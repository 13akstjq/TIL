<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>뉴럴 네트워크 손실 함수 이해 및 구현 방법 기초부터 | TIL</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://13akstjq.github.io/TIL//post/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="뉴럴 네트워크 손실 함수 이해 및 구현 방법 기초부터 | TIL" data-gatsby-head="true"/><meta property="og:title" content="뉴럴 네트워크 손실 함수 이해 및 구현 방법 기초부터 | TIL" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://TIL.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://13akstjq.github.io/TIL//post/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch" data-gatsby-head="true"/><meta name="twitter:title" content="뉴럴 네트워크 손실 함수 이해 및 구현 방법 기초부터 | TIL" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | TIL" data-gatsby-head="true"/><meta name="article:published_time" content="2024-07-09 15:01" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/TIL/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/TIL/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/TIL/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/TIL/favicons/favicon-96x96.png"/><link rel="icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/TIL/favicons/browserconfig.xml"/><link rel="preload" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/TIL/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/TIL/_next/static/chunks/webpack-21ffe88bdca56cba.js" defer=""></script><script src="/TIL/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/TIL/_next/static/chunks/main-a5eeabb286676ce6.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/TIL/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/TIL/_next/static/chunks/348-02483b66b493dd81.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/post/%5Bslug%5D-8ded8b979ba73586.js" defer=""></script><script src="/TIL/_next/static/o6AmBAY_j9v9JmbaRA39X/_buildManifest.js" defer=""></script><script src="/TIL/_next/static/o6AmBAY_j9v9JmbaRA39X/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/TIL">TIL</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/TIL/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">뉴럴 네트워크 손실 함수 이해 및 구현 방법 기초부터</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="뉴럴 네트워크 손실 함수 이해 및 구현 방법 기초부터" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/TIL/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">TIL</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jul 9, 2024</span><span class="posts_reading_time__f7YPP">10<!-- --> min read</span></span></div></div></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<img src="/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_0.png">
<p>신경망의 매혹적인 세계에서 손실 함수는 훈련 과정을 정확한 예측으로 이끄는 안내 나침반 같은 역할을 합니다. 이러한 함수들을 숙달하는 것은 딥러닝에 진지하게 임하는 사람들에게 중요합니다. 왜냐하면 손실 함수를 선택하는 것은 모델의 성능에 상당한 영향을 미칠 수 있기 때문입니다. 신진 데이터 과학자든 경험 많은 기계 학습 엔지니어든, 이 블로그는 다양한 손실 함수를 해독하고 그 목적을 설명하며, 처음부터 구현하는 방법을 보여줍니다.</p>
<p>자, 이러한 수학적 도구가 어떻게 신경망 훈련을 변화시킬 수 있는지 알아봅시다!</p>
<p>그래서, 우리 마음에 떠오르는 첫 번째 질문은 '손실 함수란 무엇인가요?'입니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_1.png" alt="image"></p>
<p>핵심적으로, 손실 함수(J로 표시됨)는 두 개의 매개변수를 입력으로 받는 수학 함수입니다:</p>
<ul>
<li>예측된 출력</li>
<li>실제 출력</li>
</ul>
<p>이 함수는 모델의 예측 값과 모델이 생성해야 하는 실제 값과 비교하여 모델이 얼마나 잘 작동하는지를 평가하기 위해 사용됩니다. 예측 값이 실제 값과 크게 다를 경우 손실 값은 크게 나타납니다. 반면, 낮은 손실 값은 두 값이 거의 유사할 때 발생합니다. 따라서 효율적인 손실 함수를 사용하여 모델을 올바르게 훈련시키는 것이 중요합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>높은 손실 값은 모델의 예측이 부정확하다는 것을 시사하며, 네트워크를 크게 재조정해야 한다는 것을 의미합니다. 반면에 낮은 손실 값은 모델이 효과적으로 작동하고 있으며, 가중치를 매우 약간만 조정해야 한다는 것을 나타냅니다.</p>
<p>이러한 시나리오는 새로운 요리법을 만드는 것과 유사합니다. 요리물이 실패하면 "손실"이 높아지고, 요리사는 다음에 요리물을 개선하기 위해 재료나 조리 방법에 상당한 변화를 해야 합니다. 그러나 요리물이 잘 나오면 이미 효과적인 레시피와 기술이기 때문에 필요한 경우에는 작은 조정만 필요합니다. 이 조정은 향후 블로그에서 논의할 하이퍼파라미터를 조정함으로써 수행할 수 있습니다. 이제 손실 함수가 무엇인지 이해했으므로, 다음으로 궁금한 것은 어떤 종류의 손실 함수가 있고 이를 어떻게 구현하는지에 대한 것입니다.</p>
<p>신경망에서의 손실 함수 유형</p>
<p>회귀 손실 함수</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>평균 제곱 오차 (MSE)</li>
</ul>
<p>MSE는 회귀 문제에 사용되는 가장 인기 있는 손실 함수 중 하나입니다. 이는 예측 값과 실제 값 사이의 오차의 제곱의 평균을 측정합니다. MSE는 이상치에 민감합니다.</p>
<p>사용 사례: MSE는 주택 가격이나 온도와 같은 연속적인 값을 예측하는 회귀 문제에서 흔히 사용됩니다.</p>
<p>수학적 공식:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>아래는 Markdown 형식으로 표시된 내용입니다.</p>
<p><img src="/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_2.png" alt="이미지"></p>
<p>where,</p>
<p><img src="/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_3.png" alt="이미지"></p>
<p>코드 구현:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_squared_error</span>(<span class="hljs-params">y_true, y_predicted</span>):
    total_error = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> yt, yp <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(y_true, y_predicted):
        total_error += (yt-yp)**<span class="hljs-number">2</span>
    mse = total_error/<span class="hljs-built_in">len</span>(y_true)
    <span class="hljs-keyword">return</span> mse
</code></pre>
<ol start="2">
<li>Mean Absolute Error (MAE)</li>
</ol>
<p>MAE는 예측 값과 실제 값 사이의 절대 오차의 평균을 측정합니다. MSE보다 이상치에 민감하지 않습니다.</p>
<p>사용 사례: MAE는 중앙값 주택 가격을 예측하는 경우와 같이, 이상치에 민감하지 않은 손실 함수를 원할 때 사용됩니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>수학 공식:</p>
<p><img src="/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_4.png" alt="수식"></p>
<p>코드 구현:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_abs_error</span>(<span class="hljs-params">y_predicted, y_true</span>):
    total_error = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> yp, yt <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(y_predicted, y_true):
        total_error += <span class="hljs-built_in">abs</span>(yp - yt)
    mae = total_error/<span class="hljs-built_in">len</span>(y_predicted)
    <span class="hljs-keyword">return</span> mae
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ol start="3">
<li>휴버 손실</li>
</ol>
<p>휴버 손실은 MSE와 MAE의 우수한 특성을 결합하여, MSE보다 이상치에 민감하지 않고 MAE보다 원점 주변에서 부드럽습니다.</p>
<p>사용 사례: 휴버 손실은 이상치의 영향을 줄이고 손실 함수를 미분 가능하게 유지하고자 하는 견고한 회귀 문제에서 자주 사용됩니다.</p>
<p>수학적 공식:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_5.png" alt="Image"></p>
<p>코드 구현:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">huber_loss</span>(<span class="hljs-params">y_true, y_pred, delta=<span class="hljs-number">1.0</span></span>):
    n = <span class="hljs-built_in">len</span>(y_true)
    loss = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):
        diff = y_true[i] - y_pred[i]
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">abs</span>(diff) &#x3C;= delta:
            loss += <span class="hljs-number">0.5</span> * diff ** <span class="hljs-number">2</span>
        <span class="hljs-keyword">else</span>:
            loss += delta * (<span class="hljs-built_in">abs</span>(diff) - <span class="hljs-number">0.5</span> * delta)
    <span class="hljs-keyword">return</span> loss / n
</code></pre>
<p>분류 손실 함수</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ol>
<li>Binary Cross-Entropy 손실</li>
</ol>
<p>Binary Cross-Entropy 손실은 이진 분류 작업에 사용됩니다. 출력이 0과 1 사이의 확률 값인 분류 모델의 성능을 측정합니다.</p>
<p>사용 사례: 스팸 감지나 사기 탐지와 같은 이진 분류 문제에 이 손실 함수가 이상적입니다.</p>
<p>수학적 공식:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이미지 태그를 Markdown 형식으로 변경하세요.</p>
<p><img src="/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_6.png" alt="UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch"></p>
<p>코드 구현:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> math

<span class="hljs-keyword">def</span> <span class="hljs-title function_">binary_crossentropy</span>(<span class="hljs-params">y_true, y_pred</span>):
    n = <span class="hljs-built_in">len</span>(y_true)
    loss = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):
        loss += y_true[i] * math.log(y_pred[i]) + (<span class="hljs-number">1</span> - y_true[i]) * math.log(<span class="hljs-number">1</span> - y_pred[i])
    <span class="hljs-keyword">return</span> -loss / n
</code></pre>
<ol start="2">
<li>범주형 Cross-Entropy Loss</li>
</ol>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>범주형 크로스 엔트로피 손실은 다중 클래스 분류 작업에 사용됩니다. 이는 여러 클래스에 대한 확률 분포인 분류 모델의 성능을 측정합니다.</p>
<p>활용 사례: 이 손실 함수는 숫자 인식(MNIST)이나 물체 검출과 같은 다중 클래스 분류 문제에 적합합니다.</p>
<p>수학적 공식:</p>
<p><img src="/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_7.png" alt="수식"></p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>구현된 코드:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> math

def <span class="hljs-title function_">categorical_crossentropy</span>(y_true, y_pred):
    n = <span class="hljs-title function_">len</span>(y_true)
    c = <span class="hljs-title function_">len</span>(y_true[<span class="hljs-number">0</span>])
    loss = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(n):
        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(c):
            <span class="hljs-keyword">if</span> y_true[i][j] == <span class="hljs-number">1</span>:
                loss += y_true[i][j] * math.<span class="hljs-title function_">log</span>(y_pred[i][j])
    <span class="hljs-keyword">return</span> -loss / n
</code></pre>
<ol start="3">
<li>Sparse Categorical Cross-Entropy Loss</li>
</ol>
<p>Sparse Categorical Cross-Entropy는 목표 레이블이 원-핫 인코딩된 벡터가 아닌 정수일 때 사용됩니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>사용 사례: 이 손실 함수는 타겟 레이블이 one-hot 벡터가 아닌 정수 레이블 형태인 다중 클래스 분류 문제에서 사용됩니다.</p>
<p>수학적 공식:</p>
<p><img src="/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_8.png" alt="이미지"></p>
<p>코드 구현:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">def <span class="hljs-title function_">sparse_categorical_cross_entropy</span>(y_true, y_pred):
    n = <span class="hljs-title function_">len</span>(y_true)
    loss = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(n):
        loss += y_true[i] * math.<span class="hljs-title function_">log</span>(y_pred[i]))
    <span class="hljs-keyword">return</span> -loss / n
</code></pre>
<p>거기에는 몇 가지 더 특별한 손실 함수가 있어요:</p>
<ul>
<li>Kullback-Leibler Divergence</li>
<li>Cosine Similarity Loss</li>
<li>Dice Loss</li>
<li>Quantile Loss</li>
<li>Hinge Loss</li>
</ul>
<p>이제 손실 함수에 대해 간결하고 명확하게 이해하셨기를 바랍니다. 다가오는 블로그에서는 특별한 손실 함수, 사용 사례 및 구현에 대해 논의할 예정이에요. 컴퓨터 비전과 딥 러닝에 관한 더 많은 기사를 위해 블로그를 팔로우해 주세요. 궁금한 사항이 있거나 특정 부분에 대한 추가 정보가 필요하면 언제든지 의견을 남겨주세요!</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>마크다운 형식으로 표 태그를 변경하세요: Linkedin</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"뉴럴 네트워크 손실 함수 이해 및 구현 방법 기초부터","description":"","date":"2024-07-09 15:01","slug":"2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch","content":"\n\u003cimg src=\"/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_0.png\" /\u003e\n\n신경망의 매혹적인 세계에서 손실 함수는 훈련 과정을 정확한 예측으로 이끄는 안내 나침반 같은 역할을 합니다. 이러한 함수들을 숙달하는 것은 딥러닝에 진지하게 임하는 사람들에게 중요합니다. 왜냐하면 손실 함수를 선택하는 것은 모델의 성능에 상당한 영향을 미칠 수 있기 때문입니다. 신진 데이터 과학자든 경험 많은 기계 학습 엔지니어든, 이 블로그는 다양한 손실 함수를 해독하고 그 목적을 설명하며, 처음부터 구현하는 방법을 보여줍니다.\n\n자, 이러한 수학적 도구가 어떻게 신경망 훈련을 변화시킬 수 있는지 알아봅시다!\n\n그래서, 우리 마음에 떠오르는 첫 번째 질문은 '손실 함수란 무엇인가요?'입니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![image](/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_1.png)\n\n핵심적으로, 손실 함수(J로 표시됨)는 두 개의 매개변수를 입력으로 받는 수학 함수입니다:\n\n- 예측된 출력\n- 실제 출력\n\n이 함수는 모델의 예측 값과 모델이 생성해야 하는 실제 값과 비교하여 모델이 얼마나 잘 작동하는지를 평가하기 위해 사용됩니다. 예측 값이 실제 값과 크게 다를 경우 손실 값은 크게 나타납니다. 반면, 낮은 손실 값은 두 값이 거의 유사할 때 발생합니다. 따라서 효율적인 손실 함수를 사용하여 모델을 올바르게 훈련시키는 것이 중요합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n높은 손실 값은 모델의 예측이 부정확하다는 것을 시사하며, 네트워크를 크게 재조정해야 한다는 것을 의미합니다. 반면에 낮은 손실 값은 모델이 효과적으로 작동하고 있으며, 가중치를 매우 약간만 조정해야 한다는 것을 나타냅니다.\n\n이러한 시나리오는 새로운 요리법을 만드는 것과 유사합니다. 요리물이 실패하면 \"손실\"이 높아지고, 요리사는 다음에 요리물을 개선하기 위해 재료나 조리 방법에 상당한 변화를 해야 합니다. 그러나 요리물이 잘 나오면 이미 효과적인 레시피와 기술이기 때문에 필요한 경우에는 작은 조정만 필요합니다. 이 조정은 향후 블로그에서 논의할 하이퍼파라미터를 조정함으로써 수행할 수 있습니다. 이제 손실 함수가 무엇인지 이해했으므로, 다음으로 궁금한 것은 어떤 종류의 손실 함수가 있고 이를 어떻게 구현하는지에 대한 것입니다.\n\n신경망에서의 손실 함수 유형\n\n회귀 손실 함수\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 평균 제곱 오차 (MSE)\n\nMSE는 회귀 문제에 사용되는 가장 인기 있는 손실 함수 중 하나입니다. 이는 예측 값과 실제 값 사이의 오차의 제곱의 평균을 측정합니다. MSE는 이상치에 민감합니다.\n\n사용 사례: MSE는 주택 가격이나 온도와 같은 연속적인 값을 예측하는 회귀 문제에서 흔히 사용됩니다.\n\n수학적 공식:\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아래는 Markdown 형식으로 표시된 내용입니다.\n\n![이미지](/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_2.png)\n\nwhere,\n\n![이미지](/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_3.png)\n\n코드 구현:\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```python\ndef mean_squared_error(y_true, y_predicted):\n    total_error = 0\n    for yt, yp in zip(y_true, y_predicted):\n        total_error += (yt-yp)**2\n    mse = total_error/len(y_true)\n    return mse\n```\n\n2. Mean Absolute Error (MAE)\n\nMAE는 예측 값과 실제 값 사이의 절대 오차의 평균을 측정합니다. MSE보다 이상치에 민감하지 않습니다.\n\n사용 사례: MAE는 중앙값 주택 가격을 예측하는 경우와 같이, 이상치에 민감하지 않은 손실 함수를 원할 때 사용됩니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n수학 공식:\n\n![수식](/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_4.png)\n\n코드 구현:\n\n```python\ndef mean_abs_error(y_predicted, y_true):\n    total_error = 0\n    for yp, yt in zip(y_predicted, y_true):\n        total_error += abs(yp - yt)\n    mae = total_error/len(y_predicted)\n    return mae\n```\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n3. 휴버 손실\n\n휴버 손실은 MSE와 MAE의 우수한 특성을 결합하여, MSE보다 이상치에 민감하지 않고 MAE보다 원점 주변에서 부드럽습니다.\n\n사용 사례: 휴버 손실은 이상치의 영향을 줄이고 손실 함수를 미분 가능하게 유지하고자 하는 견고한 회귀 문제에서 자주 사용됩니다.\n\n수학적 공식:\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Image](/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_5.png)\n\n코드 구현:\n\n```python\ndef huber_loss(y_true, y_pred, delta=1.0):\n    n = len(y_true)\n    loss = 0\n    for i in range(n):\n        diff = y_true[i] - y_pred[i]\n        if abs(diff) \u003c= delta:\n            loss += 0.5 * diff ** 2\n        else:\n            loss += delta * (abs(diff) - 0.5 * delta)\n    return loss / n\n```\n\n분류 손실 함수\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n1. Binary Cross-Entropy 손실\n\nBinary Cross-Entropy 손실은 이진 분류 작업에 사용됩니다. 출력이 0과 1 사이의 확률 값인 분류 모델의 성능을 측정합니다.\n\n사용 사례: 스팸 감지나 사기 탐지와 같은 이진 분류 문제에 이 손실 함수가 이상적입니다.\n\n수학적 공식:\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이미지 태그를 Markdown 형식으로 변경하세요.\n\n![UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch](/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_6.png)\n\n코드 구현:\n\n```python\nimport math\n\ndef binary_crossentropy(y_true, y_pred):\n    n = len(y_true)\n    loss = 0\n    for i in range(n):\n        loss += y_true[i] * math.log(y_pred[i]) + (1 - y_true[i]) * math.log(1 - y_pred[i])\n    return -loss / n\n```\n\n2. 범주형 Cross-Entropy Loss\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n범주형 크로스 엔트로피 손실은 다중 클래스 분류 작업에 사용됩니다. 이는 여러 클래스에 대한 확률 분포인 분류 모델의 성능을 측정합니다.\n\n활용 사례: 이 손실 함수는 숫자 인식(MNIST)이나 물체 검출과 같은 다중 클래스 분류 문제에 적합합니다.\n\n수학적 공식:\n\n![수식](/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_7.png)\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n구현된 코드:\n\n```js\nimport math\n\ndef categorical_crossentropy(y_true, y_pred):\n    n = len(y_true)\n    c = len(y_true[0])\n    loss = 0\n    for i in range(n):\n        for j in range(c):\n            if y_true[i][j] == 1:\n                loss += y_true[i][j] * math.log(y_pred[i][j])\n    return -loss / n\n```\n\n3. Sparse Categorical Cross-Entropy Loss\n\nSparse Categorical Cross-Entropy는 목표 레이블이 원-핫 인코딩된 벡터가 아닌 정수일 때 사용됩니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n사용 사례: 이 손실 함수는 타겟 레이블이 one-hot 벡터가 아닌 정수 레이블 형태인 다중 클래스 분류 문제에서 사용됩니다.\n\n수학적 공식:\n\n![이미지](/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_8.png)\n\n코드 구현:\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ndef sparse_categorical_cross_entropy(y_true, y_pred):\n    n = len(y_true)\n    loss = 0\n    for i in range(n):\n        loss += y_true[i] * math.log(y_pred[i]))\n    return -loss / n\n```\n\n거기에는 몇 가지 더 특별한 손실 함수가 있어요:\n\n- Kullback-Leibler Divergence\n- Cosine Similarity Loss\n- Dice Loss\n- Quantile Loss\n- Hinge Loss\n\n이제 손실 함수에 대해 간결하고 명확하게 이해하셨기를 바랍니다. 다가오는 블로그에서는 특별한 손실 함수, 사용 사례 및 구현에 대해 논의할 예정이에요. 컴퓨터 비전과 딥 러닝에 관한 더 많은 기사를 위해 블로그를 팔로우해 주세요. 궁금한 사항이 있거나 특정 부분에 대한 추가 정보가 필요하면 언제든지 의견을 남겨주세요!\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n마크다운 형식으로 표 태그를 변경하세요: Linkedin\n","ogImage":{"url":"/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_0.png","tag":["Tech"],"readingTime":10},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cimg src=\"/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_0.png\"\u003e\n\u003cp\u003e신경망의 매혹적인 세계에서 손실 함수는 훈련 과정을 정확한 예측으로 이끄는 안내 나침반 같은 역할을 합니다. 이러한 함수들을 숙달하는 것은 딥러닝에 진지하게 임하는 사람들에게 중요합니다. 왜냐하면 손실 함수를 선택하는 것은 모델의 성능에 상당한 영향을 미칠 수 있기 때문입니다. 신진 데이터 과학자든 경험 많은 기계 학습 엔지니어든, 이 블로그는 다양한 손실 함수를 해독하고 그 목적을 설명하며, 처음부터 구현하는 방법을 보여줍니다.\u003c/p\u003e\n\u003cp\u003e자, 이러한 수학적 도구가 어떻게 신경망 훈련을 변화시킬 수 있는지 알아봅시다!\u003c/p\u003e\n\u003cp\u003e그래서, 우리 마음에 떠오르는 첫 번째 질문은 '손실 함수란 무엇인가요?'입니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_1.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e핵심적으로, 손실 함수(J로 표시됨)는 두 개의 매개변수를 입력으로 받는 수학 함수입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e예측된 출력\u003c/li\u003e\n\u003cli\u003e실제 출력\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이 함수는 모델의 예측 값과 모델이 생성해야 하는 실제 값과 비교하여 모델이 얼마나 잘 작동하는지를 평가하기 위해 사용됩니다. 예측 값이 실제 값과 크게 다를 경우 손실 값은 크게 나타납니다. 반면, 낮은 손실 값은 두 값이 거의 유사할 때 발생합니다. 따라서 효율적인 손실 함수를 사용하여 모델을 올바르게 훈련시키는 것이 중요합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e높은 손실 값은 모델의 예측이 부정확하다는 것을 시사하며, 네트워크를 크게 재조정해야 한다는 것을 의미합니다. 반면에 낮은 손실 값은 모델이 효과적으로 작동하고 있으며, 가중치를 매우 약간만 조정해야 한다는 것을 나타냅니다.\u003c/p\u003e\n\u003cp\u003e이러한 시나리오는 새로운 요리법을 만드는 것과 유사합니다. 요리물이 실패하면 \"손실\"이 높아지고, 요리사는 다음에 요리물을 개선하기 위해 재료나 조리 방법에 상당한 변화를 해야 합니다. 그러나 요리물이 잘 나오면 이미 효과적인 레시피와 기술이기 때문에 필요한 경우에는 작은 조정만 필요합니다. 이 조정은 향후 블로그에서 논의할 하이퍼파라미터를 조정함으로써 수행할 수 있습니다. 이제 손실 함수가 무엇인지 이해했으므로, 다음으로 궁금한 것은 어떤 종류의 손실 함수가 있고 이를 어떻게 구현하는지에 대한 것입니다.\u003c/p\u003e\n\u003cp\u003e신경망에서의 손실 함수 유형\u003c/p\u003e\n\u003cp\u003e회귀 손실 함수\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e평균 제곱 오차 (MSE)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMSE는 회귀 문제에 사용되는 가장 인기 있는 손실 함수 중 하나입니다. 이는 예측 값과 실제 값 사이의 오차의 제곱의 평균을 측정합니다. MSE는 이상치에 민감합니다.\u003c/p\u003e\n\u003cp\u003e사용 사례: MSE는 주택 가격이나 온도와 같은 연속적인 값을 예측하는 회귀 문제에서 흔히 사용됩니다.\u003c/p\u003e\n\u003cp\u003e수학적 공식:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e아래는 Markdown 형식으로 표시된 내용입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003ewhere,\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_3.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e코드 구현:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003emean_squared_error\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ey_true, y_predicted\u003c/span\u003e):\n    total_error = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e yt, yp \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003ezip\u003c/span\u003e(y_true, y_predicted):\n        total_error += (yt-yp)**\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e\n    mse = total_error/\u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(y_true)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e mse\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eMean Absolute Error (MAE)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMAE는 예측 값과 실제 값 사이의 절대 오차의 평균을 측정합니다. MSE보다 이상치에 민감하지 않습니다.\u003c/p\u003e\n\u003cp\u003e사용 사례: MAE는 중앙값 주택 가격을 예측하는 경우와 같이, 이상치에 민감하지 않은 손실 함수를 원할 때 사용됩니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e수학 공식:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_4.png\" alt=\"수식\"\u003e\u003c/p\u003e\n\u003cp\u003e코드 구현:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003emean_abs_error\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ey_predicted, y_true\u003c/span\u003e):\n    total_error = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e yp, yt \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003ezip\u003c/span\u003e(y_predicted, y_true):\n        total_error += \u003cspan class=\"hljs-built_in\"\u003eabs\u003c/span\u003e(yp - yt)\n    mae = total_error/\u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(y_predicted)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e mae\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e휴버 손실\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e휴버 손실은 MSE와 MAE의 우수한 특성을 결합하여, MSE보다 이상치에 민감하지 않고 MAE보다 원점 주변에서 부드럽습니다.\u003c/p\u003e\n\u003cp\u003e사용 사례: 휴버 손실은 이상치의 영향을 줄이고 손실 함수를 미분 가능하게 유지하고자 하는 견고한 회귀 문제에서 자주 사용됩니다.\u003c/p\u003e\n\u003cp\u003e수학적 공식:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_5.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003e코드 구현:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ehuber_loss\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ey_true, y_pred, delta=\u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e\u003c/span\u003e):\n    n = \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(y_true)\n    loss = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(n):\n        diff = y_true[i] - y_pred[i]\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eabs\u003c/span\u003e(diff) \u0026#x3C;= delta:\n            loss += \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e * diff ** \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n            loss += delta * (\u003cspan class=\"hljs-built_in\"\u003eabs\u003c/span\u003e(diff) - \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e * delta)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e loss / n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e분류 손실 함수\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003col\u003e\n\u003cli\u003eBinary Cross-Entropy 손실\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBinary Cross-Entropy 손실은 이진 분류 작업에 사용됩니다. 출력이 0과 1 사이의 확률 값인 분류 모델의 성능을 측정합니다.\u003c/p\u003e\n\u003cp\u003e사용 사례: 스팸 감지나 사기 탐지와 같은 이진 분류 문제에 이 손실 함수가 이상적입니다.\u003c/p\u003e\n\u003cp\u003e수학적 공식:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이미지 태그를 Markdown 형식으로 변경하세요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_6.png\" alt=\"UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch\"\u003e\u003c/p\u003e\n\u003cp\u003e코드 구현:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e math\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ebinary_crossentropy\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ey_true, y_pred\u003c/span\u003e):\n    n = \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(y_true)\n    loss = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(n):\n        loss += y_true[i] * math.log(y_pred[i]) + (\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e - y_true[i]) * math.log(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e - y_pred[i])\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e -loss / n\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e범주형 Cross-Entropy Loss\u003c/li\u003e\n\u003c/ol\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e범주형 크로스 엔트로피 손실은 다중 클래스 분류 작업에 사용됩니다. 이는 여러 클래스에 대한 확률 분포인 분류 모델의 성능을 측정합니다.\u003c/p\u003e\n\u003cp\u003e활용 사례: 이 손실 함수는 숫자 인식(MNIST)이나 물체 검출과 같은 다중 클래스 분류 문제에 적합합니다.\u003c/p\u003e\n\u003cp\u003e수학적 공식:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_7.png\" alt=\"수식\"\u003e\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e구현된 코드:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e math\n\ndef \u003cspan class=\"hljs-title function_\"\u003ecategorical_crossentropy\u003c/span\u003e(y_true, y_pred):\n    n = \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(y_true)\n    c = \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(y_true[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\n    loss = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(n):\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e j \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(c):\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e y_true[i][j] == \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e:\n                loss += y_true[i][j] * math.\u003cspan class=\"hljs-title function_\"\u003elog\u003c/span\u003e(y_pred[i][j])\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e -loss / n\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eSparse Categorical Cross-Entropy Loss\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSparse Categorical Cross-Entropy는 목표 레이블이 원-핫 인코딩된 벡터가 아닌 정수일 때 사용됩니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e사용 사례: 이 손실 함수는 타겟 레이블이 one-hot 벡터가 아닌 정수 레이블 형태인 다중 클래스 분류 문제에서 사용됩니다.\u003c/p\u003e\n\u003cp\u003e수학적 공식:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch_8.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e코드 구현:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003esparse_categorical_cross_entropy\u003c/span\u003e(y_true, y_pred):\n    n = \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(y_true)\n    loss = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(n):\n        loss += y_true[i] * math.\u003cspan class=\"hljs-title function_\"\u003elog\u003c/span\u003e(y_pred[i]))\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e -loss / n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e거기에는 몇 가지 더 특별한 손실 함수가 있어요:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKullback-Leibler Divergence\u003c/li\u003e\n\u003cli\u003eCosine Similarity Loss\u003c/li\u003e\n\u003cli\u003eDice Loss\u003c/li\u003e\n\u003cli\u003eQuantile Loss\u003c/li\u003e\n\u003cli\u003eHinge Loss\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이제 손실 함수에 대해 간결하고 명확하게 이해하셨기를 바랍니다. 다가오는 블로그에서는 특별한 손실 함수, 사용 사례 및 구현에 대해 논의할 예정이에요. 컴퓨터 비전과 딥 러닝에 관한 더 많은 기사를 위해 블로그를 팔로우해 주세요. 궁금한 사항이 있거나 특정 부분에 대한 추가 정보가 필요하면 언제든지 의견을 남겨주세요!\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e마크다운 형식으로 표 태그를 변경하세요: Linkedin\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-07-09-UnderstandingImplementingDifferentLossFunctionsofNeuralNetworksfromScratch"},"buildId":"o6AmBAY_j9v9JmbaRA39X","assetPrefix":"/TIL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>