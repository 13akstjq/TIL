<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>Python과 OpenCV, FER을 사용한 실시간 감정 인식 방법 | TIL</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://13akstjq.github.io/TIL//post/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="Python과 OpenCV, FER을 사용한 실시간 감정 인식 방법 | TIL" data-gatsby-head="true"/><meta property="og:title" content="Python과 OpenCV, FER을 사용한 실시간 감정 인식 방법 | TIL" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/TIL/assets/img/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://TIL.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://13akstjq.github.io/TIL//post/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER" data-gatsby-head="true"/><meta name="twitter:title" content="Python과 OpenCV, FER을 사용한 실시간 감정 인식 방법 | TIL" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/TIL/assets/img/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | TIL" data-gatsby-head="true"/><meta name="article:published_time" content="2024-07-13 19:28" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/TIL/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/TIL/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/TIL/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/TIL/favicons/favicon-96x96.png"/><link rel="icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/TIL/favicons/browserconfig.xml"/><link rel="preload" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/TIL/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/TIL/_next/static/chunks/webpack-21ffe88bdca56cba.js" defer=""></script><script src="/TIL/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/TIL/_next/static/chunks/main-a5eeabb286676ce6.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/TIL/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/TIL/_next/static/chunks/348-02483b66b493dd81.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/post/%5Bslug%5D-8ded8b979ba73586.js" defer=""></script><script src="/TIL/_next/static/X4OrKfmLtU3-3BijwfHe6/_buildManifest.js" defer=""></script><script src="/TIL/_next/static/X4OrKfmLtU3-3BijwfHe6/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/TIL">TIL</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/TIL/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">Python과 OpenCV, FER을 사용한 실시간 감정 인식 방법</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="Python과 OpenCV, FER을 사용한 실시간 감정 인식 방법" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/TIL/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">TIL</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jul 13, 2024</span><span class="posts_reading_time__f7YPP">17<!-- --> min read</span></span></div></div></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/TIL/assets/img/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER_0.png" alt="이미지"></p>
<p>감정 인식 기술은 심리학, 인공지능 및 컴퓨터 과학의 흥미로운 교차점을 제시합니다. 우리는 영상 처리를 위해 OpenCV의 기능을 이용하고, 얼굴 감정 인식(FER) 라이브러리를 활용하여 비디오 피드에서 실시간 감정 검출을 제공합니다.</p>
<p>이 방법은 얼굴 표정을 캡처하고, 딥러닝 모델을 사용하여 감정 상태를 해석하며, 이러한 감정을 동적으로 시각화하는 것을 포함합니다. 실용적인 응용 프로그램은 소프트웨어에서 사용자 경험을 향상시키는 것부터 감정 인식 AI 시스템에 대한 통찰력을 제공하는 것까지 다양합니다.</p>
<p>이 기사는 실용적인, 처음부터 끝까지의 코드 구현을 제공합니다. 웹캠이나 화면 녹화, 비디오 파일 등을 통해 실시간으로 감정을 캡처하고 분석할 수 있는 플러그 앤 플레이 솔루션을 개발자 및 열성가 모두에게 제공합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1. 기술 스택</h1>
<h2>1.1 FER (Facial Emotion Recognition)</h2>
<p>FER은 얼굴 표정에서 감정을 감지하는 데 중점을 둔 Python 라이브러리입니다. 사전 학습된 딥 러닝 모델을 활용하여 FER은 이미지와 비디오를 분석하여 분노, 혐오, 두려움, 행복, 슬픔, 놀라움, 중립 등 다양한 감정을 식별합니다.</p>
<p>이는 사용 편의를 고려하여 설계되었으며, 감정 감지가 필요한 프로젝트에 간편하게 통합할 수 있습니다. 출처: PyPI — FER.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>1.2 OpenCV (오픈 소스 컴퓨터 비전 라이브러리):</h2>
<p>OpenCV는 컴퓨터 비전 분야에서 핵심적인 라이브러리입니다. 인텔에서 처음 개발되었으며, 이미지 및 비디오 처리에 널리 사용됩니다. OpenCV는 Python을 포함한 다양한 프로그래밍 언어를 지원하며, 실시간 응용 프로그램에서 높은 효율성으로 알려져 있습니다.</p>
<p>이 라이브러리는 이미지 및 비디오 조작에서 중요한 역할을 하며, 웹캠 피드 캡처, 비디오 처리, 이미지 위에 주석 그리기와 같은 작업에 이상적입니다. 출처: OpenCV 문서.</p>
<h2>1.4 MediaPipe (여기에서 사용되지 않음)</h2>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>저희가 실시간 감정 인식 기술 스택에 대해 논의한 맥락에서, 이 구체적인 구현에는 사용되지는 않지만, MediaPipe를 언급할 가치가 있습니다. 독자들이 정보를 얻는 데 도움이 될 것이라고 생각합니다.</p>
<p>MediaPipe는 Google이 개발한 프레임워크로서 멀티모달(오디오, 비디오, 시계열 등) 적용 기계 학습 파이프라인을 구축하는 데 사용됩니다. 실시간 및 스트리밍 미디어에 대한 사용자 정의 가능한 기계 학습 솔루션을 제공하며, 특히 얼굴 인식, 손 추적, 자세 추정 기능으로 잘 알려져 있습니다.</p>
<p>MediaPipe는 실시간 이미지 및 비디오 처리에 대해 자세히 탐구하는 독자들이 가치 있는 도구로 여길 수 있습니다.</p>
<p>특히 얼굴 감정 인식 이상의 더 복잡하거나 다양한 유형의 시각 데이터 처리 및 인식 작업이 요구되는 시나리오에서 특히 강력한 도구입니다. 출처: MediaPipe Github.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2. 파이썬 구현</h1>
<p>구현은 간단하며 주로 네 가지 라이브러리가 필요합니다: FER, OpenCV, matplotlib 및 imageio.</p>
<p>감정 인식 코드를 실행하기 위해 환경을 설정하려면 필요한 라이브러리를 설치해야 합니다. 명령 프롬프트 또는 터미널을 통해 다음 명령을 실행하세요:</p>
<pre><code class="hljs language-js">pip install fer
pip install opencv-python
pip install matplotlib
pip install imageio
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>2.1 실시간 감정 감지</h2>
<p>파이썬을 사용하여 실시간 감정 감지의 기본 개념을 소개합니다. 지금은 간단하고 액세스하기 쉽도록 유지하기 위해 핵심 기능을 보여주는 기본 스크립트부터 시작합니다.</p>
<p>이 초기 예제는 웹캠에서 비디오를 캡처하고 FER 라이브러리를 사용하여 실시간으로 감정을 감지하는 데 초점을 맞출 것입니다.</p>
<p>우리의 예제에서는 라이브 웹캠 피드를 사용하지만, 이 스크립트를 다른 소스와 함께 사용할 수 있습니다. 예를 들어, 웹캠 피드를 비디오 파일이나 라이브 화면 녹화로 대체할 수도 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>웹캠 피드 시작: 먼저 웹캠에서 비디오를 캡쳐하기 위해 OpenCV를 사용합니다. OpenCV의 VideoCapture 함수는 웹캠 피드를 초기화합니다. 대부분의 경우, VideoCapture에 0을 전달하면 기본 웹캠이 선택됩니다.</li>
<li>감정 감지: 그 다음으로는 비디오 프레임에서 감정을 감지하는 간단한 인터페이스를 제공하는 FER 라이브러리를 활용합니다. 웹캠에서 프레임이 캡쳐되면 FER는 프레임을 처리하여 얼굴과 해당 감정을 감지합니다.</li>
<li>감지된 감정 강조: 감정이 감지되면 OpenCV 함수를 사용하여 비디오 프레임에서 감지된 얼굴에 경계 상자와 텍스트 주석을 그립니다. 텍스트 레이블은 감지된 감정과 해당 신뢰 수준을 표시합니다.</li>
</ul>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> fer <span class="hljs-keyword">import</span> <span class="hljs-variable constant_">FER</span>
<span class="hljs-keyword">import</span> cv2

# 감정 감지기 초기화
detector = <span class="hljs-title function_">FER</span>(mtcnn=<span class="hljs-title class_">True</span>)

# 웹캠 시작
cap = cv2.<span class="hljs-title class_">VideoCapture</span>(<span class="hljs-number">0</span>)

<span class="hljs-attr">try</span>:
    <span class="hljs-keyword">while</span> <span class="hljs-title class_">True</span>:
        ret, frame = cap.<span class="hljs-title function_">read</span>()
        <span class="hljs-keyword">if</span> not <span class="hljs-attr">ret</span>:
            <span class="hljs-keyword">break</span>

        # 프레임에서 감정 감지
        result = detector.<span class="hljs-title function_">detect_emotions</span>(frame)
        <span class="hljs-keyword">for</span> face <span class="hljs-keyword">in</span> <span class="hljs-attr">result</span>:
            # 값을 추출
            box = face[<span class="hljs-string">"box"</span>]
            emotions = face[<span class="hljs-string">"emotions"</span>]

            x, y, w, h = box
            cv2.<span class="hljs-title function_">rectangle</span>(frame, (x, y), (x+w, y+h), (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">2</span>)
            
            # 가장 높은 점수의 감정 찾기
            emotion_type = <span class="hljs-title function_">max</span>(emotions, key=emotions.<span class="hljs-property">get</span>)
            emotion_score = emotions[emotion_type]

            # 감정 유형과 신뢰 수준 표시
            emotion_text = f<span class="hljs-string">"{emotion_type}: {emotion_score:.2f}"</span>
            cv2.<span class="hljs-title function_">putText</span>(frame, emotion_text, (x, y - <span class="hljs-number">10</span>), cv2.<span class="hljs-property">FONT_HERSHEY_SIMPLEX</span>, <span class="hljs-number">0.9</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">2</span>)

        # 결과 프레임 표시
        cv2.<span class="hljs-title function_">imshow</span>(<span class="hljs-string">'Emotion Detection'</span>, frame)

        # 루프 종료
        <span class="hljs-keyword">if</span> cv2.<span class="hljs-title function_">waitKey</span>(<span class="hljs-number">1</span>) &#x26; <span class="hljs-number">0xFF</span> == <span class="hljs-title function_">ord</span>(<span class="hljs-string">'q'</span>):
            <span class="hljs-keyword">break</span>
except <span class="hljs-title class_">KeyboardInterrupt</span>:
    <span class="hljs-title function_">print</span>(<span class="hljs-string">"사용자에 의해 중지됨"</span>)
<span class="hljs-attr">finally</span>:
    # 모든 작업이 완료되면 캡처 해제
    cap.<span class="hljs-title function_">release</span>()
    cv2.<span class="hljs-title function_">destroyAllWindows</span>()
</code></pre>
<h2>2.2 실시간 감정 시각화</h2>
<p>기본적인 실시간 감정 감지 스크립트를 기반으로, 실시간 감정 시각화 기능을 추가한 것입니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이 업데이트는 감정 감지 응용 프로그램에 더 다이내믹하고 상호작용 가능한 측면을 추가하여 데이터를 더 매력적이고 통찰력 있게 만듭니다.</p>
<p>실시간 감정 막대 차트 생성: 각 프레임에서 감지된 감정을 시각화하기 위해, 우리는 파이썬의 강력한 플로팅 라이브러리인 matplotlib을 사용합니다. 다음은 설정하는 방법입니다:</p>
<ul>
<li>우리는 matplotlib 피겨를 초기화하고 각 감정에 대한 자리 표시자를 가진 막대 차트를 만듭니다.</li>
<li>차트의 각 막대는 감정을 나타내며, FER에 의해 감지된 신뢰 수준을 실시간으로 업데이트하여 높이가 조정됩니다.</li>
</ul>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

plt.ion()  <span class="hljs-comment"># 대화형 모드를 활성화</span>
fig, ax = plt.subplots()
emotion_labels = [<span class="hljs-string">'angry'</span>, <span class="hljs-string">'disgust'</span>, <span class="hljs-string">'fear'</span>, <span class="hljs-string">'happy'</span>, <span class="hljs-string">'sad'</span>, <span class="hljs-string">'surprise'</span>, <span class="hljs-string">'neutral'</span>]
bars = ax.bar(emotion_labels, [<span class="hljs-number">0</span>]*<span class="hljs-number">7</span>, color=<span class="hljs-string">'lightblue'</span>)
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Matplotlib의 대화형 모드: 대화형 모드를 활성화하면 (plt.ion()), matplotlib의 플롯이 실시간으로 업데이트됩니다. 이를 통해 바 차트가 감정 감지 알고리즘에 의해 처리된 각 새 프레임마다 동적으로 새로 고쳐질 수 있습니다.</p>
<p>차트 업데이트: 감지된 감정을 가져와 각 막대의 높이를 업데이트하는 update_chart 함수를 만듭니다. 이 함수는 처리된 각 프레임에서 호출되어 차트가 현재 감지된 감정을 정확히 반영하도록 합니다.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">update_chart</span>(detected_emotions, bars, ax, fig):
    ax.<span class="hljs-title function_">clear</span>()
    ax.<span class="hljs-title function_">bar</span>(emotion_labels, [detected_emotions.<span class="hljs-title function_">get</span>(emotion, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> emotion <span class="hljs-keyword">in</span> emotion_labels], color=<span class="hljs-string">'lightblue'</span>)
    ### [차트 서식의 나머지 부분]
    fig.<span class="hljs-property">canvas</span>.<span class="hljs-title function_">flush_events</span>()
</code></pre>
<p>주요 루프에서 차트 업데이트 통합: 스크립트의 주요 루프에서 각 프레임에서 감정을 감지한 후, 최신 감정 데이터로 update_chart를 호출합니다. 이렇게 함으로써 시각화가 비디오 피드와 동기화되도록 유지됩니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js"># 감정 감지 및 시각화를 위한 주요 루프
<span class="hljs-keyword">while</span> <span class="hljs-title class_">True</span>:
    # [웹캠 캡처 및 감정 감지 코드]

    <span class="hljs-keyword">if</span> <span class="hljs-attr">largest_face</span>:
        # [얼굴 처리 및 감정 점수 매기기]
        
        # 최신 감정 데이터로 실시간 막대 차트 업데이트
        <span class="hljs-title function_">update_chart</span>(current_emotions, bars, ax, fig)
</code></pre>
<p>모두 합쳐서 다음과 같은 포괄적인 Python 스크립트를 얻습니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> fer <span class="hljs-keyword">import</span> <span class="hljs-variable constant_">FER</span>
<span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">import</span> matplotlib.<span class="hljs-property">pyplot</span> <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> imageio
<span class="hljs-keyword">import</span> matplotlib
<span class="hljs-keyword">import</span> time

<span class="hljs-string">""</span><span class="hljs-string">"
실시간 감정 감지 및 시각화

이 스크립트는 웹캠에서 비디오를 캡처하고 실시간으로 얼굴의 감정을 감지하여 결과를 실시간으로 시각화하는 기능을 제공합니다. 
또한 감지된 감정과 함께 비디오 자체에도 표시된 상자와 감정 레이블을 저장합니다. 또한 실시간 감정 막대 차트를 GIF로, 
누적 감정 통계도 정적 차트로 저장합니다. 이 스크립트는 비디오 처리를 위해 OpenCV, 감정 감지를 위해 FER, 라이브 차트 시각화를 
위해 matplotlib, GIF 제작을 위해 imageio를 사용합니다.

주요 기능:
- 웹캠에서 실시간 감정 감지.
- 막대 차트에서 감정 확신 수준을 실시간으로 업데이트.
- 얼굴 주위의 경계 상자 및 감정 레이블이 있는 비디오 캡처 저장.
- 라이브 감정 막대 차트의 GIF 생성.
- 시간에 따른 감정 통계의 누적 차트 저장.
"</span><span class="hljs-string">""</span>

# matplotlib의 백엔드를 다양한 환경과 호환되도록 <span class="hljs-string">'TkAgg'</span>로 설정
matplotlib.<span class="hljs-title function_">use</span>(<span class="hljs-string">'TkAgg'</span>)

# <span class="hljs-variable constant_">MTCNN</span>을 사용하는 <span class="hljs-variable constant_">FER</span> (<span class="hljs-title class_">Face</span> <span class="hljs-title class_">Emotion</span> <span class="hljs-title class_">Recognition</span>) 탐지기를 초기화
detector = <span class="hljs-title function_">FER</span>(mtcnn=<span class="hljs-title class_">True</span>)

# 웹캠에서 비디오 캡처 시작 (장치 <span class="hljs-number">0</span>)
cap = cv2.<span class="hljs-title class_">VideoCapture</span>(<span class="hljs-number">0</span>)

# 비디오 녹화용 프레임 속도 설정 (웹캠의 성능에 따라 조정)
frame_rate = <span class="hljs-number">4.3</span>

# 지정된 프레임 속도로 비디오를 저장하는 <span class="hljs-title class_">OpenCV</span>의 <span class="hljs-title class_">VideoWriter</span> 초기화
fourcc = cv2.<span class="hljs-title class_">VideoWriter</span>_fourcc(*<span class="hljs-string">'XVID'</span>)
out = cv2.<span class="hljs-title class_">VideoWriter</span>(<span class="hljs-string">'emotion_video.avi'</span>, fourcc, frame_rate, (<span class="hljs-number">640</span>, <span class="hljs-number">480</span>))

# 라이브 감정 감지 결과를 표시할 matplotlib 피겨 설정
plt.<span class="hljs-title function_">ion</span>()  # 라이브 업데이트를 위한 대화형 모드 활성화
fig, ax = plt.<span class="hljs-title function_">subplots</span>()
emotion_labels = [<span class="hljs-string">'angry'</span>, <span class="hljs-string">'disgust'</span>, <span class="hljs-string">'fear'</span>, <span class="hljs-string">'happy'</span>, <span class="hljs-string">'sad'</span>, <span class="hljs-string">'surprise'</span>, <span class="hljs-string">'neutral'</span>]
bars = ax.<span class="hljs-title function_">bar</span>(emotion_labels, [<span class="hljs-number">0</span>]*<span class="hljs-number">7</span>, color=<span class="hljs-string">'lightblue'</span>)  # 각 감정을 위한 막대 초기화
plt.<span class="hljs-title function_">ylim</span>(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)
plt.<span class="hljs-title function_">ylabel</span>(<span class="hljs-string">'신뢰도'</span>)
plt.<span class="hljs-title function_">title</span>(<span class="hljs-string">'실시간 감정 감지'</span>)
ax.<span class="hljs-title function_">set_xticklabels</span>(emotion_labels, rotation=<span class="hljs-number">45</span>)

# 라이브 차트 업데이트를 <span class="hljs-variable constant_">GIF</span>로 저장하기 위해 imageio 라이터 초기화
gif_writer = imageio.<span class="hljs-title function_">get_writer</span>(<span class="hljs-string">'emotion_chart.gif'</span>, mode=<span class="hljs-string">'I'</span>, duration=<span class="hljs-number">0.1</span>)

# 각 프레임에 대한 누적 감정 통계를 저장할 리스트
emotion_statistics = []

# 라이브 차트 업데이트 함수
def <span class="hljs-title function_">update_chart</span>(detected_emotions, bars, ax, fig):
    ax.<span class="hljs-title function_">clear</span>()  # 현재 축 지우고 다시 막대 차트 설정
    ax.<span class="hljs-title function_">bar</span>(emotion_labels, [detected_emotions.<span class="hljs-title function_">get</span>(emotion, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> emotion <span class="hljs-keyword">in</span> emotion_labels], color=<span class="hljs-string">'lightblue'</span>)
    plt.<span class="hljs-title function_">ylim</span>(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)
    plt.<span class="hljs-title function_">ylabel</span>(<span class="hljs-string">'신뢰도'</span>)
    plt.<span class="hljs-title function_">title</span>(<span class="hljs-string">'실시간 감정 감지'</span>)
    ax.<span class="hljs-title function_">set_xticklabels</span>(emotion_labels, rotation=<span class="hljs-number">45</span>)
    fig.<span class="hljs-property">canvas</span>.<span class="hljs-title function_">draw</span>()
    fig.<span class="hljs-property">canvas</span>.<span class="hljs-title function_">flush_events</span>()

# 웹캠 작동 시간을 측정하기 위해 타이머 시작
webcam_start_time = time.<span class="hljs-title function_">time</span>()

<span class="hljs-attr">try</span>:
    <span class="hljs-keyword">while</span> <span class="hljs-title class_">True</span>:
        ret, frame = cap.<span class="hljs-title function_">read</span>()  # 웹캠에서 프레임 읽기
        <span class="hljs-keyword">if</span> not <span class="hljs-attr">ret</span>:
            <span class="hljs-keyword">break</span>  # 캡처된 프레임이 없으면 루프 종료

        # 프레임에서 감정 감지
        result = detector.<span class="hljs-title function_">detect_emotions</span>(frame)
        largest_face = <span class="hljs-title class_">None</span>
        max_area = <span class="hljs-number">0</span>

        # 주요 감정 분석을 위해 프레임에서 가장 큰 얼굴 찾기
        <span class="hljs-keyword">for</span> face <span class="hljs-keyword">in</span> <span class="hljs-attr">result</span>:
            box = face[<span class="hljs-string">"box"</span>]
            x, y, w, h = box
            area = w * h
            <span class="hljs-keyword">if</span> area > <span class="hljs-attr">max_area</span>:
                max_area = area
                largest_face = face

        # 얼굴이 감지되면 감정 표시 및 차트 업데이트
        <span class="hljs-keyword">if</span> <span class="hljs-attr">largest_face</span>:
            box = largest_face[<span class="hljs-string">"box"</span>]
            current_emotions = largest_face[<span class="hljs-string">"emotions"</span>]

            # 감정 데이터 저장
            emotion_statistics.<span class="hljs-title function_">append</span>(current_emotions)

            x, y, w, h = box
            cv2.<span class="hljs-title function_">rectangle</span>(frame, (x, y), (x+w, y+h), (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">2</span>)
            
            emotion_type = <span class="hljs-title function_">max</span>(current_emotions, key=current_emotions.<span class="hljs-property">get</span>)
            emotion_score = current_emotions[emotion_type]

            emotion_text = f<span class="hljs-string">"{emotion_type}: {emotion_score:.2f}"</span>
            cv2.<span class="hljs-title function_">putText</span>(frame, emotion_text, (x, y - <span class="hljs-number">10</span>), cv2.<span class="hljs-property">FONT_HERSHEY_SIMPLEX</span>, <span class="hljs-number">0.9</span>, (<span class="hljs-number">0</span>, <span class="hljs-number">255</span>, <span class="hljs-number">0</span>), <span class="hljs-number">2</span>)

            <span class="hljs-title function_">update_chart</span>(current_emotions, bars, ax, fig)

            out.<span class="hljs-title function_">write</span>(frame)  # 프레임을 비디오 파일에 씀

            # 현재 막대 차트 상태를 <span class="hljs-variable constant_">GIF</span>의 한 프레임으로 저장
            fig.<span class="hljs-property">canvas</span>.<span class="hljs-title function_">draw</span>()
            image = np.<span class="hljs-title function_">frombuffer</span>(fig.<span class="hljs-property">canvas</span>.<span class="hljs-title function_">tostring_rgb</span>(), dtype=<span class="hljs-string">'uint8'</span>)
            image = image.<span class="hljs-title function_">reshape</span>(fig.<span class="hljs-property">canvas</span>.<span class="hljs-title function_">get_width_height</span>()[::-<span class="hljs-number">1</span>] + (<span class="hljs-number">3</span>,))
            gif_writer.<span class="hljs-title function_">append_data</span>(image)

        cv2.<span class="hljs-title function_">imshow</span>(<span class="hljs-string">'감정 감지'</span>, frame)  # 프레임 표시

        <span class="hljs-keyword">if</span> cv2.<span class="hljs-title function_">waitKey</span>(<span class="hljs-number">1</span>) &#x26; <span class="hljs-number">0xFF</span> == <span class="hljs-title function_">ord</span>(<span class="hljs-string">'q'</span>):
            <span class="hljs-keyword">break</span>
        
except <span class="hljs-title class_">KeyboardInterrupt</span>:
    <span class="hljs-title function_">print</span>(<span class="hljs-string">"사용자에 의해 중단됨"</span>)

<span class="hljs-attr">finally</span>:
    webcam_end_time = time.<span class="hljs-title function_">time</span>()  # 웹캠 창이 닫힐 때 타이머 종료
    <span class="hljs-title function_">print</span>(f<span class="hljs-string">"웹캠 활동 시간: {webcam_end_time - webcam_start_time:.2f} 초"</span>)

    cap.<span class="hljs-title function_">release</span>()
    cv2.<span class="hljs-title function_">destroyAllWindows</span>()
    plt.<span class="hljs-title function_">close</span>(fig)

    out.<span class="hljs-title function_">release</span>()
    gif_writer.<span class="hljs-title function_">close</span>()

    emotion_df = pd.<span class="hljs-title class_">DataFrame</span>(emotion_statistics)

    plt.<span class="hljs-title function_">figure</span>(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>))
    <span class="hljs-keyword">for</span> emotion <span class="hljs-keyword">in</span> <span class="hljs-attr">emotion_labels</span>:
        plt.<span class="hljs-title function_">plot</span>(emotion_df[emotion].<span class="hljs-title function_">cumsum</span>(), label=emotion)
    plt.<span class="hljs-title function_">title</span>(<span class="hljs-string">'시간에 따른 누적 감정 통계'</span>)
    plt.<span class="hljs-title function_">xlabel</span>(<span class="hljs-string">'프레임'</span>)
    plt.<span class="hljs-title function_">ylabel</span>(<span class="hljs-string">'누적 신뢰도'</span>)
    plt.<span class="hljs-title function_">legend</span>()
    plt.<span class="hljs-title function_">savefig</span>(<span class="hljs-string">'cumulative_emotions.jpg'</span>)
    plt.<span class="hljs-title function_">close</span>()
</code></pre>
<h2>2.3 보너스 — 결과 병합하기</h2>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>감정 감지 기술의 진정한 힘은 데이터를 포괄적이고 매력적인 방식으로 시각화할 때 발휘됩니다. 이를 달성하기 위해 다양한 출력물을 하나의 시각적 프레젠테이션으로 효과적으로 결합하는 스크립트를 개발했습니다. 이 스크립트는 세 가지 요소를 효과적으로 조화시킵니다:</p>
<ul>
<li>실시간 비디오: 웹캠 피드에서 캡처한 감정 감지 결과가 'emotion_video.avi'로 저장됩니다.</li>
<li>동적 막대 차트 GIF: 감지된 감정을 실시간으로 업데이트하는 차트가 'emotion_chart.gif'로 저장됩니다.</li>
<li>정적 누적 감정 차트: 'cumulative_emotions.jpg'라는 이미지 파일에 시간에 따른 집계된 감정 데이터가 표시됩니다.</li>
</ul>
<p>스크립트에서 중요한 코드 일부:</p>
<ul>
<li>입력값 읽기 및 처리:</li>
</ul>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">static_chart = <span class="hljs-title class_">Image</span>.<span class="hljs-title function_">open</span>(<span class="hljs-string">'cumulative_emotions.jpg'</span>)
video = cv2.<span class="hljs-title class_">VideoCapture</span>(<span class="hljs-string">'emotion_video.avi'</span>)
bar_chart_gif = imageio.<span class="hljs-title function_">mimread</span>(<span class="hljs-string">'emotion_chart.gif'</span>)
</code></pre>
<ul>
<li>Frames를 조합하는 로직:</li>
</ul>
<pre><code class="hljs language-js">combined_frame = <span class="hljs-title class_">Image</span>.<span class="hljs-title function_">new</span>(<span class="hljs-string">'RGB'</span>, (<span class="hljs-number">3</span> * desired_width, desired_height))
combined_frame.<span class="hljs-title function_">paste</span>(video_frame_resized, (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>))
combined_frame.<span class="hljs-title function_">paste</span>(gif_resized, (desired_width, <span class="hljs-number">0</span>))
combined_frame.<span class="hljs-title function_">paste</span>(static_chart_resized, (<span class="hljs-number">2</span> * desired_width, <span class="hljs-number">0</span>))
</code></pre>
<p>더 자세한 코드와 기술적인 세부 정보를 탐험하고 싶은 분들을 위해, AI, 데이터 과학 및 기술에 대한 다양한 기술 튜토리얼 및 실용적인 가이드가 포함된 리소스인 우리 웹사이트인 Entreprenerdly.com을 방문하시기를 권장합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이 프로젝트와 다른 많은 프로젝트의 완전한 코드를 찾을 수 있습니다. 최신 기술 솔루션을 배우고 구현하는 실전적인 방법을 제공합니다.</p>
<p><img src="/TIL/assets/img/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER_1.png" alt="이미지"></p>
<h2>5. 실용적인 응용</h2>
<ul>
<li>보안 시스템: 감정 인식은 보안 시스템에 추가적인 보호층을 추가할 수 있습니다. 감정 신호에 기반하여 의심스러운 또는 이례적인 행동을 식별할 수 있습니다.</li>
<li>의료 및 정신 건강: 임상 환경에서 감정 인식은 환자의 정신 건강 상태를 모니터링하는 데 도움을 줄 수 있습니다. 특히 텔레치료 세션에서 유용하며, 환자의 감정 반응에 대한 추가적인 통찰을 제공하여 요법사에게 도움이 될 수 있습니다.</li>
<li>사용자 경험 및 인터페이스 디자인: 웹사이트와 애플리케이션은 감정 인식을 사용하여 사용자 경험을 맞춤화할 수 있습니다. 혼란 또는 불만의 징후를 감지하여 유용한 프롬프트를 트리거하거나 사용자가 더 관련 있는 콘텐츠로 안내할 수 있습니다.</li>
<li>로봇과 인공지능: 로봇 공학에서 감정 인식은 AI 및 로봇과의 상호 작용을 더 자연스럽고 직관적으로 만들 수 있습니다. 특히 요양이나 고객 서비스 로봇에서 유용합니다.</li>
<li>접근성 기술: 언어 또는 청각 장애가 있는 사람들을 위해 감정 인식 기술은 화자의 감정 상태에 대한 추가적인 문맥을 제공하여 의사 소통을 용이하게 할 수 있습니다.</li>
</ul>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6. 한계와 개선점</h1>
<h2>6.1 한계</h2>
<ul>
<li>정확도와 데이터 의존성: 감정 감지의 정확도는 FER 모델의 훈련 데이터에 크게 의존합니다. 이 데이터의 편견은 잘못된 또는 일관되지 않은 감정 인식으로 이어질 수 있으며, 특히 다양한 인구 통계군 사이에서 발생할 수 있습니다.</li>
<li>맥락적 이해: 시스템은 감정의 맥락을 이해하기 항해 합니다. 얼굴 표정을 인식하지만 이러한 표정 뒤의 이유를 추론하거나 진짜 감정과 가짜 감정을 구별할 수 없습니다.</li>
<li>조명 및 영상 품질: 웹캠 영상의 품질, 포함하여 조명 조건과 해상도는 감지 정확도에 큰 영향을 미칠 수 있습니다. 낮은 영상 품질은 신뢰할 수 없는 감정 인식으로 이어질 수 있습니다.</li>
<li>개인정보 우려: 감정 인식 기술을 사용함에 있어, 특히 공개적 또는 준공개적 공간에서는 개인정보 문제가 우려됩니다. 이러한 시스템을 구현할 때는 동의와 윤리적 고려가 매우 중요합니다.</li>
</ul>
<h2>6.2 개선 가능한 부분:</h2>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>향상된 머신러닝 모델: 더 발전된 머신러닝 모델을 통합하거나 기존의 FER 모델을 사용자 정의함으로써 정확성을 향상시키고 편향을 줄일 수 있습니다.</li>
<li>맥락을 인지하는 알고리즘: 상황의 맥락을 고려하는 알고리즘을 개발하면 보다 세밀한 감정 분석을 제공할 수 있습니다.</li>
<li>실시간 처리 최적화: 코드를 효율적으로 최적화하거나 더 강력한 처리 하드웨어를 사용함으로써 실시간 응용 프로그램에서의 대기 시간을 최소화할 수 있습니다.</li>
<li>개인 정보 보호 대책: 엄격한 개인 정보 보호 규정을 시행하고 데이터 사용에 대한 투명성을 보장함으로써 개인 정보 보호 우려를 완화할 수 있습니다.</li>
</ul>
<h1>결론</h1>
<p>이러한 강력한 도구들을 결합하여 감정을 실시간으로 감지할 뿐만 아니라 이러한 데이터를 매력적이고 유익한 방식으로 시각화할 수 있다는 것을 확인했습니다. 이 기술의 실용적인 응용은 건강 관리부터 마케팅에 이르기까지 다양하며 다양합니다.</p>
<p>독자들에게는 이 기사가 현재 기술의 능력을 이해하고 그 잠재적인 응용 분야를 상상해 볼 수 있는 시작점으로 제공됩니다. 개발자, 연구자 또는 기술과 감정이 교차하는 부분에 관심이 있는 사람이라면 상상력만큼의 가능성이 넓게 열려 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/TIL/assets/img/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER_2.png" alt="Real-Time Emotion Recognition in Python with OpenCV and FER"></p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Python과 OpenCV, FER을 사용한 실시간 감정 인식 방법","description":"","date":"2024-07-13 19:28","slug":"2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER","content":"\n\n![이미지](/TIL/assets/img/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER_0.png)\n\n감정 인식 기술은 심리학, 인공지능 및 컴퓨터 과학의 흥미로운 교차점을 제시합니다. 우리는 영상 처리를 위해 OpenCV의 기능을 이용하고, 얼굴 감정 인식(FER) 라이브러리를 활용하여 비디오 피드에서 실시간 감정 검출을 제공합니다.\n\n이 방법은 얼굴 표정을 캡처하고, 딥러닝 모델을 사용하여 감정 상태를 해석하며, 이러한 감정을 동적으로 시각화하는 것을 포함합니다. 실용적인 응용 프로그램은 소프트웨어에서 사용자 경험을 향상시키는 것부터 감정 인식 AI 시스템에 대한 통찰력을 제공하는 것까지 다양합니다.\n\n이 기사는 실용적인, 처음부터 끝까지의 코드 구현을 제공합니다. 웹캠이나 화면 녹화, 비디오 파일 등을 통해 실시간으로 감정을 캡처하고 분석할 수 있는 플러그 앤 플레이 솔루션을 개발자 및 열성가 모두에게 제공합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 1. 기술 스택\n\n## 1.1 FER (Facial Emotion Recognition)\n\nFER은 얼굴 표정에서 감정을 감지하는 데 중점을 둔 Python 라이브러리입니다. 사전 학습된 딥 러닝 모델을 활용하여 FER은 이미지와 비디오를 분석하여 분노, 혐오, 두려움, 행복, 슬픔, 놀라움, 중립 등 다양한 감정을 식별합니다.\n\n이는 사용 편의를 고려하여 설계되었으며, 감정 감지가 필요한 프로젝트에 간편하게 통합할 수 있습니다. 출처: PyPI — FER.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 1.2 OpenCV (오픈 소스 컴퓨터 비전 라이브러리):\n\nOpenCV는 컴퓨터 비전 분야에서 핵심적인 라이브러리입니다. 인텔에서 처음 개발되었으며, 이미지 및 비디오 처리에 널리 사용됩니다. OpenCV는 Python을 포함한 다양한 프로그래밍 언어를 지원하며, 실시간 응용 프로그램에서 높은 효율성으로 알려져 있습니다.\n\n이 라이브러리는 이미지 및 비디오 조작에서 중요한 역할을 하며, 웹캠 피드 캡처, 비디오 처리, 이미지 위에 주석 그리기와 같은 작업에 이상적입니다. 출처: OpenCV 문서.\n\n## 1.4 MediaPipe (여기에서 사용되지 않음)\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n저희가 실시간 감정 인식 기술 스택에 대해 논의한 맥락에서, 이 구체적인 구현에는 사용되지는 않지만, MediaPipe를 언급할 가치가 있습니다. 독자들이 정보를 얻는 데 도움이 될 것이라고 생각합니다.\n\nMediaPipe는 Google이 개발한 프레임워크로서 멀티모달(오디오, 비디오, 시계열 등) 적용 기계 학습 파이프라인을 구축하는 데 사용됩니다. 실시간 및 스트리밍 미디어에 대한 사용자 정의 가능한 기계 학습 솔루션을 제공하며, 특히 얼굴 인식, 손 추적, 자세 추정 기능으로 잘 알려져 있습니다.\n\nMediaPipe는 실시간 이미지 및 비디오 처리에 대해 자세히 탐구하는 독자들이 가치 있는 도구로 여길 수 있습니다.\n\n특히 얼굴 감정 인식 이상의 더 복잡하거나 다양한 유형의 시각 데이터 처리 및 인식 작업이 요구되는 시나리오에서 특히 강력한 도구입니다. 출처: MediaPipe Github.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 2. 파이썬 구현\n\n구현은 간단하며 주로 네 가지 라이브러리가 필요합니다: FER, OpenCV, matplotlib 및 imageio.\n\n감정 인식 코드를 실행하기 위해 환경을 설정하려면 필요한 라이브러리를 설치해야 합니다. 명령 프롬프트 또는 터미널을 통해 다음 명령을 실행하세요:\n\n```js\npip install fer\npip install opencv-python\npip install matplotlib\npip install imageio\n```\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 2.1 실시간 감정 감지\n\n파이썬을 사용하여 실시간 감정 감지의 기본 개념을 소개합니다. 지금은 간단하고 액세스하기 쉽도록 유지하기 위해 핵심 기능을 보여주는 기본 스크립트부터 시작합니다.\n\n이 초기 예제는 웹캠에서 비디오를 캡처하고 FER 라이브러리를 사용하여 실시간으로 감정을 감지하는 데 초점을 맞출 것입니다.\n\n우리의 예제에서는 라이브 웹캠 피드를 사용하지만, 이 스크립트를 다른 소스와 함께 사용할 수 있습니다. 예를 들어, 웹캠 피드를 비디오 파일이나 라이브 화면 녹화로 대체할 수도 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 웹캠 피드 시작: 먼저 웹캠에서 비디오를 캡쳐하기 위해 OpenCV를 사용합니다. OpenCV의 VideoCapture 함수는 웹캠 피드를 초기화합니다. 대부분의 경우, VideoCapture에 0을 전달하면 기본 웹캠이 선택됩니다.\n- 감정 감지: 그 다음으로는 비디오 프레임에서 감정을 감지하는 간단한 인터페이스를 제공하는 FER 라이브러리를 활용합니다. 웹캠에서 프레임이 캡쳐되면 FER는 프레임을 처리하여 얼굴과 해당 감정을 감지합니다.\n- 감지된 감정 강조: 감정이 감지되면 OpenCV 함수를 사용하여 비디오 프레임에서 감지된 얼굴에 경계 상자와 텍스트 주석을 그립니다. 텍스트 레이블은 감지된 감정과 해당 신뢰 수준을 표시합니다.\n\n```js\nfrom fer import FER\nimport cv2\n\n# 감정 감지기 초기화\ndetector = FER(mtcnn=True)\n\n# 웹캠 시작\ncap = cv2.VideoCapture(0)\n\ntry:\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # 프레임에서 감정 감지\n        result = detector.detect_emotions(frame)\n        for face in result:\n            # 값을 추출\n            box = face[\"box\"]\n            emotions = face[\"emotions\"]\n\n            x, y, w, h = box\n            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n            \n            # 가장 높은 점수의 감정 찾기\n            emotion_type = max(emotions, key=emotions.get)\n            emotion_score = emotions[emotion_type]\n\n            # 감정 유형과 신뢰 수준 표시\n            emotion_text = f\"{emotion_type}: {emotion_score:.2f}\"\n            cv2.putText(frame, emotion_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n        # 결과 프레임 표시\n        cv2.imshow('Emotion Detection', frame)\n\n        # 루프 종료\n        if cv2.waitKey(1) \u0026 0xFF == ord('q'):\n            break\nexcept KeyboardInterrupt:\n    print(\"사용자에 의해 중지됨\")\nfinally:\n    # 모든 작업이 완료되면 캡처 해제\n    cap.release()\n    cv2.destroyAllWindows()\r\n```\n\n## 2.2 실시간 감정 시각화\n\n기본적인 실시간 감정 감지 스크립트를 기반으로, 실시간 감정 시각화 기능을 추가한 것입니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 업데이트는 감정 감지 응용 프로그램에 더 다이내믹하고 상호작용 가능한 측면을 추가하여 데이터를 더 매력적이고 통찰력 있게 만듭니다.\n\n실시간 감정 막대 차트 생성: 각 프레임에서 감지된 감정을 시각화하기 위해, 우리는 파이썬의 강력한 플로팅 라이브러리인 matplotlib을 사용합니다. 다음은 설정하는 방법입니다:\n\n- 우리는 matplotlib 피겨를 초기화하고 각 감정에 대한 자리 표시자를 가진 막대 차트를 만듭니다.\n- 차트의 각 막대는 감정을 나타내며, FER에 의해 감지된 신뢰 수준을 실시간으로 업데이트하여 높이가 조정됩니다.\n\n```python\nimport matplotlib.pyplot as plt\n\nplt.ion()  # 대화형 모드를 활성화\nfig, ax = plt.subplots()\nemotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\nbars = ax.bar(emotion_labels, [0]*7, color='lightblue')\n```\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nMatplotlib의 대화형 모드: 대화형 모드를 활성화하면 (plt.ion()), matplotlib의 플롯이 실시간으로 업데이트됩니다. 이를 통해 바 차트가 감정 감지 알고리즘에 의해 처리된 각 새 프레임마다 동적으로 새로 고쳐질 수 있습니다.\n\n차트 업데이트: 감지된 감정을 가져와 각 막대의 높이를 업데이트하는 update_chart 함수를 만듭니다. 이 함수는 처리된 각 프레임에서 호출되어 차트가 현재 감지된 감정을 정확히 반영하도록 합니다.\n\n```js\ndef update_chart(detected_emotions, bars, ax, fig):\n    ax.clear()\n    ax.bar(emotion_labels, [detected_emotions.get(emotion, 0) for emotion in emotion_labels], color='lightblue')\n    ### [차트 서식의 나머지 부분]\n    fig.canvas.flush_events()\n```\n\n주요 루프에서 차트 업데이트 통합: 스크립트의 주요 루프에서 각 프레임에서 감정을 감지한 후, 최신 감정 데이터로 update_chart를 호출합니다. 이렇게 함으로써 시각화가 비디오 피드와 동기화되도록 유지됩니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\r\n# 감정 감지 및 시각화를 위한 주요 루프\nwhile True:\n    # [웹캠 캡처 및 감정 감지 코드]\n\n    if largest_face:\n        # [얼굴 처리 및 감정 점수 매기기]\n        \n        # 최신 감정 데이터로 실시간 막대 차트 업데이트\n        update_chart(current_emotions, bars, ax, fig)\r\n```\n\n모두 합쳐서 다음과 같은 포괄적인 Python 스크립트를 얻습니다.\n\n```js\r\nfrom fer import FER\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport imageio\nimport matplotlib\nimport time\n\n\"\"\"\n실시간 감정 감지 및 시각화\n\n이 스크립트는 웹캠에서 비디오를 캡처하고 실시간으로 얼굴의 감정을 감지하여 결과를 실시간으로 시각화하는 기능을 제공합니다. \n또한 감지된 감정과 함께 비디오 자체에도 표시된 상자와 감정 레이블을 저장합니다. 또한 실시간 감정 막대 차트를 GIF로, \n누적 감정 통계도 정적 차트로 저장합니다. 이 스크립트는 비디오 처리를 위해 OpenCV, 감정 감지를 위해 FER, 라이브 차트 시각화를 \n위해 matplotlib, GIF 제작을 위해 imageio를 사용합니다.\n\n주요 기능:\n- 웹캠에서 실시간 감정 감지.\n- 막대 차트에서 감정 확신 수준을 실시간으로 업데이트.\n- 얼굴 주위의 경계 상자 및 감정 레이블이 있는 비디오 캡처 저장.\n- 라이브 감정 막대 차트의 GIF 생성.\n- 시간에 따른 감정 통계의 누적 차트 저장.\n\"\"\"\n\n# matplotlib의 백엔드를 다양한 환경과 호환되도록 'TkAgg'로 설정\nmatplotlib.use('TkAgg')\n\n# MTCNN을 사용하는 FER (Face Emotion Recognition) 탐지기를 초기화\ndetector = FER(mtcnn=True)\n\n# 웹캠에서 비디오 캡처 시작 (장치 0)\ncap = cv2.VideoCapture(0)\n\n# 비디오 녹화용 프레임 속도 설정 (웹캠의 성능에 따라 조정)\nframe_rate = 4.3\n\n# 지정된 프레임 속도로 비디오를 저장하는 OpenCV의 VideoWriter 초기화\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter('emotion_video.avi', fourcc, frame_rate, (640, 480))\n\n# 라이브 감정 감지 결과를 표시할 matplotlib 피겨 설정\nplt.ion()  # 라이브 업데이트를 위한 대화형 모드 활성화\nfig, ax = plt.subplots()\nemotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\nbars = ax.bar(emotion_labels, [0]*7, color='lightblue')  # 각 감정을 위한 막대 초기화\nplt.ylim(0, 1)\nplt.ylabel('신뢰도')\nplt.title('실시간 감정 감지')\nax.set_xticklabels(emotion_labels, rotation=45)\n\n# 라이브 차트 업데이트를 GIF로 저장하기 위해 imageio 라이터 초기화\ngif_writer = imageio.get_writer('emotion_chart.gif', mode='I', duration=0.1)\n\n# 각 프레임에 대한 누적 감정 통계를 저장할 리스트\nemotion_statistics = []\n\n# 라이브 차트 업데이트 함수\ndef update_chart(detected_emotions, bars, ax, fig):\n    ax.clear()  # 현재 축 지우고 다시 막대 차트 설정\n    ax.bar(emotion_labels, [detected_emotions.get(emotion, 0) for emotion in emotion_labels], color='lightblue')\n    plt.ylim(0, 1)\n    plt.ylabel('신뢰도')\n    plt.title('실시간 감정 감지')\n    ax.set_xticklabels(emotion_labels, rotation=45)\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\n# 웹캠 작동 시간을 측정하기 위해 타이머 시작\nwebcam_start_time = time.time()\n\ntry:\n    while True:\n        ret, frame = cap.read()  # 웹캠에서 프레임 읽기\n        if not ret:\n            break  # 캡처된 프레임이 없으면 루프 종료\n\n        # 프레임에서 감정 감지\n        result = detector.detect_emotions(frame)\n        largest_face = None\n        max_area = 0\n\n        # 주요 감정 분석을 위해 프레임에서 가장 큰 얼굴 찾기\n        for face in result:\n            box = face[\"box\"]\n            x, y, w, h = box\n            area = w * h\n            if area \u003e max_area:\n                max_area = area\n                largest_face = face\n\n        # 얼굴이 감지되면 감정 표시 및 차트 업데이트\n        if largest_face:\n            box = largest_face[\"box\"]\n            current_emotions = largest_face[\"emotions\"]\n\n            # 감정 데이터 저장\n            emotion_statistics.append(current_emotions)\n\n            x, y, w, h = box\n            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n            \n            emotion_type = max(current_emotions, key=current_emotions.get)\n            emotion_score = current_emotions[emotion_type]\n\n            emotion_text = f\"{emotion_type}: {emotion_score:.2f}\"\n            cv2.putText(frame, emotion_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n            update_chart(current_emotions, bars, ax, fig)\n\n            out.write(frame)  # 프레임을 비디오 파일에 씀\n\n            # 현재 막대 차트 상태를 GIF의 한 프레임으로 저장\n            fig.canvas.draw()\n            image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n            image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n            gif_writer.append_data(image)\n\n        cv2.imshow('감정 감지', frame)  # 프레임 표시\n\n        if cv2.waitKey(1) \u0026 0xFF == ord('q'):\n            break\n        \nexcept KeyboardInterrupt:\n    print(\"사용자에 의해 중단됨\")\n\nfinally:\n    webcam_end_time = time.time()  # 웹캠 창이 닫힐 때 타이머 종료\n    print(f\"웹캠 활동 시간: {webcam_end_time - webcam_start_time:.2f} 초\")\n\n    cap.release()\n    cv2.destroyAllWindows()\n    plt.close(fig)\n\n    out.release()\n    gif_writer.close()\n\n    emotion_df = pd.DataFrame(emotion_statistics)\n\n    plt.figure(figsize=(10, 10))\n    for emotion in emotion_labels:\n        plt.plot(emotion_df[emotion].cumsum(), label=emotion)\n    plt.title('시간에 따른 누적 감정 통계')\n    plt.xlabel('프레임')\n    plt.ylabel('누적 신뢰도')\n    plt.legend()\n    plt.savefig('cumulative_emotions.jpg')\n    plt.close()\r\n```\n\n## 2.3 보너스 — 결과 병합하기\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n감정 감지 기술의 진정한 힘은 데이터를 포괄적이고 매력적인 방식으로 시각화할 때 발휘됩니다. 이를 달성하기 위해 다양한 출력물을 하나의 시각적 프레젠테이션으로 효과적으로 결합하는 스크립트를 개발했습니다. 이 스크립트는 세 가지 요소를 효과적으로 조화시킵니다:\n\n- 실시간 비디오: 웹캠 피드에서 캡처한 감정 감지 결과가 'emotion_video.avi'로 저장됩니다.\n- 동적 막대 차트 GIF: 감지된 감정을 실시간으로 업데이트하는 차트가 'emotion_chart.gif'로 저장됩니다.\n- 정적 누적 감정 차트: 'cumulative_emotions.jpg'라는 이미지 파일에 시간에 따른 집계된 감정 데이터가 표시됩니다.\n\n스크립트에서 중요한 코드 일부:\n\n- 입력값 읽기 및 처리:\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nstatic_chart = Image.open('cumulative_emotions.jpg')\nvideo = cv2.VideoCapture('emotion_video.avi')\nbar_chart_gif = imageio.mimread('emotion_chart.gif')\n```\n\n- Frames를 조합하는 로직:\n\n```js\ncombined_frame = Image.new('RGB', (3 * desired_width, desired_height))\ncombined_frame.paste(video_frame_resized, (0, 0))\ncombined_frame.paste(gif_resized, (desired_width, 0))\ncombined_frame.paste(static_chart_resized, (2 * desired_width, 0))\n```\n\n더 자세한 코드와 기술적인 세부 정보를 탐험하고 싶은 분들을 위해, AI, 데이터 과학 및 기술에 대한 다양한 기술 튜토리얼 및 실용적인 가이드가 포함된 리소스인 우리 웹사이트인 Entreprenerdly.com을 방문하시기를 권장합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 프로젝트와 다른 많은 프로젝트의 완전한 코드를 찾을 수 있습니다. 최신 기술 솔루션을 배우고 구현하는 실전적인 방법을 제공합니다.\n\n![이미지](/TIL/assets/img/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER_1.png)\n\n## 5. 실용적인 응용\n\n- 보안 시스템: 감정 인식은 보안 시스템에 추가적인 보호층을 추가할 수 있습니다. 감정 신호에 기반하여 의심스러운 또는 이례적인 행동을 식별할 수 있습니다.\n- 의료 및 정신 건강: 임상 환경에서 감정 인식은 환자의 정신 건강 상태를 모니터링하는 데 도움을 줄 수 있습니다. 특히 텔레치료 세션에서 유용하며, 환자의 감정 반응에 대한 추가적인 통찰을 제공하여 요법사에게 도움이 될 수 있습니다.\n- 사용자 경험 및 인터페이스 디자인: 웹사이트와 애플리케이션은 감정 인식을 사용하여 사용자 경험을 맞춤화할 수 있습니다. 혼란 또는 불만의 징후를 감지하여 유용한 프롬프트를 트리거하거나 사용자가 더 관련 있는 콘텐츠로 안내할 수 있습니다.\n- 로봇과 인공지능: 로봇 공학에서 감정 인식은 AI 및 로봇과의 상호 작용을 더 자연스럽고 직관적으로 만들 수 있습니다. 특히 요양이나 고객 서비스 로봇에서 유용합니다.\n- 접근성 기술: 언어 또는 청각 장애가 있는 사람들을 위해 감정 인식 기술은 화자의 감정 상태에 대한 추가적인 문맥을 제공하여 의사 소통을 용이하게 할 수 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 6. 한계와 개선점\n\n## 6.1 한계\n\n- 정확도와 데이터 의존성: 감정 감지의 정확도는 FER 모델의 훈련 데이터에 크게 의존합니다. 이 데이터의 편견은 잘못된 또는 일관되지 않은 감정 인식으로 이어질 수 있으며, 특히 다양한 인구 통계군 사이에서 발생할 수 있습니다.\n- 맥락적 이해: 시스템은 감정의 맥락을 이해하기 항해 합니다. 얼굴 표정을 인식하지만 이러한 표정 뒤의 이유를 추론하거나 진짜 감정과 가짜 감정을 구별할 수 없습니다.\n- 조명 및 영상 품질: 웹캠 영상의 품질, 포함하여 조명 조건과 해상도는 감지 정확도에 큰 영향을 미칠 수 있습니다. 낮은 영상 품질은 신뢰할 수 없는 감정 인식으로 이어질 수 있습니다.\n- 개인정보 우려: 감정 인식 기술을 사용함에 있어, 특히 공개적 또는 준공개적 공간에서는 개인정보 문제가 우려됩니다. 이러한 시스템을 구현할 때는 동의와 윤리적 고려가 매우 중요합니다.\n\n## 6.2 개선 가능한 부분:\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 향상된 머신러닝 모델: 더 발전된 머신러닝 모델을 통합하거나 기존의 FER 모델을 사용자 정의함으로써 정확성을 향상시키고 편향을 줄일 수 있습니다.\n- 맥락을 인지하는 알고리즘: 상황의 맥락을 고려하는 알고리즘을 개발하면 보다 세밀한 감정 분석을 제공할 수 있습니다.\n- 실시간 처리 최적화: 코드를 효율적으로 최적화하거나 더 강력한 처리 하드웨어를 사용함으로써 실시간 응용 프로그램에서의 대기 시간을 최소화할 수 있습니다.\n- 개인 정보 보호 대책: 엄격한 개인 정보 보호 규정을 시행하고 데이터 사용에 대한 투명성을 보장함으로써 개인 정보 보호 우려를 완화할 수 있습니다.\n\n# 결론\n\n이러한 강력한 도구들을 결합하여 감정을 실시간으로 감지할 뿐만 아니라 이러한 데이터를 매력적이고 유익한 방식으로 시각화할 수 있다는 것을 확인했습니다. 이 기술의 실용적인 응용은 건강 관리부터 마케팅에 이르기까지 다양하며 다양합니다. \n\n독자들에게는 이 기사가 현재 기술의 능력을 이해하고 그 잠재적인 응용 분야를 상상해 볼 수 있는 시작점으로 제공됩니다. 개발자, 연구자 또는 기술과 감정이 교차하는 부분에 관심이 있는 사람이라면 상상력만큼의 가능성이 넓게 열려 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![Real-Time Emotion Recognition in Python with OpenCV and FER](/TIL/assets/img/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER_2.png)","ogImage":{"url":"/TIL/assets/img/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER_0.png"},"coverImage":"/TIL/assets/img/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER_0.png","tag":["Tech"],"readingTime":17},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e감정 인식 기술은 심리학, 인공지능 및 컴퓨터 과학의 흥미로운 교차점을 제시합니다. 우리는 영상 처리를 위해 OpenCV의 기능을 이용하고, 얼굴 감정 인식(FER) 라이브러리를 활용하여 비디오 피드에서 실시간 감정 검출을 제공합니다.\u003c/p\u003e\n\u003cp\u003e이 방법은 얼굴 표정을 캡처하고, 딥러닝 모델을 사용하여 감정 상태를 해석하며, 이러한 감정을 동적으로 시각화하는 것을 포함합니다. 실용적인 응용 프로그램은 소프트웨어에서 사용자 경험을 향상시키는 것부터 감정 인식 AI 시스템에 대한 통찰력을 제공하는 것까지 다양합니다.\u003c/p\u003e\n\u003cp\u003e이 기사는 실용적인, 처음부터 끝까지의 코드 구현을 제공합니다. 웹캠이나 화면 녹화, 비디오 파일 등을 통해 실시간으로 감정을 캡처하고 분석할 수 있는 플러그 앤 플레이 솔루션을 개발자 및 열성가 모두에게 제공합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e1. 기술 스택\u003c/h1\u003e\n\u003ch2\u003e1.1 FER (Facial Emotion Recognition)\u003c/h2\u003e\n\u003cp\u003eFER은 얼굴 표정에서 감정을 감지하는 데 중점을 둔 Python 라이브러리입니다. 사전 학습된 딥 러닝 모델을 활용하여 FER은 이미지와 비디오를 분석하여 분노, 혐오, 두려움, 행복, 슬픔, 놀라움, 중립 등 다양한 감정을 식별합니다.\u003c/p\u003e\n\u003cp\u003e이는 사용 편의를 고려하여 설계되었으며, 감정 감지가 필요한 프로젝트에 간편하게 통합할 수 있습니다. 출처: PyPI — FER.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e1.2 OpenCV (오픈 소스 컴퓨터 비전 라이브러리):\u003c/h2\u003e\n\u003cp\u003eOpenCV는 컴퓨터 비전 분야에서 핵심적인 라이브러리입니다. 인텔에서 처음 개발되었으며, 이미지 및 비디오 처리에 널리 사용됩니다. OpenCV는 Python을 포함한 다양한 프로그래밍 언어를 지원하며, 실시간 응용 프로그램에서 높은 효율성으로 알려져 있습니다.\u003c/p\u003e\n\u003cp\u003e이 라이브러리는 이미지 및 비디오 조작에서 중요한 역할을 하며, 웹캠 피드 캡처, 비디오 처리, 이미지 위에 주석 그리기와 같은 작업에 이상적입니다. 출처: OpenCV 문서.\u003c/p\u003e\n\u003ch2\u003e1.4 MediaPipe (여기에서 사용되지 않음)\u003c/h2\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e저희가 실시간 감정 인식 기술 스택에 대해 논의한 맥락에서, 이 구체적인 구현에는 사용되지는 않지만, MediaPipe를 언급할 가치가 있습니다. 독자들이 정보를 얻는 데 도움이 될 것이라고 생각합니다.\u003c/p\u003e\n\u003cp\u003eMediaPipe는 Google이 개발한 프레임워크로서 멀티모달(오디오, 비디오, 시계열 등) 적용 기계 학습 파이프라인을 구축하는 데 사용됩니다. 실시간 및 스트리밍 미디어에 대한 사용자 정의 가능한 기계 학습 솔루션을 제공하며, 특히 얼굴 인식, 손 추적, 자세 추정 기능으로 잘 알려져 있습니다.\u003c/p\u003e\n\u003cp\u003eMediaPipe는 실시간 이미지 및 비디오 처리에 대해 자세히 탐구하는 독자들이 가치 있는 도구로 여길 수 있습니다.\u003c/p\u003e\n\u003cp\u003e특히 얼굴 감정 인식 이상의 더 복잡하거나 다양한 유형의 시각 데이터 처리 및 인식 작업이 요구되는 시나리오에서 특히 강력한 도구입니다. 출처: MediaPipe Github.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e2. 파이썬 구현\u003c/h1\u003e\n\u003cp\u003e구현은 간단하며 주로 네 가지 라이브러리가 필요합니다: FER, OpenCV, matplotlib 및 imageio.\u003c/p\u003e\n\u003cp\u003e감정 인식 코드를 실행하기 위해 환경을 설정하려면 필요한 라이브러리를 설치해야 합니다. 명령 프롬프트 또는 터미널을 통해 다음 명령을 실행하세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epip install fer\npip install opencv-python\npip install matplotlib\npip install imageio\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e2.1 실시간 감정 감지\u003c/h2\u003e\n\u003cp\u003e파이썬을 사용하여 실시간 감정 감지의 기본 개념을 소개합니다. 지금은 간단하고 액세스하기 쉽도록 유지하기 위해 핵심 기능을 보여주는 기본 스크립트부터 시작합니다.\u003c/p\u003e\n\u003cp\u003e이 초기 예제는 웹캠에서 비디오를 캡처하고 FER 라이브러리를 사용하여 실시간으로 감정을 감지하는 데 초점을 맞출 것입니다.\u003c/p\u003e\n\u003cp\u003e우리의 예제에서는 라이브 웹캠 피드를 사용하지만, 이 스크립트를 다른 소스와 함께 사용할 수 있습니다. 예를 들어, 웹캠 피드를 비디오 파일이나 라이브 화면 녹화로 대체할 수도 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e웹캠 피드 시작: 먼저 웹캠에서 비디오를 캡쳐하기 위해 OpenCV를 사용합니다. OpenCV의 VideoCapture 함수는 웹캠 피드를 초기화합니다. 대부분의 경우, VideoCapture에 0을 전달하면 기본 웹캠이 선택됩니다.\u003c/li\u003e\n\u003cli\u003e감정 감지: 그 다음으로는 비디오 프레임에서 감정을 감지하는 간단한 인터페이스를 제공하는 FER 라이브러리를 활용합니다. 웹캠에서 프레임이 캡쳐되면 FER는 프레임을 처리하여 얼굴과 해당 감정을 감지합니다.\u003c/li\u003e\n\u003cli\u003e감지된 감정 강조: 감정이 감지되면 OpenCV 함수를 사용하여 비디오 프레임에서 감지된 얼굴에 경계 상자와 텍스트 주석을 그립니다. 텍스트 레이블은 감지된 감정과 해당 신뢰 수준을 표시합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e fer \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eFER\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e cv2\n\n# 감정 감지기 초기화\ndetector = \u003cspan class=\"hljs-title function_\"\u003eFER\u003c/span\u003e(mtcnn=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\n# 웹캠 시작\ncap = cv2.\u003cspan class=\"hljs-title class_\"\u003eVideoCapture\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n\n\u003cspan class=\"hljs-attr\"\u003etry\u003c/span\u003e:\n    \u003cspan class=\"hljs-keyword\"\u003ewhile\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e:\n        ret, frame = cap.\u003cspan class=\"hljs-title function_\"\u003eread\u003c/span\u003e()\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e not \u003cspan class=\"hljs-attr\"\u003eret\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003ebreak\u003c/span\u003e\n\n        # 프레임에서 감정 감지\n        result = detector.\u003cspan class=\"hljs-title function_\"\u003edetect_emotions\u003c/span\u003e(frame)\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e face \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eresult\u003c/span\u003e:\n            # 값을 추출\n            box = face[\u003cspan class=\"hljs-string\"\u003e\"box\"\u003c/span\u003e]\n            emotions = face[\u003cspan class=\"hljs-string\"\u003e\"emotions\"\u003c/span\u003e]\n\n            x, y, w, h = box\n            cv2.\u003cspan class=\"hljs-title function_\"\u003erectangle\u003c/span\u003e(frame, (x, y), (x+w, y+h), (\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e), \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n            \n            # 가장 높은 점수의 감정 찾기\n            emotion_type = \u003cspan class=\"hljs-title function_\"\u003emax\u003c/span\u003e(emotions, key=emotions.\u003cspan class=\"hljs-property\"\u003eget\u003c/span\u003e)\n            emotion_score = emotions[emotion_type]\n\n            # 감정 유형과 신뢰 수준 표시\n            emotion_text = f\u003cspan class=\"hljs-string\"\u003e\"{emotion_type}: {emotion_score:.2f}\"\u003c/span\u003e\n            cv2.\u003cspan class=\"hljs-title function_\"\u003eputText\u003c/span\u003e(frame, emotion_text, (x, y - \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e), cv2.\u003cspan class=\"hljs-property\"\u003eFONT_HERSHEY_SIMPLEX\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.9\u003c/span\u003e, (\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e), \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n\n        # 결과 프레임 표시\n        cv2.\u003cspan class=\"hljs-title function_\"\u003eimshow\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'Emotion Detection'\u003c/span\u003e, frame)\n\n        # 루프 종료\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e cv2.\u003cspan class=\"hljs-title function_\"\u003ewaitKey\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) \u0026#x26; \u003cspan class=\"hljs-number\"\u003e0xFF\u003c/span\u003e == \u003cspan class=\"hljs-title function_\"\u003eord\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'q'\u003c/span\u003e):\n            \u003cspan class=\"hljs-keyword\"\u003ebreak\u003c/span\u003e\nexcept \u003cspan class=\"hljs-title class_\"\u003eKeyboardInterrupt\u003c/span\u003e:\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"사용자에 의해 중지됨\"\u003c/span\u003e)\n\u003cspan class=\"hljs-attr\"\u003efinally\u003c/span\u003e:\n    # 모든 작업이 완료되면 캡처 해제\n    cap.\u003cspan class=\"hljs-title function_\"\u003erelease\u003c/span\u003e()\n    cv2.\u003cspan class=\"hljs-title function_\"\u003edestroyAllWindows\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e2.2 실시간 감정 시각화\u003c/h2\u003e\n\u003cp\u003e기본적인 실시간 감정 감지 스크립트를 기반으로, 실시간 감정 시각화 기능을 추가한 것입니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이 업데이트는 감정 감지 응용 프로그램에 더 다이내믹하고 상호작용 가능한 측면을 추가하여 데이터를 더 매력적이고 통찰력 있게 만듭니다.\u003c/p\u003e\n\u003cp\u003e실시간 감정 막대 차트 생성: 각 프레임에서 감지된 감정을 시각화하기 위해, 우리는 파이썬의 강력한 플로팅 라이브러리인 matplotlib을 사용합니다. 다음은 설정하는 방법입니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e우리는 matplotlib 피겨를 초기화하고 각 감정에 대한 자리 표시자를 가진 막대 차트를 만듭니다.\u003c/li\u003e\n\u003cli\u003e차트의 각 막대는 감정을 나타내며, FER에 의해 감지된 신뢰 수준을 실시간으로 업데이트하여 높이가 조정됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\nplt.ion()  \u003cspan class=\"hljs-comment\"\u003e# 대화형 모드를 활성화\u003c/span\u003e\nfig, ax = plt.subplots()\nemotion_labels = [\u003cspan class=\"hljs-string\"\u003e'angry'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'disgust'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'fear'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'happy'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'sad'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'surprise'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'neutral'\u003c/span\u003e]\nbars = ax.bar(emotion_labels, [\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]*\u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e, color=\u003cspan class=\"hljs-string\"\u003e'lightblue'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eMatplotlib의 대화형 모드: 대화형 모드를 활성화하면 (plt.ion()), matplotlib의 플롯이 실시간으로 업데이트됩니다. 이를 통해 바 차트가 감정 감지 알고리즘에 의해 처리된 각 새 프레임마다 동적으로 새로 고쳐질 수 있습니다.\u003c/p\u003e\n\u003cp\u003e차트 업데이트: 감지된 감정을 가져와 각 막대의 높이를 업데이트하는 update_chart 함수를 만듭니다. 이 함수는 처리된 각 프레임에서 호출되어 차트가 현재 감지된 감정을 정확히 반영하도록 합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eupdate_chart\u003c/span\u003e(detected_emotions, bars, ax, fig):\n    ax.\u003cspan class=\"hljs-title function_\"\u003eclear\u003c/span\u003e()\n    ax.\u003cspan class=\"hljs-title function_\"\u003ebar\u003c/span\u003e(emotion_labels, [detected_emotions.\u003cspan class=\"hljs-title function_\"\u003eget\u003c/span\u003e(emotion, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e emotion \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e emotion_labels], color=\u003cspan class=\"hljs-string\"\u003e'lightblue'\u003c/span\u003e)\n    ### [차트 서식의 나머지 부분]\n    fig.\u003cspan class=\"hljs-property\"\u003ecanvas\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eflush_events\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e주요 루프에서 차트 업데이트 통합: 스크립트의 주요 루프에서 각 프레임에서 감정을 감지한 후, 최신 감정 데이터로 update_chart를 호출합니다. 이렇게 함으로써 시각화가 비디오 피드와 동기화되도록 유지됩니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 감정 감지 및 시각화를 위한 주요 루프\n\u003cspan class=\"hljs-keyword\"\u003ewhile\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e:\n    # [웹캠 캡처 및 감정 감지 코드]\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003elargest_face\u003c/span\u003e:\n        # [얼굴 처리 및 감정 점수 매기기]\n        \n        # 최신 감정 데이터로 실시간 막대 차트 업데이트\n        \u003cspan class=\"hljs-title function_\"\u003eupdate_chart\u003c/span\u003e(current_emotions, bars, ax, fig)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e모두 합쳐서 다음과 같은 포괄적인 Python 스크립트를 얻습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e fer \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eFER\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e cv2\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.\u003cspan class=\"hljs-property\"\u003epyplot\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e pd\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e imageio\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e time\n\n\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n실시간 감정 감지 및 시각화\n\n이 스크립트는 웹캠에서 비디오를 캡처하고 실시간으로 얼굴의 감정을 감지하여 결과를 실시간으로 시각화하는 기능을 제공합니다. \n또한 감지된 감정과 함께 비디오 자체에도 표시된 상자와 감정 레이블을 저장합니다. 또한 실시간 감정 막대 차트를 GIF로, \n누적 감정 통계도 정적 차트로 저장합니다. 이 스크립트는 비디오 처리를 위해 OpenCV, 감정 감지를 위해 FER, 라이브 차트 시각화를 \n위해 matplotlib, GIF 제작을 위해 imageio를 사용합니다.\n\n주요 기능:\n- 웹캠에서 실시간 감정 감지.\n- 막대 차트에서 감정 확신 수준을 실시간으로 업데이트.\n- 얼굴 주위의 경계 상자 및 감정 레이블이 있는 비디오 캡처 저장.\n- 라이브 감정 막대 차트의 GIF 생성.\n- 시간에 따른 감정 통계의 누적 차트 저장.\n\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\n# matplotlib의 백엔드를 다양한 환경과 호환되도록 \u003cspan class=\"hljs-string\"\u003e'TkAgg'\u003c/span\u003e로 설정\nmatplotlib.\u003cspan class=\"hljs-title function_\"\u003euse\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'TkAgg'\u003c/span\u003e)\n\n# \u003cspan class=\"hljs-variable constant_\"\u003eMTCNN\u003c/span\u003e을 사용하는 \u003cspan class=\"hljs-variable constant_\"\u003eFER\u003c/span\u003e (\u003cspan class=\"hljs-title class_\"\u003eFace\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eEmotion\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eRecognition\u003c/span\u003e) 탐지기를 초기화\ndetector = \u003cspan class=\"hljs-title function_\"\u003eFER\u003c/span\u003e(mtcnn=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\n# 웹캠에서 비디오 캡처 시작 (장치 \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\ncap = cv2.\u003cspan class=\"hljs-title class_\"\u003eVideoCapture\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n\n# 비디오 녹화용 프레임 속도 설정 (웹캠의 성능에 따라 조정)\nframe_rate = \u003cspan class=\"hljs-number\"\u003e4.3\u003c/span\u003e\n\n# 지정된 프레임 속도로 비디오를 저장하는 \u003cspan class=\"hljs-title class_\"\u003eOpenCV\u003c/span\u003e의 \u003cspan class=\"hljs-title class_\"\u003eVideoWriter\u003c/span\u003e 초기화\nfourcc = cv2.\u003cspan class=\"hljs-title class_\"\u003eVideoWriter\u003c/span\u003e_fourcc(*\u003cspan class=\"hljs-string\"\u003e'XVID'\u003c/span\u003e)\nout = cv2.\u003cspan class=\"hljs-title class_\"\u003eVideoWriter\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'emotion_video.avi'\u003c/span\u003e, fourcc, frame_rate, (\u003cspan class=\"hljs-number\"\u003e640\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e480\u003c/span\u003e))\n\n# 라이브 감정 감지 결과를 표시할 matplotlib 피겨 설정\nplt.\u003cspan class=\"hljs-title function_\"\u003eion\u003c/span\u003e()  # 라이브 업데이트를 위한 대화형 모드 활성화\nfig, ax = plt.\u003cspan class=\"hljs-title function_\"\u003esubplots\u003c/span\u003e()\nemotion_labels = [\u003cspan class=\"hljs-string\"\u003e'angry'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'disgust'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'fear'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'happy'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'sad'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'surprise'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'neutral'\u003c/span\u003e]\nbars = ax.\u003cspan class=\"hljs-title function_\"\u003ebar\u003c/span\u003e(emotion_labels, [\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]*\u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e, color=\u003cspan class=\"hljs-string\"\u003e'lightblue'\u003c/span\u003e)  # 각 감정을 위한 막대 초기화\nplt.\u003cspan class=\"hljs-title function_\"\u003eylim\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003eylabel\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'신뢰도'\u003c/span\u003e)\nplt.\u003cspan class=\"hljs-title function_\"\u003etitle\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'실시간 감정 감지'\u003c/span\u003e)\nax.\u003cspan class=\"hljs-title function_\"\u003eset_xticklabels\u003c/span\u003e(emotion_labels, rotation=\u003cspan class=\"hljs-number\"\u003e45\u003c/span\u003e)\n\n# 라이브 차트 업데이트를 \u003cspan class=\"hljs-variable constant_\"\u003eGIF\u003c/span\u003e로 저장하기 위해 imageio 라이터 초기화\ngif_writer = imageio.\u003cspan class=\"hljs-title function_\"\u003eget_writer\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'emotion_chart.gif'\u003c/span\u003e, mode=\u003cspan class=\"hljs-string\"\u003e'I'\u003c/span\u003e, duration=\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e)\n\n# 각 프레임에 대한 누적 감정 통계를 저장할 리스트\nemotion_statistics = []\n\n# 라이브 차트 업데이트 함수\ndef \u003cspan class=\"hljs-title function_\"\u003eupdate_chart\u003c/span\u003e(detected_emotions, bars, ax, fig):\n    ax.\u003cspan class=\"hljs-title function_\"\u003eclear\u003c/span\u003e()  # 현재 축 지우고 다시 막대 차트 설정\n    ax.\u003cspan class=\"hljs-title function_\"\u003ebar\u003c/span\u003e(emotion_labels, [detected_emotions.\u003cspan class=\"hljs-title function_\"\u003eget\u003c/span\u003e(emotion, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e emotion \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e emotion_labels], color=\u003cspan class=\"hljs-string\"\u003e'lightblue'\u003c/span\u003e)\n    plt.\u003cspan class=\"hljs-title function_\"\u003eylim\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n    plt.\u003cspan class=\"hljs-title function_\"\u003eylabel\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'신뢰도'\u003c/span\u003e)\n    plt.\u003cspan class=\"hljs-title function_\"\u003etitle\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'실시간 감정 감지'\u003c/span\u003e)\n    ax.\u003cspan class=\"hljs-title function_\"\u003eset_xticklabels\u003c/span\u003e(emotion_labels, rotation=\u003cspan class=\"hljs-number\"\u003e45\u003c/span\u003e)\n    fig.\u003cspan class=\"hljs-property\"\u003ecanvas\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003edraw\u003c/span\u003e()\n    fig.\u003cspan class=\"hljs-property\"\u003ecanvas\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eflush_events\u003c/span\u003e()\n\n# 웹캠 작동 시간을 측정하기 위해 타이머 시작\nwebcam_start_time = time.\u003cspan class=\"hljs-title function_\"\u003etime\u003c/span\u003e()\n\n\u003cspan class=\"hljs-attr\"\u003etry\u003c/span\u003e:\n    \u003cspan class=\"hljs-keyword\"\u003ewhile\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e:\n        ret, frame = cap.\u003cspan class=\"hljs-title function_\"\u003eread\u003c/span\u003e()  # 웹캠에서 프레임 읽기\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e not \u003cspan class=\"hljs-attr\"\u003eret\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003ebreak\u003c/span\u003e  # 캡처된 프레임이 없으면 루프 종료\n\n        # 프레임에서 감정 감지\n        result = detector.\u003cspan class=\"hljs-title function_\"\u003edetect_emotions\u003c/span\u003e(frame)\n        largest_face = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e\n        max_area = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\n        # 주요 감정 분석을 위해 프레임에서 가장 큰 얼굴 찾기\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e face \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eresult\u003c/span\u003e:\n            box = face[\u003cspan class=\"hljs-string\"\u003e\"box\"\u003c/span\u003e]\n            x, y, w, h = box\n            area = w * h\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e area \u003e \u003cspan class=\"hljs-attr\"\u003emax_area\u003c/span\u003e:\n                max_area = area\n                largest_face = face\n\n        # 얼굴이 감지되면 감정 표시 및 차트 업데이트\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003elargest_face\u003c/span\u003e:\n            box = largest_face[\u003cspan class=\"hljs-string\"\u003e\"box\"\u003c/span\u003e]\n            current_emotions = largest_face[\u003cspan class=\"hljs-string\"\u003e\"emotions\"\u003c/span\u003e]\n\n            # 감정 데이터 저장\n            emotion_statistics.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(current_emotions)\n\n            x, y, w, h = box\n            cv2.\u003cspan class=\"hljs-title function_\"\u003erectangle\u003c/span\u003e(frame, (x, y), (x+w, y+h), (\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e), \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n            \n            emotion_type = \u003cspan class=\"hljs-title function_\"\u003emax\u003c/span\u003e(current_emotions, key=current_emotions.\u003cspan class=\"hljs-property\"\u003eget\u003c/span\u003e)\n            emotion_score = current_emotions[emotion_type]\n\n            emotion_text = f\u003cspan class=\"hljs-string\"\u003e\"{emotion_type}: {emotion_score:.2f}\"\u003c/span\u003e\n            cv2.\u003cspan class=\"hljs-title function_\"\u003eputText\u003c/span\u003e(frame, emotion_text, (x, y - \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e), cv2.\u003cspan class=\"hljs-property\"\u003eFONT_HERSHEY_SIMPLEX\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.9\u003c/span\u003e, (\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e), \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n\n            \u003cspan class=\"hljs-title function_\"\u003eupdate_chart\u003c/span\u003e(current_emotions, bars, ax, fig)\n\n            out.\u003cspan class=\"hljs-title function_\"\u003ewrite\u003c/span\u003e(frame)  # 프레임을 비디오 파일에 씀\n\n            # 현재 막대 차트 상태를 \u003cspan class=\"hljs-variable constant_\"\u003eGIF\u003c/span\u003e의 한 프레임으로 저장\n            fig.\u003cspan class=\"hljs-property\"\u003ecanvas\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003edraw\u003c/span\u003e()\n            image = np.\u003cspan class=\"hljs-title function_\"\u003efrombuffer\u003c/span\u003e(fig.\u003cspan class=\"hljs-property\"\u003ecanvas\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003etostring_rgb\u003c/span\u003e(), dtype=\u003cspan class=\"hljs-string\"\u003e'uint8'\u003c/span\u003e)\n            image = image.\u003cspan class=\"hljs-title function_\"\u003ereshape\u003c/span\u003e(fig.\u003cspan class=\"hljs-property\"\u003ecanvas\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eget_width_height\u003c/span\u003e()[::-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e] + (\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e,))\n            gif_writer.\u003cspan class=\"hljs-title function_\"\u003eappend_data\u003c/span\u003e(image)\n\n        cv2.\u003cspan class=\"hljs-title function_\"\u003eimshow\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'감정 감지'\u003c/span\u003e, frame)  # 프레임 표시\n\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e cv2.\u003cspan class=\"hljs-title function_\"\u003ewaitKey\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) \u0026#x26; \u003cspan class=\"hljs-number\"\u003e0xFF\u003c/span\u003e == \u003cspan class=\"hljs-title function_\"\u003eord\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'q'\u003c/span\u003e):\n            \u003cspan class=\"hljs-keyword\"\u003ebreak\u003c/span\u003e\n        \nexcept \u003cspan class=\"hljs-title class_\"\u003eKeyboardInterrupt\u003c/span\u003e:\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"사용자에 의해 중단됨\"\u003c/span\u003e)\n\n\u003cspan class=\"hljs-attr\"\u003efinally\u003c/span\u003e:\n    webcam_end_time = time.\u003cspan class=\"hljs-title function_\"\u003etime\u003c/span\u003e()  # 웹캠 창이 닫힐 때 타이머 종료\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"웹캠 활동 시간: {webcam_end_time - webcam_start_time:.2f} 초\"\u003c/span\u003e)\n\n    cap.\u003cspan class=\"hljs-title function_\"\u003erelease\u003c/span\u003e()\n    cv2.\u003cspan class=\"hljs-title function_\"\u003edestroyAllWindows\u003c/span\u003e()\n    plt.\u003cspan class=\"hljs-title function_\"\u003eclose\u003c/span\u003e(fig)\n\n    out.\u003cspan class=\"hljs-title function_\"\u003erelease\u003c/span\u003e()\n    gif_writer.\u003cspan class=\"hljs-title function_\"\u003eclose\u003c/span\u003e()\n\n    emotion_df = pd.\u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e(emotion_statistics)\n\n    plt.\u003cspan class=\"hljs-title function_\"\u003efigure\u003c/span\u003e(figsize=(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e))\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e emotion \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eemotion_labels\u003c/span\u003e:\n        plt.\u003cspan class=\"hljs-title function_\"\u003eplot\u003c/span\u003e(emotion_df[emotion].\u003cspan class=\"hljs-title function_\"\u003ecumsum\u003c/span\u003e(), label=emotion)\n    plt.\u003cspan class=\"hljs-title function_\"\u003etitle\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'시간에 따른 누적 감정 통계'\u003c/span\u003e)\n    plt.\u003cspan class=\"hljs-title function_\"\u003exlabel\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'프레임'\u003c/span\u003e)\n    plt.\u003cspan class=\"hljs-title function_\"\u003eylabel\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'누적 신뢰도'\u003c/span\u003e)\n    plt.\u003cspan class=\"hljs-title function_\"\u003elegend\u003c/span\u003e()\n    plt.\u003cspan class=\"hljs-title function_\"\u003esavefig\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'cumulative_emotions.jpg'\u003c/span\u003e)\n    plt.\u003cspan class=\"hljs-title function_\"\u003eclose\u003c/span\u003e()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e2.3 보너스 — 결과 병합하기\u003c/h2\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e감정 감지 기술의 진정한 힘은 데이터를 포괄적이고 매력적인 방식으로 시각화할 때 발휘됩니다. 이를 달성하기 위해 다양한 출력물을 하나의 시각적 프레젠테이션으로 효과적으로 결합하는 스크립트를 개발했습니다. 이 스크립트는 세 가지 요소를 효과적으로 조화시킵니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e실시간 비디오: 웹캠 피드에서 캡처한 감정 감지 결과가 'emotion_video.avi'로 저장됩니다.\u003c/li\u003e\n\u003cli\u003e동적 막대 차트 GIF: 감지된 감정을 실시간으로 업데이트하는 차트가 'emotion_chart.gif'로 저장됩니다.\u003c/li\u003e\n\u003cli\u003e정적 누적 감정 차트: 'cumulative_emotions.jpg'라는 이미지 파일에 시간에 따른 집계된 감정 데이터가 표시됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e스크립트에서 중요한 코드 일부:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e입력값 읽기 및 처리:\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003estatic_chart = \u003cspan class=\"hljs-title class_\"\u003eImage\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eopen\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'cumulative_emotions.jpg'\u003c/span\u003e)\nvideo = cv2.\u003cspan class=\"hljs-title class_\"\u003eVideoCapture\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'emotion_video.avi'\u003c/span\u003e)\nbar_chart_gif = imageio.\u003cspan class=\"hljs-title function_\"\u003emimread\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'emotion_chart.gif'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eFrames를 조합하는 로직:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ecombined_frame = \u003cspan class=\"hljs-title class_\"\u003eImage\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003enew\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'RGB'\u003c/span\u003e, (\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e * desired_width, desired_height))\ncombined_frame.\u003cspan class=\"hljs-title function_\"\u003epaste\u003c/span\u003e(video_frame_resized, (\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e))\ncombined_frame.\u003cspan class=\"hljs-title function_\"\u003epaste\u003c/span\u003e(gif_resized, (desired_width, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e))\ncombined_frame.\u003cspan class=\"hljs-title function_\"\u003epaste\u003c/span\u003e(static_chart_resized, (\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e * desired_width, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e더 자세한 코드와 기술적인 세부 정보를 탐험하고 싶은 분들을 위해, AI, 데이터 과학 및 기술에 대한 다양한 기술 튜토리얼 및 실용적인 가이드가 포함된 리소스인 우리 웹사이트인 Entreprenerdly.com을 방문하시기를 권장합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이 프로젝트와 다른 많은 프로젝트의 완전한 코드를 찾을 수 있습니다. 최신 기술 솔루션을 배우고 구현하는 실전적인 방법을 제공합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER_1.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch2\u003e5. 실용적인 응용\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e보안 시스템: 감정 인식은 보안 시스템에 추가적인 보호층을 추가할 수 있습니다. 감정 신호에 기반하여 의심스러운 또는 이례적인 행동을 식별할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e의료 및 정신 건강: 임상 환경에서 감정 인식은 환자의 정신 건강 상태를 모니터링하는 데 도움을 줄 수 있습니다. 특히 텔레치료 세션에서 유용하며, 환자의 감정 반응에 대한 추가적인 통찰을 제공하여 요법사에게 도움이 될 수 있습니다.\u003c/li\u003e\n\u003cli\u003e사용자 경험 및 인터페이스 디자인: 웹사이트와 애플리케이션은 감정 인식을 사용하여 사용자 경험을 맞춤화할 수 있습니다. 혼란 또는 불만의 징후를 감지하여 유용한 프롬프트를 트리거하거나 사용자가 더 관련 있는 콘텐츠로 안내할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e로봇과 인공지능: 로봇 공학에서 감정 인식은 AI 및 로봇과의 상호 작용을 더 자연스럽고 직관적으로 만들 수 있습니다. 특히 요양이나 고객 서비스 로봇에서 유용합니다.\u003c/li\u003e\n\u003cli\u003e접근성 기술: 언어 또는 청각 장애가 있는 사람들을 위해 감정 인식 기술은 화자의 감정 상태에 대한 추가적인 문맥을 제공하여 의사 소통을 용이하게 할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e6. 한계와 개선점\u003c/h1\u003e\n\u003ch2\u003e6.1 한계\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e정확도와 데이터 의존성: 감정 감지의 정확도는 FER 모델의 훈련 데이터에 크게 의존합니다. 이 데이터의 편견은 잘못된 또는 일관되지 않은 감정 인식으로 이어질 수 있으며, 특히 다양한 인구 통계군 사이에서 발생할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e맥락적 이해: 시스템은 감정의 맥락을 이해하기 항해 합니다. 얼굴 표정을 인식하지만 이러한 표정 뒤의 이유를 추론하거나 진짜 감정과 가짜 감정을 구별할 수 없습니다.\u003c/li\u003e\n\u003cli\u003e조명 및 영상 품질: 웹캠 영상의 품질, 포함하여 조명 조건과 해상도는 감지 정확도에 큰 영향을 미칠 수 있습니다. 낮은 영상 품질은 신뢰할 수 없는 감정 인식으로 이어질 수 있습니다.\u003c/li\u003e\n\u003cli\u003e개인정보 우려: 감정 인식 기술을 사용함에 있어, 특히 공개적 또는 준공개적 공간에서는 개인정보 문제가 우려됩니다. 이러한 시스템을 구현할 때는 동의와 윤리적 고려가 매우 중요합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e6.2 개선 가능한 부분:\u003c/h2\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e향상된 머신러닝 모델: 더 발전된 머신러닝 모델을 통합하거나 기존의 FER 모델을 사용자 정의함으로써 정확성을 향상시키고 편향을 줄일 수 있습니다.\u003c/li\u003e\n\u003cli\u003e맥락을 인지하는 알고리즘: 상황의 맥락을 고려하는 알고리즘을 개발하면 보다 세밀한 감정 분석을 제공할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e실시간 처리 최적화: 코드를 효율적으로 최적화하거나 더 강력한 처리 하드웨어를 사용함으로써 실시간 응용 프로그램에서의 대기 시간을 최소화할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e개인 정보 보호 대책: 엄격한 개인 정보 보호 규정을 시행하고 데이터 사용에 대한 투명성을 보장함으로써 개인 정보 보호 우려를 완화할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003e이러한 강력한 도구들을 결합하여 감정을 실시간으로 감지할 뿐만 아니라 이러한 데이터를 매력적이고 유익한 방식으로 시각화할 수 있다는 것을 확인했습니다. 이 기술의 실용적인 응용은 건강 관리부터 마케팅에 이르기까지 다양하며 다양합니다.\u003c/p\u003e\n\u003cp\u003e독자들에게는 이 기사가 현재 기술의 능력을 이해하고 그 잠재적인 응용 분야를 상상해 볼 수 있는 시작점으로 제공됩니다. 개발자, 연구자 또는 기술과 감정이 교차하는 부분에 관심이 있는 사람이라면 상상력만큼의 가능성이 넓게 열려 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER_2.png\" alt=\"Real-Time Emotion Recognition in Python with OpenCV and FER\"\u003e\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-07-13-Real-TimeEmotionRecognitioninPythonwithOpenCVandFER"},"buildId":"X4OrKfmLtU3-3BijwfHe6","assetPrefix":"/TIL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>