<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>로컬 LLMs 실행이 생각보다 더 유용하고 쉬운 이유 | TIL</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://13akstjq.github.io/TIL//post/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="로컬 LLMs 실행이 생각보다 더 유용하고 쉬운 이유 | TIL" data-gatsby-head="true"/><meta property="og:title" content="로컬 LLMs 실행이 생각보다 더 유용하고 쉬운 이유 | TIL" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://TIL.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://13akstjq.github.io/TIL//post/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink" data-gatsby-head="true"/><meta name="twitter:title" content="로컬 LLMs 실행이 생각보다 더 유용하고 쉬운 이유 | TIL" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | TIL" data-gatsby-head="true"/><meta name="article:published_time" content="2024-07-12 19:27" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/TIL/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/TIL/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/TIL/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/TIL/favicons/favicon-96x96.png"/><link rel="icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/TIL/favicons/browserconfig.xml"/><link rel="preload" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/TIL/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/TIL/_next/static/chunks/webpack-21ffe88bdca56cba.js" defer=""></script><script src="/TIL/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/TIL/_next/static/chunks/main-a5eeabb286676ce6.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/TIL/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/TIL/_next/static/chunks/348-02483b66b493dd81.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/post/%5Bslug%5D-8ded8b979ba73586.js" defer=""></script><script src="/TIL/_next/static/N1mNhRlQaHCliEGDvPEpG/_buildManifest.js" defer=""></script><script src="/TIL/_next/static/N1mNhRlQaHCliEGDvPEpG/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/TIL">TIL</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/TIL/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">로컬 LLMs 실행이 생각보다 더 유용하고 쉬운 이유</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="로컬 LLMs 실행이 생각보다 더 유용하고 쉬운 이유" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/TIL/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">TIL</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jul 12, 2024</span><span class="posts_reading_time__f7YPP">9<!-- --> min read</span></span></div></div></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_0.png" alt="image"></p>
<h1>#1 로컬 LLM을 사용해야 하는 이유</h1>
<p>ChatGPT은 정말 멋지죠. 그런데 한 가지 치명적인 단점이 있습니다: 작성하거나 업로드하는 모든 것이 OpenAI의 서버에 저장됩니다. 이는 많은 경우에는 문제가 되지 않을 수 있지만, 민감한 데이터를 다룰 때 문제가 될 수 있습니다.</p>
<p>그래서 저는 개인 컴퓨터에서 로컬로 실행할 수 있는 오픈소스 LLM을 탐구하기 시작했습니다. 실제로 그것들이 왜 훌륭한지에 대해 많은 이유가 있다는 것을 발견했습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ol>
<li>
<p>데이터 개인 정보 보호: 귀하의 정보는 귀하의 기기에 유지됩니다.</p>
</li>
<li>
<p>비용 효율적: 가입비나 API 비용이 없으며 무료로 사용할 수 있습니다.</p>
</li>
<li>
<p>맞춤화: 모델은 귀하의 특정 시스템 프롬프트나 데이터 세트로 세밀하게 조정할 수 있습니다.</p>
</li>
<li>
<p>오프라인 기능: 인터넷 연결이 필요하지 않습니다.</p>
</li>
</ol>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ol start="5">
<li>제약 사항이 없는 사용: 외부 API에서 부과된 제한이 없습니다.</li>
</ol>
<p>시작해 봅시다!</p>
<h2>2. Ollama 설치 및 Llama 3 실행하기</h2>
<p>Ollama는 개인 컴퓨터에서 쉽게 대형 언어 모델(LLM)을 로컬에서 실행할 수 있게 해주는 오픈 소스 프로젝트입니다. 사용자 친화적이고 매우 가벼우며 Meta(럼마 3)와 구글(젬마 2)의 최신 및 최고의 사전 학습 모델을 포함한 다양한 모델을 제공하는 것으로 알려져 있습니다. 이러한 회사들이 이러한 모델을 교육하는 데 수백만 달러를 투자하여 우리가 자신의 기기에서 재미있게 사용할 수 있도록 했습니다. 대단하지 않나요?</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Ollama는 그 자체로는 빈 껍데기에 불과하며 작동하려면 LLM이 필요합니다.</p>
<p>설치 프로세스에 들어가기 전에 사용 가능한 모델들을 살펴보겠습니다:</p>
<p><img src="/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_1.png" alt=""></p>
<p>그리고 더 많은 모델이 있습니다!</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이 글에서는 Meta사의 최신 모델인 Llama 3에 초점을 맞추어 놀라운 성능을 약속하는 이 모델이 이 플랫폼에서 가장 인기 있는 모델이라고 합니다. 이 글을 작성하는 시점에 이 모델은 440만 회 이상의 다운로드를 기록하고 있습니다.</p>
<p><img src="/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_2.png" alt="이미지"></p>
<p>다음 단계에서는 컴퓨터에 Ollama를 설치하고 Llama3로 공급하여 마침내 그 모델을 ChatGPT처럼 사용하는 방법을 보여줍니다.</p>
<p>단계 1/2:</p>
<ol>
<li>ollama.com에 가서 "다운로드"를 클릭합니다. 저는 macOS를 사용하고 있으므로 이후 튜토리얼에서 이 옵션에 초점을 맞출 것이지만, Linux나 Windows에서 할 때도 크게 다르지 않을 것입니다.</li>
<li>"macOS용 다운로드"를 클릭합니다.</li>
</ol>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>마크다운 형식의 표를 사용해보세요.</p>
<p>STEP 3/4/5:
다른 앱들과 마찬가지로 매우 간단한 설치 단계를 따르기만 하면 됩니다.</p>
<ol>
<li>"설치"를 클릭합니다.</li>
<li>"다음"을 클릭합니다.</li>
<li>터미널에서 "ollama run llama3"을 실행합니다.</li>
</ol>
<p>마지막 단계에서는 먼저 llama3의 8B 버전(약 4.7GB)을 컴퓨터에 다운로드한 다음 실행됩니다. 이렇게 간단합니다!</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_5.png" alt="Running Local LLM is More Useful and Easier Than You Think"></p>
<p>And this article could stop right here. A few clicks and a line of code later, here we are running an LLM locally!</p>
<p>You can ask it anything, like explaining the differences between the 8 billion and 70 billion parameters versions of Llama 3.</p>
<p><img src="/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_6.png" alt="Running Local LLM is More Useful and Easier Than You Think"></p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>해당 모델의 응답 시간은 일반적으로 컴퓨터의 GPU / RAM에 의존합니다.</p>
<h2>#3 몇 가지 유용한 명령어</h2>
<p>터미널 내에서 계속 LLMs를 사용하고 싶다면 몇 가지 기본 명령어가 필요하다고 생각됩니다:</p>
<ul>
<li>ollama run llama3
이 경우 llama3 모델을 실행합니다.</li>
<li>ollama list
로컬로 이미 설치된 모든 모델을 나열합니다.</li>
<li>ollama pull mistral
플랫폼에서 다른 사용 가능한 모델을 가져옵니다. 이 경우 mistral 모델을 가져옵니다.</li>
<li>/clear (모델이 실행 중일 때)
세션의 컨텍스트를 지워 처음부터 시작합니다.</li>
<li>/bye (모델이 실행 중일 때)
ollama를 종료합니다.</li>
<li>/? (모델이 실행 중일 때)
사용 가능한 모든 명령어를 나열합니다.</li>
</ul>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>더 복잡한 사용 사례를 위해 더 많은 명령이 존재합니다. 새로운 미세 조정 모델을 생성하는 것과 같은 경우가 있습니다.</p>
<p>Ollama의 Github 저장소에는 매우 완벽한 설명서가 있습니다.</p>
<p>기본 사용 사례에 대해서는 CLI가 충분할 수도 있지만 더 많은 기능이 있습니다...</p>
<h1>#4 Jupyter Notebook에서 Llama 3</h1>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>터미널을 통해 LLMs를 사용하는 것도 좋지만 파이썬 코드를 통해 모델과 상호 작용하면 더 많은 가능성이 열립니다.</p>
<p>이를 위해 langchain_community 라이브러리를 pip으로 설치해야 합니다 (pip install langchain_community) 그리고 Ollama 패키지를 가져와야 합니다.</p>
<p>예를 들어, 어떤 사람의 이름, 나이, 직업을 제공하여 짧은 자기소개를 만들고 싶다고 가정해봅시다. 이 예제에서는 다음과 같이 코드가 작성됩니다:</p>
<pre><code class="hljs language-js"># !pip install langchain_community

# 필요한 패키지 가져오기
<span class="hljs-keyword">from</span> langchain_community.<span class="hljs-property">llms</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">Ollama</span>

# 모델 인스턴스 생성
llm = <span class="hljs-title class_">Ollama</span>(model=<span class="hljs-string">"llama3"</span>)

# 프롬프트와 함께 모델 사용
llm.<span class="hljs-title function_">invoke</span>(<span class="hljs-string">"Alice의 나이가 25세이고 엔지니어로 일하는 짧은 2문장 자기소개 생성"</span>)
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>친근한 톤으로 번역해 드릴게요.</p>
<p>조금 더 다듬어보면 더 완벽해질거에요.</p>
<p>단일 인물을 위한 소개를 터미널에서 쉽게 만들 수 있지만, 많은 사람들의 경우엔 파이썬 없이 같은 작업을 반복해야 할 수도 있어요. 파이썬을 사용하면 프롬프트를 매개변수화시키고, 많은 사람들에 대해 자동화된 프로세스를 실행할 수 있어요.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>예를 들어:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

<span class="hljs-comment"># 샘플 DataFrame 생성</span>
df = pd.DataFrame({
    <span class="hljs-string">'name'</span>: [<span class="hljs-string">'Alice'</span>, <span class="hljs-string">'Bob'</span>, <span class="hljs-string">'Charlie'</span>],
    <span class="hljs-string">'age'</span>: [<span class="hljs-number">25</span>, <span class="hljs-number">30</span>, <span class="hljs-number">35</span>],
    <span class="hljs-string">'occupation'</span>: [<span class="hljs-string">'Engineer'</span>, <span class="hljs-string">'Teacher'</span>, <span class="hljs-string">'Artist'</span>]
})

<span class="hljs-comment"># DataFrame에 적용할 수 있는 함수 생성</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_bio</span>(<span class="hljs-params">name, age, occupation</span>):
    prompt = <span class="hljs-string">f"<span class="hljs-subst">{name}</span>님에 대한 간단한 2문장 소개 생성, <span class="hljs-subst">{age}</span>세이고 <span class="hljs-subst">{occupation}</span>로 근무 중"</span>
    <span class="hljs-keyword">return</span> llm.invoke(prompt)

<span class="hljs-comment"># DataFrame에 함수 적용</span>
df[<span class="hljs-string">'bio'</span>] = df.apply(<span class="hljs-keyword">lambda</span> row: generate_bio(row[<span class="hljs-string">'name'</span>], row[<span class="hljs-string">'age'</span>], row[<span class="hljs-string">'occupation'</span>]), axis=<span class="hljs-number">1</span>)

df.head()
</code></pre>
<p>이제 DataFrame의 각 행에 대해 모델이 바이오를 생성합니다!</p>
<h1>5번째 단계 최종 소견</h1>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이 기사의 목적은 Ollama 덕분에 로컬에서 완전히 기능적인 LLM(Large Language Model)을 구현하는 간편함을 강조하는 것이었습니다.</p>
<p>간단한 요청을 위해 터미널을 통해 이 모델을 사용하거나 Python을 사용하여 더 복잡하거나 자동화된 작업을 수행할 수 있습니다. 프로세스는 간단합니다.</p>
<p>Open WebUI와 같은 오픈 소스 프로젝트 덕분에 우리만의 ChatGPT와 같은 그래픽 인터페이스를 구현할 수도 있습니다.</p>
<p>저는 그저 몇 번의 클릭과 몇 줄의 코드로 이렇게 유용한 것을 얻을 수 있다는 것이 놀라워요! 여러분도 즐기셨으면 좋겠네요.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>기사 끝까지 읽어 주셔서 감사합니다.
더 많은 내용을 보려면 팔로우해주세요!
질문이나 의견이 있으시면 아래에 메시지를 남겨 주시거나 LinkedIn / X를 통해 연락해 주세요!</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"로컬 LLMs 실행이 생각보다 더 유용하고 쉬운 이유","description":"","date":"2024-07-12 19:27","slug":"2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink","content":"\n\n![image](/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_0.png)  \n\n# #1 로컬 LLM을 사용해야 하는 이유  \n\nChatGPT은 정말 멋지죠. 그런데 한 가지 치명적인 단점이 있습니다: 작성하거나 업로드하는 모든 것이 OpenAI의 서버에 저장됩니다. 이는 많은 경우에는 문제가 되지 않을 수 있지만, 민감한 데이터를 다룰 때 문제가 될 수 있습니다.  \n\n그래서 저는 개인 컴퓨터에서 로컬로 실행할 수 있는 오픈소스 LLM을 탐구하기 시작했습니다. 실제로 그것들이 왜 훌륭한지에 대해 많은 이유가 있다는 것을 발견했습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n1. 데이터 개인 정보 보호: 귀하의 정보는 귀하의 기기에 유지됩니다.\n\n2. 비용 효율적: 가입비나 API 비용이 없으며 무료로 사용할 수 있습니다.\n\n3. 맞춤화: 모델은 귀하의 특정 시스템 프롬프트나 데이터 세트로 세밀하게 조정할 수 있습니다.\n\n4. 오프라인 기능: 인터넷 연결이 필요하지 않습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n5. 제약 사항이 없는 사용: 외부 API에서 부과된 제한이 없습니다.\n\n시작해 봅시다!\n\n## 2. Ollama 설치 및 Llama 3 실행하기\n\nOllama는 개인 컴퓨터에서 쉽게 대형 언어 모델(LLM)을 로컬에서 실행할 수 있게 해주는 오픈 소스 프로젝트입니다. 사용자 친화적이고 매우 가벼우며 Meta(럼마 3)와 구글(젬마 2)의 최신 및 최고의 사전 학습 모델을 포함한 다양한 모델을 제공하는 것으로 알려져 있습니다. 이러한 회사들이 이러한 모델을 교육하는 데 수백만 달러를 투자하여 우리가 자신의 기기에서 재미있게 사용할 수 있도록 했습니다. 대단하지 않나요?\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nOllama는 그 자체로는 빈 껍데기에 불과하며 작동하려면 LLM이 필요합니다.\n\n설치 프로세스에 들어가기 전에 사용 가능한 모델들을 살펴보겠습니다:\n\n![](/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_1.png)\n\n그리고 더 많은 모델이 있습니다!\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 글에서는 Meta사의 최신 모델인 Llama 3에 초점을 맞추어 놀라운 성능을 약속하는 이 모델이 이 플랫폼에서 가장 인기 있는 모델이라고 합니다. 이 글을 작성하는 시점에 이 모델은 440만 회 이상의 다운로드를 기록하고 있습니다.\n\n![이미지](/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_2.png)\n\n다음 단계에서는 컴퓨터에 Ollama를 설치하고 Llama3로 공급하여 마침내 그 모델을 ChatGPT처럼 사용하는 방법을 보여줍니다.\n\n단계 1/2:\n1. ollama.com에 가서 \"다운로드\"를 클릭합니다. 저는 macOS를 사용하고 있으므로 이후 튜토리얼에서 이 옵션에 초점을 맞출 것이지만, Linux나 Windows에서 할 때도 크게 다르지 않을 것입니다.\n2. \"macOS용 다운로드\"를 클릭합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n마크다운 형식의 표를 사용해보세요.\n\nSTEP 3/4/5:\n다른 앱들과 마찬가지로 매우 간단한 설치 단계를 따르기만 하면 됩니다.\n1. \"설치\"를 클릭합니다.\n2. \"다음\"을 클릭합니다.\n3. 터미널에서 \"ollama run llama3\"을 실행합니다.\n\n마지막 단계에서는 먼저 llama3의 8B 버전(약 4.7GB)을 컴퓨터에 다운로드한 다음 실행됩니다. 이렇게 간단합니다!\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n![Running Local LLM is More Useful and Easier Than You Think](/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_5.png)\n\nAnd this article could stop right here. A few clicks and a line of code later, here we are running an LLM locally!\n\nYou can ask it anything, like explaining the differences between the 8 billion and 70 billion parameters versions of Llama 3.\n\n![Running Local LLM is More Useful and Easier Than You Think](/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_6.png)\n\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n해당 모델의 응답 시간은 일반적으로 컴퓨터의 GPU / RAM에 의존합니다.\n\n## #3 몇 가지 유용한 명령어\n\n터미널 내에서 계속 LLMs를 사용하고 싶다면 몇 가지 기본 명령어가 필요하다고 생각됩니다:\n\n- ollama run llama3\n이 경우 llama3 모델을 실행합니다.\n- ollama list\n로컬로 이미 설치된 모든 모델을 나열합니다.\n- ollama pull mistral\n플랫폼에서 다른 사용 가능한 모델을 가져옵니다. 이 경우 mistral 모델을 가져옵니다.\n- /clear (모델이 실행 중일 때)\n세션의 컨텍스트를 지워 처음부터 시작합니다.\n- /bye (모델이 실행 중일 때)\nollama를 종료합니다.\n- /? (모델이 실행 중일 때)\n사용 가능한 모든 명령어를 나열합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n더 복잡한 사용 사례를 위해 더 많은 명령이 존재합니다. 새로운 미세 조정 모델을 생성하는 것과 같은 경우가 있습니다. \n\nOllama의 Github 저장소에는 매우 완벽한 설명서가 있습니다.\n\n기본 사용 사례에 대해서는 CLI가 충분할 수도 있지만 더 많은 기능이 있습니다...\n\n# #4 Jupyter Notebook에서 Llama 3\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n터미널을 통해 LLMs를 사용하는 것도 좋지만 파이썬 코드를 통해 모델과 상호 작용하면 더 많은 가능성이 열립니다.\n\n이를 위해 langchain_community 라이브러리를 pip으로 설치해야 합니다 (pip install langchain_community) 그리고 Ollama 패키지를 가져와야 합니다.\n\n예를 들어, 어떤 사람의 이름, 나이, 직업을 제공하여 짧은 자기소개를 만들고 싶다고 가정해봅시다. 이 예제에서는 다음과 같이 코드가 작성됩니다:\n\n```js\n# !pip install langchain_community\n\n# 필요한 패키지 가져오기\nfrom langchain_community.llms import Ollama\n\n# 모델 인스턴스 생성\nllm = Ollama(model=\"llama3\")\n\n# 프롬프트와 함께 모델 사용\nllm.invoke(\"Alice의 나이가 25세이고 엔지니어로 일하는 짧은 2문장 자기소개 생성\")\n```\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n친근한 톤으로 번역해 드릴게요.\n\n조금 더 다듬어보면 더 완벽해질거에요.\n\n단일 인물을 위한 소개를 터미널에서 쉽게 만들 수 있지만, 많은 사람들의 경우엔 파이썬 없이 같은 작업을 반복해야 할 수도 있어요. 파이썬을 사용하면 프롬프트를 매개변수화시키고, 많은 사람들에 대해 자동화된 프로세스를 실행할 수 있어요.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n예를 들어:\n\n```python\nimport pandas as pd\n\n# 샘플 DataFrame 생성\ndf = pd.DataFrame({\n    'name': ['Alice', 'Bob', 'Charlie'],\n    'age': [25, 30, 35],\n    'occupation': ['Engineer', 'Teacher', 'Artist']\n})\n\n# DataFrame에 적용할 수 있는 함수 생성\ndef generate_bio(name, age, occupation):\n    prompt = f\"{name}님에 대한 간단한 2문장 소개 생성, {age}세이고 {occupation}로 근무 중\"\n    return llm.invoke(prompt)\n\n# DataFrame에 함수 적용\ndf['bio'] = df.apply(lambda row: generate_bio(row['name'], row['age'], row['occupation']), axis=1)\n\ndf.head()\n```\n\n이제 DataFrame의 각 행에 대해 모델이 바이오를 생성합니다!\n\n# 5번째 단계 최종 소견\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 기사의 목적은 Ollama 덕분에 로컬에서 완전히 기능적인 LLM(Large Language Model)을 구현하는 간편함을 강조하는 것이었습니다.\n\n간단한 요청을 위해 터미널을 통해 이 모델을 사용하거나 Python을 사용하여 더 복잡하거나 자동화된 작업을 수행할 수 있습니다. 프로세스는 간단합니다.\n\nOpen WebUI와 같은 오픈 소스 프로젝트 덕분에 우리만의 ChatGPT와 같은 그래픽 인터페이스를 구현할 수도 있습니다.\n\n저는 그저 몇 번의 클릭과 몇 줄의 코드로 이렇게 유용한 것을 얻을 수 있다는 것이 놀라워요! 여러분도 즐기셨으면 좋겠네요.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n기사 끝까지 읽어 주셔서 감사합니다.\n더 많은 내용을 보려면 팔로우해주세요!\n질문이나 의견이 있으시면 아래에 메시지를 남겨 주시거나 LinkedIn / X를 통해 연락해 주세요!","ogImage":{"url":"/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_0.png"},"coverImage":"/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_0.png","tag":["Tech"],"readingTime":9},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_0.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch1\u003e#1 로컬 LLM을 사용해야 하는 이유\u003c/h1\u003e\n\u003cp\u003eChatGPT은 정말 멋지죠. 그런데 한 가지 치명적인 단점이 있습니다: 작성하거나 업로드하는 모든 것이 OpenAI의 서버에 저장됩니다. 이는 많은 경우에는 문제가 되지 않을 수 있지만, 민감한 데이터를 다룰 때 문제가 될 수 있습니다.\u003c/p\u003e\n\u003cp\u003e그래서 저는 개인 컴퓨터에서 로컬로 실행할 수 있는 오픈소스 LLM을 탐구하기 시작했습니다. 실제로 그것들이 왜 훌륭한지에 대해 많은 이유가 있다는 것을 발견했습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e데이터 개인 정보 보호: 귀하의 정보는 귀하의 기기에 유지됩니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e비용 효율적: 가입비나 API 비용이 없으며 무료로 사용할 수 있습니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e맞춤화: 모델은 귀하의 특정 시스템 프롬프트나 데이터 세트로 세밀하게 조정할 수 있습니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e오프라인 기능: 인터넷 연결이 필요하지 않습니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003col start=\"5\"\u003e\n\u003cli\u003e제약 사항이 없는 사용: 외부 API에서 부과된 제한이 없습니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e시작해 봅시다!\u003c/p\u003e\n\u003ch2\u003e2. Ollama 설치 및 Llama 3 실행하기\u003c/h2\u003e\n\u003cp\u003eOllama는 개인 컴퓨터에서 쉽게 대형 언어 모델(LLM)을 로컬에서 실행할 수 있게 해주는 오픈 소스 프로젝트입니다. 사용자 친화적이고 매우 가벼우며 Meta(럼마 3)와 구글(젬마 2)의 최신 및 최고의 사전 학습 모델을 포함한 다양한 모델을 제공하는 것으로 알려져 있습니다. 이러한 회사들이 이러한 모델을 교육하는 데 수백만 달러를 투자하여 우리가 자신의 기기에서 재미있게 사용할 수 있도록 했습니다. 대단하지 않나요?\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eOllama는 그 자체로는 빈 껍데기에 불과하며 작동하려면 LLM이 필요합니다.\u003c/p\u003e\n\u003cp\u003e설치 프로세스에 들어가기 전에 사용 가능한 모델들을 살펴보겠습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_1.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e그리고 더 많은 모델이 있습니다!\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이 글에서는 Meta사의 최신 모델인 Llama 3에 초점을 맞추어 놀라운 성능을 약속하는 이 모델이 이 플랫폼에서 가장 인기 있는 모델이라고 합니다. 이 글을 작성하는 시점에 이 모델은 440만 회 이상의 다운로드를 기록하고 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_2.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e다음 단계에서는 컴퓨터에 Ollama를 설치하고 Llama3로 공급하여 마침내 그 모델을 ChatGPT처럼 사용하는 방법을 보여줍니다.\u003c/p\u003e\n\u003cp\u003e단계 1/2:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eollama.com에 가서 \"다운로드\"를 클릭합니다. 저는 macOS를 사용하고 있으므로 이후 튜토리얼에서 이 옵션에 초점을 맞출 것이지만, Linux나 Windows에서 할 때도 크게 다르지 않을 것입니다.\u003c/li\u003e\n\u003cli\u003e\"macOS용 다운로드\"를 클릭합니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e마크다운 형식의 표를 사용해보세요.\u003c/p\u003e\n\u003cp\u003eSTEP 3/4/5:\n다른 앱들과 마찬가지로 매우 간단한 설치 단계를 따르기만 하면 됩니다.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\"설치\"를 클릭합니다.\u003c/li\u003e\n\u003cli\u003e\"다음\"을 클릭합니다.\u003c/li\u003e\n\u003cli\u003e터미널에서 \"ollama run llama3\"을 실행합니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e마지막 단계에서는 먼저 llama3의 8B 버전(약 4.7GB)을 컴퓨터에 다운로드한 다음 실행됩니다. 이렇게 간단합니다!\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_5.png\" alt=\"Running Local LLM is More Useful and Easier Than You Think\"\u003e\u003c/p\u003e\n\u003cp\u003eAnd this article could stop right here. A few clicks and a line of code later, here we are running an LLM locally!\u003c/p\u003e\n\u003cp\u003eYou can ask it anything, like explaining the differences between the 8 billion and 70 billion parameters versions of Llama 3.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink_6.png\" alt=\"Running Local LLM is More Useful and Easier Than You Think\"\u003e\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e해당 모델의 응답 시간은 일반적으로 컴퓨터의 GPU / RAM에 의존합니다.\u003c/p\u003e\n\u003ch2\u003e#3 몇 가지 유용한 명령어\u003c/h2\u003e\n\u003cp\u003e터미널 내에서 계속 LLMs를 사용하고 싶다면 몇 가지 기본 명령어가 필요하다고 생각됩니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eollama run llama3\n이 경우 llama3 모델을 실행합니다.\u003c/li\u003e\n\u003cli\u003eollama list\n로컬로 이미 설치된 모든 모델을 나열합니다.\u003c/li\u003e\n\u003cli\u003eollama pull mistral\n플랫폼에서 다른 사용 가능한 모델을 가져옵니다. 이 경우 mistral 모델을 가져옵니다.\u003c/li\u003e\n\u003cli\u003e/clear (모델이 실행 중일 때)\n세션의 컨텍스트를 지워 처음부터 시작합니다.\u003c/li\u003e\n\u003cli\u003e/bye (모델이 실행 중일 때)\nollama를 종료합니다.\u003c/li\u003e\n\u003cli\u003e/? (모델이 실행 중일 때)\n사용 가능한 모든 명령어를 나열합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e더 복잡한 사용 사례를 위해 더 많은 명령이 존재합니다. 새로운 미세 조정 모델을 생성하는 것과 같은 경우가 있습니다.\u003c/p\u003e\n\u003cp\u003eOllama의 Github 저장소에는 매우 완벽한 설명서가 있습니다.\u003c/p\u003e\n\u003cp\u003e기본 사용 사례에 대해서는 CLI가 충분할 수도 있지만 더 많은 기능이 있습니다...\u003c/p\u003e\n\u003ch1\u003e#4 Jupyter Notebook에서 Llama 3\u003c/h1\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e터미널을 통해 LLMs를 사용하는 것도 좋지만 파이썬 코드를 통해 모델과 상호 작용하면 더 많은 가능성이 열립니다.\u003c/p\u003e\n\u003cp\u003e이를 위해 langchain_community 라이브러리를 pip으로 설치해야 합니다 (pip install langchain_community) 그리고 Ollama 패키지를 가져와야 합니다.\u003c/p\u003e\n\u003cp\u003e예를 들어, 어떤 사람의 이름, 나이, 직업을 제공하여 짧은 자기소개를 만들고 싶다고 가정해봅시다. 이 예제에서는 다음과 같이 코드가 작성됩니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# !pip install langchain_community\n\n# 필요한 패키지 가져오기\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain_community.\u003cspan class=\"hljs-property\"\u003ellms\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOllama\u003c/span\u003e\n\n# 모델 인스턴스 생성\nllm = \u003cspan class=\"hljs-title class_\"\u003eOllama\u003c/span\u003e(model=\u003cspan class=\"hljs-string\"\u003e\"llama3\"\u003c/span\u003e)\n\n# 프롬프트와 함께 모델 사용\nllm.\u003cspan class=\"hljs-title function_\"\u003einvoke\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Alice의 나이가 25세이고 엔지니어로 일하는 짧은 2문장 자기소개 생성\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e친근한 톤으로 번역해 드릴게요.\u003c/p\u003e\n\u003cp\u003e조금 더 다듬어보면 더 완벽해질거에요.\u003c/p\u003e\n\u003cp\u003e단일 인물을 위한 소개를 터미널에서 쉽게 만들 수 있지만, 많은 사람들의 경우엔 파이썬 없이 같은 작업을 반복해야 할 수도 있어요. 파이썬을 사용하면 프롬프트를 매개변수화시키고, 많은 사람들에 대해 자동화된 프로세스를 실행할 수 있어요.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e예를 들어:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e pd\n\n\u003cspan class=\"hljs-comment\"\u003e# 샘플 DataFrame 생성\u003c/span\u003e\ndf = pd.DataFrame({\n    \u003cspan class=\"hljs-string\"\u003e'name'\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'Alice'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Bob'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Charlie'\u003c/span\u003e],\n    \u003cspan class=\"hljs-string\"\u003e'age'\u003c/span\u003e: [\u003cspan class=\"hljs-number\"\u003e25\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e35\u003c/span\u003e],\n    \u003cspan class=\"hljs-string\"\u003e'occupation'\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'Engineer'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Teacher'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Artist'\u003c/span\u003e]\n})\n\n\u003cspan class=\"hljs-comment\"\u003e# DataFrame에 적용할 수 있는 함수 생성\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003egenerate_bio\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ename, age, occupation\u003c/span\u003e):\n    prompt = \u003cspan class=\"hljs-string\"\u003ef\"\u003cspan class=\"hljs-subst\"\u003e{name}\u003c/span\u003e님에 대한 간단한 2문장 소개 생성, \u003cspan class=\"hljs-subst\"\u003e{age}\u003c/span\u003e세이고 \u003cspan class=\"hljs-subst\"\u003e{occupation}\u003c/span\u003e로 근무 중\"\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e llm.invoke(prompt)\n\n\u003cspan class=\"hljs-comment\"\u003e# DataFrame에 함수 적용\u003c/span\u003e\ndf[\u003cspan class=\"hljs-string\"\u003e'bio'\u003c/span\u003e] = df.apply(\u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e row: generate_bio(row[\u003cspan class=\"hljs-string\"\u003e'name'\u003c/span\u003e], row[\u003cspan class=\"hljs-string\"\u003e'age'\u003c/span\u003e], row[\u003cspan class=\"hljs-string\"\u003e'occupation'\u003c/span\u003e]), axis=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n\ndf.head()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 DataFrame의 각 행에 대해 모델이 바이오를 생성합니다!\u003c/p\u003e\n\u003ch1\u003e5번째 단계 최종 소견\u003c/h1\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이 기사의 목적은 Ollama 덕분에 로컬에서 완전히 기능적인 LLM(Large Language Model)을 구현하는 간편함을 강조하는 것이었습니다.\u003c/p\u003e\n\u003cp\u003e간단한 요청을 위해 터미널을 통해 이 모델을 사용하거나 Python을 사용하여 더 복잡하거나 자동화된 작업을 수행할 수 있습니다. 프로세스는 간단합니다.\u003c/p\u003e\n\u003cp\u003eOpen WebUI와 같은 오픈 소스 프로젝트 덕분에 우리만의 ChatGPT와 같은 그래픽 인터페이스를 구현할 수도 있습니다.\u003c/p\u003e\n\u003cp\u003e저는 그저 몇 번의 클릭과 몇 줄의 코드로 이렇게 유용한 것을 얻을 수 있다는 것이 놀라워요! 여러분도 즐기셨으면 좋겠네요.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e기사 끝까지 읽어 주셔서 감사합니다.\n더 많은 내용을 보려면 팔로우해주세요!\n질문이나 의견이 있으시면 아래에 메시지를 남겨 주시거나 LinkedIn / X를 통해 연락해 주세요!\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-07-12-RunningLocalLLMsisMoreUsefulandEasierThanYouThink"},"buildId":"N1mNhRlQaHCliEGDvPEpG","assetPrefix":"/TIL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>