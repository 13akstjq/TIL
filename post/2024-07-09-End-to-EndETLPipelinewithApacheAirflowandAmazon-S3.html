<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>Apache Airflow와 Amazon-S3를 사용한 End-to-End ETL 파이프라인 구축 하는 방법 | TIL</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://13akstjq.github.io/TIL//post/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="Apache Airflow와 Amazon-S3를 사용한 End-to-End ETL 파이프라인 구축 하는 방법 | TIL" data-gatsby-head="true"/><meta property="og:title" content="Apache Airflow와 Amazon-S3를 사용한 End-to-End ETL 파이프라인 구축 하는 방법 | TIL" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://TIL.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://13akstjq.github.io/TIL//post/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3" data-gatsby-head="true"/><meta name="twitter:title" content="Apache Airflow와 Amazon-S3를 사용한 End-to-End ETL 파이프라인 구축 하는 방법 | TIL" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | TIL" data-gatsby-head="true"/><meta name="article:published_time" content="2024-07-09 09:06" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/TIL/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/TIL/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/TIL/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/TIL/favicons/favicon-96x96.png"/><link rel="icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/TIL/favicons/browserconfig.xml"/><link rel="preload" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/TIL/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/TIL/_next/static/chunks/webpack-21ffe88bdca56cba.js" defer=""></script><script src="/TIL/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/TIL/_next/static/chunks/main-a5eeabb286676ce6.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/TIL/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/TIL/_next/static/chunks/463-925361deb4cec4b1.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/post/%5Bslug%5D-9d7ebbd29b9e08ce.js" defer=""></script><script src="/TIL/_next/static/xwOwpfNxF5xANgUpiyc2H/_buildManifest.js" defer=""></script><script src="/TIL/_next/static/xwOwpfNxF5xANgUpiyc2H/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/TIL">TIL</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/TIL/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">Apache Airflow와 Amazon-S3를 사용한 End-to-End ETL 파이프라인 구축 하는 방법</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="Apache Airflow와 Amazon-S3를 사용한 End-to-End ETL 파이프라인 구축 하는 방법" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/TIL/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">TIL</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jul 9, 2024</span><span class="posts_reading_time__f7YPP">9<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h1>프로젝트 개요</h1>
<p>이 프로젝트는 Apache Airflow와 Amazon S3를 사용하여 end-to-end ETL (추출, 변환, 로드) 파이프라인을 개발하는 데 중점을 둡니다.</p>
<p>이 파이프라인은 OpenWeather API에서 날씨 데이터를 검색하여 구조화된 형식으로 변환하고 S3 버킷에로드합니다. 이 프로젝트를 완료하면 희망하는 빈도로 예약된 파이프라인을 실행할 수 있는 완전히 기능하는 파이프라인을 보유하게 됩니다.</p>
<h1>프로젝트 구조</h1>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>날씨 데이터 가져오기: OpenWeather API에서 날씨 데이터를 가져옵니다.</li>
<li>날씨 데이터 변환: API에서 가져온 데이터는 JSON 형식이며, 변환 작업은 JSON 개체에서 데이터프레임을 만들고 데이터프레임을 CSV 파일로 변환하는 작업을 포함합니다.</li>
<li>S3에 데이터로드: 변환된 데이터를 S3 버킷에 저장합니다.</li>
</ul>
<h1>사전 준비 및 사용된 도구</h1>
<ul>
<li>Apache Airflow: 워크플로우를 프로그래밍 방식으로 작성, 예약, 모니터링할 수 있는 강력하고 유연한 플랫폼입니다.</li>
<li>OpenWeather API: 여러 도시의 날씨 데이터를 제공하는 서비스입니다.</li>
<li>Amazon S3: Amazon Web Services (AWS)의 확장 가능한 객체 스토리지 서비스입니다.</li>
<li>Pandas: 데이터 조작 및 분석을 위해 사용되는 Python 라이브러리입니다.</li>
<li>Boto3: Python 개발자가 S3와 같은 Amazon 서비스를 활용하는 소프트웨어를 작성할 수 있게 해주는 AWS SDK for Python입니다.</li>
<li>DAG 파일: Apache Airflow에서 작업 및 종속성을 정의하는 작업열로서 사용되는 중요한 개념인 Directed Acyclic Graph(DAG) 파일입니다.</li>
</ul>
<h1>구현</h1>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>API와 연결하여 데이터를 가져오는 DAG 스크립트를 작성하세요. 데이터는 데이터프레임에 저장됩니다. DAG 코드는 이 페이지의 맨 아래에서 찾을 수 있습니다.</li>
<li>EC2 인스턴스를 생성하고 인스턴스를 시작하여 콘솔에 연결하세요. 저는 무료티어 AWS를 사용하여 이 인스턴스를 생성했습니다. 사양은 t2.micro 및 우분투 22 버전입니다.</li>
</ul>
<p><img src="/TIL/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_0.png" alt="이미지"></p>
<p>인스턴스가 실행되면 콘솔에 연결하여 다음을 설치하세요.</p>
<pre><code class="hljs language-js">sudo apt-get update
sudo install python3-pip
sudo pip install requests pandas boto3 s3fs pyarrow apache-airflow
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ol start="3">
<li>한 번 설치되면, Airflow가 올바르게 설치되었는지 확인하세요. airflow 명령어를 사용하여 확인하고 초기 로그인 자격 증명을 위해 스탠드얼론 명령어를 실행하십시오. 자격 증명을 복사하여 나중에 사용하세요.</li>
</ol>
<pre><code class="hljs language-bash">airflow
airflow standalone
</code></pre>
<ol start="4">
<li>
<p>실행 중인 인스턴스에서 보안으로 이동하여 보안 그룹에 액세스하세요. 인바운드 규칙을 편집하고 새 역할을 생성하세요. "모든 트래픽", "IPv4 어디서나"로 설정하세요.</p>
</li>
<li>
<p>Airflow 서버와 스케줄러를 시작하세요.</p>
</li>
</ol>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">에어플로우 스케줄러 및 에어플로우 웹서버 - 포트 <span class="hljs-number">8080</span>

<span class="hljs-number">6.</span> 공개 <span class="hljs-variable constant_">IP</span> 주소를 복사하고 포트를 추가하세요. 예: <span class="hljs-number">172.31</span><span class="hljs-number">.22</span><span class="hljs-number">.254</span>:<span class="hljs-number">8080.</span> 이로써 에어플로우 어플리케이션을 열 수 있습니다. 기본 자격 증명을 사용하여 로그인하고 마음에 드는 비밀번호로 재설정하세요.

<span class="hljs-number">7.</span> <span class="hljs-variable constant_">AWS</span>에서 데이터를 저장할 <span class="hljs-variable constant_">S3</span> 버킷을 만드세요. <span class="hljs-variable constant_">IAM</span> 역할을 사용하여 권한을 조정하세요. 새 <span class="hljs-variable constant_">IAM</span> 역할을 만들고 <span class="hljs-variable constant_">S3</span> 및 <span class="hljs-title class_">EC2</span>에 권한을 부여하세요.

<span class="hljs-number">8.</span> <span class="hljs-variable constant_">DAG</span> 파일에 관련 있는 <span class="hljs-variable constant_">S3</span> 버킷 이름을 추가하고 저장하세요.
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ol start="9">
<li>인스턴스 콘솔에서 서버를 중지하고 명령을 실행하세요. 이렇게 하면 airflow의 DAGs 폴더에 액세스할 수 있어요. 원하는 경우 DAG 파일을 추가하고 필요할 때 수정할 수 있어요.</li>
</ol>
<pre><code class="hljs language-js">airflow
cd airflow
ls
sudo nano airflow.<span class="hljs-property">cfg</span>
</code></pre>
<p>DAGs 폴더에서 파일 이름을 조정하세요. 수정된 버퍼를 저장하고 종료하세요.</p>
<ol start="10">
<li>파일을 저장한 후 airflow 명령을 다시 실행하고 로그인하세요. 그러면 airflow 내에서 Dag 파일을 볼 수 있어요. 보이는 형태는 이렇습니다:</li>
</ol>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>아래는 Markdown 형식으로 변경된 테이블입니다.</p>
<ol start="11">
<li>파일을 열어서 수동으로 실행할 수 있어야 합니다. Airflow의 내장 그래프 기능을 사용하여 DAG 파일의 상태를 모니터링할 수 있습니다. 초록 테두리는 성공적인 실행을 나타냅니다.</li>
</ol>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ol start="12">
<li>실행이 성공하면 데이터가 S3 버킷에 표시됩니다.</li>
</ol>
<p><img src="/TIL/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_4.png" alt="Airflow-S3"></p>
<p>DAG 파일과 설명:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime, timedelta
<span class="hljs-keyword">from</span> airflow <span class="hljs-keyword">import</span> <span class="hljs-variable constant_">DAG</span>
<span class="hljs-keyword">from</span> airflow.<span class="hljs-property">operators</span>.<span class="hljs-property">python_operator</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">PythonOperator</span>
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> boto3
<span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> <span class="hljs-title class_">StringIO</span>

# <span class="hljs-title class_">Configuration</span>
<span class="hljs-variable constant_">API_KEY</span> = <span class="hljs-string">'xxxxxxxxxxxxxxxxxxxxxxxxxxx'</span>  # <span class="hljs-title class_">OpenWeather</span>의 <span class="hljs-variable constant_">API</span> 키
<span class="hljs-variable constant_">CITY</span> = <span class="hljs-string">'Arizona'</span>  # 날씨 데이터를 가져올 도시
<span class="hljs-variable constant_">S3_BUCKET</span> = <span class="hljs-string">'open-weather-s3-bucket'</span>  # 데이터가 저장될 <span class="hljs-variable constant_">S3</span> 버킷 이름
<span class="hljs-variable constant_">S3_KEY</span> = <span class="hljs-string">'weather_data/weather.csv'</span>  # <span class="hljs-variable constant_">S3</span> 오브젝트 키 (버킷 내 파일 경로)

# <span class="hljs-variable constant_">DAG</span>를 위한 기본 인수
default_args = {
    <span class="hljs-string">'owner'</span>: <span class="hljs-string">'airflow'</span>,  # <span class="hljs-variable constant_">DAG</span> 소유자
    <span class="hljs-string">'depends_on_past'</span>: <span class="hljs-title class_">False</span>,  # 작업 인스턴스는 과거 실행에 의존하지 않음
    <span class="hljs-string">'start_date'</span>: <span class="hljs-title function_">datetime</span>(<span class="hljs-number">2024</span>, <span class="hljs-number">7</span>, <span class="hljs-number">5</span>),  # <span class="hljs-variable constant_">DAG</span> 시작 날짜
    <span class="hljs-string">'email_on_failure'</span>: <span class="hljs-title class_">False</span>,  # 실패 시 이메일 알림 비활성화
    <span class="hljs-string">'email_on_retry'</span>: <span class="hljs-title class_">False</span>,  # 재시도 시 이메일 알림 비활성화
    <span class="hljs-string">'retries'</span>: <span class="hljs-number">1</span>,  # 실패 시 재시도 횟수
    <span class="hljs-string">'retry_delay'</span>: <span class="hljs-title function_">timedelta</span>(minutes=<span class="hljs-number">5</span>),  # 재시도 간의 지연
}

# 스케줄러가 없는 <span class="hljs-variable constant_">DAG</span> 정의
dag = <span class="hljs-title function_">DAG</span>(
    <span class="hljs-string">'OpenWeather_to_s3'</span>,
    default_args=default_args,
    description=<span class="hljs-string">'날씨 데이터를 가져와 변환한 후 S3로 로드합니다.'</span>,
    schedule_interval=<span class="hljs-title class_">None</span>,  # schedule_interval을 <span class="hljs-title class_">None</span>으로 설정하여 스케줄러 비활성화
)

def <span class="hljs-title function_">fetch_weather_data</span>():
    <span class="hljs-string">""</span><span class="hljs-string">"OpenWeather API에서 날씨 데이터 가져오기"</span><span class="hljs-string">""</span>
    url = f<span class="hljs-string">"http://api.openweathermap.org/data/2.5/weather?q={CITY}&#x26;appid={API_KEY}"</span>  # 도시와 <span class="hljs-variable constant_">API</span> 키를 포함한 <span class="hljs-variable constant_">API</span> 엔드포인트
    response = requests.<span class="hljs-title function_">get</span>(url)  # <span class="hljs-variable constant_">API</span>에 <span class="hljs-variable constant_">GET</span> 요청 보내기
    data = response.<span class="hljs-title function_">json</span>()  # 응답을 <span class="hljs-title class_">JSON</span>으로 변환
    <span class="hljs-keyword">return</span> data  # 데이터 반환

def <span class="hljs-title function_">transform_weather_data</span>(**kwargs):
    <span class="hljs-string">""</span><span class="hljs-string">"가져온 날씨 데이터 변환하기"</span><span class="hljs-string">""</span>
    ti = kwargs[<span class="hljs-string">'ti'</span>]  # 작업 인스턴스 가져오기
    data = ti.<span class="hljs-title function_">xcom_pull</span>(task_ids=<span class="hljs-string">'fetch_weather_data'</span>)  # <span class="hljs-title class_">XCom</span>을 사용하여 <span class="hljs-string">'fetch_weather_data'</span> 작업에서 데이터 가져오기

    weather = {
        <span class="hljs-string">'city'</span>: data[<span class="hljs-string">'name'</span>],  # 도시 이름 추출
        <span class="hljs-string">'temperature'</span>: data[<span class="hljs-string">'main'</span>][<span class="hljs-string">'temp'</span>],  # 온도 추출
        <span class="hljs-string">'pressure'</span>: data[<span class="hljs-string">'main'</span>][<span class="hljs-string">'pressure'</span>],  # 기압 추출
        <span class="hljs-string">'humidity'</span>: data[<span class="hljs-string">'main'</span>][<span class="hljs-string">'humidity'</span>],  # 습도 추출
        <span class="hljs-string">'weather'</span>: data[<span class="hljs-string">'weather'</span>][<span class="hljs-number">0</span>][<span class="hljs-string">'description'</span>],  # 날씨 설명 추출
        <span class="hljs-string">'wind_speed'</span>: data[<span class="hljs-string">'wind'</span>][<span class="hljs-string">'speed'</span>],  # 풍속 추출
        <span class="hljs-string">'date'</span>: datetime.<span class="hljs-title function_">utcfromtimestamp</span>(data[<span class="hljs-string">'dt'</span>]).<span class="hljs-title function_">strftime</span>(<span class="hljs-string">'%Y-%m-%d %H:%M:%S'</span>)  # 타임스탬프를 읽기 가능한 날짜로 변환
    }

    df = pd.<span class="hljs-title class_">DataFrame</span>([weather])  # 날씨 데이터를 판다스 <span class="hljs-title class_">DataFrame</span>으로 변환
    <span class="hljs-keyword">return</span> df  # <span class="hljs-title class_">DataFrame</span> 반환

def <span class="hljs-title function_">load_data_to_s3</span>(**kwargs):
    <span class="hljs-string">""</span><span class="hljs-string">"변환된 데이터를 S3 버킷에 로드하기"</span><span class="hljs-string">""</span>
    ti = kwargs[<span class="hljs-string">'ti'</span>]  # 작업 인스턴스 가져오기
    df = ti.<span class="hljs-title function_">xcom_pull</span>(task_ids=<span class="hljs-string">'transform_weather_data'</span>)  # <span class="hljs-title class_">XCom</span>을 사용하여 <span class="hljs-string">'transform_weather_data'</span> 작업에서 변환된 데이터 가져오기

    csv_buffer = <span class="hljs-title class_">StringIO</span>()  # 인메모리 버퍼 생성
    df.<span class="hljs-title function_">to_csv</span>(csv_buffer, index=<span class="hljs-title class_">False</span>)  # <span class="hljs-title class_">DataFrame</span>을 <span class="hljs-variable constant_">CSV</span>로 버퍼에 작성
    <span class="hljs-title function_">print</span>(df)  # <span class="hljs-title class_">DataFrame</span> 출력 (선택 사항)

    s3_resource = boto3.<span class="hljs-title function_">resource</span>(<span class="hljs-string">'s3'</span>)  # boto3 <span class="hljs-variable constant_">S3</span> 리소스 생성
    s3_resource.<span class="hljs-title class_">Object</span>(<span class="hljs-variable constant_">S3_BUCKET</span>, <span class="hljs-variable constant_">S3_KEY</span>).<span class="hljs-title function_">put</span>(<span class="hljs-title class_">Body</span>=csv_buffer.<span class="hljs-title function_">getvalue</span>())  # <span class="hljs-variable constant_">CSV</span> 데이터를 지정한 <span class="hljs-variable constant_">S3</span> 버킷 및 키에 업로드

# <span class="hljs-title class_">PythonOperator</span>를 사용하여 작업 정의
fetch_task = <span class="hljs-title class_">PythonOperator</span>(
    task_id=<span class="hljs-string">'fetch_weather_data'</span>,  # 작업 <span class="hljs-variable constant_">ID</span>
    python_callable=fetch_weather_data,  # 호출 가능한 함수
    dag=dag,  # 작업이 속한 <span class="hljs-variable constant_">DAG</span>
)

transform_task = <span class="hljs-title class_">PythonOperator</span>(
    task_id=<span class="hljs-string">'transform_weather_data'</span>,  # 작업 <span class="hljs-variable constant_">ID</span>
    python_callable=transform_weather_data,  # 호출 가능한 함수
    provide_context=<span class="hljs-title class_">True</span>,  # 호출 가능한 함수에 컨텍스트 제공
    dag=dag,  # 작업이 속한 <span class="hljs-variable constant_">DAG</span>
)

load_task = <span class="hljs-title class_">PythonOperator</span>(
    task_id=<span class="hljs-string">'load_data_to_s3'</span>,  # 작업 <span class="hljs-variable constant_">ID</span>
    python_callable=load_data_to_s3,  # 호출 가능한 함수
    provide_context=<span class="hljs-title class_">True</span>,  # 호출 가능한 함수에 컨텍스트 제공
    dag=dag,  # 작업이 속한 <span class="hljs-variable constant_">DAG</span>
)

# 작업 간 의존성 정의 (작업 실행 순서)
fetch_task >> transform_task >> load_task  # 작업 실행 순서 설정: 가져오기 -> 변환 -> 로드
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>설명:</p>
<ul>
<li>
<p>Imports 및 구성: 필요한 라이브러리를 import하고 구성 변수를 설정합니다.</p>
</li>
<li>
<p>기본 인수: DAG의 기본 인수를 정의합니다. 예를 들어, 소유자, 시작 날짜, 재시도 정책 등이 포함됩니다.</p>
</li>
<li>
<p>DAG 정의: DAG 개체를 설명과 일정 간격으로 생성합니다 (일정을 비활성화하려면 None으로 설정 가능합니다).</p>
</li>
<li>
<p>작업 함수: 사용할 Python 함수를 작업으로 정의합니다.</p>
</li>
<li>
<p>fetch_weather_data: OpenWeather API에서 날씨 데이터를 가져옵니다.</p>
</li>
<li>
<p>transform_weather_data: 가져온 데이터를 Pandas DataFrame으로 변환합니다.</p>
</li>
<li>
<p>load_data_to_s3: 변환된 데이터를 S3 버킷에 로드합니다.</p>
</li>
</ul>
<ol start="5">
<li>작업 생성: PythonOperator를 사용하여 정의된 함수를 호출하는 작업을 생성합니다.</li>
</ol>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ol start="6">
<li>종속성 설정: 비트 시프트 연산자를 사용하여 작업을 실행할 순서를 정의하세요.</li>
</ol>
<h1>자료들</h1>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Apache Airflow와 Amazon-S3를 사용한 End-to-End ETL 파이프라인 구축 하는 방법","description":"","date":"2024-07-09 09:06","slug":"2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3","content":"\n# 프로젝트 개요\n\n이 프로젝트는 Apache Airflow와 Amazon S3를 사용하여 end-to-end ETL (추출, 변환, 로드) 파이프라인을 개발하는 데 중점을 둡니다.\n\n이 파이프라인은 OpenWeather API에서 날씨 데이터를 검색하여 구조화된 형식으로 변환하고 S3 버킷에로드합니다. 이 프로젝트를 완료하면 희망하는 빈도로 예약된 파이프라인을 실행할 수 있는 완전히 기능하는 파이프라인을 보유하게 됩니다.\n\n# 프로젝트 구조\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 날씨 데이터 가져오기: OpenWeather API에서 날씨 데이터를 가져옵니다.\n- 날씨 데이터 변환: API에서 가져온 데이터는 JSON 형식이며, 변환 작업은 JSON 개체에서 데이터프레임을 만들고 데이터프레임을 CSV 파일로 변환하는 작업을 포함합니다.\n- S3에 데이터로드: 변환된 데이터를 S3 버킷에 저장합니다.\n\n# 사전 준비 및 사용된 도구\n\n- Apache Airflow: 워크플로우를 프로그래밍 방식으로 작성, 예약, 모니터링할 수 있는 강력하고 유연한 플랫폼입니다.\n- OpenWeather API: 여러 도시의 날씨 데이터를 제공하는 서비스입니다.\n- Amazon S3: Amazon Web Services (AWS)의 확장 가능한 객체 스토리지 서비스입니다.\n- Pandas: 데이터 조작 및 분석을 위해 사용되는 Python 라이브러리입니다.\n- Boto3: Python 개발자가 S3와 같은 Amazon 서비스를 활용하는 소프트웨어를 작성할 수 있게 해주는 AWS SDK for Python입니다.\n- DAG 파일: Apache Airflow에서 작업 및 종속성을 정의하는 작업열로서 사용되는 중요한 개념인 Directed Acyclic Graph(DAG) 파일입니다.\n\n# 구현\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- API와 연결하여 데이터를 가져오는 DAG 스크립트를 작성하세요. 데이터는 데이터프레임에 저장됩니다. DAG 코드는 이 페이지의 맨 아래에서 찾을 수 있습니다.\n- EC2 인스턴스를 생성하고 인스턴스를 시작하여 콘솔에 연결하세요. 저는 무료티어 AWS를 사용하여 이 인스턴스를 생성했습니다. 사양은 t2.micro 및 우분투 22 버전입니다.\n\n![이미지](/TIL/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_0.png)\n\n인스턴스가 실행되면 콘솔에 연결하여 다음을 설치하세요.\n\n```js\nsudo apt-get update\nsudo install python3-pip\nsudo pip install requests pandas boto3 s3fs pyarrow apache-airflow\n```\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n3. 한 번 설치되면, Airflow가 올바르게 설치되었는지 확인하세요. airflow 명령어를 사용하여 확인하고 초기 로그인 자격 증명을 위해 스탠드얼론 명령어를 실행하십시오. 자격 증명을 복사하여 나중에 사용하세요.\n\n```bash\nairflow\nairflow standalone\n```\n\n4. 실행 중인 인스턴스에서 보안으로 이동하여 보안 그룹에 액세스하세요. 인바운드 규칙을 편집하고 새 역할을 생성하세요. \"모든 트래픽\", \"IPv4 어디서나\"로 설정하세요.\n\n5. Airflow 서버와 스케줄러를 시작하세요.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n에어플로우 스케줄러 및 에어플로우 웹서버 - 포트 8080\n\n6. 공개 IP 주소를 복사하고 포트를 추가하세요. 예: 172.31.22.254:8080. 이로써 에어플로우 어플리케이션을 열 수 있습니다. 기본 자격 증명을 사용하여 로그인하고 마음에 드는 비밀번호로 재설정하세요.\n\n7. AWS에서 데이터를 저장할 S3 버킷을 만드세요. IAM 역할을 사용하여 권한을 조정하세요. 새 IAM 역할을 만들고 S3 및 EC2에 권한을 부여하세요.\n\n8. DAG 파일에 관련 있는 S3 버킷 이름을 추가하고 저장하세요.\n```\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n9. 인스턴스 콘솔에서 서버를 중지하고 명령을 실행하세요. 이렇게 하면 airflow의 DAGs 폴더에 액세스할 수 있어요. 원하는 경우 DAG 파일을 추가하고 필요할 때 수정할 수 있어요.\n\n```js\nairflow\ncd airflow\nls\nsudo nano airflow.cfg\n```\n\nDAGs 폴더에서 파일 이름을 조정하세요. 수정된 버퍼를 저장하고 종료하세요.\n\n10. 파일을 저장한 후 airflow 명령을 다시 실행하고 로그인하세요. 그러면 airflow 내에서 Dag 파일을 볼 수 있어요. 보이는 형태는 이렇습니다:\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아래는 Markdown 형식으로 변경된 테이블입니다.\n\n11. 파일을 열어서 수동으로 실행할 수 있어야 합니다. Airflow의 내장 그래프 기능을 사용하여 DAG 파일의 상태를 모니터링할 수 있습니다. 초록 테두리는 성공적인 실행을 나타냅니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n12. 실행이 성공하면 데이터가 S3 버킷에 표시됩니다.\n\n![Airflow-S3](/TIL/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_4.png)\n\nDAG 파일과 설명:\n\n```js\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nimport requests\nimport pandas as pd\nimport boto3\nfrom io import StringIO\n\n# Configuration\nAPI_KEY = 'xxxxxxxxxxxxxxxxxxxxxxxxxxx'  # OpenWeather의 API 키\nCITY = 'Arizona'  # 날씨 데이터를 가져올 도시\nS3_BUCKET = 'open-weather-s3-bucket'  # 데이터가 저장될 S3 버킷 이름\nS3_KEY = 'weather_data/weather.csv'  # S3 오브젝트 키 (버킷 내 파일 경로)\n\n# DAG를 위한 기본 인수\ndefault_args = {\n    'owner': 'airflow',  # DAG 소유자\n    'depends_on_past': False,  # 작업 인스턴스는 과거 실행에 의존하지 않음\n    'start_date': datetime(2024, 7, 5),  # DAG 시작 날짜\n    'email_on_failure': False,  # 실패 시 이메일 알림 비활성화\n    'email_on_retry': False,  # 재시도 시 이메일 알림 비활성화\n    'retries': 1,  # 실패 시 재시도 횟수\n    'retry_delay': timedelta(minutes=5),  # 재시도 간의 지연\n}\n\n# 스케줄러가 없는 DAG 정의\ndag = DAG(\n    'OpenWeather_to_s3',\n    default_args=default_args,\n    description='날씨 데이터를 가져와 변환한 후 S3로 로드합니다.',\n    schedule_interval=None,  # schedule_interval을 None으로 설정하여 스케줄러 비활성화\n)\n\ndef fetch_weather_data():\n    \"\"\"OpenWeather API에서 날씨 데이터 가져오기\"\"\"\n    url = f\"http://api.openweathermap.org/data/2.5/weather?q={CITY}\u0026appid={API_KEY}\"  # 도시와 API 키를 포함한 API 엔드포인트\n    response = requests.get(url)  # API에 GET 요청 보내기\n    data = response.json()  # 응답을 JSON으로 변환\n    return data  # 데이터 반환\n\ndef transform_weather_data(**kwargs):\n    \"\"\"가져온 날씨 데이터 변환하기\"\"\"\n    ti = kwargs['ti']  # 작업 인스턴스 가져오기\n    data = ti.xcom_pull(task_ids='fetch_weather_data')  # XCom을 사용하여 'fetch_weather_data' 작업에서 데이터 가져오기\n\n    weather = {\n        'city': data['name'],  # 도시 이름 추출\n        'temperature': data['main']['temp'],  # 온도 추출\n        'pressure': data['main']['pressure'],  # 기압 추출\n        'humidity': data['main']['humidity'],  # 습도 추출\n        'weather': data['weather'][0]['description'],  # 날씨 설명 추출\n        'wind_speed': data['wind']['speed'],  # 풍속 추출\n        'date': datetime.utcfromtimestamp(data['dt']).strftime('%Y-%m-%d %H:%M:%S')  # 타임스탬프를 읽기 가능한 날짜로 변환\n    }\n\n    df = pd.DataFrame([weather])  # 날씨 데이터를 판다스 DataFrame으로 변환\n    return df  # DataFrame 반환\n\ndef load_data_to_s3(**kwargs):\n    \"\"\"변환된 데이터를 S3 버킷에 로드하기\"\"\"\n    ti = kwargs['ti']  # 작업 인스턴스 가져오기\n    df = ti.xcom_pull(task_ids='transform_weather_data')  # XCom을 사용하여 'transform_weather_data' 작업에서 변환된 데이터 가져오기\n\n    csv_buffer = StringIO()  # 인메모리 버퍼 생성\n    df.to_csv(csv_buffer, index=False)  # DataFrame을 CSV로 버퍼에 작성\n    print(df)  # DataFrame 출력 (선택 사항)\n\n    s3_resource = boto3.resource('s3')  # boto3 S3 리소스 생성\n    s3_resource.Object(S3_BUCKET, S3_KEY).put(Body=csv_buffer.getvalue())  # CSV 데이터를 지정한 S3 버킷 및 키에 업로드\n\n# PythonOperator를 사용하여 작업 정의\nfetch_task = PythonOperator(\n    task_id='fetch_weather_data',  # 작업 ID\n    python_callable=fetch_weather_data,  # 호출 가능한 함수\n    dag=dag,  # 작업이 속한 DAG\n)\n\ntransform_task = PythonOperator(\n    task_id='transform_weather_data',  # 작업 ID\n    python_callable=transform_weather_data,  # 호출 가능한 함수\n    provide_context=True,  # 호출 가능한 함수에 컨텍스트 제공\n    dag=dag,  # 작업이 속한 DAG\n)\n\nload_task = PythonOperator(\n    task_id='load_data_to_s3',  # 작업 ID\n    python_callable=load_data_to_s3,  # 호출 가능한 함수\n    provide_context=True,  # 호출 가능한 함수에 컨텍스트 제공\n    dag=dag,  # 작업이 속한 DAG\n)\n\n# 작업 간 의존성 정의 (작업 실행 순서)\nfetch_task \u003e\u003e transform_task \u003e\u003e load_task  # 작업 실행 순서 설정: 가져오기 -\u003e 변환 -\u003e 로드\n```\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n설명:\n\n- Imports 및 구성: 필요한 라이브러리를 import하고 구성 변수를 설정합니다.\n- 기본 인수: DAG의 기본 인수를 정의합니다. 예를 들어, 소유자, 시작 날짜, 재시도 정책 등이 포함됩니다.\n- DAG 정의: DAG 개체를 설명과 일정 간격으로 생성합니다 (일정을 비활성화하려면 None으로 설정 가능합니다).\n- 작업 함수: 사용할 Python 함수를 작업으로 정의합니다.\n\n- fetch_weather_data: OpenWeather API에서 날씨 데이터를 가져옵니다.\n- transform_weather_data: 가져온 데이터를 Pandas DataFrame으로 변환합니다.\n- load_data_to_s3: 변환된 데이터를 S3 버킷에 로드합니다.\n\n5. 작업 생성: PythonOperator를 사용하여 정의된 함수를 호출하는 작업을 생성합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n6. 종속성 설정: 비트 시프트 연산자를 사용하여 작업을 실행할 순서를 정의하세요.\n\n# 자료들\n","ogImage":{"url":"/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_0.png","tag":["Tech"],"readingTime":9},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch1\u003e프로젝트 개요\u003c/h1\u003e\n\u003cp\u003e이 프로젝트는 Apache Airflow와 Amazon S3를 사용하여 end-to-end ETL (추출, 변환, 로드) 파이프라인을 개발하는 데 중점을 둡니다.\u003c/p\u003e\n\u003cp\u003e이 파이프라인은 OpenWeather API에서 날씨 데이터를 검색하여 구조화된 형식으로 변환하고 S3 버킷에로드합니다. 이 프로젝트를 완료하면 희망하는 빈도로 예약된 파이프라인을 실행할 수 있는 완전히 기능하는 파이프라인을 보유하게 됩니다.\u003c/p\u003e\n\u003ch1\u003e프로젝트 구조\u003c/h1\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e날씨 데이터 가져오기: OpenWeather API에서 날씨 데이터를 가져옵니다.\u003c/li\u003e\n\u003cli\u003e날씨 데이터 변환: API에서 가져온 데이터는 JSON 형식이며, 변환 작업은 JSON 개체에서 데이터프레임을 만들고 데이터프레임을 CSV 파일로 변환하는 작업을 포함합니다.\u003c/li\u003e\n\u003cli\u003eS3에 데이터로드: 변환된 데이터를 S3 버킷에 저장합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e사전 준비 및 사용된 도구\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eApache Airflow: 워크플로우를 프로그래밍 방식으로 작성, 예약, 모니터링할 수 있는 강력하고 유연한 플랫폼입니다.\u003c/li\u003e\n\u003cli\u003eOpenWeather API: 여러 도시의 날씨 데이터를 제공하는 서비스입니다.\u003c/li\u003e\n\u003cli\u003eAmazon S3: Amazon Web Services (AWS)의 확장 가능한 객체 스토리지 서비스입니다.\u003c/li\u003e\n\u003cli\u003ePandas: 데이터 조작 및 분석을 위해 사용되는 Python 라이브러리입니다.\u003c/li\u003e\n\u003cli\u003eBoto3: Python 개발자가 S3와 같은 Amazon 서비스를 활용하는 소프트웨어를 작성할 수 있게 해주는 AWS SDK for Python입니다.\u003c/li\u003e\n\u003cli\u003eDAG 파일: Apache Airflow에서 작업 및 종속성을 정의하는 작업열로서 사용되는 중요한 개념인 Directed Acyclic Graph(DAG) 파일입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e구현\u003c/h1\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003eAPI와 연결하여 데이터를 가져오는 DAG 스크립트를 작성하세요. 데이터는 데이터프레임에 저장됩니다. DAG 코드는 이 페이지의 맨 아래에서 찾을 수 있습니다.\u003c/li\u003e\n\u003cli\u003eEC2 인스턴스를 생성하고 인스턴스를 시작하여 콘솔에 연결하세요. 저는 무료티어 AWS를 사용하여 이 인스턴스를 생성했습니다. 사양은 t2.micro 및 우분투 22 버전입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e인스턴스가 실행되면 콘솔에 연결하여 다음을 설치하세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003esudo apt-get update\nsudo install python3-pip\nsudo pip install requests pandas boto3 s3fs pyarrow apache-airflow\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e한 번 설치되면, Airflow가 올바르게 설치되었는지 확인하세요. airflow 명령어를 사용하여 확인하고 초기 로그인 자격 증명을 위해 스탠드얼론 명령어를 실행하십시오. 자격 증명을 복사하여 나중에 사용하세요.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003eairflow\nairflow standalone\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003e\n\u003cp\u003e실행 중인 인스턴스에서 보안으로 이동하여 보안 그룹에 액세스하세요. 인바운드 규칙을 편집하고 새 역할을 생성하세요. \"모든 트래픽\", \"IPv4 어디서나\"로 설정하세요.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAirflow 서버와 스케줄러를 시작하세요.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e에어플로우 스케줄러 및 에어플로우 웹서버 - 포트 \u003cspan class=\"hljs-number\"\u003e8080\u003c/span\u003e\n\n\u003cspan class=\"hljs-number\"\u003e6.\u003c/span\u003e 공개 \u003cspan class=\"hljs-variable constant_\"\u003eIP\u003c/span\u003e 주소를 복사하고 포트를 추가하세요. 예: \u003cspan class=\"hljs-number\"\u003e172.31\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.22\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.254\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e8080.\u003c/span\u003e 이로써 에어플로우 어플리케이션을 열 수 있습니다. 기본 자격 증명을 사용하여 로그인하고 마음에 드는 비밀번호로 재설정하세요.\n\n\u003cspan class=\"hljs-number\"\u003e7.\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eAWS\u003c/span\u003e에서 데이터를 저장할 \u003cspan class=\"hljs-variable constant_\"\u003eS3\u003c/span\u003e 버킷을 만드세요. \u003cspan class=\"hljs-variable constant_\"\u003eIAM\u003c/span\u003e 역할을 사용하여 권한을 조정하세요. 새 \u003cspan class=\"hljs-variable constant_\"\u003eIAM\u003c/span\u003e 역할을 만들고 \u003cspan class=\"hljs-variable constant_\"\u003eS3\u003c/span\u003e 및 \u003cspan class=\"hljs-title class_\"\u003eEC2\u003c/span\u003e에 권한을 부여하세요.\n\n\u003cspan class=\"hljs-number\"\u003e8.\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eDAG\u003c/span\u003e 파일에 관련 있는 \u003cspan class=\"hljs-variable constant_\"\u003eS3\u003c/span\u003e 버킷 이름을 추가하고 저장하세요.\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003col start=\"9\"\u003e\n\u003cli\u003e인스턴스 콘솔에서 서버를 중지하고 명령을 실행하세요. 이렇게 하면 airflow의 DAGs 폴더에 액세스할 수 있어요. 원하는 경우 DAG 파일을 추가하고 필요할 때 수정할 수 있어요.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eairflow\ncd airflow\nls\nsudo nano airflow.\u003cspan class=\"hljs-property\"\u003ecfg\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDAGs 폴더에서 파일 이름을 조정하세요. 수정된 버퍼를 저장하고 종료하세요.\u003c/p\u003e\n\u003col start=\"10\"\u003e\n\u003cli\u003e파일을 저장한 후 airflow 명령을 다시 실행하고 로그인하세요. 그러면 airflow 내에서 Dag 파일을 볼 수 있어요. 보이는 형태는 이렇습니다:\u003c/li\u003e\n\u003c/ol\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e아래는 Markdown 형식으로 변경된 테이블입니다.\u003c/p\u003e\n\u003col start=\"11\"\u003e\n\u003cli\u003e파일을 열어서 수동으로 실행할 수 있어야 합니다. Airflow의 내장 그래프 기능을 사용하여 DAG 파일의 상태를 모니터링할 수 있습니다. 초록 테두리는 성공적인 실행을 나타냅니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003col start=\"12\"\u003e\n\u003cli\u003e실행이 성공하면 데이터가 S3 버킷에 표시됩니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_4.png\" alt=\"Airflow-S3\"\u003e\u003c/p\u003e\n\u003cp\u003eDAG 파일과 설명:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e datetime \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e datetime, timedelta\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e airflow \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eDAG\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e airflow.\u003cspan class=\"hljs-property\"\u003eoperators\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003epython_operator\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003ePythonOperator\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e requests\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e pd\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e boto3\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e io \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eStringIO\u003c/span\u003e\n\n# \u003cspan class=\"hljs-title class_\"\u003eConfiguration\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eAPI_KEY\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e'xxxxxxxxxxxxxxxxxxxxxxxxxxx'\u003c/span\u003e  # \u003cspan class=\"hljs-title class_\"\u003eOpenWeather\u003c/span\u003e의 \u003cspan class=\"hljs-variable constant_\"\u003eAPI\u003c/span\u003e 키\n\u003cspan class=\"hljs-variable constant_\"\u003eCITY\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e'Arizona'\u003c/span\u003e  # 날씨 데이터를 가져올 도시\n\u003cspan class=\"hljs-variable constant_\"\u003eS3_BUCKET\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e'open-weather-s3-bucket'\u003c/span\u003e  # 데이터가 저장될 \u003cspan class=\"hljs-variable constant_\"\u003eS3\u003c/span\u003e 버킷 이름\n\u003cspan class=\"hljs-variable constant_\"\u003eS3_KEY\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e'weather_data/weather.csv'\u003c/span\u003e  # \u003cspan class=\"hljs-variable constant_\"\u003eS3\u003c/span\u003e 오브젝트 키 (버킷 내 파일 경로)\n\n# \u003cspan class=\"hljs-variable constant_\"\u003eDAG\u003c/span\u003e를 위한 기본 인수\ndefault_args = {\n    \u003cspan class=\"hljs-string\"\u003e'owner'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'airflow'\u003c/span\u003e,  # \u003cspan class=\"hljs-variable constant_\"\u003eDAG\u003c/span\u003e 소유자\n    \u003cspan class=\"hljs-string\"\u003e'depends_on_past'\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e,  # 작업 인스턴스는 과거 실행에 의존하지 않음\n    \u003cspan class=\"hljs-string\"\u003e'start_date'\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003edatetime\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e2024\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e),  # \u003cspan class=\"hljs-variable constant_\"\u003eDAG\u003c/span\u003e 시작 날짜\n    \u003cspan class=\"hljs-string\"\u003e'email_on_failure'\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e,  # 실패 시 이메일 알림 비활성화\n    \u003cspan class=\"hljs-string\"\u003e'email_on_retry'\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e,  # 재시도 시 이메일 알림 비활성화\n    \u003cspan class=\"hljs-string\"\u003e'retries'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,  # 실패 시 재시도 횟수\n    \u003cspan class=\"hljs-string\"\u003e'retry_delay'\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003etimedelta\u003c/span\u003e(minutes=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e),  # 재시도 간의 지연\n}\n\n# 스케줄러가 없는 \u003cspan class=\"hljs-variable constant_\"\u003eDAG\u003c/span\u003e 정의\ndag = \u003cspan class=\"hljs-title function_\"\u003eDAG\u003c/span\u003e(\n    \u003cspan class=\"hljs-string\"\u003e'OpenWeather_to_s3'\u003c/span\u003e,\n    default_args=default_args,\n    description=\u003cspan class=\"hljs-string\"\u003e'날씨 데이터를 가져와 변환한 후 S3로 로드합니다.'\u003c/span\u003e,\n    schedule_interval=\u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e,  # schedule_interval을 \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e으로 설정하여 스케줄러 비활성화\n)\n\ndef \u003cspan class=\"hljs-title function_\"\u003efetch_weather_data\u003c/span\u003e():\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"OpenWeather API에서 날씨 데이터 가져오기\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    url = f\u003cspan class=\"hljs-string\"\u003e\"http://api.openweathermap.org/data/2.5/weather?q={CITY}\u0026#x26;appid={API_KEY}\"\u003c/span\u003e  # 도시와 \u003cspan class=\"hljs-variable constant_\"\u003eAPI\u003c/span\u003e 키를 포함한 \u003cspan class=\"hljs-variable constant_\"\u003eAPI\u003c/span\u003e 엔드포인트\n    response = requests.\u003cspan class=\"hljs-title function_\"\u003eget\u003c/span\u003e(url)  # \u003cspan class=\"hljs-variable constant_\"\u003eAPI\u003c/span\u003e에 \u003cspan class=\"hljs-variable constant_\"\u003eGET\u003c/span\u003e 요청 보내기\n    data = response.\u003cspan class=\"hljs-title function_\"\u003ejson\u003c/span\u003e()  # 응답을 \u003cspan class=\"hljs-title class_\"\u003eJSON\u003c/span\u003e으로 변환\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e data  # 데이터 반환\n\ndef \u003cspan class=\"hljs-title function_\"\u003etransform_weather_data\u003c/span\u003e(**kwargs):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"가져온 날씨 데이터 변환하기\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    ti = kwargs[\u003cspan class=\"hljs-string\"\u003e'ti'\u003c/span\u003e]  # 작업 인스턴스 가져오기\n    data = ti.\u003cspan class=\"hljs-title function_\"\u003excom_pull\u003c/span\u003e(task_ids=\u003cspan class=\"hljs-string\"\u003e'fetch_weather_data'\u003c/span\u003e)  # \u003cspan class=\"hljs-title class_\"\u003eXCom\u003c/span\u003e을 사용하여 \u003cspan class=\"hljs-string\"\u003e'fetch_weather_data'\u003c/span\u003e 작업에서 데이터 가져오기\n\n    weather = {\n        \u003cspan class=\"hljs-string\"\u003e'city'\u003c/span\u003e: data[\u003cspan class=\"hljs-string\"\u003e'name'\u003c/span\u003e],  # 도시 이름 추출\n        \u003cspan class=\"hljs-string\"\u003e'temperature'\u003c/span\u003e: data[\u003cspan class=\"hljs-string\"\u003e'main'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'temp'\u003c/span\u003e],  # 온도 추출\n        \u003cspan class=\"hljs-string\"\u003e'pressure'\u003c/span\u003e: data[\u003cspan class=\"hljs-string\"\u003e'main'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'pressure'\u003c/span\u003e],  # 기압 추출\n        \u003cspan class=\"hljs-string\"\u003e'humidity'\u003c/span\u003e: data[\u003cspan class=\"hljs-string\"\u003e'main'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'humidity'\u003c/span\u003e],  # 습도 추출\n        \u003cspan class=\"hljs-string\"\u003e'weather'\u003c/span\u003e: data[\u003cspan class=\"hljs-string\"\u003e'weather'\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'description'\u003c/span\u003e],  # 날씨 설명 추출\n        \u003cspan class=\"hljs-string\"\u003e'wind_speed'\u003c/span\u003e: data[\u003cspan class=\"hljs-string\"\u003e'wind'\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'speed'\u003c/span\u003e],  # 풍속 추출\n        \u003cspan class=\"hljs-string\"\u003e'date'\u003c/span\u003e: datetime.\u003cspan class=\"hljs-title function_\"\u003eutcfromtimestamp\u003c/span\u003e(data[\u003cspan class=\"hljs-string\"\u003e'dt'\u003c/span\u003e]).\u003cspan class=\"hljs-title function_\"\u003estrftime\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'%Y-%m-%d %H:%M:%S'\u003c/span\u003e)  # 타임스탬프를 읽기 가능한 날짜로 변환\n    }\n\n    df = pd.\u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e([weather])  # 날씨 데이터를 판다스 \u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e으로 변환\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e df  # \u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e 반환\n\ndef \u003cspan class=\"hljs-title function_\"\u003eload_data_to_s3\u003c/span\u003e(**kwargs):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"변환된 데이터를 S3 버킷에 로드하기\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    ti = kwargs[\u003cspan class=\"hljs-string\"\u003e'ti'\u003c/span\u003e]  # 작업 인스턴스 가져오기\n    df = ti.\u003cspan class=\"hljs-title function_\"\u003excom_pull\u003c/span\u003e(task_ids=\u003cspan class=\"hljs-string\"\u003e'transform_weather_data'\u003c/span\u003e)  # \u003cspan class=\"hljs-title class_\"\u003eXCom\u003c/span\u003e을 사용하여 \u003cspan class=\"hljs-string\"\u003e'transform_weather_data'\u003c/span\u003e 작업에서 변환된 데이터 가져오기\n\n    csv_buffer = \u003cspan class=\"hljs-title class_\"\u003eStringIO\u003c/span\u003e()  # 인메모리 버퍼 생성\n    df.\u003cspan class=\"hljs-title function_\"\u003eto_csv\u003c/span\u003e(csv_buffer, index=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)  # \u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e을 \u003cspan class=\"hljs-variable constant_\"\u003eCSV\u003c/span\u003e로 버퍼에 작성\n    \u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(df)  # \u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e 출력 (선택 사항)\n\n    s3_resource = boto3.\u003cspan class=\"hljs-title function_\"\u003eresource\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e's3'\u003c/span\u003e)  # boto3 \u003cspan class=\"hljs-variable constant_\"\u003eS3\u003c/span\u003e 리소스 생성\n    s3_resource.\u003cspan class=\"hljs-title class_\"\u003eObject\u003c/span\u003e(\u003cspan class=\"hljs-variable constant_\"\u003eS3_BUCKET\u003c/span\u003e, \u003cspan class=\"hljs-variable constant_\"\u003eS3_KEY\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003eput\u003c/span\u003e(\u003cspan class=\"hljs-title class_\"\u003eBody\u003c/span\u003e=csv_buffer.\u003cspan class=\"hljs-title function_\"\u003egetvalue\u003c/span\u003e())  # \u003cspan class=\"hljs-variable constant_\"\u003eCSV\u003c/span\u003e 데이터를 지정한 \u003cspan class=\"hljs-variable constant_\"\u003eS3\u003c/span\u003e 버킷 및 키에 업로드\n\n# \u003cspan class=\"hljs-title class_\"\u003ePythonOperator\u003c/span\u003e를 사용하여 작업 정의\nfetch_task = \u003cspan class=\"hljs-title class_\"\u003ePythonOperator\u003c/span\u003e(\n    task_id=\u003cspan class=\"hljs-string\"\u003e'fetch_weather_data'\u003c/span\u003e,  # 작업 \u003cspan class=\"hljs-variable constant_\"\u003eID\u003c/span\u003e\n    python_callable=fetch_weather_data,  # 호출 가능한 함수\n    dag=dag,  # 작업이 속한 \u003cspan class=\"hljs-variable constant_\"\u003eDAG\u003c/span\u003e\n)\n\ntransform_task = \u003cspan class=\"hljs-title class_\"\u003ePythonOperator\u003c/span\u003e(\n    task_id=\u003cspan class=\"hljs-string\"\u003e'transform_weather_data'\u003c/span\u003e,  # 작업 \u003cspan class=\"hljs-variable constant_\"\u003eID\u003c/span\u003e\n    python_callable=transform_weather_data,  # 호출 가능한 함수\n    provide_context=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,  # 호출 가능한 함수에 컨텍스트 제공\n    dag=dag,  # 작업이 속한 \u003cspan class=\"hljs-variable constant_\"\u003eDAG\u003c/span\u003e\n)\n\nload_task = \u003cspan class=\"hljs-title class_\"\u003ePythonOperator\u003c/span\u003e(\n    task_id=\u003cspan class=\"hljs-string\"\u003e'load_data_to_s3'\u003c/span\u003e,  # 작업 \u003cspan class=\"hljs-variable constant_\"\u003eID\u003c/span\u003e\n    python_callable=load_data_to_s3,  # 호출 가능한 함수\n    provide_context=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,  # 호출 가능한 함수에 컨텍스트 제공\n    dag=dag,  # 작업이 속한 \u003cspan class=\"hljs-variable constant_\"\u003eDAG\u003c/span\u003e\n)\n\n# 작업 간 의존성 정의 (작업 실행 순서)\nfetch_task \u003e\u003e transform_task \u003e\u003e load_task  # 작업 실행 순서 설정: 가져오기 -\u003e 변환 -\u003e 로드\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e설명:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eImports 및 구성: 필요한 라이브러리를 import하고 구성 변수를 설정합니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e기본 인수: DAG의 기본 인수를 정의합니다. 예를 들어, 소유자, 시작 날짜, 재시도 정책 등이 포함됩니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDAG 정의: DAG 개체를 설명과 일정 간격으로 생성합니다 (일정을 비활성화하려면 None으로 설정 가능합니다).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e작업 함수: 사용할 Python 함수를 작업으로 정의합니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003efetch_weather_data: OpenWeather API에서 날씨 데이터를 가져옵니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etransform_weather_data: 가져온 데이터를 Pandas DataFrame으로 변환합니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eload_data_to_s3: 변환된 데이터를 S3 버킷에 로드합니다.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"5\"\u003e\n\u003cli\u003e작업 생성: PythonOperator를 사용하여 정의된 함수를 호출하는 작업을 생성합니다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003e종속성 설정: 비트 시프트 연산자를 사용하여 작업을 실행할 순서를 정의하세요.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e자료들\u003c/h1\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3"},"buildId":"xwOwpfNxF5xANgUpiyc2H","assetPrefix":"/TIL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>