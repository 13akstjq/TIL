<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>딥 뉴럴 네트워크 파인튜닝의 수학적 원리 | TIL</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://13akstjq.github.io/TIL//post/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="딥 뉴럴 네트워크 파인튜닝의 수학적 원리 | TIL" data-gatsby-head="true"/><meta property="og:title" content="딥 뉴럴 네트워크 파인튜닝의 수학적 원리 | TIL" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://TIL.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://13akstjq.github.io/TIL//post/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks" data-gatsby-head="true"/><meta name="twitter:title" content="딥 뉴럴 네트워크 파인튜닝의 수학적 원리 | TIL" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | TIL" data-gatsby-head="true"/><meta name="article:published_time" content="2024-07-09 19:56" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/TIL/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/TIL/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/TIL/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/TIL/favicons/favicon-96x96.png"/><link rel="icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/TIL/favicons/browserconfig.xml"/><link rel="preload" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/TIL/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/TIL/_next/static/chunks/webpack-21ffe88bdca56cba.js" defer=""></script><script src="/TIL/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/TIL/_next/static/chunks/main-a5eeabb286676ce6.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/TIL/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/TIL/_next/static/chunks/463-925361deb4cec4b1.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/post/%5Bslug%5D-9d7ebbd29b9e08ce.js" defer=""></script><script src="/TIL/_next/static/FuXRqV9h16krA5Mvtd6Dn/_buildManifest.js" defer=""></script><script src="/TIL/_next/static/FuXRqV9h16krA5Mvtd6Dn/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/TIL">TIL</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/TIL/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">딥 뉴럴 네트워크 파인튜닝의 수학적 원리</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="딥 뉴럴 네트워크 파인튜닝의 수학적 원리" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/TIL/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">TIL</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jul 9, 2024</span><span class="posts_reading_time__f7YPP">51<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_0.png" alt="이미지"></p>
<p>머신 러닝에서는 몇 가지 모델을 시도해 가장 성능이 좋은 것을 선택하고 몇 가지 설정을 조정하여 그나마 성공할 수 있을지도 모릅니다. 그러나 딥러닝은 그런 룰에 맞지 않습니다. 신경망을 실험해 본 적이 있다면, 성능이 꽤 불안정할 수 있다는 것을 눈치챌 수 있습니다. 어쩌면 로지스틱 회귀와 같이 간단한 모델이 멋진 200층 심층 신경망을 이길 수도 있습니다.</p>
<p>이게 왜 그럴까요? 딥러닝은 우리가 가지고 있는 가장 고급 인공 지능 기술 중 하나이지만, 철저한 이해와 조심스러운 다룸이 필요합니다. 신경망을 세밀하게 조정하고, 내부 작동 방식을 파악하고, 그 사용법을 마스터하는 것이 중요합니다. 오늘은 이에 대해 자세히 살펴보겠습니다!</p>
<p>글을 읽기 전에 Jupyter Notebook을 여시는 것을 제안합니다. 오늘 다룰 모든 코드가 담겨 있으므로 함께 따라가는 데 도움이 될 것입니다:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>인덱스</p>
<ul>
<li>
<p>1: 소개</p>
<ul>
<li>1.1: 기본 신경망의 발전</li>
<li>1.2: 복잡성으로의 길</li>
</ul>
</li>
<li>
<p>2: 모델 복잡성 확장</p>
<ul>
<li>2.1: 레이어 추가</li>
</ul>
</li>
<li>
<p>3: 향상된 학습을 위한 최적화 기법</p>
<ul>
<li>3.1: 학습률</li>
<li>3.2: 조기 중단 기법</li>
<li>3.3: 초기화 방법</li>
<li>3.4: 드롭아웃</li>
<li>3.5: 그래디언트 클리핑</li>
</ul>
</li>
</ul>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>
<p>4: 최적 레이어 수 결정하기</p>
<ul>
<li>4.1: 레이어 깊이와 모델 성능</li>
<li>4.2: 적절한 깊이 선택을 위한 테스트 전략</li>
</ul>
</li>
<li>
<p>5: Optuna를 활용한 자동 세부 조정</p>
<ul>
<li>5.1: Optuna 소개</li>
<li>5.2: 신경망 최적화를 위한 Optuna 통합</li>
<li>5.3: 실제 적용</li>
<li>5.4: 장점과 결과</li>
<li>5.5: 제한 사항</li>
</ul>
</li>
<li>
<p>6: 결론</p>
<ul>
<li>6.1: 다음 단계</li>
</ul>
</li>
<li>
<p>추가 자료</p>
</li>
</ul>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1: 소개</h1>
<h2>1.1: 기본 신경망의 발전</h2>
<p>인공 지능에 대한 최근 탐구에서 우리는 기초부터 신경망을 구축했습니다. 이 기본 모델은 오늘날 인공 지능 기술의 핵심인 신경망의 세계를 열어 주었습니다. 우리는 입력, 은닉 및 출력 레이어와 활성화 함수가 어떻게 정보를 처리하고 예측하는 데 결합되는지 간단하게 다루었습니다. 그리고 나서 우리는 컴퓨터 비전 작업을 위해 숫자 데이터셋에서 훈련된 간단한 신경망으로 이론을 실제로 적용했습니다.</p>
<p>이제 이러한 기초 위에 계속해서 더 진보해 나갈 것입니다. 우리는 레이어를 추가하고, 초기화, 정규화 및 최적화에 대한 다양한 기술을 탐구함으로써 더 많은 복잡성을 도입할 것입니다. 물론, 이러한 수정이 우리의 신경망 성능에 어떻게 영향을 미치는지 확인하기 위해 코드를 테스트할 것입니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>제가 이전 기사를 확인하지 않으셨다면, 우리가 처음부터 신경망을 만들어본 기사를 꼭 읽어보시기를 추천합니다. 이번에는 그 작업을 기반으로 계속해서 진행할 것이며, 이미 우리가 다룬 개념에 익숙하다고 가정할게요.</p>
<h2>1.2: 복잡성으로의 길</h2>
<p>신경망을 기본 구성에서 더 정교한 구조로 변환하는 것은 단순히 더 많은 층이나 노드를 추가하는 것만으로는 이루어지지 않습니다. 이것은 신경망의 구조와 그 데이터를 다루는 미묘한 차이를 체화하는 세심한 작업이 필요한 미묘한 춤이죠. 더 깊게 파고들수록, 우리의 목표는 신경망의 깊이를 풍부하게 함으로써 데이터의 복잡한 패턴과 연결을 더 잘 분별하는 데 있습니다.</p>
<p>하지만 복잡성을 높이는 것은 그리 수월한 일이 아닙니다. 우리가 도입할 때마다, 세련된 최적화 기술의 필요성이 커집니다. 이는 효과적인 학습뿐만 아니라 새로운 보이지 않는 데이터에 적응하기 위한 모델 능력에 필수적입니다. 이 안내서는 우리의 기반 신경망을 강화하는 과정을 안내해 드릴 것입니다. 우리는 신경망을 세밀하게 조정하는 정교한 전략에 대해 살펴볼 것이며, 학습 속도 조정, 조기 종료 도입, 그리고 SGD(확률적 경사 하강법)와 Adam과 같은 다양한 최적화 알고리즘을 활용하는 방법에 대해 살펴볼 것입니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>우리는 초기화 방법으로 시작하는 중요성, 오버피팅을 피하는 데 드롭아웃 사용의 장점, 그리고 클리핑 및 정규화로 네트워크의 그래디언트를 체크하여 안정성을 유지하는 것이 모델의 안정성에 얼마나 중요한지에 대해 다룰 예정입니다. 또한 학습을 향상시키기 위한 레이어의 최적 개수를 찾는 도전과정 및 불필요한 복잡성으로 빠져들지 않도록 할 것입니다.</p>
<p>이전 게시물에서 함께 만든 Neural Network 및 Trainer 클래스를 아래에서 확인할 수 있습니다. 우리는 이를 조정하고 각 수정이 모델의 성능에 어떤 영향을 미치는지 실제로 살펴볼 것입니다:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NeuralNetwork</span>:
    def <span class="hljs-title function_">__init__</span>(self, input_size, hidden_size, output_size, loss_func=<span class="hljs-string">'mse'</span>):
        self.<span class="hljs-property">input_size</span> = input_size
        self.<span class="hljs-property">hidden_size</span> = hidden_size
        self.<span class="hljs-property">output_size</span> = output_size
        self.<span class="hljs-property">loss_func</span> = loss_func

        # 가중치 및 편향 초기화
        self.<span class="hljs-property">weights1</span> = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">randn</span>(self.<span class="hljs-property">input_size</span>, self.<span class="hljs-property">hidden_size</span>)
        self.<span class="hljs-property">bias1</span> = np.<span class="hljs-title function_">zeros</span>((<span class="hljs-number">1</span>, self.<span class="hljs-property">hidden_size</span>))
        self.<span class="hljs-property">weights2</span> = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">randn</span>(self.<span class="hljs-property">hidden_size</span>, self.<span class="hljs-property">output_size</span>)
        self.<span class="hljs-property">bias2</span> = np.<span class="hljs-title function_">zeros</span>((<span class="hljs-number">1</span>, self.<span class="hljs-property">output_size</span>))

        # 손실 추적
        self.<span class="hljs-property">train_loss</span> = []
        self.<span class="hljs-property">test_loss</span> = []

    def <span class="hljs-title function_">__str__</span>(self):
        <span class="hljs-keyword">return</span> f<span class="hljs-string">"Neural Network Layout:\n입력 레이어: {self.input_size} 뉴런\n은닉 레이어: {self.hidden_size} 뉴런\n출력 레이어: {self.output_size} 뉴런\n손실 함수: {self.loss_func}"</span>

    def <span class="hljs-title function_">forward</span>(self, X):
        # 순방향 전파 수행
        self.<span class="hljs-property">z1</span> = np.<span class="hljs-title function_">dot</span>(X, self.<span class="hljs-property">weights1</span>) + self.<span class="hljs-property">bias1</span>
        self.<span class="hljs-property">a1</span> = self.<span class="hljs-title function_">sigmoid</span>(self.<span class="hljs-property">z1</span>)
        self.<span class="hljs-property">z2</span> = np.<span class="hljs-title function_">dot</span>(self.<span class="hljs-property">a1</span>, self.<span class="hljs-property">weights2</span>) + self.<span class="hljs-property">bias2</span>
        <span class="hljs-keyword">if</span> self.<span class="hljs-property">loss_func</span> == <span class="hljs-string">'categorical_crossentropy'</span>:
            self.<span class="hljs-property">a2</span> = self.<span class="hljs-title function_">softmax</span>(self.<span class="hljs-property">z2</span>)
        <span class="hljs-attr">else</span>:
            self.<span class="hljs-property">a2</span> = self.<span class="hljs-title function_">sigmoid</span>(self.<span class="hljs-property">z2</span>)
        <span class="hljs-keyword">return</span> self.<span class="hljs-property">a2</span>

    def <span class="hljs-title function_">backward</span>(self, X, y, learning_rate):
        # 역전파 수행
        m = X.<span class="hljs-property">shape</span>[<span class="hljs-number">0</span>]

        # 기울기 계산
        <span class="hljs-keyword">if</span> self.<span class="hljs-property">loss_func</span> == <span class="hljs-string">'mse'</span>:
            self.<span class="hljs-property">dz2</span> = self.<span class="hljs-property">a2</span> - y
        elif self.<span class="hljs-property">loss_func</span> == <span class="hljs-string">'log_loss'</span>:
            self.<span class="hljs-property">dz2</span> = -(y/self.<span class="hljs-property">a2</span> - (<span class="hljs-number">1</span>-y)/(<span class="hljs-number">1</span>-self.<span class="hljs-property">a2</span>))
        elif self.<span class="hljs-property">loss_func</span> == <span class="hljs-string">'categorical_crossentropy'</span>:
            self.<span class="hljs-property">dz2</span> = self.<span class="hljs-property">a2</span> - y
        <span class="hljs-attr">else</span>:
            raise <span class="hljs-title class_">ValueError</span>(<span class="hljs-string">'잘못된 손실 함수'</span>)

        self.<span class="hljs-property">dw2</span> = (<span class="hljs-number">1</span> / m) * np.<span class="hljs-title function_">dot</span>(self.<span class="hljs-property">a1</span>.<span class="hljs-property">T</span>, self.<span class="hljs-property">dz2</span>)
        self.<span class="hljs-property">db2</span> = (<span class="hljs-number">1</span> / m) * np.<span class="hljs-title function_">sum</span>(self.<span class="hljs-property">dz2</span>, axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-title class_">True</span>)
        self.<span class="hljs-property">dz1</span> = np.<span class="hljs-title function_">dot</span>(self.<span class="hljs-property">dz2</span>, self.<span class="hljs-property">weights2</span>.<span class="hljs-property">T</span>) * self.<span class="hljs-title function_">sigmoid_derivative</span>(self.<span class="hljs-property">a1</span>)
        self.<span class="hljs-property">dw1</span> = (<span class="hljs-number">1</span> / m) * np.<span class="hljs-title function_">dot</span>(X.<span class="hljs-property">T</span>, self.<span class="hljs-property">dz1</span>)
        self.<span class="hljs-property">db1</span> = (<span class="hljs-number">1</span> / m) * np.<span class="hljs-title function_">sum</span>(self.<span class="hljs-property">dz1</span>, axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-title class_">True</span>)

        # 가중치 및 편향 업데이트
        self.<span class="hljs-property">weights2</span> -= learning_rate * self.<span class="hljs-property">dw2</span>
        self.<span class="hljs-property">bias2</span> -= learning_rate * self.<span class="hljs-property">db2</span>
        self.<span class="hljs-property">weights1</span> -= learning_rate * self.<span class="hljs-property">dw1</span>
        self.<span class="hljs-property">bias1</span> -= learning_rate * self.<span class="hljs-property">db1</span>

    def <span class="hljs-title function_">sigmoid</span>(self, x):
        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.<span class="hljs-title function_">exp</span>(-x))

    def <span class="hljs-title function_">sigmoid_derivative</span>(self, x):
        <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span> - x)

    def <span class="hljs-title function_">softmax</span>(self, x):
        exps = np.<span class="hljs-title function_">exp</span>(x - np.<span class="hljs-title function_">max</span>(x, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-title class_">True</span>))
        <span class="hljs-keyword">return</span> exps/np.<span class="hljs-title function_">sum</span>(exps, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-title class_">True</span>)

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Trainer</span>:
    def <span class="hljs-title function_">__init__</span>(self, model, loss_func=<span class="hljs-string">'mse'</span>):
        self.<span class="hljs-property">model</span> = model
        self.<span class="hljs-property">loss_func</span> = loss_func
        self.<span class="hljs-property">train_loss</span> = []
        self.<span class="hljs-property">val_loss</span> = []

    def <span class="hljs-title function_">calculate_loss</span>(self, y_true, y_pred):
        <span class="hljs-keyword">if</span> self.<span class="hljs-property">loss_func</span> == <span class="hljs-string">'mse'</span>:
            <span class="hljs-keyword">return</span> np.<span class="hljs-title function_">mean</span>((y_pred - y_true)**<span class="hljs-number">2</span>)
        elif self.<span class="hljs-property">loss_func</span> == <span class="hljs-string">'log_loss'</span>:
            <span class="hljs-keyword">return</span> -np.<span class="hljs-title function_">mean</span>(y_true*np.<span class="hljs-title function_">log</span>(y_pred) + (<span class="hljs-number">1</span>-y_true)*np.<span class="hljs-title function_">log</span>(<span class="hljs-number">1</span>-y_pred))
        elif self.<span class="hljs-property">loss_func</span> == <span class="hljs-string">'categorical_crossentropy'</span>:
            <span class="hljs-keyword">return</span> -np.<span class="hljs-title function_">mean</span>(y_true*np.<span class="hljs-title function_">log</span>(y_pred))
        <span class="hljs-attr">else</span>:
            raise <span class="hljs-title class_">ValueError</span>(<span class="hljs-string">'잘못된 손실 함수'</span>)

    def <span class="hljs-title function_">train</span>(self, X_train, y_train, X_test, y_test, epochs, learning_rate):
        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(epochs):
            self.<span class="hljs-property">model</span>.<span class="hljs-title function_">forward</span>(X_train)
            self.<span class="hljs-property">model</span>.<span class="hljs-title function_">backward</span>(X_train, y_train, learning_rate)
            train_loss = self.<span class="hljs-title function_">calculate_loss</span>(y_train, self.<span class="hljs-property">model</span>.<span class="hljs-property">a2</span>)
            self.<span class="hljs-property">train_loss</span>.<span class="hljs-title function_">append</span>(train_loss)

            self.<span class="hljs-property">model</span>.<span class="hljs-title function_">forward</span>(X_test)
            test_loss = self.<span class="hljs-title function_">calculate_loss</span>(y_test, self.<span class="hljs-property">model</span>.<span class="hljs-property">a2</span>)
            self.<span class="hljs-property">val_loss</span>.<span class="hljs-title function_">append</span>(val_loss)
</code></pre>
<h1>2: 모델 복잡성 확대</h1>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>신경망을 더 정교하게 개선하기 위해 깊이 파고들면서, 게임을 바꾸는 전략을 발견했습니다: 레벨을 더 쌓아 복잡성을 높이는 것입니다. 이 동작은 모델을 강화하는 것뿐만 아니라, 데이터의 미묘한 변화를 더 정교하게 이해하고 해석하는 능력을 키우는 것입니다.</p>
<h2>2.1: 레이어 추가</h2>
<p>증가된 네트워크 깊이의 근거
딥러닝의 핵심은 계층적 데이터 표현을 조합하는 능력에 있습니다. 더 많은 레이어를 추가함으로써, 우리는 신경망에 성장하는 복잡성의 패턴을 해체하고 이해하기 위한 도구를 제공하는 셈입니다. 간단한 형태와 질감을 인식하는 데서 시작해 데이터 속에서 더 복잡한 관계와 특징을 풀어가는 데로 발전하는 것으로 생각해보세요. 이러한 계층적 학습 접근법은 어느 정도 인간이 정보를 해석하는 방식과 유사하며, 기본적인 이해에서 복잡한 해석으로 진화합니다.</p>
<p>더 많은 레이어를 쌓으면 네트워크의 "학습 용량"이 증가하여 보다 넓은 범위의 데이터 관계를 매핑하고 소화하는 능력을 갖추게 됩니다. 이를 통해 더 복잡한 작업을 처리할 수 있습니다. 하지만 마구 레이어를 추가하는 것은 아니며, 모델의 지능에 의미 있는 기여를 하지 않고 무분별하게 레이어를 추가하면 학습 과정을 혼란시키는 것이 아니라 명료하게 해야 합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><code>NeuralNetwork</code> 클래스를 더 많은 층을 통합하는 방법 안내</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NeuralNetwork</span>:
    def <span class="hljs-title function_">__init__</span>(self, layers, loss_func=<span class="hljs-string">'mse'</span>):
        self.<span class="hljs-property">layers</span> = []
        self.<span class="hljs-property">loss_func</span> = loss_func

        # 레이어 초기화
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-title function_">len</span>(layers) - <span class="hljs-number">1</span>):
            self.<span class="hljs-property">layers</span>.<span class="hljs-title function_">append</span>({
                <span class="hljs-string">'weights'</span>: np.<span class="hljs-property">random</span>.<span class="hljs-title function_">randn</span>(layers[i], layers[i + <span class="hljs-number">1</span>]),
                <span class="hljs-string">'biases'</span>: np.<span class="hljs-title function_">zeros</span>((<span class="hljs-number">1</span>, layers[i + <span class="hljs-number">1</span>]))
            })

        # 손실 추적
        self.<span class="hljs-property">train_loss</span> = []
        self.<span class="hljs-property">test_loss</span> = []

    def <span class="hljs-title function_">forward</span>(self, X):
        self.<span class="hljs-property">a</span> = [X]
        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.<span class="hljs-property">layers</span>:
            self.<span class="hljs-property">a</span>.<span class="hljs-title function_">append</span>(self.<span class="hljs-title function_">sigmoid</span>(np.<span class="hljs-title function_">dot</span>(self.<span class="hljs-property">a</span>[-<span class="hljs-number">1</span>], layer[<span class="hljs-string">'weights'</span>]) + layer[<span class="hljs-string">'biases'</span>]))
        <span class="hljs-keyword">return</span> self.<span class="hljs-property">a</span>[-<span class="hljs-number">1</span>]

    def <span class="hljs-title function_">backward</span>(self, X, y, learning_rate):
        m = X.<span class="hljs-property">shape</span>[<span class="hljs-number">0</span>]
        self.<span class="hljs-property">dz</span> = [self.<span class="hljs-property">a</span>[-<span class="hljs-number">1</span>] - y]

        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">reversed</span>(<span class="hljs-title function_">range</span>(<span class="hljs-title function_">len</span>(self.<span class="hljs-property">layers</span>) - <span class="hljs-number">1</span>)):
            self.<span class="hljs-property">dz</span>.<span class="hljs-title function_">append</span>(np.<span class="hljs-title function_">dot</span>(self.<span class="hljs-property">dz</span>[-<span class="hljs-number">1</span>], self.<span class="hljs-property">layers</span>[i + <span class="hljs-number">1</span>][<span class="hljs-string">'weights'</span>].<span class="hljs-property">T</span>) * self.<span class="hljs-title function_">sigmoid_derivative</span>(self.<span class="hljs-property">a</span>[i + <span class="hljs-number">1</span>]))

        self.<span class="hljs-property">dz</span> = self.<span class="hljs-property">dz</span>[::-<span class="hljs-number">1</span>]

        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-title function_">len</span>(self.<span class="hljs-property">layers</span>)):
            self.<span class="hljs-property">layers</span>[i][<span class="hljs-string">'weights'</span>] -= learning_rate * np.<span class="hljs-title function_">dot</span>(self.<span class="hljs-property">a</span>[i].<span class="hljs-property">T</span>, self.<span class="hljs-property">dz</span>[i]) / m
            self.<span class="hljs-property">layers</span>[i][<span class="hljs-string">'biases'</span>] -= learning_rate * np.<span class="hljs-title function_">sum</span>(self.<span class="hljs-property">dz</span>[i], axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-title class_">True</span>) / m

    def <span class="hljs-title function_">sigmoid</span>(self, x):
        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.<span class="hljs-title function_">exp</span>(-x))

    def <span class="hljs-title function_">sigmoid_derivative</span>(self, x):
        <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span> - x)
</code></pre>
<p>이 섹션에서는 신경망의 작동 방식에 중요한 조정을 가했으며, 임의의 층 수를 유연하게 지원하는 모델을 목표로했습니다. 변경된 사항은 다음과 같습니다:</p>
<p>먼저, 이전에 각 층의 노드 수를 정의했던 self.input, self.hidden, self.output 변수를 삭제했습니다. 이제 목표는 임의의 층 수를 관리할 수 있는 다목적 모델입니다. 예를 들어, 이전에 숫자 데이터셋에 사용했던 모델인 64개의 입력 노드, 64개의 은닉 노드 및 10개의 출력 노드를 사용하는 경우, 다음과 같이 설정할 수 있습니다:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">nn = <span class="hljs-title class_">NeuralNetwork</span>((layers = [<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">10</span>]));
</code></pre>
<p>이제 코드가 각 레이어를 세 번씩 순환하며 다른 목적으로 사용됨을 알 수 있습니다:</p>
<p>초기화 과정 중에는 모든 레이어의 가중치와 편향이 설정됩니다. 이 단계는 학습 프로세스를 위해 초기 매개변수로 네트워크를 준비하는 데 중요합니다.</p>
<p>순방향 패스 동안 활성화 self.a는 리스트에 수집됩니다. 입력 레이어의 활성화(본질적으로 입력 데이터 X)로 시작합니다. 각 레이어에 대해, np.dot(self.a[-1], layer['weights']) + layer['biases']를 사용하여 가중치 합과 편향을 계산하고 시그모이드 활성화 함수를 적용하여 결과를 self.a에 첨부합니다. 네트워크의 결과는 self.a의 마지막 요소로, 최종 출력을 나타냅니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>역 전파 동안, 이 단계는 마지막 레이어의 활성화에 대한 손실의 도함수를 계산하고 출력 레이어의 오차 목록을 준비함으로써 시작합니다 (self.dz). 그런 다음 네트워크를 역방향으로 거슬러 올라가며 (reversed(range(len(self.layers) - 1))를 사용하여), 숨은 레이어에 대한 오차 항목을 계산합니다. 이 과정은 현재 오차 항목을 다음 레이어의 가중치와 점곱(역방향)하여 시그모이드 함수의 도함수로 비선형성을 처리하는 작업을 포함합니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Trainer</span>:
    ...
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, X_train, y_train, X_test, y_test, epochs, learning_rate</span>):
        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
            self.model.forward(X_train)
            self.model.backward(X_train, y_train, learning_rate)
            train_loss = self.calculate_loss(y_train, self.model.a[-<span class="hljs-number">1</span>])
            self.train_loss.append(train_loss)

            self.model.forward(X_test)
            test_loss = self.calculate_loss(y_test, self.model.a[-<span class="hljs-number">1</span>])
            self.test_loss.append(test_loss)
</code></pre>
<p>마지막으로, NeuralNetwork 클래스의 변화에 따라 Trainer 클래스를 업데이트했습니다. 주요한 수정 사항은 특히 train 메서드에 있으며, 네트워크의 출력이 이제 self.model.a[-1]에서 가져온다는 점 때문에 훈련 및 테스트 손실을 다시 계산하는 방식입니다.</p>
<p>이러한 수정 사항은 우리의 신경망을 다양한 아키텍처에 적응할 수 있도록 할뿐만 아니라 데이터와 그래디언트의 흐름을 이해하는 중요성을 강조합니다. 구조를 간소화함으로써, 각종 작업에서 네트워크의 성능을 실험하고 최적화할 수 있는 능력을 향상시킵니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3: 향상된 학습을 위한 최적화 기법</h1>
<p>신경망을 최적화하는 것은 그들이 배우는 능력을 향상시키고 효율적인 학습을 보장하며 최상의 버전으로 이끄는 데 중요합니다. 저희 모델이 얼마나 잘 수행되는지에 상당한 영향을 미치는 몇 가지 중요한 최적화 기술에 대해 알아보겠습니다.</p>
<h2>3.1: 학습률</h2>
<p>학습률은 손실 경사에 기반하여 네트워크의 가중치를 조정하는 제어 장치입니다. 이는 모델이 학습하는 속도를 결정하며 최적화 중에 취하는 단계가 얼마나 큰지 작은지를 결정합니다. 학습률을 적절하게 설정하면 모델이 빠르게 낮은 오차의 해결책을 찾을 수 있습니다. 그러나 올바르게 설정하지 않으면 모델이 수렴하는 데 시간이 오래 걸리거나 아예 좋은 해결책을 찾지 못할 수 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>너무 높은 학습률을 설정하면 모델이 최적해를 뛰어넘을 수 있어 불안정한 행동을 일으킬 수 있습니다. 이는 정확도나 손실이 훈련 중에 급격하게 변하는 것으로 나타날 수 있어요.</p>
<p>학습률이 너무 낮으면 훈련 과정이 지나치게 느리게 진행될 수 있어요. 이 경우, 훈련 손실이 시간이 지남에 따라 거의 변하지 않는 것을 볼 수 있어요.</p>
<p>관건은 훈련 및 검증 손실을 추적하면서 학습률이 어떻게 작동하는지에 대한 단서를 얻는 것이에요. 훈련 중에 일정 간격으로 이러한 손실을 기록하고 나중에 이를 플로팅하여 손실 landscape가 얼마나 매끄럽거나 불안정한지 보다 명확히 파악할 수 있어요. 우리의 코드에서는 이러한 메트릭을 추적하기 위해 Python의 logging 라이브러리를 사용하고 있어요. 이렇게 생겼답니다:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> logging
<span class="hljs-comment"># Logger 설정</span>
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Trainer</span>:
    ...
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, X_train, y_train, X_val, y_val, epochs, learning_rate</span>):
        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
            ...
            <span class="hljs-comment"># 50 에폭마다 손실 및 검증 손실을 로그로 남깁니다</span>
            <span class="hljs-keyword">if</span> epoch % <span class="hljs-number">50</span> == <span class="hljs-number">0</span>:
                logger.info(<span class="hljs-string">f'에폭 <span class="hljs-subst">{epoch}</span>: 손실 = <span class="hljs-subst">{train_loss}</span>, 검증 손실 = <span class="hljs-subst">{val_loss}</span>'</span>)
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>시작할 때, 우리는 훈련 업데이트를 캡처하고 표시하기 위해 로거를 설정했습니다. 이 설정을 통해 우리는 훈련 및 검증 손실을 매 50번째 에포크마다 기록하여 모델의 진행 상황에 대한 안정적인 피드백을 받을 수 있습니다. 이 피드백을 통해 손실이 잘 감소하고 있는지, 아니면 너무 불규칙하게 변동하는지 파악할 수 있어서 학습률을 조정해야 할 필요가 있을지도 모릅니다.</p>
<p>위의 코드는 훈련 및 검증 손실을 그래프로 플로팅하여 훈련 중에 손실이 어떻게 변화하는지 더 잘 이해할 수 있도록 해줍니다. 많은 반복에서 약간의 잡음이 예상되므로 부드러운(스무딩) 효과를 추가했습니다. 잡음을 부드럽게 처리하여 그래프를 더 잘 분석할 수 있도록 도와줄 것입니다.</p>
<p>이러한 방식을 따르면 훈련을 시작하면 로그가 나타나면서 우리의 진행 상황을 한 눈에 볼 수 있고 조정할 수 있는 정보를 제공하여 우리가 방향을 수정하는 데 도움이 될 것입니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_1.png">
<p>그런 다음, 훈련이 끝난 후 손실을 그래프로 그려볼 수 있습니다:</p>
<img src="/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_2.png">
<p>훈련 및 검증 손실이 꾸준히 감소하는 것을 보는 것은 좋은 신호입니다. 이는 에포크 수를 늘리고 학습률 스텝 크기를 증가시킨다면 잘 작동할 수 있다는 신호일 수 있습니다. 그러나 반대로 손실이 감소한 후 급상승하는 것을 관찰하면, 학습률 스텝 크기를 줄이는 것이 명백한 신호입니다. 그렇지만 재미있는 점이 있습니다: 에포크 0부터 50까지 우리의 손실이 어떤 이상한 일이 일어나고 있습니다. 우리는 그 부분을 확인하기 위해 다시 살펴보겠습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>그 달콤한 학습률의 최적값을 찾기 위해서는 학습률 앤달링 또는 적응형 학습률 기법과 같은 방법이 정말 유용할 수 있어요. 이러한 방법들은 학습률을 실시간으로 세밀하게 조정하여 훈련 중에 최적의 속도를 유지하도록 도와줘요.</p>
<h2>3.2: 조기 중단 기법</h2>
<p>조기 중단은 마치 안전망 같아요 — 유효성 검사 세트에서 모델의 성능을 보고, 더 이상 성능이 개선되지 않을 때 훈련을 중지하는 것이에요. 이는 과적합에 대한 안전 장치이며, 우리 모델이 이전에 본 적 없는 데이터에서도 잘 작동하도록 보장해줘요.</p>
<p>여기에 이를 실행하는 방법이 있어요:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>검증 세트: 교육 데이터의 일부를 분리하여 검증 세트로 사용합니다. 이것은 중요합니다. 왜냐하면 이렇게 하면 멈춤 결정이 신선한 보이지 않는 데이터에 기반하기 때문입니다.</li>
<li>모니터링: 각 학습 에포크 후 모델이 검증 세트에서 어떻게 수행되는지 주시하세요. 성능이 향상되고 있나요, 아니면 정체되었나요?</li>
<li>멈춤 기준: 멈출 시점을 결정하세요. 일반적으로 "연속적으로 50번의 에포크 동안 검증 손실이 향상되지 않음"이 있습니다.</li>
</ul>
<p>이를 위한 코드를 살펴보죠:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Trainer</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, X_train, y_train, X_val, y_val, epochs, learning_rate,
              early_stopping=<span class="hljs-literal">True</span>, patience=<span class="hljs-number">10</span></span>):
        best_loss = np.inf
        epochs_no_improve = <span class="hljs-number">0</span>

        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
           ...

            <span class="hljs-comment"># 조기 중단</span>
            <span class="hljs-keyword">if</span> early_stopping:
                <span class="hljs-keyword">if</span> val_loss &#x3C; best_loss:
                    best_loss = val_loss
                    best_weights = [layer[<span class="hljs-string">'weights'</span>] <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.model.layers]
                    epochs_no_improve = <span class="hljs-number">0</span>
                <span class="hljs-keyword">else</span>:
                    epochs_no_improve += <span class="hljs-number">1</span>

                <span class="hljs-keyword">if</span> epochs_no_improve == patience:
                    <span class="hljs-built_in">print</span>(<span class="hljs-string">'조기 중단!'</span>)
                    <span class="hljs-comment"># 최적의 가중치로 복원</span>
                    <span class="hljs-keyword">for</span> i, layer <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.model.layers):
                        layer[<span class="hljs-string">'weights'</span>] = best_weights[i]
                    <span class="hljs-keyword">break</span>
</code></pre>
<p>train 메서드에서 두 가지 새로운 옵션을 소개했습니다:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>early_stopping: 이는 조기 중단을 켜거나 끄는 여부를 결정하는 이진 플래그입니다.</li>
<li>patience: 이는 훈련을 중단하기 전에 유효성 검사 손실이 향상되지 않은 라운드 수를 설정합니다.</li>
</ul>
<p>우리는 가장 낮은 유효성 검사 손실을 현재까지 본 최저치로 설정하기 위해 best_loss를 무한대로 설정합니다. 한편, epochs_no_improve는 얼마 동안 유효성 검사 손실이 개선되지 않은 에포크 수를 기록합니다.</p>
<p>모델을 훈련하기 위해 각 에포크를 순회하는 동안에는 실제 훈련 단계(순방향 전파 및 역전파)가 여기에 자세히 나와 있지는 않지만 프로세스의 중요한 부분입니다.</p>
<p>매 에포크가 끝날 때마다 현재 에포크의 유효성 검사 손실(val_loss)이 best_loss보다 낮아졌다면, 이는 우리가 진전을 이루고 있다는 뜻입니다. 우리는 best_loss를 이 새로운 최솟값으로 업데이트하고, 또한 현재 모델 가중치를 best_weights로 저장합니다. 이렇게 하면 모델이 최상의 성능을 발휘한 시점의 스냅샷을 항상 가지게 됩니다. 그리고 우리는 방금 개선을 보았기 때문에 epochs_no_improve 카운트를 다시 0으로 재설정합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>만약 val_loss에 감소가 없다면, epochs_no_improve를 하나씩 증가시켜서 다른 epoch가 향상되지 않은 것으로 표시합니다.</p>
<p>만약 우리가 설정한 인내심 한계치에 epochs_no_improve 카운트가 달성하면, 모델이 더 나아질 가능성이 낮다는 신호로 조기 종료를 시작합니다. 메시지와 함께 알림을 표시하고, 모델의 가중치를 최적의 가중치인 best_weights로 되돌립니다. 그런 다음 학습 루프를 종료합니다.</p>
<p>이 접근 방식은 학습을 중단하는 균형있는 방법을 제공합니다. 모델에 학습의 공정한 기회를 제공하여 너무 일찍 중단하지 않으면서, 너무 늦지도 않아 시간을 낭비하거나 과적합의 위험을 가져올 수 있습니다.</p>
<h2>3.3: 초기화 방법</h2>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>신경망을 설정할 때, 가중치를 어떻게 시작하느냐에 따라 네트워크가 얼마나 잘, 그리고 얼마나 빨리 학습하는지가 달라질 수 있어요. 가중치를 초기화하는 몇 가지 다른 방법 – 랜덤, 영, Glorot(Xavier), 그리고 He 초기화 – 에 대해 알아봐요.</p>
<p>랜덤 초기화
랜덤 방식을 선택하면 주로 균일하거나 정규 분포에서 숫자를 추출하여 초기 가중치를 설정하는 것을 의미해요. 이러한 무작위성은 모든 뉴런이 동일한 위치에서 시작하지 않도록하여 네트워크가 학습함에 따라 서로 다른 것을 배울 수 있도록 도와줘요. 핵심은 적절한 분산을 선택하는 것인데, 너무 많으면 기울기가 폭발할 위험이 있고, 너무 적으면 사라질 수도 있어요.</p>
<pre><code class="hljs language-js">weights = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">randn</span>(layers[i], layers[i + <span class="hljs-number">1</span>]);
</code></pre>
<p>이 코드 라인은 표준 정규 분포에서 가중치를 추출하여 각 뉴런이 학습의 길로 나아갈 수 있도록 준비를 합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>장점: 뉴런이 서로 모방하는 것을 방지하는 간단한 방법입니다.</p>
<p>단점: 분산을 잘못 설정하면 학습 과정이 불안정해질 수 있습니다.</p>
<p>제로 초기화
모든 가중치를 0으로 설정하는 방법은 매우 간단합니다. 그러나 이 방법에는 주요 단점이 있습니다: 이로 인해 층의 모든 뉴런이 사실상 동일해집니다. 이러한 동일성으로 인해 네트워크의 학습이 저해될 수 있으며, 모든 층의 뉴런이 학습 중에 동일하게 업데이트될 수 있습니다.</p>
<pre><code class="hljs language-js">weights = np.<span class="hljs-title function_">zeros</span>((layers[i], layers[i + <span class="hljs-number">1</span>]));
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>마지막으로, 우리는 모두 0으로 채워진 가중치 행렬을 얻습니다. 깔끔하고 정돈되어 있지만, 네트워크를 통해 나아가는 모든 경로가 처음에는 동일한 가중치를 갖게 되어 학습 다양성을 위한 좋지 않은 결과를 초래할 수 있습니다.</p>
<p>장점: 구현이 매우 쉽습니다.</p>
<p>단점: 학습 과정을 제약시켜 네트워크의 성능이 보통 좋지 않게끔 만듭니다.</p>
<p>Glorot 초기화
시그모이드 활성화 함수를 사용하는 네트워크를 위해 설계된 Glorot 초기화는 네트워크 내 입력 단위와 출력 단위의 수에 기반하여 가중치를 설정합니다. 이 초기화는 활성화와 역전파된 그래디언트의 분산을 유지하고 vanishing 또는 exploding 그래디언트 문제를 방지하기 위해 레이어를 통해 전달됩니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>글로럿 초기화에서의 가중치는 균일 분포나 정규 분포로 생성할 수 있습니다. 균일 분포를 사용하는 경우, 가중치는 [-a, a] 범위로 초기화됩니다. 여기서 a 값은:</p>
<p><img src="/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_3.png" alt="식"></p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">glorot_uniform</span>(self, fan_in, fan_out):
    limit = np.<span class="hljs-title function_">sqrt</span>(<span class="hljs-number">6</span> / (fan_in + fan_out))
    <span class="hljs-keyword">return</span> np.<span class="hljs-property">random</span>.<span class="hljs-title function_">uniform</span>(-limit, limit, (fan_in, fan_out))

weights = <span class="hljs-title function_">glorot_uniform</span>(layers[i - <span class="hljs-number">1</span>], layers[i])
</code></pre>
<p>이 공식은 가중치가 균등하게 분포되고, 가져올 수 있으며, 좋은 기울기 흐름을 유지할 수 있도록 보장합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>정상 분포에 대한 정보입니다:</p>
<p><img src="/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_4.png" alt=""></p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">glorot_normal</span>(self, fan_in, fan_out):
    stddev = np.<span class="hljs-title function_">sqrt</span>(<span class="hljs-number">2.</span> / (fan_in + fan_out))
    <span class="hljs-keyword">return</span> np.<span class="hljs-property">random</span>.<span class="hljs-title function_">normal</span>(<span class="hljs-number">0.</span>, stddev, size=(fan_in, fan_out))

weights = self.<span class="hljs-title function_">glorot_normal</span>(layers[i - <span class="hljs-number">1</span>], layers[i])
</code></pre>
<p>이 조정은 시그모이드 활성화 함수를 사용하는 네트워크에서 적절하게 가중치를 유지합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>장점: 합리적인 범위 내 그라디언트 변화를 유지하여 심층 신경망의 안정성을 향상시킵니다.</p>
<p>단점: ReLU(또는 변형) 활성화를 사용하는 레이어에는 신호 전파 특성이 다르기 때문에 최적이 아닐 수 있습니다.</p>
<p>He 초기화
He 초기화는 ReLU 활성화 함수를 사용하는 레이어에 적합하게 설계되었으며, ReLU의 비선형 특성을 고려하여 가중치의 분산을 조정합니다. 이 전략은 특히 ReLU가 일반적으로 사용되는 깊은 신경망에서 그라디언트 흐름을 유지하는 데 도움이 됩니다.</p>
<p>Glorot 초기화와 마찬가지로, 가중치는 균등 분포 또는 정규 분포에서 선택할 수 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>균일 분포를 위해 가중치는 [-a, a] 범위를 사용하여 초기화됩니다. 여기서 a는 다음과 같이 계산됩니다:</p>
<p><img src="/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_5.png" alt="a 계산 공식"></p>
<p>따라서 가중치 W는 균일 분포에서 추출됩니다:</p>
<p><img src="/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_6.png" alt="균일 분포에서 가중치 추출 공식"></p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">def <span class="hljs-title function_">he_uniform</span>(self, fan_in, fan_out):
    limit = np.<span class="hljs-title function_">sqrt</span>(<span class="hljs-number">2</span> / fan_in)
    <span class="hljs-keyword">return</span> np.<span class="hljs-property">random</span>.<span class="hljs-title function_">uniform</span>(-limit, limit, (fan_in, fan_out))

weights = self.<span class="hljs-title function_">he_uniform</span>(layers[i - <span class="hljs-number">1</span>], layers[i])
</code></pre>
<p>일반 분포를 사용할 때, 가중치는 다음과 같은 수식에 따라 초기화됩니다:</p>
<p><img src="/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_7.png" alt="수식"></p>
<p>여기서 W는 가중치를, N은 정규 분포를, 0은 분포의 평균을, 그리고 2/n은 분산을 나타냅니다. n-in은 레이어로 들어오는 입력 단위의 수를 나타냅니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">he_normal</span>(<span class="hljs-params">self, fan_in, fan_out</span>):
    stddev = np.sqrt(<span class="hljs-number">2.</span> / fan_in)
    <span class="hljs-keyword">return</span> np.random.normal(<span class="hljs-number">0.</span>, stddev, size=(fan_in, fan_out))

weights = self.he_normal(layers[i - <span class="hljs-number">1</span>], layers[i])
</code></pre>
<p>양쪽 경우 모두 초기화 전략은 ReLU 활성화 함수의 특성을 반영하려고 합니다. 이 함수는 양수 입력에 대해 비활성화된 뉴런을 가지기 때문에 초기 가중치의 분산 조정은 깊은 네트워크에서 발생할 수 있는 그래디언트의 소실 또는 폭발을 방지하고 더 안정적이고 효율적인 훈련 과정을 촉진합니다.</p>
<p>장점: ReLU 활성화 함수를 사용하는 네트워크에서 그래디언트 크기를 유지하여 깊은 학습 모델을 용이하게 학습시킵니다.</p>
<p>단점: 특히 ReLU에 최적화되어 있어 다른 활성화 함수만큼 효과적이지 않을 수 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이제 초기화를 소개한 후 NeuralNetwork 클래스가 어떻게 보이는지 살펴보겠습니다:</p>
<pre><code class="hljs language-js">클래스 <span class="hljs-title class_">NeuralNetwork</span>:
    def <span class="hljs-title function_">__init__</span>(self,
                 layers,
                 init_method=<span class="hljs-string">'glorot_uniform'</span>, # <span class="hljs-string">'zeros'</span>, <span class="hljs-string">'random'</span>, <span class="hljs-string">'glorot_uniform'</span>, <span class="hljs-string">'glorot_normal'</span>, <span class="hljs-string">'he_uniform'</span>, <span class="hljs-string">'he_normal'</span>
                 loss_func=<span class="hljs-string">'mse'</span>,
                 ):
        ...

        self.<span class="hljs-property">init_method</span> = init_method

        # 레이어 초기화
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-title function_">len</span>(layers) - <span class="hljs-number">1</span>):
            <span class="hljs-keyword">if</span> self.<span class="hljs-property">init_method</span> == <span class="hljs-string">'zeros'</span>:
                weights = np.<span class="hljs-title function_">zeros</span>((layers[i], layers[i + <span class="hljs-number">1</span>]))
            elif self.<span class="hljs-property">init_method</span> == <span class="hljs-string">'random'</span>:
                weights = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">randn</span>(layers[i], layers[i + <span class="hljs-number">1</span>])
            elif self.<span class="hljs-property">init_method</span> == <span class="hljs-string">'glorot_uniform'</span>:
                weights = self.<span class="hljs-title function_">glorot_uniform</span>(layers[i], layers[i + <span class="hljs-number">1</span>])
            elif self.<span class="hljs-property">init_method</span> == <span class="hljs-string">'glorot_normal'</span>:
                weights = self.<span class="hljs-title function_">glorot_normal</span>(layers[i], layers[i + <span class="hljs-number">1</span>])
            elif self.<span class="hljs-property">init_method</span> == <span class="hljs-string">'he_uniform'</span>:
                weights = self.<span class="hljs-title function_">he_uniform</span>(layers[i], layers[i + <span class="hljs-number">1</span>])
            elif self.<span class="hljs-property">init_method</span> == <span class="hljs-string">'he_normal'</span>:
                weights = self.<span class="hljs-title function_">he_normal</span>(layers[i], layers[i + <span class="hljs-number">1</span>])

            <span class="hljs-attr">else</span>:
                raise <span class="hljs-title class_">ValueError</span>(f<span class="hljs-string">'알 수없는 초기화 방법 {self.init_method}'</span>)

            self.<span class="hljs-property">layers</span>.<span class="hljs-title function_">append</span>({
                <span class="hljs-string">'weights'</span>: weights,
                <span class="hljs-string">'biases'</span>: np.<span class="hljs-title function_">zeros</span>((<span class="hljs-number">1</span>, layers[i + <span class="hljs-number">1</span>]))
            })

        ...

    ...

    def <span class="hljs-title function_">glorot_uniform</span>(self, fan_in, fan_out):
        limit = np.<span class="hljs-title function_">sqrt</span>(<span class="hljs-number">6</span> / (fan_in + fan_out))
        <span class="hljs-keyword">return</span> np.<span class="hljs-property">random</span>.<span class="hljs-title function_">uniform</span>(-limit, limit, (fan_in, fan_out))

    def <span class="hljs-title function_">he_uniform</span>(self, fan_in, fan_out):
        limit = np.<span class="hljs-title function_">sqrt</span>(<span class="hljs-number">2</span> / fan_in)
        <span class="hljs-keyword">return</span> np.<span class="hljs-property">random</span>.<span class="hljs-title function_">uniform</span>(-limit, limit, (fan_in, fan_out))

    def <span class="hljs-title function_">glorot_normal</span>(self, fan_in, fan_out):
        stddev = np.<span class="hljs-title function_">sqrt</span>(<span class="hljs-number">2.</span> / (fan_in + fan_out))
        <span class="hljs-keyword">return</span> np.<span class="hljs-property">random</span>.<span class="hljs-title function_">normal</span>(<span class="hljs-number">0.</span>, stddev, size=(fan_in, fan_out))

    def <span class="hljs-title function_">he_normal</span>(self, fan_in, fan_out):
        stddev = np.<span class="hljs-title function_">sqrt</span>(<span class="hljs-number">2.</span> / fan_in)
        <span class="hljs-keyword">return</span> np.<span class="hljs-property">random</span>.<span class="hljs-title function_">normal</span>(<span class="hljs-number">0.</span>, stddev, size=(fan_in, fan_out))

    ...
</code></pre>
<p>적절한 가중치 초기화 전략을 선택하는 것은 효과적인 신경망 학습에 중요합니다. 무작위 및 영점 초기화는 기본적인 접근법을 제공하지만 항상 최적의 학습 동역학을 이끌어내지 않을 수 있습니다. 반면, Glorot/Xavier 및 He 초기화는 신경망 아키텍처 및 사용된 활성화 함수를 고려하여 딥 러닝 모델의 특정 요구 사항을 고려하는 더 소박한 솔루션을 제공합니다. 이러한 전략은 너무 빠른 학습과 너무 느린 학습 사이의 절충안을 균형있게 맞추어 더 신뢰할 수 있는 수렴 방향으로 학습 프로세스를 이끕니다.</p>
<h2>3.4: 드롭아웃</h2>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Dropout은 신경망에서 오버피팅을 방지하기 위해 설계된 정규화 기술로, 훈련 단계에서 네트워크에서 임시로 그리고 무작위로 유닛(뉴런)과 해당 연결을 제거함으로써 사용합니다. 이 방법은 Srivastava 및 그 동료들이 2014 년 논문에서 고안한 간단하면서도 효과적인 방법으로 견고한 신경망을 훈련하는 데 사용됩니다.</p>
<p><img src="/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_8.png" alt="이미지"></p>
<p>각 훈련 반복에서 각 뉴런(입력 단위 포함되지만 보통 출력 단위는 제외)은 일시적으로 "드랍아웃"될 확률 p를 가집니다. 이는 해당 뉴런이 이 전방 및 역방향 패스 동안 완전히 무시된다는 것을 의미합니다. 이 확률 p은 "드랍아웃 비율"로 불리며 성능을 최적화하기 위해 조절할 수 있는 하이퍼파라미터입니다. 예를 들어, 0.5의 드랍아웃 비율은 각 뉴런이 각 훈련 패스에서 계산에서 제외될 확률이 50% 라는 것을 의미합니다.</p>
<p>이 과정의 효과는 네트워크가 개별 뉴런의 특정 가중치에 덜 민감해진다는 것입니다. 이것은 예측을 할 때 개별 뉴런의 출력에 의존할 수 없으므로 네트워크가 뉴런들 사이에 중요성을 분산시키도록 장려합니다. 이는 실제로 가중치를 공유하는 신경망의 의사앙상블을 훈련하며, 각 훈련 반복에서 네트워크의 다른 "드랍아웃된" 버전이 포함됩니다. 시험 시간에는 드랍아웃이 적용되지 않고, 대신 가중치는 일반적으로 드랍아웃 비율 p에 의해 조정되어 더 많은 유닛이 활성화되었다는 사실을 균형 있게 합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>올바른 드롭아웃 비율 선택하기
드롭아웃 비율은 각 신경망 구조와 데이터셋에 대해 조정이 필요한 하이퍼파라미터입니다. 일반적으로, 숨겨진 유닛에 대해 시작점으로 0.5의 비율이 사용되며, 이는 원래 드롭아웃 논문에서 제안되었습니다.</p>
<p>높은 드롭아웃 비율 (1에 가까운 값)은 학습 중에 더 많은 뉴런이 제거되는 것을 의미합니다. 이는 네트워크가 데이터를 충분히 학습하지 못할 수 있어서, 훈련 데이터의 복잡성을 모델링하는 데 어려움을 겪어 과소적합을 초래할 수 있습니다.</p>
<p>반대로, 낮은 드롭아웉 비율 (0에 가까운 값)은 더 적은 뉴런이 제거되어 드롭아웃의 정규화 효과가 줄어들 수 있으며, 이는 모델이 훈련 데이터에서 잘 수행되지만 보이지 않는 데이터에서 성능이 나빠질 수 있는 과적합을 초래할 수 있습니다.</p>
<p>코드 구현
우리 코드에서 어떻게 보이는지 살펴보겠습니다:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NeuralNetwork</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,
                 layers,
                 init_method=<span class="hljs-string">'glorot_uniform'</span>, <span class="hljs-comment"># 'zeros', 'random', 'glorot_uniform', 'glorot_normal', 'he_uniform', 'he_normal'</span>
                 loss_func=<span class="hljs-string">'mse'</span>,
                 dropout_rate=<span class="hljs-number">0.5</span>
                 </span>):
        ...

        self.dropout_rate = dropout_rate

        ...

    ...


    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, is_training=<span class="hljs-literal">True</span></span>):
        self.a = [X]
        <span class="hljs-keyword">for</span> i, layer <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.layers):
            z = np.dot(self.a[-<span class="hljs-number">1</span>], layer[<span class="hljs-string">'weights'</span>]) + layer[<span class="hljs-string">'biases'</span>]
            a = self.sigmoid(z)
            <span class="hljs-keyword">if</span> is_training <span class="hljs-keyword">and</span> i &#x3C; <span class="hljs-built_in">len</span>(self.layers) - <span class="hljs-number">1</span>:  <span class="hljs-comment"># apply dropout to all layers except the output layer</span>
                dropout_mask = np.random.rand(*a.shape) > self.dropout_rate
                a *= dropout_mask
            self.a.append(a)
        <span class="hljs-keyword">return</span> self.a[-<span class="hljs-number">1</span>]

    ...
</code></pre>
<p>저희 신경망 클래스는 새로운 초기화 매개변수와 드롭아웃 정규화를 포함한 새로운 순전파 메서드로 업그레이드되었습니다.</p>
<p>dropout_rate : 이것은 훈련 중에 신경세포들이 네트워크에서 일시적으로 제거될 가능성을 결정하는 설정입니다. 오버피팅을 피하는 데 도움이 됩니다. 0.5로 설정함으로써 어떤 신경세포가 한 번의 훈련 라운드에서 "제거"될 확률이 50%라고 말하고 있습니다. 이 무작위성은 네트워크가 어떤 단일 신경세포에 너무 의존하지 않도록 보장하여 더 견고한 학습 과정을 촉진합니다.</p>
<p>is_training 부울 플래그는 네트워크가 현재 훈련되고 있는지를 알려줍니다. 이것은 훈련 중에만 드롭아웃이 발생해야 하므로 새 데이터에 대한 네트워크 성능을 평가할 때는 드롭아웃이 일어나서는 안 된다는 점이 중요합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>네트워크를 통해 데이터(X로 표시)가 전달되면, 네트워크는 들어오는 데이터와 레이어의 편향을 가중합(z)으로 계산합니다. 그런 다음 이 합계를 시그모이드 활성화 함수를 통해 활성화(a)로 변환하여 다음 레이어로 전달할 신호를 얻습니다.</p>
<p>하지만 훈련 중에 다음 레이어로 진행하기 전에 드롭아웃을 적용할 수 있습니다:</p>
<ul>
<li>is_training이 true이고 출력 레이어를 다루고 있지 않다면, 각 뉴런에 대해 주사위를 굴려 떨어뜨릴지 여부를 확인합니다. 이를 위해 무작위 수가 드롭아웃 비율을 초과하는지 확인하여 드롭아웃 마스크(모양은 a와 같은 배열)를 생성합니다.</li>
<li>이 마스크를 사용하여 a의 일부 활성화를 0으로 만들어 네트워크에서 일시적으로 뉴런을 제거하는 것을 흉내냅니다.</li>
</ul>
<p>드롭아웃을 적용한 후(해당하는 경우), 생성된 활성화를 self.a에 추가하여 모든 레이어를 통해 활성화를 추적하는 리스트를 유지합니다. 이렇게 하면 신호를 그냥 한 레이어에서 다음 레이어로 무작정 이동시키는 것이 아니라, 네트워크가 더 견고하게 학습하도록 장려하는 기술을 적용하여 특정 경로의 뉴런에 지나치게 의존하지 않도록 합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>3.5: 그레이디언트 클리핑</h2>
<p>그레이디언트 클리핑은 깊은 신경망을 훈련할 때 중요한 기술로, 특히 폭주하는 그레이디언트 문제를 해결할 때 주로 사용됩니다. 폭주하는 그레이디언트는 신경망의 매개변수에 대한 손실 함수의 미분이나 그레이디언트가 층을 거치면서 지수적으로 증가하여 훈련 중에 가중치에 대해 매우 큰 업데이트를 유도할 때 발생합니다. 이는 학습 과정을 불안정하게 만들 수 있으며, 종종 가중치나 손실에서 NaN 값의 형태로 나타나 수치 오버플로우 때문에 발산하여 모델이 해결책으로 수렴하지 못하도록 방해할 수 있습니다.</p>
<p>그레이디언트 클리핑은 값에 의한 클리핑과 법에 의한 클리핑 두 가지 주요 방법으로 구현할 수 있으며, 각각 폭주하는 그레이디언트 문제를 완화하는 전략을 가지고 있습니다.</p>
<p>값에 의한 클리핑
이 방법은 미리 정의된 임계값을 설정하고, 각 그레이디언트 구성 요소를 해당 임계값을 초과하는 경우 지정된 범위 내로 직접 클리핑하는 접근 방식입니다. 예를 들어, 임계값이 1로 설정되면, 1보다 큰 모든 그레이디언트 구성 요소를 1로 설정하고, -1보다 작은 모든 구성 요소를 -1로 설정합니다. 이는 모든 그레이디언트가 [-1, 1] 범위 내에 유지되도록 보장하여 너무 커지는 그레이디언트를 효과적으로 방지합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_9.png">
<p>gi는 기울기 벡터의 각 구성 요소를 나타냅니다.</p>
<p>노름에 의한 클리핑
이 방법은 각 기울기 구성 요소를 개별적으로 클리핑하는 대신, 일정 임계값을 초과하는 경우 전체 기울기를 조절합니다. 이렇게 하면 기울기의 방향을 보존한 채 크기가 지정된 한도를 초과하지 않도록 할 수 있습니다. 이는 모든 매개변수를 통해 업데이트의 상대적 방향을 유지하는 데 특히 유용하며, 값에 의한 클리핑보다 학습 과정에 더 유익할 수 있습니다.</p>
<img src="/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_10.png">
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>그래디언트 벡터를 나타내는 g이고 ∥g∥는 그 노름값입니다.</p>
<p>훈련에의 응용</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NeuralNetwork</span>:
    def <span class="hljs-title function_">__init__</span>(self,
                 layers,
                 init_method=<span class="hljs-string">'glorot_uniform'</span>, # <span class="hljs-string">'zeros'</span>, <span class="hljs-string">'random'</span>, <span class="hljs-string">'glorot_uniform'</span>, <span class="hljs-string">'glorot_normal'</span>, <span class="hljs-string">'he_uniform'</span>, <span class="hljs-string">'he_normal'</span>
                 loss_func=<span class="hljs-string">'mse'</span>,
                 dropout_rate=<span class="hljs-number">0.5</span>,
                 clip_type=<span class="hljs-string">'value'</span>,
                 grad_clip=<span class="hljs-number">5.0</span>
                 ):
        ...

        self.<span class="hljs-property">clip_type</span> = clip_type
        self.<span class="hljs-property">grad_clip</span> = grad_clip

        ...

    ...

    def <span class="hljs-title function_">backward</span>(self, X, y, learning_rate):
        m = X.<span class="hljs-property">shape</span>[<span class="hljs-number">0</span>]
        self.<span class="hljs-property">dz</span> = [self.<span class="hljs-property">a</span>[-<span class="hljs-number">1</span>] - y]
        self.<span class="hljs-property">gradient_norms</span> = []  # 그래디언트 노름을 저장하는 리스트

        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">reversed</span>(<span class="hljs-title function_">range</span>(<span class="hljs-title function_">len</span>(self.<span class="hljs-property">layers</span>) - <span class="hljs-number">1</span>)):
            self.<span class="hljs-property">dz</span>.<span class="hljs-title function_">append</span>(np.<span class="hljs-title function_">dot</span>(self.<span class="hljs-property">dz</span>[-<span class="hljs-number">1</span>], self.<span class="hljs-property">layers</span>[i + <span class="hljs-number">1</span>][<span class="hljs-string">'weights'</span>].<span class="hljs-property">T</span>) * self.<span class="hljs-title function_">sigmoid_derivative</span>(self.<span class="hljs-property">a</span>[i + <span class="hljs-number">1</span>]))
            self.<span class="hljs-property">gradient_norms</span>.<span class="hljs-title function_">append</span>(np.<span class="hljs-property">linalg</span>.<span class="hljs-title function_">norm</span>(self.<span class="hljs-property">layers</span>[i + <span class="hljs-number">1</span>][<span class="hljs-string">'weights'</span>]))  # 그래디언트 노름을 계산하고 저장

        self.<span class="hljs-property">dz</span> = self.<span class="hljs-property">dz</span>[::-<span class="hljs-number">1</span>]
        self.<span class="hljs-property">gradient_norms</span> = self.<span class="hljs-property">gradient_norms</span>[::-<span class="hljs-number">1</span>]  # 리스트를 뒤집어서 레이어의 순서와 일치시킴

        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-title function_">len</span>(self.<span class="hljs-property">layers</span>)):
            grads_w = np.<span class="hljs-title function_">dot</span>(self.<span class="hljs-property">a</span>[i].<span class="hljs-property">T</span>, self.<span class="hljs-property">dz</span>[i]) / m
            grads_b = np.<span class="hljs-title function_">sum</span>(self.<span class="hljs-property">dz</span>[i], axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-title class_">True</span>) / m

            # 그래디언트 클리핑
            <span class="hljs-keyword">if</span> self.<span class="hljs-property">clip_type</span> == <span class="hljs-string">'value'</span>:
                grads_w = np.<span class="hljs-title function_">clip</span>(grads_w, -self.<span class="hljs-property">grad_clip</span>, self.<span class="hljs-property">grad_clip</span>)
                grads_b = np.<span class="hljs-title function_">clip</span>(grads_b, -self.<span class="hljs-property">grad_clip</span>, self.<span class="hljs-property">grad_clip</span>)
            elif self.<span class="hljs-property">clip_type</span> == <span class="hljs-string">'norm'</span>:
                grads_w = self.<span class="hljs-title function_">clip_by_norm</span>(grads_w, self.<span class="hljs-property">grad_clip</span>)
                grads_b = self.<span class="hljs-title function_">clip_by_norm</span>(grads_b, self.<span class="hljs-property">grad_clip</span>)

            self.<span class="hljs-property">layers</span>[i][<span class="hljs-string">'weights'</span>] -= learning_rate * grads_w
            self.<span class="hljs-property">layers</span>[i][<span class="hljs-string">'biases'</span>] -= learning_rate * grads_b

    def <span class="hljs-title function_">clip_by_norm</span>(self, grads, clip_norm):
        l2_norm = np.<span class="hljs-property">linalg</span>.<span class="hljs-title function_">norm</span>(grads)
        <span class="hljs-keyword">if</span> l2_norm > <span class="hljs-attr">clip_norm</span>:
            grads = grads / l2_norm * clip_norm
        <span class="hljs-keyword">return</span> grads

    ...
</code></pre>
<p>초기화 중에 이제 사용할 그래디언트 클리핑 유형(clip_type)과 그래디언트 클리핑 임계값(grad_clip)이 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><code>clip_type</code>은 그레디언트를 값으로 자르는 경우에는 <code>value</code>, 또는 L2 노름에 의해 그레디언트를 자르는 경우에는 <code>norm</code>이 될 수 있습니다. grad_clip은 자르는 임계값이나 한계를 지정합니다.</p>
<p>그런 다음, 역전파 중에 함수는 네트워크의 각 레이어에 대한 그레디언트를 계산합니다. 가중치(grads_w)와 편향(grads_b)에 대한 손실의 미분 값을 각 레이어마다 계산합니다.</p>
<p>만약 <code>clip_type</code>이 <code>value</code>인 경우, np.clip을 사용하여 그레디언트를 [-grad_clip, grad_clip] 범위로 자릅니다. 이렇게 하면 그레디언트 성분이 이 한계를 초과하지 않도록 합니다.</p>
<p>만약 <code>clip_type</code>이 <code>norm</code>인 경우, 그레디언트의 노름이 grad_clip을 초과하는 경우 이 방향을 유지하면서 그에 대한 크기를 제한하기 위해 clip_by_norm 메서드가 호출됩니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>클리핑 이후, 각 층의 가중치와 편향을 학습률에 의해 조정하는 데 그래디언트가 사용됩니다.</p>
<p>마지막으로, 그래디언트의 L2 노름이 지정된 clip_norm을 초과하는 경우 그래디언트를 스케일링하는 clip_by_norm 메서드를 만듭니다. 이 메서드는 그래디언트의 L2 노름을 계산하고, clip_norm보다 크면 그래디언트를 clip_norm까지 스케일 다운시키면서 방향을 유지합니다. 이는 그래디언트를 그들의 L2 노름으로 나누고 clip_norm을 곱해 달성됩니다.</p>
<p>그래디언트 클리핑의 장점
모델의 가중치에 대한 지나치게 큰 업데이트를 방지함으로써, 그래디언트 클리핑은 더 안정적이고 신뢰할 수 있는 훈련 과정에 기여합니다. 그래디언트의 계산이 큰 업데이트로 인해 불안정성을 초래할 수 있는 경우에도 손실 함수를 최소화하여 옵티마이저가 일관된 진전을 이룰 수 있도록 합니다. 이는 훈련하는 동안 그래디언트의 스케일이 큰 문제로 인해 불안정성 문제에 직면하는 순환 신경망(RNNs) 훈련과 같은 과제에서 특히 유용한 도구로 작용합니다.</p>
<p>그래디언트 클리핑은 신경망 훈련의 안정성과 성능을 향상시키는 간단하면서도 강력한 기술입니다. 그래디언트가 지나치게 커지지 않도록 보장함으로써, 훈련 불안정성(과적합, 과소적합, 수렴 속도 저하 등)의 문제를 피하고, 신경망이 효과적이고 효율적으로 학습하기 쉽도록 돕습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>4: 층의 최적 개수 결정하기</h1>
<p>신경망을 설계하는 중요한 결정 중 하나는 올바른 층의 개수를 결정하는 것입니다. 이 측면은 네트워크의 데이터로부터 학습하고 새로운, 보지 못한 데이터를 일반화하는 능력에 상당한 영향을 미칩니다. 신경망의 깊이 - 얼마나 많은 층이 있는지 - 능력을 강화시키거나 과적합 또는 학습이 부족하다는 문제로 이어질 수 있습니다.</p>
<h2>4.1: 층의 깊이와 모델 성능</h2>
<p>신경망에 더 많은 층을 추가하면 학습 능력이 향상되어 데이터의 더 복잡한 패턴과 관계를 파악할 수 있습니다. 이는 추가적인 층이 입력 데이터의 보다 추상적인 표현을 만들 수 있기 때문에 단순한 기능에서 더 복잡한 조합으로 이동할 수 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>더 깊은 신경망은 복잡한 패턴을 모델링할 수 있지만, 추가적인 깊이가 오버피팅으로 이어지는 기묘한 지점이 있습니다. 오버피팅은 모델이 훈련 데이터를 너무 잘 학습하여 그 잡음까지 포함해 새로운 데이터에서 성능이 나빠지는 현상입니다.</p>
<p>궁극적인 목표는 훈련 데이터로부터 잘 학습하는 모델을 갖는 것뿐만 아니라 이 학습을 새로운 데이터에서도 정확하게 수행할 수 있는 범용성을 갖는 것입니다. 이를 위해서는 층의 깊이에 대한 적절한 균형을 찾는 것이 중요합니다. 너무 적은 층은 과소적합될 수 있고, 너무 많은 층은 오버피팅될 수 있습니다.</p>
<h2>4.2: 적절한 깊이를 테스트하고 선택하는 전략</h2>
<p>점진적인 접근 방식
간단한 모델부터 시작하여 점진적으로 층을 추가하고 검증 성능이 크게 향상될 때까지 관찰합니다. 이 접근 방식은 각 층이 전체 성능에 어떤 기여를 하는지 이해하는 데 도움이 됩니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>모델의 성능을 판단하기 위한 기준으로 검증 세트(학습 중에 사용되지 않은 학습 데이터의 하위 집합)에서 모델이 일반화하는 능력을 향상시키는지 여부를 결정합니다.</p>
<p>정규화 기법
더 많은 레이어를 추가할 때 드롭아웃 또는 L2 정규화와 같은 정규화 방법을 사용하세요. 이러한 기법은 오버피팅의 위험을 줄일 수 있어 추가된 레이어가 모델의 학습 능력에 어떤 가치를 더하는지를 공정하게 평가할 수 있게 해줍니다.</p>
<p>학습 동태 관찰
더 많은 레이어를 추가할 때 학습과 검증 손실을 모니터링하세요. 이 두 지표 사이에 차이가 발생하는 경우 — 학습 손실이 감소하지만 검증 손실이 그렇지 않을 때 — 오버피팅을 나타낼 수 있으며, 현재 깊이가 지나칠 수 있다는 것을 시사할 수 있습니다.</p>
<p><img src="/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_11.png" alt="이미지"></p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이 두 그래프는 기계 학습 모델을 훈련하는 과정에서 발생할 수 있는 두 가지 시나리오를 나타냅니다.</p>
<p>첫 번째 그래프에서는 훈련 손실과 검증 손실이 모두 감소하여 비슷한 값으로 수렴합니다. 이것은 이상적인 시나리오로, 모델이 잘 학습하고 적절하게 일반화되고 있음을 나타냅니다. 모델의 성능이 훈련 데이터와 보지 않은 검증 데이터 모두에서 향상되고 있는 것을 의미합니다. 이는 모델이 데이터를 과소적합하거나 과적합하지 않고 있다는 것을 시사합니다.</p>
<p>두 번째 그래프에서는 훈련 손실은 감소하지만 검증 손실이 증가합니다. 이는 과적합의 전형적인 징후입니다. 모델이 훈련 데이터를 너무 잘 학습하여 노이즈와 이상점을 포함하고 있으며 보지 않은 데이터에 대한 일반화를 실패합니다. 결과적으로, 검증 데이터에서의 성능이 시간이 지남에 따라 악화됩니다. 이는 모델의 복잡성을 줄이거나 정규화나 드롭아웃과 같은 과적합 방지 기술을 적용해야 할 수도 있다는 것을 나타냅니다.</p>
<p>자동화 아키텍처 탐색
신경망 아키텍처 탐색(NAS) 도구나 Optuna와 같은 하이퍼파라미터 최적화 프레임워크를 활용하여 서로 다른 아키텍처를 체계적으로 탐색하십시오. 이러한 도구는 다양한 구성을 평가하고 검증 지표에서 최상의 성능을 발휘하는 구성을 선택함으로써 최적의 레이어 수를 자동화할 수 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>신경망의 최적 레이어 수를 결정하는 것은 모델의 복잡성과 학습 및 일반화 능력을 균형있게 고려하는 세심한 프로세스입니다. 레이어 추가에 체계적인 방법론을 채택하고 교차 검증을 활용하며 정칙화 기법을 통합함으로써 특정 문제에 적합한 네트워크 깊이를 결정할 수 있습니다. 이를 통해 보이지 않는 데이터에 대한 모델 성능을 최적화할 수 있습니다.</p>
<h1>5: Optuna을 활용한 자동 미세 튜닝</h1>
<p>최적 성능을 달성하기 위해 신경망을 미세 조정하는 것은 다양한 하이퍼파라미터의 섬세한 균형을 찾는 과정으로, 종종 방대한 탐색 공간 속에서 바늘을 찾는 것처럼 느껴질 수 있습니다. 이때 Optuna와 같은 자동 하이퍼파라미터 최적화 도구가 필요합니다.</p>
<h2>5.1: Optuna 소개</h2>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>옵투나는 최적화 하이퍼파라미터 선택을 자동화하기 위해 설계된 오픈소스 최적화 프레임워크입니다. 이는 가장 효율적인 신경망 모델로 이어지는 매개변수 조합을 식별하는 복잡한 작업을 간단화합니다. 옵투나는 고급 알고리즘을 활용하여 최적화 하이퍼파라미터 공간을 보다 효과적으로 탐색하여, 필요한 계산 자원과 수렴 시간을 줄입니다.</p>
<h2>5.2: 옵투나를 활용한 신경망 최적화 통합</h2>
<p>옵투나는 베이지안 최적화, 트리 구조 파르젠 추정기, 진화 알고리즘 등 다양한 전략을 활용하여 하이퍼파라미터 공간을 지능적으로 탐색합니다. 이 접근 방식을 통해 옵투나는 가장 유망한 하이퍼파라미터를 빠르게 식별하여 최적화 과정을 크게 가속화할 수 있습니다.</p>
<p>옵투나를 신경망 훈련 워크플로우에 통합하는 것은 옵투나가 최소화 또는 최대화하려는 목적 함수를 정의하는 과정을 포함합니다. 이 함수에는 일반적으로 모델 훈련 및 검증 과정이 포함되며, 목표는 검증 손실을 최소화하거나 검증 정확도를 최대화하는 것입니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>검색 공간 정의: 각 하이퍼파라미터 값 범위를 지정하여 Optuna가 탐색할 것입니다 (예: 레이어 수, 학습률, 드롭아웃 비율).</li>
<li>시험과 평가: Optuna는 모델을 훈련시키기 위해 매번 새로운 하이퍼파라미터 세트를 선택하는 시험을 진행합니다. 검증 세트에서 모델의 성능을 평가하고 이 정보를 사용하여 탐색을 안내합니다.</li>
</ul>
<h2>5.3: 실제 구현</h2>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> optuna

<span class="hljs-keyword">def</span> <span class="hljs-title function_">objective</span>(<span class="hljs-params">trial</span>):
    <span class="hljs-comment"># 하이퍼파라미터 정의</span>
    n_layers = trial.suggest_int(<span class="hljs-string">'n_layers'</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>)
    hidden_sizes = [trial.suggest_int(<span class="hljs-string">f'hidden_size_<span class="hljs-subst">{i}</span>'</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_layers)]
    dropout_rate = trial.suggest_uniform(<span class="hljs-string">'dropout_rate'</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.5</span>)  <span class="hljs-comment"># 모든 레이어에 대한 단일 드롭아웃 비율</span>
    learning_rate = trial.suggest_loguniform(<span class="hljs-string">'learning_rate'</span>, <span class="hljs-number">1e-3</span>, <span class="hljs-number">1e-1</span>)
    init_method = trial.suggest_categorical(<span class="hljs-string">'init_method'</span>, [<span class="hljs-string">'glorot_uniform'</span>, <span class="hljs-string">'glorot_normal'</span>, <span class="hljs-string">'he_uniform'</span>, <span class="hljs-string">'he_normal'</span>, <span class="hljs-string">'random'</span>])
    clip_type = trial.suggest_categorical(<span class="hljs-string">'clip_type'</span>, [<span class="hljs-string">'value'</span>, <span class="hljs-string">'norm'</span>])
    clip_value = trial.suggest_uniform(<span class="hljs-string">'clip_value'</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>)
    epochs = <span class="hljs-number">10000</span>

    layers = [input_size] + hidden_sizes + [output_size]

    <span class="hljs-comment"># 신경망 생성 및 훈련</span>
    nn = NeuralNetwork(layers=layers, loss_func=loss_func, dropout_rate=dropout_rate, init_method=init_method, clip_type=clip_type, grad_clip=clip_value)
    trainer = Trainer(nn, loss_func)
    trainer.train(X_train, y_train, X_test, y_test, epochs, learning_rate, early_stopping=<span class="hljs-literal">False</span>)

    <span class="hljs-comment"># 신경망 성능 평가</span>
    predictions = np.argmax(nn.forward(X_test), axis=<span class="hljs-number">1</span>)
    accuracy = np.mean(predictions == y_test_labels)

    <span class="hljs-keyword">return</span> accuracy

<span class="hljs-comment"># Study 객체 생성 및 목적 함수 최적화</span>
study = optuna.create_study(study_name=<span class="hljs-string">'nn_study'</span>, direction=<span class="hljs-string">'maximize'</span>)
study.optimize(objective, n_trials=<span class="hljs-number">100</span>)

<span class="hljs-comment"># 최적 하이퍼파라미터 출력</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"Best trial: <span class="hljs-subst">{study.best_trial.params}</span>"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"Best value: <span class="hljs-subst">{study.best_trial.value:<span class="hljs-number">.3</span>f}</span>"</span>)
</code></pre>
<p>Optuna 최적화 프로세스의 핵심은 목적 함수입니다. 이 함수는 시험 목표를 정의하고 각 시험에 대해 Optuna에 의해 호출됩니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><strong>Heren_layers</strong>은 신경망의 은닉층의 수이며, 1에서 10 사이를 추천합니다. 층의 수를 변화시킴으로써 얕은 네트워크와 깊은 네트워크 아키텍처를 탐색할 수 있습니다.</p>
<p><strong>hidden_sizes</strong>는 각 층의 크기(뉴런 수)를 저장하며, 32에서 128 사이의 숫자를 제안하여 모델이 다양한 용량을 탐색하게 합니다.</p>
<p><strong>dropout_rate</strong>는 균일하게 0.0(드롭아웃 없음)에서 0.5 사이를 제안하여 시험을 통해 정규화 유연성을 가능케 합니다.</p>
<p><strong>learning_rate</strong>는 로그 스케일로 1e-3에서 1e-1 사이를 제안하여, 학습률 최적화에 대한 공통적인 민감도로 인해 크기의 범위를 포괄하는 넓은 탐색 공간을 보장합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>신경망 가중치의 init_method은 일련의 일반적인 전략 중에서 선택됩니다. 이 선택은 훈련의 시작점과 수렴 동작을 영향을 줍니다.</p>
<p>clip_type과 clip_value는 그래디언트 클리핑 전략과 값으로, 값이나 노름을 기준으로 클리핑하여 폭발하는 그래디언트를 방지하는 데 도움이 됩니다.</p>
<p>그런 다음, 정의된 하이퍼파라미터를 사용하여 NeuralNetwork 인스턴스가 생성되고 훈련됩니다. 각 시행이 일정한 에포크 수동안 실행될 수 있도록 조기 중지가 비활성화되며, 일관된 비교를 보장합니다. 성능은 테스트 세트에서 모델의 예측 정확도를 기반으로 평가됩니다.</p>
<p>목적 함수와 NeuralNetwork 인스턴스가 정의된 후 Optuna 스터디로 이동할 수 있습니다. Optuna 스터디 객체는 목적 함수를 최대화(<code>maximize</code>)하는 데 사용되며, 이 문맥에서는 신경망의 정확도가 목적 함수입니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>연구는 목적 함수를 여러 번 호출합니다(n_trials=100), 매번 옵튜나 내부 최적화 알고리즘에서 제안한 다른 하이퍼파라미터 세트로 호출합니다. 옵튜나는 시험 이력에 기반하여 지능적으로 제안을 조정하여 하이퍼파라미터 공간을 효율적으로 탐색합니다.</p>
<p>이 프로세스를 통해 모든 실험에서 찾은 가장 좋은 하이퍼파라미터 세트(study.best_trial.params)와 달성한 최고 정확도(study.best_trial.value)가 생성됩니다. 이 출력은 주어진 작업에 대한 신경망의 최적 구성에 대한 통찰을 제공합니다.</p>
<h2>5.4: 혜택 및 결과</h2>
<p>옵튜나를 통합함으로써, 개발자는 하이퍼파라미터 튜닝 프로세스를 자동화할뿐만 아니라 어떻게 다른 매개변수가 모델에 영향을 미치는지에 대한 깊은 통찰을 얻을 수 있습니다. 이를 통해 수동 실험을 통해 걸릴 시간의 일부로 최적화된, 더 견고하고 정확한 신경망이 생성됩니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>옵투나의 체계적인 파라미터 조정 접근법은 신경망 개발에 새로운 수준의 정밀성과 효율성을 제공하여 개발자들이 더 높은 성능 표준을 달성하고 모델이 이룰 수 있는 한계를 뛰어넘을 수 있도록 돕습니다.</p>
<h2>5.5: 한계</h2>
<p>옵투나는 하이퍼파라미터 최적화에 강력하고 유연한 접근 방식을 제공하지만, 기계 학습 워크플로에 통합할 때 고려해야 할 몇 가지 한계점과 주의 사항이 있습니다.</p>
<p>계산 리소스
각 시도는 신경망을 처음부터 훈련해야 하므로, 특히 심층 신경망이나 대규모 데이터셋의 경우에는 계산 리소스가 많이 필요할 수 있습니다. 하이퍼파라미터 공간을 철저히 탐색하기 위해 수백 번이나 수천 번의 시도를 실행하는 것은 상당한 계산 리소스와 시간이 필요할 수 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>하이퍼파라미터 검색 공간
옵투나의 검색 효과는 검색 공간이 어떻게 정의되는지에 매우 의존합니다. 하이퍼파라미터 값의 범위가 너무 넓거나 문제와 제대로 일치하지 않으면 옵투나가 비최적 영역을 탐색하는 데 시간을 낭비할 수 있습니다. 반대로 검색 공간이 너무 좁으면 최적의 구성을 놓칠 수 있습니다.</p>
<p>하이퍼파라미터 수가 증가함에 따라 검색 공간이 기하급수적으로 증가하는데, 이를 "차원의 저주"라고 합니다. 이로 인해 옵투나가 공간을 효율적으로 탐색하고 합리적인 횟수의 시도 내에서 최적의 하이퍼파라미터를 찾는 것이 어렵다는 도전이 생길 수 있습니다.</p>
<p>평가 지표
목적 함수와 평가 지표의 선택은 최적화 결과에 상당한 영향을 미칠 수 있습니다. 모델의 성능이나 작업 목표를 적절히 포착하지 못하는 지표는 하이퍼파라미터 구성을 부적절하게 만들 수 있습니다.</p>
<p>모델의 성능 평가는 무작위 초기화, 데이터 섞기, 또는 데이터셋 내 잡음과 같은 요소로 인해 달라질 수 있습니다. 이러한 변동성은 최적화 과정에 잡음을 도입하여 결과의 신뢰성에 영향을 줄 수 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>알고리즘 제한사항
Optuna은 검색 공간을 탐색하기 위해 정교한 알고리즘을 사용하지만, 이러한 알고리즘의 효율성과 효과는 문제에 따라 다를 수 있습니다. 경우에 따라 특정 알고리즘이 지역 최적점으로 수렴하거나 하이퍼파라미터 공간의 특정 특성에 더 잘 맞도록 설정을 조정해야 할 수도 있습니다.</p>
<h1>6: 결론</h1>
<p>신경망의 세밀한 조정에 대해 심층적으로 살펴본 후에 우리가 걸어온 길을 돌아보는 좋은 시기입니다. 우리는 신경망이 어떻게 작동하는지에 대한 기본 사항부터 시작하여 그들의 성능과 효율성을 높이는 더 정교한 기술로 점진적으로 발전해왔습니다.</p>
<h2>6.1: 다음 단계</h2>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>우리는 신경망 최적화에 많은 영역을 다루었지만, 명백히 우리는 겨우 표면만 긁은 것 뿐입니다. 신경망 최적화의 영역은 방대하며 지속적으로 진화하고 있으며, 아직 탐험하지 않은 기술과 전략으로 넘쳐납니다. 다가오는 기사에서 더 심층적으로 파고들어 복잡한 신경망 구조와 더 높은 성능과 효율성을 끌어올릴 수 있는 고급 기술을 탐구할 예정입니다.</p>
<p>저희가 파헤치고자 하는 최적화 기술과 개념의 다양한 범위에는 다음과 같은 것들이 포함됩니다:</p>
<ul>
<li>배치 정규화: 입력 레이어를 정규화해 활성화를 조정하고 스케일링하여 훈련 속도를 높이고 안정성을 향상시키는 방법입니다.</li>
<li>최적화 알고리즘: SGD 및 Adam을 포함한 최적화 알고리즘은 복잡한 손실 함수의 영역을 더 효과적으로 탐색할 수 있는 도구를 제공하여 더 효율적인 훈련 주기와 더 나은 모델 성능을 보장합니다.</li>
<li>전이 학습 및 파인 튜닝: 사전 훈련된 모델을 활용하여 새로운 작업에 적응시키면 훈련 시간을 크게 단축하고 데이터가 제한적인 작업에서 모델 정확도를 향상시킬 수 있습니다.</li>
<li>신경 아키텍처 탐색(NAS): 자동화를 사용하여 신경망을 위한 최상의 아키텍처를 발견함으로써 직관적이지 않은 효율적인 모델을 발견할 수 있습니다.</li>
</ul>
<p>이러한 주제들은 단지 저희가 다루는 것 중 일부에 불과하며, 각각 고유한 이점과 도전을 제공합니다. 앞으로 나아가면서, 이러한 기술을 자세히 살펴보고, 언제 사용해야 하는지, 그들이 어떻게 작용하는지, 그리고 당신의 신경망 프로젝트에 미치는 영향에 대한 통찰력을 제공할 것을 목표로 합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>추가 자료</h1>
<ul>
<li>“Deep Learning” - Ian Goodfellow, Yoshua Bengio, Aaron Courville 저: 깊은 학습 기술과 원리에 대한 깊이 있는 개요를 제공하는 이 근본적인 문헌은 고급 신경망 구조 및 최적화 방법을 다룹니다.</li>
<li>“Neural Networks and Deep Learning: A Textbook” - Charu C. Aggarwal 저: 신경망에 대한 상세한 탐구를 제공하며, 깊은 학습과 그 응용에 중점을 둡니다. 신경망 디자인 및 최적화의 복잡한 개념을 이해하는 데 탁월한 자료입니다.</li>
</ul>
<p>여기까지 왔습니다. 축하해요! 이 기사를 즐기셨다면 좋아요를 누르고 팔로우해주시면 감사하겠습니다. 저는 정기적으로 유사한 기사를 게시할 예정이니 많은 관심 부탁드립니다. 제 목표는 가장 인기 있는 알고리즘을 다시 처음부터 만들어 머신 러닝을 모든 사람이 접근 가능하도록 하는 것입니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"딥 뉴럴 네트워크 파인튜닝의 수학적 원리","description":"","date":"2024-07-09 19:56","slug":"2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks","content":"\n![이미지](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_0.png)\n\n머신 러닝에서는 몇 가지 모델을 시도해 가장 성능이 좋은 것을 선택하고 몇 가지 설정을 조정하여 그나마 성공할 수 있을지도 모릅니다. 그러나 딥러닝은 그런 룰에 맞지 않습니다. 신경망을 실험해 본 적이 있다면, 성능이 꽤 불안정할 수 있다는 것을 눈치챌 수 있습니다. 어쩌면 로지스틱 회귀와 같이 간단한 모델이 멋진 200층 심층 신경망을 이길 수도 있습니다.\n\n이게 왜 그럴까요? 딥러닝은 우리가 가지고 있는 가장 고급 인공 지능 기술 중 하나이지만, 철저한 이해와 조심스러운 다룸이 필요합니다. 신경망을 세밀하게 조정하고, 내부 작동 방식을 파악하고, 그 사용법을 마스터하는 것이 중요합니다. 오늘은 이에 대해 자세히 살펴보겠습니다!\n\n글을 읽기 전에 Jupyter Notebook을 여시는 것을 제안합니다. 오늘 다룰 모든 코드가 담겨 있으므로 함께 따라가는 데 도움이 될 것입니다:\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n인덱스\n\n- 1: 소개\n\n  - 1.1: 기본 신경망의 발전\n  - 1.2: 복잡성으로의 길\n\n- 2: 모델 복잡성 확장\n\n  - 2.1: 레이어 추가\n\n- 3: 향상된 학습을 위한 최적화 기법\n  - 3.1: 학습률\n  - 3.2: 조기 중단 기법\n  - 3.3: 초기화 방법\n  - 3.4: 드롭아웃\n  - 3.5: 그래디언트 클리핑\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 4: 최적 레이어 수 결정하기\n\n  - 4.1: 레이어 깊이와 모델 성능\n  - 4.2: 적절한 깊이 선택을 위한 테스트 전략\n\n- 5: Optuna를 활용한 자동 세부 조정\n\n  - 5.1: Optuna 소개\n  - 5.2: 신경망 최적화를 위한 Optuna 통합\n  - 5.3: 실제 적용\n  - 5.4: 장점과 결과\n  - 5.5: 제한 사항\n\n- 6: 결론\n\n  - 6.1: 다음 단계\n\n- 추가 자료\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 1: 소개\n\n## 1.1: 기본 신경망의 발전\n\n인공 지능에 대한 최근 탐구에서 우리는 기초부터 신경망을 구축했습니다. 이 기본 모델은 오늘날 인공 지능 기술의 핵심인 신경망의 세계를 열어 주었습니다. 우리는 입력, 은닉 및 출력 레이어와 활성화 함수가 어떻게 정보를 처리하고 예측하는 데 결합되는지 간단하게 다루었습니다. 그리고 나서 우리는 컴퓨터 비전 작업을 위해 숫자 데이터셋에서 훈련된 간단한 신경망으로 이론을 실제로 적용했습니다.\n\n이제 이러한 기초 위에 계속해서 더 진보해 나갈 것입니다. 우리는 레이어를 추가하고, 초기화, 정규화 및 최적화에 대한 다양한 기술을 탐구함으로써 더 많은 복잡성을 도입할 것입니다. 물론, 이러한 수정이 우리의 신경망 성능에 어떻게 영향을 미치는지 확인하기 위해 코드를 테스트할 것입니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n제가 이전 기사를 확인하지 않으셨다면, 우리가 처음부터 신경망을 만들어본 기사를 꼭 읽어보시기를 추천합니다. 이번에는 그 작업을 기반으로 계속해서 진행할 것이며, 이미 우리가 다룬 개념에 익숙하다고 가정할게요.\n\n## 1.2: 복잡성으로의 길\n\n신경망을 기본 구성에서 더 정교한 구조로 변환하는 것은 단순히 더 많은 층이나 노드를 추가하는 것만으로는 이루어지지 않습니다. 이것은 신경망의 구조와 그 데이터를 다루는 미묘한 차이를 체화하는 세심한 작업이 필요한 미묘한 춤이죠. 더 깊게 파고들수록, 우리의 목표는 신경망의 깊이를 풍부하게 함으로써 데이터의 복잡한 패턴과 연결을 더 잘 분별하는 데 있습니다.\n\n하지만 복잡성을 높이는 것은 그리 수월한 일이 아닙니다. 우리가 도입할 때마다, 세련된 최적화 기술의 필요성이 커집니다. 이는 효과적인 학습뿐만 아니라 새로운 보이지 않는 데이터에 적응하기 위한 모델 능력에 필수적입니다. 이 안내서는 우리의 기반 신경망을 강화하는 과정을 안내해 드릴 것입니다. 우리는 신경망을 세밀하게 조정하는 정교한 전략에 대해 살펴볼 것이며, 학습 속도 조정, 조기 종료 도입, 그리고 SGD(확률적 경사 하강법)와 Adam과 같은 다양한 최적화 알고리즘을 활용하는 방법에 대해 살펴볼 것입니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n우리는 초기화 방법으로 시작하는 중요성, 오버피팅을 피하는 데 드롭아웃 사용의 장점, 그리고 클리핑 및 정규화로 네트워크의 그래디언트를 체크하여 안정성을 유지하는 것이 모델의 안정성에 얼마나 중요한지에 대해 다룰 예정입니다. 또한 학습을 향상시키기 위한 레이어의 최적 개수를 찾는 도전과정 및 불필요한 복잡성으로 빠져들지 않도록 할 것입니다.\n\n이전 게시물에서 함께 만든 Neural Network 및 Trainer 클래스를 아래에서 확인할 수 있습니다. 우리는 이를 조정하고 각 수정이 모델의 성능에 어떤 영향을 미치는지 실제로 살펴볼 것입니다:\n\n```js\nclass NeuralNetwork:\n    def __init__(self, input_size, hidden_size, output_size, loss_func='mse'):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.loss_func = loss_func\n\n        # 가중치 및 편향 초기화\n        self.weights1 = np.random.randn(self.input_size, self.hidden_size)\n        self.bias1 = np.zeros((1, self.hidden_size))\n        self.weights2 = np.random.randn(self.hidden_size, self.output_size)\n        self.bias2 = np.zeros((1, self.output_size))\n\n        # 손실 추적\n        self.train_loss = []\n        self.test_loss = []\n\n    def __str__(self):\n        return f\"Neural Network Layout:\\n입력 레이어: {self.input_size} 뉴런\\n은닉 레이어: {self.hidden_size} 뉴런\\n출력 레이어: {self.output_size} 뉴런\\n손실 함수: {self.loss_func}\"\n\n    def forward(self, X):\n        # 순방향 전파 수행\n        self.z1 = np.dot(X, self.weights1) + self.bias1\n        self.a1 = self.sigmoid(self.z1)\n        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n        if self.loss_func == 'categorical_crossentropy':\n            self.a2 = self.softmax(self.z2)\n        else:\n            self.a2 = self.sigmoid(self.z2)\n        return self.a2\n\n    def backward(self, X, y, learning_rate):\n        # 역전파 수행\n        m = X.shape[0]\n\n        # 기울기 계산\n        if self.loss_func == 'mse':\n            self.dz2 = self.a2 - y\n        elif self.loss_func == 'log_loss':\n            self.dz2 = -(y/self.a2 - (1-y)/(1-self.a2))\n        elif self.loss_func == 'categorical_crossentropy':\n            self.dz2 = self.a2 - y\n        else:\n            raise ValueError('잘못된 손실 함수')\n\n        self.dw2 = (1 / m) * np.dot(self.a1.T, self.dz2)\n        self.db2 = (1 / m) * np.sum(self.dz2, axis=0, keepdims=True)\n        self.dz1 = np.dot(self.dz2, self.weights2.T) * self.sigmoid_derivative(self.a1)\n        self.dw1 = (1 / m) * np.dot(X.T, self.dz1)\n        self.db1 = (1 / m) * np.sum(self.dz1, axis=0, keepdims=True)\n\n        # 가중치 및 편향 업데이트\n        self.weights2 -= learning_rate * self.dw2\n        self.bias2 -= learning_rate * self.db2\n        self.weights1 -= learning_rate * self.dw1\n        self.bias1 -= learning_rate * self.db1\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(self, x):\n        return x * (1 - x)\n\n    def softmax(self, x):\n        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return exps/np.sum(exps, axis=1, keepdims=True)\n\nclass Trainer:\n    def __init__(self, model, loss_func='mse'):\n        self.model = model\n        self.loss_func = loss_func\n        self.train_loss = []\n        self.val_loss = []\n\n    def calculate_loss(self, y_true, y_pred):\n        if self.loss_func == 'mse':\n            return np.mean((y_pred - y_true)**2)\n        elif self.loss_func == 'log_loss':\n            return -np.mean(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n        elif self.loss_func == 'categorical_crossentropy':\n            return -np.mean(y_true*np.log(y_pred))\n        else:\n            raise ValueError('잘못된 손실 함수')\n\n    def train(self, X_train, y_train, X_test, y_test, epochs, learning_rate):\n        for _ in range(epochs):\n            self.model.forward(X_train)\n            self.model.backward(X_train, y_train, learning_rate)\n            train_loss = self.calculate_loss(y_train, self.model.a2)\n            self.train_loss.append(train_loss)\n\n            self.model.forward(X_test)\n            test_loss = self.calculate_loss(y_test, self.model.a2)\n            self.val_loss.append(val_loss)\n```\n\n# 2: 모델 복잡성 확대\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n신경망을 더 정교하게 개선하기 위해 깊이 파고들면서, 게임을 바꾸는 전략을 발견했습니다: 레벨을 더 쌓아 복잡성을 높이는 것입니다. 이 동작은 모델을 강화하는 것뿐만 아니라, 데이터의 미묘한 변화를 더 정교하게 이해하고 해석하는 능력을 키우는 것입니다.\n\n## 2.1: 레이어 추가\n\n증가된 네트워크 깊이의 근거\n딥러닝의 핵심은 계층적 데이터 표현을 조합하는 능력에 있습니다. 더 많은 레이어를 추가함으로써, 우리는 신경망에 성장하는 복잡성의 패턴을 해체하고 이해하기 위한 도구를 제공하는 셈입니다. 간단한 형태와 질감을 인식하는 데서 시작해 데이터 속에서 더 복잡한 관계와 특징을 풀어가는 데로 발전하는 것으로 생각해보세요. 이러한 계층적 학습 접근법은 어느 정도 인간이 정보를 해석하는 방식과 유사하며, 기본적인 이해에서 복잡한 해석으로 진화합니다.\n\n더 많은 레이어를 쌓으면 네트워크의 \"학습 용량\"이 증가하여 보다 넓은 범위의 데이터 관계를 매핑하고 소화하는 능력을 갖추게 됩니다. 이를 통해 더 복잡한 작업을 처리할 수 있습니다. 하지만 마구 레이어를 추가하는 것은 아니며, 모델의 지능에 의미 있는 기여를 하지 않고 무분별하게 레이어를 추가하면 학습 과정을 혼란시키는 것이 아니라 명료하게 해야 합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n`NeuralNetwork` 클래스를 더 많은 층을 통합하는 방법 안내\n\n```js\nclass NeuralNetwork:\n    def __init__(self, layers, loss_func='mse'):\n        self.layers = []\n        self.loss_func = loss_func\n\n        # 레이어 초기화\n        for i in range(len(layers) - 1):\n            self.layers.append({\n                'weights': np.random.randn(layers[i], layers[i + 1]),\n                'biases': np.zeros((1, layers[i + 1]))\n            })\n\n        # 손실 추적\n        self.train_loss = []\n        self.test_loss = []\n\n    def forward(self, X):\n        self.a = [X]\n        for layer in self.layers:\n            self.a.append(self.sigmoid(np.dot(self.a[-1], layer['weights']) + layer['biases']))\n        return self.a[-1]\n\n    def backward(self, X, y, learning_rate):\n        m = X.shape[0]\n        self.dz = [self.a[-1] - y]\n\n        for i in reversed(range(len(self.layers) - 1)):\n            self.dz.append(np.dot(self.dz[-1], self.layers[i + 1]['weights'].T) * self.sigmoid_derivative(self.a[i + 1]))\n\n        self.dz = self.dz[::-1]\n\n        for i in range(len(self.layers)):\n            self.layers[i]['weights'] -= learning_rate * np.dot(self.a[i].T, self.dz[i]) / m\n            self.layers[i]['biases'] -= learning_rate * np.sum(self.dz[i], axis=0, keepdims=True) / m\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(self, x):\n        return x * (1 - x)\n```\n\n이 섹션에서는 신경망의 작동 방식에 중요한 조정을 가했으며, 임의의 층 수를 유연하게 지원하는 모델을 목표로했습니다. 변경된 사항은 다음과 같습니다:\n\n먼저, 이전에 각 층의 노드 수를 정의했던 self.input, self.hidden, self.output 변수를 삭제했습니다. 이제 목표는 임의의 층 수를 관리할 수 있는 다목적 모델입니다. 예를 들어, 이전에 숫자 데이터셋에 사용했던 모델인 64개의 입력 노드, 64개의 은닉 노드 및 10개의 출력 노드를 사용하는 경우, 다음과 같이 설정할 수 있습니다:\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nnn = NeuralNetwork((layers = [64, 64, 10]));\n```\n\n이제 코드가 각 레이어를 세 번씩 순환하며 다른 목적으로 사용됨을 알 수 있습니다:\n\n초기화 과정 중에는 모든 레이어의 가중치와 편향이 설정됩니다. 이 단계는 학습 프로세스를 위해 초기 매개변수로 네트워크를 준비하는 데 중요합니다.\n\n순방향 패스 동안 활성화 self.a는 리스트에 수집됩니다. 입력 레이어의 활성화(본질적으로 입력 데이터 X)로 시작합니다. 각 레이어에 대해, np.dot(self.a[-1], layer['weights']) + layer['biases']를 사용하여 가중치 합과 편향을 계산하고 시그모이드 활성화 함수를 적용하여 결과를 self.a에 첨부합니다. 네트워크의 결과는 self.a의 마지막 요소로, 최종 출력을 나타냅니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n역 전파 동안, 이 단계는 마지막 레이어의 활성화에 대한 손실의 도함수를 계산하고 출력 레이어의 오차 목록을 준비함으로써 시작합니다 (self.dz). 그런 다음 네트워크를 역방향으로 거슬러 올라가며 (reversed(range(len(self.layers) - 1))를 사용하여), 숨은 레이어에 대한 오차 항목을 계산합니다. 이 과정은 현재 오차 항목을 다음 레이어의 가중치와 점곱(역방향)하여 시그모이드 함수의 도함수로 비선형성을 처리하는 작업을 포함합니다.\n\n```python\nclass Trainer:\n    ...\n    def train(self, X_train, y_train, X_test, y_test, epochs, learning_rate):\n        for _ in range(epochs):\n            self.model.forward(X_train)\n            self.model.backward(X_train, y_train, learning_rate)\n            train_loss = self.calculate_loss(y_train, self.model.a[-1])\n            self.train_loss.append(train_loss)\n\n            self.model.forward(X_test)\n            test_loss = self.calculate_loss(y_test, self.model.a[-1])\n            self.test_loss.append(test_loss)\n```\n\n마지막으로, NeuralNetwork 클래스의 변화에 따라 Trainer 클래스를 업데이트했습니다. 주요한 수정 사항은 특히 train 메서드에 있으며, 네트워크의 출력이 이제 self.model.a[-1]에서 가져온다는 점 때문에 훈련 및 테스트 손실을 다시 계산하는 방식입니다.\n\n이러한 수정 사항은 우리의 신경망을 다양한 아키텍처에 적응할 수 있도록 할뿐만 아니라 데이터와 그래디언트의 흐름을 이해하는 중요성을 강조합니다. 구조를 간소화함으로써, 각종 작업에서 네트워크의 성능을 실험하고 최적화할 수 있는 능력을 향상시킵니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 3: 향상된 학습을 위한 최적화 기법\n\n신경망을 최적화하는 것은 그들이 배우는 능력을 향상시키고 효율적인 학습을 보장하며 최상의 버전으로 이끄는 데 중요합니다. 저희 모델이 얼마나 잘 수행되는지에 상당한 영향을 미치는 몇 가지 중요한 최적화 기술에 대해 알아보겠습니다.\n\n## 3.1: 학습률\n\n학습률은 손실 경사에 기반하여 네트워크의 가중치를 조정하는 제어 장치입니다. 이는 모델이 학습하는 속도를 결정하며 최적화 중에 취하는 단계가 얼마나 큰지 작은지를 결정합니다. 학습률을 적절하게 설정하면 모델이 빠르게 낮은 오차의 해결책을 찾을 수 있습니다. 그러나 올바르게 설정하지 않으면 모델이 수렴하는 데 시간이 오래 걸리거나 아예 좋은 해결책을 찾지 못할 수 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n너무 높은 학습률을 설정하면 모델이 최적해를 뛰어넘을 수 있어 불안정한 행동을 일으킬 수 있습니다. 이는 정확도나 손실이 훈련 중에 급격하게 변하는 것으로 나타날 수 있어요.\n\n학습률이 너무 낮으면 훈련 과정이 지나치게 느리게 진행될 수 있어요. 이 경우, 훈련 손실이 시간이 지남에 따라 거의 변하지 않는 것을 볼 수 있어요.\n\n관건은 훈련 및 검증 손실을 추적하면서 학습률이 어떻게 작동하는지에 대한 단서를 얻는 것이에요. 훈련 중에 일정 간격으로 이러한 손실을 기록하고 나중에 이를 플로팅하여 손실 landscape가 얼마나 매끄럽거나 불안정한지 보다 명확히 파악할 수 있어요. 우리의 코드에서는 이러한 메트릭을 추적하기 위해 Python의 logging 라이브러리를 사용하고 있어요. 이렇게 생겼답니다:\n\n```python\nimport logging\n# Logger 설정\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass Trainer:\n    ...\n    def train(self, X_train, y_train, X_val, y_val, epochs, learning_rate):\n        for epoch in range(epochs):\n            ...\n            # 50 에폭마다 손실 및 검증 손실을 로그로 남깁니다\n            if epoch % 50 == 0:\n                logger.info(f'에폭 {epoch}: 손실 = {train_loss}, 검증 손실 = {val_loss}')\n```\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n시작할 때, 우리는 훈련 업데이트를 캡처하고 표시하기 위해 로거를 설정했습니다. 이 설정을 통해 우리는 훈련 및 검증 손실을 매 50번째 에포크마다 기록하여 모델의 진행 상황에 대한 안정적인 피드백을 받을 수 있습니다. 이 피드백을 통해 손실이 잘 감소하고 있는지, 아니면 너무 불규칙하게 변동하는지 파악할 수 있어서 학습률을 조정해야 할 필요가 있을지도 모릅니다.\n\n위의 코드는 훈련 및 검증 손실을 그래프로 플로팅하여 훈련 중에 손실이 어떻게 변화하는지 더 잘 이해할 수 있도록 해줍니다. 많은 반복에서 약간의 잡음이 예상되므로 부드러운(스무딩) 효과를 추가했습니다. 잡음을 부드럽게 처리하여 그래프를 더 잘 분석할 수 있도록 도와줄 것입니다.\n\n이러한 방식을 따르면 훈련을 시작하면 로그가 나타나면서 우리의 진행 상황을 한 눈에 볼 수 있고 조정할 수 있는 정보를 제공하여 우리가 방향을 수정하는 데 도움이 될 것입니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_1.png\" /\u003e\n\n그런 다음, 훈련이 끝난 후 손실을 그래프로 그려볼 수 있습니다:\n\n\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_2.png\" /\u003e\n\n훈련 및 검증 손실이 꾸준히 감소하는 것을 보는 것은 좋은 신호입니다. 이는 에포크 수를 늘리고 학습률 스텝 크기를 증가시킨다면 잘 작동할 수 있다는 신호일 수 있습니다. 그러나 반대로 손실이 감소한 후 급상승하는 것을 관찰하면, 학습률 스텝 크기를 줄이는 것이 명백한 신호입니다. 그렇지만 재미있는 점이 있습니다: 에포크 0부터 50까지 우리의 손실이 어떤 이상한 일이 일어나고 있습니다. 우리는 그 부분을 확인하기 위해 다시 살펴보겠습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그 달콤한 학습률의 최적값을 찾기 위해서는 학습률 앤달링 또는 적응형 학습률 기법과 같은 방법이 정말 유용할 수 있어요. 이러한 방법들은 학습률을 실시간으로 세밀하게 조정하여 훈련 중에 최적의 속도를 유지하도록 도와줘요.\n\n## 3.2: 조기 중단 기법\n\n조기 중단은 마치 안전망 같아요 — 유효성 검사 세트에서 모델의 성능을 보고, 더 이상 성능이 개선되지 않을 때 훈련을 중지하는 것이에요. 이는 과적합에 대한 안전 장치이며, 우리 모델이 이전에 본 적 없는 데이터에서도 잘 작동하도록 보장해줘요.\n\n여기에 이를 실행하는 방법이 있어요:\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 검증 세트: 교육 데이터의 일부를 분리하여 검증 세트로 사용합니다. 이것은 중요합니다. 왜냐하면 이렇게 하면 멈춤 결정이 신선한 보이지 않는 데이터에 기반하기 때문입니다.\n- 모니터링: 각 학습 에포크 후 모델이 검증 세트에서 어떻게 수행되는지 주시하세요. 성능이 향상되고 있나요, 아니면 정체되었나요?\n- 멈춤 기준: 멈출 시점을 결정하세요. 일반적으로 \"연속적으로 50번의 에포크 동안 검증 손실이 향상되지 않음\"이 있습니다.\n\n이를 위한 코드를 살펴보죠:\n\n```python\nclass Trainer:\n    def train(self, X_train, y_train, X_val, y_val, epochs, learning_rate,\n              early_stopping=True, patience=10):\n        best_loss = np.inf\n        epochs_no_improve = 0\n\n        for epoch in range(epochs):\n           ...\n\n            # 조기 중단\n            if early_stopping:\n                if val_loss \u003c best_loss:\n                    best_loss = val_loss\n                    best_weights = [layer['weights'] for layer in self.model.layers]\n                    epochs_no_improve = 0\n                else:\n                    epochs_no_improve += 1\n\n                if epochs_no_improve == patience:\n                    print('조기 중단!')\n                    # 최적의 가중치로 복원\n                    for i, layer in enumerate(self.model.layers):\n                        layer['weights'] = best_weights[i]\n                    break\n```\n\ntrain 메서드에서 두 가지 새로운 옵션을 소개했습니다:\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- early_stopping: 이는 조기 중단을 켜거나 끄는 여부를 결정하는 이진 플래그입니다.\n- patience: 이는 훈련을 중단하기 전에 유효성 검사 손실이 향상되지 않은 라운드 수를 설정합니다.\n\n우리는 가장 낮은 유효성 검사 손실을 현재까지 본 최저치로 설정하기 위해 best_loss를 무한대로 설정합니다. 한편, epochs_no_improve는 얼마 동안 유효성 검사 손실이 개선되지 않은 에포크 수를 기록합니다.\n\n모델을 훈련하기 위해 각 에포크를 순회하는 동안에는 실제 훈련 단계(순방향 전파 및 역전파)가 여기에 자세히 나와 있지는 않지만 프로세스의 중요한 부분입니다.\n\n매 에포크가 끝날 때마다 현재 에포크의 유효성 검사 손실(val_loss)이 best_loss보다 낮아졌다면, 이는 우리가 진전을 이루고 있다는 뜻입니다. 우리는 best_loss를 이 새로운 최솟값으로 업데이트하고, 또한 현재 모델 가중치를 best_weights로 저장합니다. 이렇게 하면 모델이 최상의 성능을 발휘한 시점의 스냅샷을 항상 가지게 됩니다. 그리고 우리는 방금 개선을 보았기 때문에 epochs_no_improve 카운트를 다시 0으로 재설정합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n만약 val_loss에 감소가 없다면, epochs_no_improve를 하나씩 증가시켜서 다른 epoch가 향상되지 않은 것으로 표시합니다.\n\n만약 우리가 설정한 인내심 한계치에 epochs_no_improve 카운트가 달성하면, 모델이 더 나아질 가능성이 낮다는 신호로 조기 종료를 시작합니다. 메시지와 함께 알림을 표시하고, 모델의 가중치를 최적의 가중치인 best_weights로 되돌립니다. 그런 다음 학습 루프를 종료합니다.\n\n이 접근 방식은 학습을 중단하는 균형있는 방법을 제공합니다. 모델에 학습의 공정한 기회를 제공하여 너무 일찍 중단하지 않으면서, 너무 늦지도 않아 시간을 낭비하거나 과적합의 위험을 가져올 수 있습니다.\n\n## 3.3: 초기화 방법\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n신경망을 설정할 때, 가중치를 어떻게 시작하느냐에 따라 네트워크가 얼마나 잘, 그리고 얼마나 빨리 학습하는지가 달라질 수 있어요. 가중치를 초기화하는 몇 가지 다른 방법 – 랜덤, 영, Glorot(Xavier), 그리고 He 초기화 – 에 대해 알아봐요.\n\n랜덤 초기화\n랜덤 방식을 선택하면 주로 균일하거나 정규 분포에서 숫자를 추출하여 초기 가중치를 설정하는 것을 의미해요. 이러한 무작위성은 모든 뉴런이 동일한 위치에서 시작하지 않도록하여 네트워크가 학습함에 따라 서로 다른 것을 배울 수 있도록 도와줘요. 핵심은 적절한 분산을 선택하는 것인데, 너무 많으면 기울기가 폭발할 위험이 있고, 너무 적으면 사라질 수도 있어요.\n\n```js\nweights = np.random.randn(layers[i], layers[i + 1]);\n```\n\n이 코드 라인은 표준 정규 분포에서 가중치를 추출하여 각 뉴런이 학습의 길로 나아갈 수 있도록 준비를 합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n장점: 뉴런이 서로 모방하는 것을 방지하는 간단한 방법입니다.\n\n단점: 분산을 잘못 설정하면 학습 과정이 불안정해질 수 있습니다.\n\n제로 초기화\n모든 가중치를 0으로 설정하는 방법은 매우 간단합니다. 그러나 이 방법에는 주요 단점이 있습니다: 이로 인해 층의 모든 뉴런이 사실상 동일해집니다. 이러한 동일성으로 인해 네트워크의 학습이 저해될 수 있으며, 모든 층의 뉴런이 학습 중에 동일하게 업데이트될 수 있습니다.\n\n```js\nweights = np.zeros((layers[i], layers[i + 1]));\n```\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n마지막으로, 우리는 모두 0으로 채워진 가중치 행렬을 얻습니다. 깔끔하고 정돈되어 있지만, 네트워크를 통해 나아가는 모든 경로가 처음에는 동일한 가중치를 갖게 되어 학습 다양성을 위한 좋지 않은 결과를 초래할 수 있습니다.\n\n장점: 구현이 매우 쉽습니다.\n\n단점: 학습 과정을 제약시켜 네트워크의 성능이 보통 좋지 않게끔 만듭니다.\n\nGlorot 초기화\n시그모이드 활성화 함수를 사용하는 네트워크를 위해 설계된 Glorot 초기화는 네트워크 내 입력 단위와 출력 단위의 수에 기반하여 가중치를 설정합니다. 이 초기화는 활성화와 역전파된 그래디언트의 분산을 유지하고 vanishing 또는 exploding 그래디언트 문제를 방지하기 위해 레이어를 통해 전달됩니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n글로럿 초기화에서의 가중치는 균일 분포나 정규 분포로 생성할 수 있습니다. 균일 분포를 사용하는 경우, 가중치는 [-a, a] 범위로 초기화됩니다. 여기서 a 값은:\n\n![식](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_3.png)\n\n```js\ndef glorot_uniform(self, fan_in, fan_out):\n    limit = np.sqrt(6 / (fan_in + fan_out))\n    return np.random.uniform(-limit, limit, (fan_in, fan_out))\n\nweights = glorot_uniform(layers[i - 1], layers[i])\n```\n\n이 공식은 가중치가 균등하게 분포되고, 가져올 수 있으며, 좋은 기울기 흐름을 유지할 수 있도록 보장합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n정상 분포에 대한 정보입니다:\n\n![](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_4.png)\n\n```js\ndef glorot_normal(self, fan_in, fan_out):\n    stddev = np.sqrt(2. / (fan_in + fan_out))\n    return np.random.normal(0., stddev, size=(fan_in, fan_out))\n\nweights = self.glorot_normal(layers[i - 1], layers[i])\n```\n\n이 조정은 시그모이드 활성화 함수를 사용하는 네트워크에서 적절하게 가중치를 유지합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n장점: 합리적인 범위 내 그라디언트 변화를 유지하여 심층 신경망의 안정성을 향상시킵니다.\n\n단점: ReLU(또는 변형) 활성화를 사용하는 레이어에는 신호 전파 특성이 다르기 때문에 최적이 아닐 수 있습니다.\n\nHe 초기화\nHe 초기화는 ReLU 활성화 함수를 사용하는 레이어에 적합하게 설계되었으며, ReLU의 비선형 특성을 고려하여 가중치의 분산을 조정합니다. 이 전략은 특히 ReLU가 일반적으로 사용되는 깊은 신경망에서 그라디언트 흐름을 유지하는 데 도움이 됩니다.\n\nGlorot 초기화와 마찬가지로, 가중치는 균등 분포 또는 정규 분포에서 선택할 수 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n균일 분포를 위해 가중치는 [-a, a] 범위를 사용하여 초기화됩니다. 여기서 a는 다음과 같이 계산됩니다:\n\n![a 계산 공식](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_5.png)\n\n따라서 가중치 W는 균일 분포에서 추출됩니다:\n\n![균일 분포에서 가중치 추출 공식](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_6.png)\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ndef he_uniform(self, fan_in, fan_out):\n    limit = np.sqrt(2 / fan_in)\n    return np.random.uniform(-limit, limit, (fan_in, fan_out))\n\nweights = self.he_uniform(layers[i - 1], layers[i])\n```\n\n일반 분포를 사용할 때, 가중치는 다음과 같은 수식에 따라 초기화됩니다:\n\n![수식](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_7.png)\n\n여기서 W는 가중치를, N은 정규 분포를, 0은 분포의 평균을, 그리고 2/n은 분산을 나타냅니다. n-in은 레이어로 들어오는 입력 단위의 수를 나타냅니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```python\ndef he_normal(self, fan_in, fan_out):\n    stddev = np.sqrt(2. / fan_in)\n    return np.random.normal(0., stddev, size=(fan_in, fan_out))\n\nweights = self.he_normal(layers[i - 1], layers[i])\n```\n\n양쪽 경우 모두 초기화 전략은 ReLU 활성화 함수의 특성을 반영하려고 합니다. 이 함수는 양수 입력에 대해 비활성화된 뉴런을 가지기 때문에 초기 가중치의 분산 조정은 깊은 네트워크에서 발생할 수 있는 그래디언트의 소실 또는 폭발을 방지하고 더 안정적이고 효율적인 훈련 과정을 촉진합니다.\n\n장점: ReLU 활성화 함수를 사용하는 네트워크에서 그래디언트 크기를 유지하여 깊은 학습 모델을 용이하게 학습시킵니다.\n\n단점: 특히 ReLU에 최적화되어 있어 다른 활성화 함수만큼 효과적이지 않을 수 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이제 초기화를 소개한 후 NeuralNetwork 클래스가 어떻게 보이는지 살펴보겠습니다:\n\n```js\n클래스 NeuralNetwork:\n    def __init__(self,\n                 layers,\n                 init_method='glorot_uniform', # 'zeros', 'random', 'glorot_uniform', 'glorot_normal', 'he_uniform', 'he_normal'\n                 loss_func='mse',\n                 ):\n        ...\n\n        self.init_method = init_method\n\n        # 레이어 초기화\n        for i in range(len(layers) - 1):\n            if self.init_method == 'zeros':\n                weights = np.zeros((layers[i], layers[i + 1]))\n            elif self.init_method == 'random':\n                weights = np.random.randn(layers[i], layers[i + 1])\n            elif self.init_method == 'glorot_uniform':\n                weights = self.glorot_uniform(layers[i], layers[i + 1])\n            elif self.init_method == 'glorot_normal':\n                weights = self.glorot_normal(layers[i], layers[i + 1])\n            elif self.init_method == 'he_uniform':\n                weights = self.he_uniform(layers[i], layers[i + 1])\n            elif self.init_method == 'he_normal':\n                weights = self.he_normal(layers[i], layers[i + 1])\n\n            else:\n                raise ValueError(f'알 수없는 초기화 방법 {self.init_method}')\n\n            self.layers.append({\n                'weights': weights,\n                'biases': np.zeros((1, layers[i + 1]))\n            })\n\n        ...\n\n    ...\n\n    def glorot_uniform(self, fan_in, fan_out):\n        limit = np.sqrt(6 / (fan_in + fan_out))\n        return np.random.uniform(-limit, limit, (fan_in, fan_out))\n\n    def he_uniform(self, fan_in, fan_out):\n        limit = np.sqrt(2 / fan_in)\n        return np.random.uniform(-limit, limit, (fan_in, fan_out))\n\n    def glorot_normal(self, fan_in, fan_out):\n        stddev = np.sqrt(2. / (fan_in + fan_out))\n        return np.random.normal(0., stddev, size=(fan_in, fan_out))\n\n    def he_normal(self, fan_in, fan_out):\n        stddev = np.sqrt(2. / fan_in)\n        return np.random.normal(0., stddev, size=(fan_in, fan_out))\n\n    ...\n```\n\n적절한 가중치 초기화 전략을 선택하는 것은 효과적인 신경망 학습에 중요합니다. 무작위 및 영점 초기화는 기본적인 접근법을 제공하지만 항상 최적의 학습 동역학을 이끌어내지 않을 수 있습니다. 반면, Glorot/Xavier 및 He 초기화는 신경망 아키텍처 및 사용된 활성화 함수를 고려하여 딥 러닝 모델의 특정 요구 사항을 고려하는 더 소박한 솔루션을 제공합니다. 이러한 전략은 너무 빠른 학습과 너무 느린 학습 사이의 절충안을 균형있게 맞추어 더 신뢰할 수 있는 수렴 방향으로 학습 프로세스를 이끕니다.\n\n## 3.4: 드롭아웃\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nDropout은 신경망에서 오버피팅을 방지하기 위해 설계된 정규화 기술로, 훈련 단계에서 네트워크에서 임시로 그리고 무작위로 유닛(뉴런)과 해당 연결을 제거함으로써 사용합니다. 이 방법은 Srivastava 및 그 동료들이 2014 년 논문에서 고안한 간단하면서도 효과적인 방법으로 견고한 신경망을 훈련하는 데 사용됩니다.\n\n![이미지](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_8.png)\n\n각 훈련 반복에서 각 뉴런(입력 단위 포함되지만 보통 출력 단위는 제외)은 일시적으로 \"드랍아웃\"될 확률 p를 가집니다. 이는 해당 뉴런이 이 전방 및 역방향 패스 동안 완전히 무시된다는 것을 의미합니다. 이 확률 p은 \"드랍아웃 비율\"로 불리며 성능을 최적화하기 위해 조절할 수 있는 하이퍼파라미터입니다. 예를 들어, 0.5의 드랍아웃 비율은 각 뉴런이 각 훈련 패스에서 계산에서 제외될 확률이 50% 라는 것을 의미합니다.\n\n이 과정의 효과는 네트워크가 개별 뉴런의 특정 가중치에 덜 민감해진다는 것입니다. 이것은 예측을 할 때 개별 뉴런의 출력에 의존할 수 없으므로 네트워크가 뉴런들 사이에 중요성을 분산시키도록 장려합니다. 이는 실제로 가중치를 공유하는 신경망의 의사앙상블을 훈련하며, 각 훈련 반복에서 네트워크의 다른 \"드랍아웃된\" 버전이 포함됩니다. 시험 시간에는 드랍아웃이 적용되지 않고, 대신 가중치는 일반적으로 드랍아웃 비율 p에 의해 조정되어 더 많은 유닛이 활성화되었다는 사실을 균형 있게 합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n올바른 드롭아웃 비율 선택하기\n드롭아웃 비율은 각 신경망 구조와 데이터셋에 대해 조정이 필요한 하이퍼파라미터입니다. 일반적으로, 숨겨진 유닛에 대해 시작점으로 0.5의 비율이 사용되며, 이는 원래 드롭아웃 논문에서 제안되었습니다.\n\n높은 드롭아웃 비율 (1에 가까운 값)은 학습 중에 더 많은 뉴런이 제거되는 것을 의미합니다. 이는 네트워크가 데이터를 충분히 학습하지 못할 수 있어서, 훈련 데이터의 복잡성을 모델링하는 데 어려움을 겪어 과소적합을 초래할 수 있습니다.\n\n반대로, 낮은 드롭아웉 비율 (0에 가까운 값)은 더 적은 뉴런이 제거되어 드롭아웃의 정규화 효과가 줄어들 수 있으며, 이는 모델이 훈련 데이터에서 잘 수행되지만 보이지 않는 데이터에서 성능이 나빠질 수 있는 과적합을 초래할 수 있습니다.\n\n코드 구현\n우리 코드에서 어떻게 보이는지 살펴보겠습니다:\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```python\nclass NeuralNetwork:\n    def __init__(self,\n                 layers,\n                 init_method='glorot_uniform', # 'zeros', 'random', 'glorot_uniform', 'glorot_normal', 'he_uniform', 'he_normal'\n                 loss_func='mse',\n                 dropout_rate=0.5\n                 ):\n        ...\n\n        self.dropout_rate = dropout_rate\n\n        ...\n\n    ...\n\n\n    def forward(self, X, is_training=True):\n        self.a = [X]\n        for i, layer in enumerate(self.layers):\n            z = np.dot(self.a[-1], layer['weights']) + layer['biases']\n            a = self.sigmoid(z)\n            if is_training and i \u003c len(self.layers) - 1:  # apply dropout to all layers except the output layer\n                dropout_mask = np.random.rand(*a.shape) \u003e self.dropout_rate\n                a *= dropout_mask\n            self.a.append(a)\n        return self.a[-1]\n\n    ...\n```\n\n저희 신경망 클래스는 새로운 초기화 매개변수와 드롭아웃 정규화를 포함한 새로운 순전파 메서드로 업그레이드되었습니다.\n\ndropout_rate : 이것은 훈련 중에 신경세포들이 네트워크에서 일시적으로 제거될 가능성을 결정하는 설정입니다. 오버피팅을 피하는 데 도움이 됩니다. 0.5로 설정함으로써 어떤 신경세포가 한 번의 훈련 라운드에서 \"제거\"될 확률이 50%라고 말하고 있습니다. 이 무작위성은 네트워크가 어떤 단일 신경세포에 너무 의존하지 않도록 보장하여 더 견고한 학습 과정을 촉진합니다.\n\nis_training 부울 플래그는 네트워크가 현재 훈련되고 있는지를 알려줍니다. 이것은 훈련 중에만 드롭아웃이 발생해야 하므로 새 데이터에 대한 네트워크 성능을 평가할 때는 드롭아웃이 일어나서는 안 된다는 점이 중요합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n네트워크를 통해 데이터(X로 표시)가 전달되면, 네트워크는 들어오는 데이터와 레이어의 편향을 가중합(z)으로 계산합니다. 그런 다음 이 합계를 시그모이드 활성화 함수를 통해 활성화(a)로 변환하여 다음 레이어로 전달할 신호를 얻습니다.\n\n하지만 훈련 중에 다음 레이어로 진행하기 전에 드롭아웃을 적용할 수 있습니다:\n\n- is_training이 true이고 출력 레이어를 다루고 있지 않다면, 각 뉴런에 대해 주사위를 굴려 떨어뜨릴지 여부를 확인합니다. 이를 위해 무작위 수가 드롭아웃 비율을 초과하는지 확인하여 드롭아웃 마스크(모양은 a와 같은 배열)를 생성합니다.\n- 이 마스크를 사용하여 a의 일부 활성화를 0으로 만들어 네트워크에서 일시적으로 뉴런을 제거하는 것을 흉내냅니다.\n\n드롭아웃을 적용한 후(해당하는 경우), 생성된 활성화를 self.a에 추가하여 모든 레이어를 통해 활성화를 추적하는 리스트를 유지합니다. 이렇게 하면 신호를 그냥 한 레이어에서 다음 레이어로 무작정 이동시키는 것이 아니라, 네트워크가 더 견고하게 학습하도록 장려하는 기술을 적용하여 특정 경로의 뉴런에 지나치게 의존하지 않도록 합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 3.5: 그레이디언트 클리핑\n\n그레이디언트 클리핑은 깊은 신경망을 훈련할 때 중요한 기술로, 특히 폭주하는 그레이디언트 문제를 해결할 때 주로 사용됩니다. 폭주하는 그레이디언트는 신경망의 매개변수에 대한 손실 함수의 미분이나 그레이디언트가 층을 거치면서 지수적으로 증가하여 훈련 중에 가중치에 대해 매우 큰 업데이트를 유도할 때 발생합니다. 이는 학습 과정을 불안정하게 만들 수 있으며, 종종 가중치나 손실에서 NaN 값의 형태로 나타나 수치 오버플로우 때문에 발산하여 모델이 해결책으로 수렴하지 못하도록 방해할 수 있습니다.\n\n그레이디언트 클리핑은 값에 의한 클리핑과 법에 의한 클리핑 두 가지 주요 방법으로 구현할 수 있으며, 각각 폭주하는 그레이디언트 문제를 완화하는 전략을 가지고 있습니다.\n\n값에 의한 클리핑\n이 방법은 미리 정의된 임계값을 설정하고, 각 그레이디언트 구성 요소를 해당 임계값을 초과하는 경우 지정된 범위 내로 직접 클리핑하는 접근 방식입니다. 예를 들어, 임계값이 1로 설정되면, 1보다 큰 모든 그레이디언트 구성 요소를 1로 설정하고, -1보다 작은 모든 구성 요소를 -1로 설정합니다. 이는 모든 그레이디언트가 [-1, 1] 범위 내에 유지되도록 보장하여 너무 커지는 그레이디언트를 효과적으로 방지합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_9.png\" /\u003e\n\ngi는 기울기 벡터의 각 구성 요소를 나타냅니다.\n\n노름에 의한 클리핑\n이 방법은 각 기울기 구성 요소를 개별적으로 클리핑하는 대신, 일정 임계값을 초과하는 경우 전체 기울기를 조절합니다. 이렇게 하면 기울기의 방향을 보존한 채 크기가 지정된 한도를 초과하지 않도록 할 수 있습니다. 이는 모든 매개변수를 통해 업데이트의 상대적 방향을 유지하는 데 특히 유용하며, 값에 의한 클리핑보다 학습 과정에 더 유익할 수 있습니다.\n\n\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_10.png\" /\u003e\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그래디언트 벡터를 나타내는 g이고 ∥g∥는 그 노름값입니다.\n\n훈련에의 응용\n\n```js\nclass NeuralNetwork:\n    def __init__(self,\n                 layers,\n                 init_method='glorot_uniform', # 'zeros', 'random', 'glorot_uniform', 'glorot_normal', 'he_uniform', 'he_normal'\n                 loss_func='mse',\n                 dropout_rate=0.5,\n                 clip_type='value',\n                 grad_clip=5.0\n                 ):\n        ...\n\n        self.clip_type = clip_type\n        self.grad_clip = grad_clip\n\n        ...\n\n    ...\n\n    def backward(self, X, y, learning_rate):\n        m = X.shape[0]\n        self.dz = [self.a[-1] - y]\n        self.gradient_norms = []  # 그래디언트 노름을 저장하는 리스트\n\n        for i in reversed(range(len(self.layers) - 1)):\n            self.dz.append(np.dot(self.dz[-1], self.layers[i + 1]['weights'].T) * self.sigmoid_derivative(self.a[i + 1]))\n            self.gradient_norms.append(np.linalg.norm(self.layers[i + 1]['weights']))  # 그래디언트 노름을 계산하고 저장\n\n        self.dz = self.dz[::-1]\n        self.gradient_norms = self.gradient_norms[::-1]  # 리스트를 뒤집어서 레이어의 순서와 일치시킴\n\n        for i in range(len(self.layers)):\n            grads_w = np.dot(self.a[i].T, self.dz[i]) / m\n            grads_b = np.sum(self.dz[i], axis=0, keepdims=True) / m\n\n            # 그래디언트 클리핑\n            if self.clip_type == 'value':\n                grads_w = np.clip(grads_w, -self.grad_clip, self.grad_clip)\n                grads_b = np.clip(grads_b, -self.grad_clip, self.grad_clip)\n            elif self.clip_type == 'norm':\n                grads_w = self.clip_by_norm(grads_w, self.grad_clip)\n                grads_b = self.clip_by_norm(grads_b, self.grad_clip)\n\n            self.layers[i]['weights'] -= learning_rate * grads_w\n            self.layers[i]['biases'] -= learning_rate * grads_b\n\n    def clip_by_norm(self, grads, clip_norm):\n        l2_norm = np.linalg.norm(grads)\n        if l2_norm \u003e clip_norm:\n            grads = grads / l2_norm * clip_norm\n        return grads\n\n    ...\n```\n\n초기화 중에 이제 사용할 그래디언트 클리핑 유형(clip_type)과 그래디언트 클리핑 임계값(grad_clip)이 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n`clip_type`은 그레디언트를 값으로 자르는 경우에는 `value`, 또는 L2 노름에 의해 그레디언트를 자르는 경우에는 `norm`이 될 수 있습니다. grad_clip은 자르는 임계값이나 한계를 지정합니다.\n\n그런 다음, 역전파 중에 함수는 네트워크의 각 레이어에 대한 그레디언트를 계산합니다. 가중치(grads_w)와 편향(grads_b)에 대한 손실의 미분 값을 각 레이어마다 계산합니다.\n\n만약 `clip_type`이 `value`인 경우, np.clip을 사용하여 그레디언트를 [-grad_clip, grad_clip] 범위로 자릅니다. 이렇게 하면 그레디언트 성분이 이 한계를 초과하지 않도록 합니다.\n\n만약 `clip_type`이 `norm`인 경우, 그레디언트의 노름이 grad_clip을 초과하는 경우 이 방향을 유지하면서 그에 대한 크기를 제한하기 위해 clip_by_norm 메서드가 호출됩니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n클리핑 이후, 각 층의 가중치와 편향을 학습률에 의해 조정하는 데 그래디언트가 사용됩니다.\n\n마지막으로, 그래디언트의 L2 노름이 지정된 clip_norm을 초과하는 경우 그래디언트를 스케일링하는 clip_by_norm 메서드를 만듭니다. 이 메서드는 그래디언트의 L2 노름을 계산하고, clip_norm보다 크면 그래디언트를 clip_norm까지 스케일 다운시키면서 방향을 유지합니다. 이는 그래디언트를 그들의 L2 노름으로 나누고 clip_norm을 곱해 달성됩니다.\n\n그래디언트 클리핑의 장점\n모델의 가중치에 대한 지나치게 큰 업데이트를 방지함으로써, 그래디언트 클리핑은 더 안정적이고 신뢰할 수 있는 훈련 과정에 기여합니다. 그래디언트의 계산이 큰 업데이트로 인해 불안정성을 초래할 수 있는 경우에도 손실 함수를 최소화하여 옵티마이저가 일관된 진전을 이룰 수 있도록 합니다. 이는 훈련하는 동안 그래디언트의 스케일이 큰 문제로 인해 불안정성 문제에 직면하는 순환 신경망(RNNs) 훈련과 같은 과제에서 특히 유용한 도구로 작용합니다.\n\n그래디언트 클리핑은 신경망 훈련의 안정성과 성능을 향상시키는 간단하면서도 강력한 기술입니다. 그래디언트가 지나치게 커지지 않도록 보장함으로써, 훈련 불안정성(과적합, 과소적합, 수렴 속도 저하 등)의 문제를 피하고, 신경망이 효과적이고 효율적으로 학습하기 쉽도록 돕습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 4: 층의 최적 개수 결정하기\n\n신경망을 설계하는 중요한 결정 중 하나는 올바른 층의 개수를 결정하는 것입니다. 이 측면은 네트워크의 데이터로부터 학습하고 새로운, 보지 못한 데이터를 일반화하는 능력에 상당한 영향을 미칩니다. 신경망의 깊이 - 얼마나 많은 층이 있는지 - 능력을 강화시키거나 과적합 또는 학습이 부족하다는 문제로 이어질 수 있습니다.\n\n## 4.1: 층의 깊이와 모델 성능\n\n신경망에 더 많은 층을 추가하면 학습 능력이 향상되어 데이터의 더 복잡한 패턴과 관계를 파악할 수 있습니다. 이는 추가적인 층이 입력 데이터의 보다 추상적인 표현을 만들 수 있기 때문에 단순한 기능에서 더 복잡한 조합으로 이동할 수 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n더 깊은 신경망은 복잡한 패턴을 모델링할 수 있지만, 추가적인 깊이가 오버피팅으로 이어지는 기묘한 지점이 있습니다. 오버피팅은 모델이 훈련 데이터를 너무 잘 학습하여 그 잡음까지 포함해 새로운 데이터에서 성능이 나빠지는 현상입니다.\n\n궁극적인 목표는 훈련 데이터로부터 잘 학습하는 모델을 갖는 것뿐만 아니라 이 학습을 새로운 데이터에서도 정확하게 수행할 수 있는 범용성을 갖는 것입니다. 이를 위해서는 층의 깊이에 대한 적절한 균형을 찾는 것이 중요합니다. 너무 적은 층은 과소적합될 수 있고, 너무 많은 층은 오버피팅될 수 있습니다.\n\n## 4.2: 적절한 깊이를 테스트하고 선택하는 전략\n\n점진적인 접근 방식\n간단한 모델부터 시작하여 점진적으로 층을 추가하고 검증 성능이 크게 향상될 때까지 관찰합니다. 이 접근 방식은 각 층이 전체 성능에 어떤 기여를 하는지 이해하는 데 도움이 됩니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모델의 성능을 판단하기 위한 기준으로 검증 세트(학습 중에 사용되지 않은 학습 데이터의 하위 집합)에서 모델이 일반화하는 능력을 향상시키는지 여부를 결정합니다.\n\n정규화 기법\n더 많은 레이어를 추가할 때 드롭아웃 또는 L2 정규화와 같은 정규화 방법을 사용하세요. 이러한 기법은 오버피팅의 위험을 줄일 수 있어 추가된 레이어가 모델의 학습 능력에 어떤 가치를 더하는지를 공정하게 평가할 수 있게 해줍니다.\n\n학습 동태 관찰\n더 많은 레이어를 추가할 때 학습과 검증 손실을 모니터링하세요. 이 두 지표 사이에 차이가 발생하는 경우 — 학습 손실이 감소하지만 검증 손실이 그렇지 않을 때 — 오버피팅을 나타낼 수 있으며, 현재 깊이가 지나칠 수 있다는 것을 시사할 수 있습니다.\n\n![이미지](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_11.png)\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 두 그래프는 기계 학습 모델을 훈련하는 과정에서 발생할 수 있는 두 가지 시나리오를 나타냅니다.\n\n첫 번째 그래프에서는 훈련 손실과 검증 손실이 모두 감소하여 비슷한 값으로 수렴합니다. 이것은 이상적인 시나리오로, 모델이 잘 학습하고 적절하게 일반화되고 있음을 나타냅니다. 모델의 성능이 훈련 데이터와 보지 않은 검증 데이터 모두에서 향상되고 있는 것을 의미합니다. 이는 모델이 데이터를 과소적합하거나 과적합하지 않고 있다는 것을 시사합니다.\n\n두 번째 그래프에서는 훈련 손실은 감소하지만 검증 손실이 증가합니다. 이는 과적합의 전형적인 징후입니다. 모델이 훈련 데이터를 너무 잘 학습하여 노이즈와 이상점을 포함하고 있으며 보지 않은 데이터에 대한 일반화를 실패합니다. 결과적으로, 검증 데이터에서의 성능이 시간이 지남에 따라 악화됩니다. 이는 모델의 복잡성을 줄이거나 정규화나 드롭아웃과 같은 과적합 방지 기술을 적용해야 할 수도 있다는 것을 나타냅니다.\n\n자동화 아키텍처 탐색\n신경망 아키텍처 탐색(NAS) 도구나 Optuna와 같은 하이퍼파라미터 최적화 프레임워크를 활용하여 서로 다른 아키텍처를 체계적으로 탐색하십시오. 이러한 도구는 다양한 구성을 평가하고 검증 지표에서 최상의 성능을 발휘하는 구성을 선택함으로써 최적의 레이어 수를 자동화할 수 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n신경망의 최적 레이어 수를 결정하는 것은 모델의 복잡성과 학습 및 일반화 능력을 균형있게 고려하는 세심한 프로세스입니다. 레이어 추가에 체계적인 방법론을 채택하고 교차 검증을 활용하며 정칙화 기법을 통합함으로써 특정 문제에 적합한 네트워크 깊이를 결정할 수 있습니다. 이를 통해 보이지 않는 데이터에 대한 모델 성능을 최적화할 수 있습니다.\n\n# 5: Optuna을 활용한 자동 미세 튜닝\n\n최적 성능을 달성하기 위해 신경망을 미세 조정하는 것은 다양한 하이퍼파라미터의 섬세한 균형을 찾는 과정으로, 종종 방대한 탐색 공간 속에서 바늘을 찾는 것처럼 느껴질 수 있습니다. 이때 Optuna와 같은 자동 하이퍼파라미터 최적화 도구가 필요합니다.\n\n## 5.1: Optuna 소개\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n옵투나는 최적화 하이퍼파라미터 선택을 자동화하기 위해 설계된 오픈소스 최적화 프레임워크입니다. 이는 가장 효율적인 신경망 모델로 이어지는 매개변수 조합을 식별하는 복잡한 작업을 간단화합니다. 옵투나는 고급 알고리즘을 활용하여 최적화 하이퍼파라미터 공간을 보다 효과적으로 탐색하여, 필요한 계산 자원과 수렴 시간을 줄입니다.\n\n## 5.2: 옵투나를 활용한 신경망 최적화 통합\n\n옵투나는 베이지안 최적화, 트리 구조 파르젠 추정기, 진화 알고리즘 등 다양한 전략을 활용하여 하이퍼파라미터 공간을 지능적으로 탐색합니다. 이 접근 방식을 통해 옵투나는 가장 유망한 하이퍼파라미터를 빠르게 식별하여 최적화 과정을 크게 가속화할 수 있습니다.\n\n옵투나를 신경망 훈련 워크플로우에 통합하는 것은 옵투나가 최소화 또는 최대화하려는 목적 함수를 정의하는 과정을 포함합니다. 이 함수에는 일반적으로 모델 훈련 및 검증 과정이 포함되며, 목표는 검증 손실을 최소화하거나 검증 정확도를 최대화하는 것입니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 검색 공간 정의: 각 하이퍼파라미터 값 범위를 지정하여 Optuna가 탐색할 것입니다 (예: 레이어 수, 학습률, 드롭아웃 비율).\n- 시험과 평가: Optuna는 모델을 훈련시키기 위해 매번 새로운 하이퍼파라미터 세트를 선택하는 시험을 진행합니다. 검증 세트에서 모델의 성능을 평가하고 이 정보를 사용하여 탐색을 안내합니다.\n\n## 5.3: 실제 구현\n\n```python\nimport optuna\n\ndef objective(trial):\n    # 하이퍼파라미터 정의\n    n_layers = trial.suggest_int('n_layers', 1, 10)\n    hidden_sizes = [trial.suggest_int(f'hidden_size_{i}', 32, 128) for i in range(n_layers)]\n    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)  # 모든 레이어에 대한 단일 드롭아웃 비율\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-3, 1e-1)\n    init_method = trial.suggest_categorical('init_method', ['glorot_uniform', 'glorot_normal', 'he_uniform', 'he_normal', 'random'])\n    clip_type = trial.suggest_categorical('clip_type', ['value', 'norm'])\n    clip_value = trial.suggest_uniform('clip_value', 0.0, 1.0)\n    epochs = 10000\n\n    layers = [input_size] + hidden_sizes + [output_size]\n\n    # 신경망 생성 및 훈련\n    nn = NeuralNetwork(layers=layers, loss_func=loss_func, dropout_rate=dropout_rate, init_method=init_method, clip_type=clip_type, grad_clip=clip_value)\n    trainer = Trainer(nn, loss_func)\n    trainer.train(X_train, y_train, X_test, y_test, epochs, learning_rate, early_stopping=False)\n\n    # 신경망 성능 평가\n    predictions = np.argmax(nn.forward(X_test), axis=1)\n    accuracy = np.mean(predictions == y_test_labels)\n\n    return accuracy\n\n# Study 객체 생성 및 목적 함수 최적화\nstudy = optuna.create_study(study_name='nn_study', direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\n# 최적 하이퍼파라미터 출력\nprint(f\"Best trial: {study.best_trial.params}\")\nprint(f\"Best value: {study.best_trial.value:.3f}\")\n```\n\nOptuna 최적화 프로세스의 핵심은 목적 함수입니다. 이 함수는 시험 목표를 정의하고 각 시험에 대해 Optuna에 의해 호출됩니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n**Heren_layers**은 신경망의 은닉층의 수이며, 1에서 10 사이를 추천합니다. 층의 수를 변화시킴으로써 얕은 네트워크와 깊은 네트워크 아키텍처를 탐색할 수 있습니다.\n\n**hidden_sizes**는 각 층의 크기(뉴런 수)를 저장하며, 32에서 128 사이의 숫자를 제안하여 모델이 다양한 용량을 탐색하게 합니다.\n\n**dropout_rate**는 균일하게 0.0(드롭아웃 없음)에서 0.5 사이를 제안하여 시험을 통해 정규화 유연성을 가능케 합니다.\n\n**learning_rate**는 로그 스케일로 1e-3에서 1e-1 사이를 제안하여, 학습률 최적화에 대한 공통적인 민감도로 인해 크기의 범위를 포괄하는 넓은 탐색 공간을 보장합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n신경망 가중치의 init_method은 일련의 일반적인 전략 중에서 선택됩니다. 이 선택은 훈련의 시작점과 수렴 동작을 영향을 줍니다.\n\nclip_type과 clip_value는 그래디언트 클리핑 전략과 값으로, 값이나 노름을 기준으로 클리핑하여 폭발하는 그래디언트를 방지하는 데 도움이 됩니다.\n\n그런 다음, 정의된 하이퍼파라미터를 사용하여 NeuralNetwork 인스턴스가 생성되고 훈련됩니다. 각 시행이 일정한 에포크 수동안 실행될 수 있도록 조기 중지가 비활성화되며, 일관된 비교를 보장합니다. 성능은 테스트 세트에서 모델의 예측 정확도를 기반으로 평가됩니다.\n\n목적 함수와 NeuralNetwork 인스턴스가 정의된 후 Optuna 스터디로 이동할 수 있습니다. Optuna 스터디 객체는 목적 함수를 최대화(`maximize`)하는 데 사용되며, 이 문맥에서는 신경망의 정확도가 목적 함수입니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n연구는 목적 함수를 여러 번 호출합니다(n_trials=100), 매번 옵튜나 내부 최적화 알고리즘에서 제안한 다른 하이퍼파라미터 세트로 호출합니다. 옵튜나는 시험 이력에 기반하여 지능적으로 제안을 조정하여 하이퍼파라미터 공간을 효율적으로 탐색합니다.\n\n이 프로세스를 통해 모든 실험에서 찾은 가장 좋은 하이퍼파라미터 세트(study.best_trial.params)와 달성한 최고 정확도(study.best_trial.value)가 생성됩니다. 이 출력은 주어진 작업에 대한 신경망의 최적 구성에 대한 통찰을 제공합니다.\n\n## 5.4: 혜택 및 결과\n\n옵튜나를 통합함으로써, 개발자는 하이퍼파라미터 튜닝 프로세스를 자동화할뿐만 아니라 어떻게 다른 매개변수가 모델에 영향을 미치는지에 대한 깊은 통찰을 얻을 수 있습니다. 이를 통해 수동 실험을 통해 걸릴 시간의 일부로 최적화된, 더 견고하고 정확한 신경망이 생성됩니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n옵투나의 체계적인 파라미터 조정 접근법은 신경망 개발에 새로운 수준의 정밀성과 효율성을 제공하여 개발자들이 더 높은 성능 표준을 달성하고 모델이 이룰 수 있는 한계를 뛰어넘을 수 있도록 돕습니다.\n\n## 5.5: 한계\n\n옵투나는 하이퍼파라미터 최적화에 강력하고 유연한 접근 방식을 제공하지만, 기계 학습 워크플로에 통합할 때 고려해야 할 몇 가지 한계점과 주의 사항이 있습니다.\n\n계산 리소스\n각 시도는 신경망을 처음부터 훈련해야 하므로, 특히 심층 신경망이나 대규모 데이터셋의 경우에는 계산 리소스가 많이 필요할 수 있습니다. 하이퍼파라미터 공간을 철저히 탐색하기 위해 수백 번이나 수천 번의 시도를 실행하는 것은 상당한 계산 리소스와 시간이 필요할 수 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n하이퍼파라미터 검색 공간\n옵투나의 검색 효과는 검색 공간이 어떻게 정의되는지에 매우 의존합니다. 하이퍼파라미터 값의 범위가 너무 넓거나 문제와 제대로 일치하지 않으면 옵투나가 비최적 영역을 탐색하는 데 시간을 낭비할 수 있습니다. 반대로 검색 공간이 너무 좁으면 최적의 구성을 놓칠 수 있습니다.\n\n하이퍼파라미터 수가 증가함에 따라 검색 공간이 기하급수적으로 증가하는데, 이를 \"차원의 저주\"라고 합니다. 이로 인해 옵투나가 공간을 효율적으로 탐색하고 합리적인 횟수의 시도 내에서 최적의 하이퍼파라미터를 찾는 것이 어렵다는 도전이 생길 수 있습니다.\n\n평가 지표\n목적 함수와 평가 지표의 선택은 최적화 결과에 상당한 영향을 미칠 수 있습니다. 모델의 성능이나 작업 목표를 적절히 포착하지 못하는 지표는 하이퍼파라미터 구성을 부적절하게 만들 수 있습니다.\n\n모델의 성능 평가는 무작위 초기화, 데이터 섞기, 또는 데이터셋 내 잡음과 같은 요소로 인해 달라질 수 있습니다. 이러한 변동성은 최적화 과정에 잡음을 도입하여 결과의 신뢰성에 영향을 줄 수 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n알고리즘 제한사항\nOptuna은 검색 공간을 탐색하기 위해 정교한 알고리즘을 사용하지만, 이러한 알고리즘의 효율성과 효과는 문제에 따라 다를 수 있습니다. 경우에 따라 특정 알고리즘이 지역 최적점으로 수렴하거나 하이퍼파라미터 공간의 특정 특성에 더 잘 맞도록 설정을 조정해야 할 수도 있습니다.\n\n# 6: 결론\n\n신경망의 세밀한 조정에 대해 심층적으로 살펴본 후에 우리가 걸어온 길을 돌아보는 좋은 시기입니다. 우리는 신경망이 어떻게 작동하는지에 대한 기본 사항부터 시작하여 그들의 성능과 효율성을 높이는 더 정교한 기술로 점진적으로 발전해왔습니다.\n\n## 6.1: 다음 단계\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n우리는 신경망 최적화에 많은 영역을 다루었지만, 명백히 우리는 겨우 표면만 긁은 것 뿐입니다. 신경망 최적화의 영역은 방대하며 지속적으로 진화하고 있으며, 아직 탐험하지 않은 기술과 전략으로 넘쳐납니다. 다가오는 기사에서 더 심층적으로 파고들어 복잡한 신경망 구조와 더 높은 성능과 효율성을 끌어올릴 수 있는 고급 기술을 탐구할 예정입니다.\n\n저희가 파헤치고자 하는 최적화 기술과 개념의 다양한 범위에는 다음과 같은 것들이 포함됩니다:\n\n- 배치 정규화: 입력 레이어를 정규화해 활성화를 조정하고 스케일링하여 훈련 속도를 높이고 안정성을 향상시키는 방법입니다.\n- 최적화 알고리즘: SGD 및 Adam을 포함한 최적화 알고리즘은 복잡한 손실 함수의 영역을 더 효과적으로 탐색할 수 있는 도구를 제공하여 더 효율적인 훈련 주기와 더 나은 모델 성능을 보장합니다.\n- 전이 학습 및 파인 튜닝: 사전 훈련된 모델을 활용하여 새로운 작업에 적응시키면 훈련 시간을 크게 단축하고 데이터가 제한적인 작업에서 모델 정확도를 향상시킬 수 있습니다.\n- 신경 아키텍처 탐색(NAS): 자동화를 사용하여 신경망을 위한 최상의 아키텍처를 발견함으로써 직관적이지 않은 효율적인 모델을 발견할 수 있습니다.\n\n이러한 주제들은 단지 저희가 다루는 것 중 일부에 불과하며, 각각 고유한 이점과 도전을 제공합니다. 앞으로 나아가면서, 이러한 기술을 자세히 살펴보고, 언제 사용해야 하는지, 그들이 어떻게 작용하는지, 그리고 당신의 신경망 프로젝트에 미치는 영향에 대한 통찰력을 제공할 것을 목표로 합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 추가 자료\n\n- “Deep Learning” - Ian Goodfellow, Yoshua Bengio, Aaron Courville 저: 깊은 학습 기술과 원리에 대한 깊이 있는 개요를 제공하는 이 근본적인 문헌은 고급 신경망 구조 및 최적화 방법을 다룹니다.\n- “Neural Networks and Deep Learning: A Textbook” - Charu C. Aggarwal 저: 신경망에 대한 상세한 탐구를 제공하며, 깊은 학습과 그 응용에 중점을 둡니다. 신경망 디자인 및 최적화의 복잡한 개념을 이해하는 데 탁월한 자료입니다.\n\n여기까지 왔습니다. 축하해요! 이 기사를 즐기셨다면 좋아요를 누르고 팔로우해주시면 감사하겠습니다. 저는 정기적으로 유사한 기사를 게시할 예정이니 많은 관심 부탁드립니다. 제 목표는 가장 인기 있는 알고리즘을 다시 처음부터 만들어 머신 러닝을 모든 사람이 접근 가능하도록 하는 것입니다.\n","ogImage":{"url":"/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_0.png","tag":["Tech"],"readingTime":51},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e머신 러닝에서는 몇 가지 모델을 시도해 가장 성능이 좋은 것을 선택하고 몇 가지 설정을 조정하여 그나마 성공할 수 있을지도 모릅니다. 그러나 딥러닝은 그런 룰에 맞지 않습니다. 신경망을 실험해 본 적이 있다면, 성능이 꽤 불안정할 수 있다는 것을 눈치챌 수 있습니다. 어쩌면 로지스틱 회귀와 같이 간단한 모델이 멋진 200층 심층 신경망을 이길 수도 있습니다.\u003c/p\u003e\n\u003cp\u003e이게 왜 그럴까요? 딥러닝은 우리가 가지고 있는 가장 고급 인공 지능 기술 중 하나이지만, 철저한 이해와 조심스러운 다룸이 필요합니다. 신경망을 세밀하게 조정하고, 내부 작동 방식을 파악하고, 그 사용법을 마스터하는 것이 중요합니다. 오늘은 이에 대해 자세히 살펴보겠습니다!\u003c/p\u003e\n\u003cp\u003e글을 읽기 전에 Jupyter Notebook을 여시는 것을 제안합니다. 오늘 다룰 모든 코드가 담겨 있으므로 함께 따라가는 데 도움이 될 것입니다:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e인덱스\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e1: 소개\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e1.1: 기본 신경망의 발전\u003c/li\u003e\n\u003cli\u003e1.2: 복잡성으로의 길\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e2: 모델 복잡성 확장\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e2.1: 레이어 추가\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e3: 향상된 학습을 위한 최적화 기법\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e3.1: 학습률\u003c/li\u003e\n\u003cli\u003e3.2: 조기 중단 기법\u003c/li\u003e\n\u003cli\u003e3.3: 초기화 방법\u003c/li\u003e\n\u003cli\u003e3.4: 드롭아웃\u003c/li\u003e\n\u003cli\u003e3.5: 그래디언트 클리핑\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e4: 최적 레이어 수 결정하기\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e4.1: 레이어 깊이와 모델 성능\u003c/li\u003e\n\u003cli\u003e4.2: 적절한 깊이 선택을 위한 테스트 전략\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e5: Optuna를 활용한 자동 세부 조정\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e5.1: Optuna 소개\u003c/li\u003e\n\u003cli\u003e5.2: 신경망 최적화를 위한 Optuna 통합\u003c/li\u003e\n\u003cli\u003e5.3: 실제 적용\u003c/li\u003e\n\u003cli\u003e5.4: 장점과 결과\u003c/li\u003e\n\u003cli\u003e5.5: 제한 사항\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e6: 결론\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e6.1: 다음 단계\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e추가 자료\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e1: 소개\u003c/h1\u003e\n\u003ch2\u003e1.1: 기본 신경망의 발전\u003c/h2\u003e\n\u003cp\u003e인공 지능에 대한 최근 탐구에서 우리는 기초부터 신경망을 구축했습니다. 이 기본 모델은 오늘날 인공 지능 기술의 핵심인 신경망의 세계를 열어 주었습니다. 우리는 입력, 은닉 및 출력 레이어와 활성화 함수가 어떻게 정보를 처리하고 예측하는 데 결합되는지 간단하게 다루었습니다. 그리고 나서 우리는 컴퓨터 비전 작업을 위해 숫자 데이터셋에서 훈련된 간단한 신경망으로 이론을 실제로 적용했습니다.\u003c/p\u003e\n\u003cp\u003e이제 이러한 기초 위에 계속해서 더 진보해 나갈 것입니다. 우리는 레이어를 추가하고, 초기화, 정규화 및 최적화에 대한 다양한 기술을 탐구함으로써 더 많은 복잡성을 도입할 것입니다. 물론, 이러한 수정이 우리의 신경망 성능에 어떻게 영향을 미치는지 확인하기 위해 코드를 테스트할 것입니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e제가 이전 기사를 확인하지 않으셨다면, 우리가 처음부터 신경망을 만들어본 기사를 꼭 읽어보시기를 추천합니다. 이번에는 그 작업을 기반으로 계속해서 진행할 것이며, 이미 우리가 다룬 개념에 익숙하다고 가정할게요.\u003c/p\u003e\n\u003ch2\u003e1.2: 복잡성으로의 길\u003c/h2\u003e\n\u003cp\u003e신경망을 기본 구성에서 더 정교한 구조로 변환하는 것은 단순히 더 많은 층이나 노드를 추가하는 것만으로는 이루어지지 않습니다. 이것은 신경망의 구조와 그 데이터를 다루는 미묘한 차이를 체화하는 세심한 작업이 필요한 미묘한 춤이죠. 더 깊게 파고들수록, 우리의 목표는 신경망의 깊이를 풍부하게 함으로써 데이터의 복잡한 패턴과 연결을 더 잘 분별하는 데 있습니다.\u003c/p\u003e\n\u003cp\u003e하지만 복잡성을 높이는 것은 그리 수월한 일이 아닙니다. 우리가 도입할 때마다, 세련된 최적화 기술의 필요성이 커집니다. 이는 효과적인 학습뿐만 아니라 새로운 보이지 않는 데이터에 적응하기 위한 모델 능력에 필수적입니다. 이 안내서는 우리의 기반 신경망을 강화하는 과정을 안내해 드릴 것입니다. 우리는 신경망을 세밀하게 조정하는 정교한 전략에 대해 살펴볼 것이며, 학습 속도 조정, 조기 종료 도입, 그리고 SGD(확률적 경사 하강법)와 Adam과 같은 다양한 최적화 알고리즘을 활용하는 방법에 대해 살펴볼 것입니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e우리는 초기화 방법으로 시작하는 중요성, 오버피팅을 피하는 데 드롭아웃 사용의 장점, 그리고 클리핑 및 정규화로 네트워크의 그래디언트를 체크하여 안정성을 유지하는 것이 모델의 안정성에 얼마나 중요한지에 대해 다룰 예정입니다. 또한 학습을 향상시키기 위한 레이어의 최적 개수를 찾는 도전과정 및 불필요한 복잡성으로 빠져들지 않도록 할 것입니다.\u003c/p\u003e\n\u003cp\u003e이전 게시물에서 함께 만든 Neural Network 및 Trainer 클래스를 아래에서 확인할 수 있습니다. 우리는 이를 조정하고 각 수정이 모델의 성능에 어떤 영향을 미치는지 실제로 살펴볼 것입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eNeuralNetwork\u003c/span\u003e:\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, input_size, hidden_size, output_size, loss_func=\u003cspan class=\"hljs-string\"\u003e'mse'\u003c/span\u003e):\n        self.\u003cspan class=\"hljs-property\"\u003einput_size\u003c/span\u003e = input_size\n        self.\u003cspan class=\"hljs-property\"\u003ehidden_size\u003c/span\u003e = hidden_size\n        self.\u003cspan class=\"hljs-property\"\u003eoutput_size\u003c/span\u003e = output_size\n        self.\u003cspan class=\"hljs-property\"\u003eloss_func\u003c/span\u003e = loss_func\n\n        # 가중치 및 편향 초기화\n        self.\u003cspan class=\"hljs-property\"\u003eweights1\u003c/span\u003e = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erandn\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003einput_size\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003ehidden_size\u003c/span\u003e)\n        self.\u003cspan class=\"hljs-property\"\u003ebias1\u003c/span\u003e = np.\u003cspan class=\"hljs-title function_\"\u003ezeros\u003c/span\u003e((\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003ehidden_size\u003c/span\u003e))\n        self.\u003cspan class=\"hljs-property\"\u003eweights2\u003c/span\u003e = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erandn\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ehidden_size\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003eoutput_size\u003c/span\u003e)\n        self.\u003cspan class=\"hljs-property\"\u003ebias2\u003c/span\u003e = np.\u003cspan class=\"hljs-title function_\"\u003ezeros\u003c/span\u003e((\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003eoutput_size\u003c/span\u003e))\n\n        # 손실 추적\n        self.\u003cspan class=\"hljs-property\"\u003etrain_loss\u003c/span\u003e = []\n        self.\u003cspan class=\"hljs-property\"\u003etest_loss\u003c/span\u003e = []\n\n    def \u003cspan class=\"hljs-title function_\"\u003e__str__\u003c/span\u003e(self):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e f\u003cspan class=\"hljs-string\"\u003e\"Neural Network Layout:\\n입력 레이어: {self.input_size} 뉴런\\n은닉 레이어: {self.hidden_size} 뉴런\\n출력 레이어: {self.output_size} 뉴런\\n손실 함수: {self.loss_func}\"\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(self, X):\n        # 순방향 전파 수행\n        self.\u003cspan class=\"hljs-property\"\u003ez1\u003c/span\u003e = np.\u003cspan class=\"hljs-title function_\"\u003edot\u003c/span\u003e(X, self.\u003cspan class=\"hljs-property\"\u003eweights1\u003c/span\u003e) + self.\u003cspan class=\"hljs-property\"\u003ebias1\u003c/span\u003e\n        self.\u003cspan class=\"hljs-property\"\u003ea1\u003c/span\u003e = self.\u003cspan class=\"hljs-title function_\"\u003esigmoid\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ez1\u003c/span\u003e)\n        self.\u003cspan class=\"hljs-property\"\u003ez2\u003c/span\u003e = np.\u003cspan class=\"hljs-title function_\"\u003edot\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ea1\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003eweights2\u003c/span\u003e) + self.\u003cspan class=\"hljs-property\"\u003ebias2\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003eloss_func\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'categorical_crossentropy'\u003c/span\u003e:\n            self.\u003cspan class=\"hljs-property\"\u003ea2\u003c/span\u003e = self.\u003cspan class=\"hljs-title function_\"\u003esoftmax\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ez2\u003c/span\u003e)\n        \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n            self.\u003cspan class=\"hljs-property\"\u003ea2\u003c/span\u003e = self.\u003cspan class=\"hljs-title function_\"\u003esigmoid\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ez2\u003c/span\u003e)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003ea2\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003ebackward\u003c/span\u003e(self, X, y, learning_rate):\n        # 역전파 수행\n        m = X.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n\n        # 기울기 계산\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003eloss_func\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'mse'\u003c/span\u003e:\n            self.\u003cspan class=\"hljs-property\"\u003edz2\u003c/span\u003e = self.\u003cspan class=\"hljs-property\"\u003ea2\u003c/span\u003e - y\n        elif self.\u003cspan class=\"hljs-property\"\u003eloss_func\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'log_loss'\u003c/span\u003e:\n            self.\u003cspan class=\"hljs-property\"\u003edz2\u003c/span\u003e = -(y/self.\u003cspan class=\"hljs-property\"\u003ea2\u003c/span\u003e - (\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e-y)/(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e-self.\u003cspan class=\"hljs-property\"\u003ea2\u003c/span\u003e))\n        elif self.\u003cspan class=\"hljs-property\"\u003eloss_func\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'categorical_crossentropy'\u003c/span\u003e:\n            self.\u003cspan class=\"hljs-property\"\u003edz2\u003c/span\u003e = self.\u003cspan class=\"hljs-property\"\u003ea2\u003c/span\u003e - y\n        \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n            raise \u003cspan class=\"hljs-title class_\"\u003eValueError\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'잘못된 손실 함수'\u003c/span\u003e)\n\n        self.\u003cspan class=\"hljs-property\"\u003edw2\u003c/span\u003e = (\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e / m) * np.\u003cspan class=\"hljs-title function_\"\u003edot\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ea1\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003edz2\u003c/span\u003e)\n        self.\u003cspan class=\"hljs-property\"\u003edb2\u003c/span\u003e = (\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e / m) * np.\u003cspan class=\"hljs-title function_\"\u003esum\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003edz2\u003c/span\u003e, axis=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, keepdims=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n        self.\u003cspan class=\"hljs-property\"\u003edz1\u003c/span\u003e = np.\u003cspan class=\"hljs-title function_\"\u003edot\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003edz2\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003eweights2\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e) * self.\u003cspan class=\"hljs-title function_\"\u003esigmoid_derivative\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ea1\u003c/span\u003e)\n        self.\u003cspan class=\"hljs-property\"\u003edw1\u003c/span\u003e = (\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e / m) * np.\u003cspan class=\"hljs-title function_\"\u003edot\u003c/span\u003e(X.\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003edz1\u003c/span\u003e)\n        self.\u003cspan class=\"hljs-property\"\u003edb1\u003c/span\u003e = (\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e / m) * np.\u003cspan class=\"hljs-title function_\"\u003esum\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003edz1\u003c/span\u003e, axis=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, keepdims=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\n        # 가중치 및 편향 업데이트\n        self.\u003cspan class=\"hljs-property\"\u003eweights2\u003c/span\u003e -= learning_rate * self.\u003cspan class=\"hljs-property\"\u003edw2\u003c/span\u003e\n        self.\u003cspan class=\"hljs-property\"\u003ebias2\u003c/span\u003e -= learning_rate * self.\u003cspan class=\"hljs-property\"\u003edb2\u003c/span\u003e\n        self.\u003cspan class=\"hljs-property\"\u003eweights1\u003c/span\u003e -= learning_rate * self.\u003cspan class=\"hljs-property\"\u003edw1\u003c/span\u003e\n        self.\u003cspan class=\"hljs-property\"\u003ebias1\u003c/span\u003e -= learning_rate * self.\u003cspan class=\"hljs-property\"\u003edb1\u003c/span\u003e\n\n    def \u003cspan class=\"hljs-title function_\"\u003esigmoid\u003c/span\u003e(self, x):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e / (\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e + np.\u003cspan class=\"hljs-title function_\"\u003eexp\u003c/span\u003e(-x))\n\n    def \u003cspan class=\"hljs-title function_\"\u003esigmoid_derivative\u003c/span\u003e(self, x):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e x * (\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e - x)\n\n    def \u003cspan class=\"hljs-title function_\"\u003esoftmax\u003c/span\u003e(self, x):\n        exps = np.\u003cspan class=\"hljs-title function_\"\u003eexp\u003c/span\u003e(x - np.\u003cspan class=\"hljs-title function_\"\u003emax\u003c/span\u003e(x, axis=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, keepdims=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e))\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e exps/np.\u003cspan class=\"hljs-title function_\"\u003esum\u003c/span\u003e(exps, axis=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, keepdims=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTrainer\u003c/span\u003e:\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, model, loss_func=\u003cspan class=\"hljs-string\"\u003e'mse'\u003c/span\u003e):\n        self.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e = model\n        self.\u003cspan class=\"hljs-property\"\u003eloss_func\u003c/span\u003e = loss_func\n        self.\u003cspan class=\"hljs-property\"\u003etrain_loss\u003c/span\u003e = []\n        self.\u003cspan class=\"hljs-property\"\u003eval_loss\u003c/span\u003e = []\n\n    def \u003cspan class=\"hljs-title function_\"\u003ecalculate_loss\u003c/span\u003e(self, y_true, y_pred):\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003eloss_func\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'mse'\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-title function_\"\u003emean\u003c/span\u003e((y_pred - y_true)**\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n        elif self.\u003cspan class=\"hljs-property\"\u003eloss_func\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'log_loss'\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e -np.\u003cspan class=\"hljs-title function_\"\u003emean\u003c/span\u003e(y_true*np.\u003cspan class=\"hljs-title function_\"\u003elog\u003c/span\u003e(y_pred) + (\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e-y_true)*np.\u003cspan class=\"hljs-title function_\"\u003elog\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e-y_pred))\n        elif self.\u003cspan class=\"hljs-property\"\u003eloss_func\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'categorical_crossentropy'\u003c/span\u003e:\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e -np.\u003cspan class=\"hljs-title function_\"\u003emean\u003c/span\u003e(y_true*np.\u003cspan class=\"hljs-title function_\"\u003elog\u003c/span\u003e(y_pred))\n        \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n            raise \u003cspan class=\"hljs-title class_\"\u003eValueError\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'잘못된 손실 함수'\u003c/span\u003e)\n\n    def \u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e(self, X_train, y_train, X_test, y_test, epochs, learning_rate):\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e _ \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(epochs):\n            self.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(X_train)\n            self.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ebackward\u003c/span\u003e(X_train, y_train, learning_rate)\n            train_loss = self.\u003cspan class=\"hljs-title function_\"\u003ecalculate_loss\u003c/span\u003e(y_train, self.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ea2\u003c/span\u003e)\n            self.\u003cspan class=\"hljs-property\"\u003etrain_loss\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(train_loss)\n\n            self.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(X_test)\n            test_loss = self.\u003cspan class=\"hljs-title function_\"\u003ecalculate_loss\u003c/span\u003e(y_test, self.\u003cspan class=\"hljs-property\"\u003emodel\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003ea2\u003c/span\u003e)\n            self.\u003cspan class=\"hljs-property\"\u003eval_loss\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(val_loss)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e2: 모델 복잡성 확대\u003c/h1\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e신경망을 더 정교하게 개선하기 위해 깊이 파고들면서, 게임을 바꾸는 전략을 발견했습니다: 레벨을 더 쌓아 복잡성을 높이는 것입니다. 이 동작은 모델을 강화하는 것뿐만 아니라, 데이터의 미묘한 변화를 더 정교하게 이해하고 해석하는 능력을 키우는 것입니다.\u003c/p\u003e\n\u003ch2\u003e2.1: 레이어 추가\u003c/h2\u003e\n\u003cp\u003e증가된 네트워크 깊이의 근거\n딥러닝의 핵심은 계층적 데이터 표현을 조합하는 능력에 있습니다. 더 많은 레이어를 추가함으로써, 우리는 신경망에 성장하는 복잡성의 패턴을 해체하고 이해하기 위한 도구를 제공하는 셈입니다. 간단한 형태와 질감을 인식하는 데서 시작해 데이터 속에서 더 복잡한 관계와 특징을 풀어가는 데로 발전하는 것으로 생각해보세요. 이러한 계층적 학습 접근법은 어느 정도 인간이 정보를 해석하는 방식과 유사하며, 기본적인 이해에서 복잡한 해석으로 진화합니다.\u003c/p\u003e\n\u003cp\u003e더 많은 레이어를 쌓으면 네트워크의 \"학습 용량\"이 증가하여 보다 넓은 범위의 데이터 관계를 매핑하고 소화하는 능력을 갖추게 됩니다. 이를 통해 더 복잡한 작업을 처리할 수 있습니다. 하지만 마구 레이어를 추가하는 것은 아니며, 모델의 지능에 의미 있는 기여를 하지 않고 무분별하게 레이어를 추가하면 학습 과정을 혼란시키는 것이 아니라 명료하게 해야 합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003ccode\u003eNeuralNetwork\u003c/code\u003e 클래스를 더 많은 층을 통합하는 방법 안내\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eNeuralNetwork\u003c/span\u003e:\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, layers, loss_func=\u003cspan class=\"hljs-string\"\u003e'mse'\u003c/span\u003e):\n        self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e = []\n        self.\u003cspan class=\"hljs-property\"\u003eloss_func\u003c/span\u003e = loss_func\n\n        # 레이어 초기화\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(layers) - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e):\n            self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e({\n                \u003cspan class=\"hljs-string\"\u003e'weights'\u003c/span\u003e: np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erandn\u003c/span\u003e(layers[i], layers[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]),\n                \u003cspan class=\"hljs-string\"\u003e'biases'\u003c/span\u003e: np.\u003cspan class=\"hljs-title function_\"\u003ezeros\u003c/span\u003e((\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, layers[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]))\n            })\n\n        # 손실 추적\n        self.\u003cspan class=\"hljs-property\"\u003etrain_loss\u003c/span\u003e = []\n        self.\u003cspan class=\"hljs-property\"\u003etest_loss\u003c/span\u003e = []\n\n    def \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(self, X):\n        self.\u003cspan class=\"hljs-property\"\u003ea\u003c/span\u003e = [X]\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e layer \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e:\n            self.\u003cspan class=\"hljs-property\"\u003ea\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(self.\u003cspan class=\"hljs-title function_\"\u003esigmoid\u003c/span\u003e(np.\u003cspan class=\"hljs-title function_\"\u003edot\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ea\u003c/span\u003e[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], layer[\u003cspan class=\"hljs-string\"\u003e'weights'\u003c/span\u003e]) + layer[\u003cspan class=\"hljs-string\"\u003e'biases'\u003c/span\u003e]))\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003ea\u003c/span\u003e[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n\n    def \u003cspan class=\"hljs-title function_\"\u003ebackward\u003c/span\u003e(self, X, y, learning_rate):\n        m = X.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n        self.\u003cspan class=\"hljs-property\"\u003edz\u003c/span\u003e = [self.\u003cspan class=\"hljs-property\"\u003ea\u003c/span\u003e[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e] - y]\n\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ereversed\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e) - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)):\n            self.\u003cspan class=\"hljs-property\"\u003edz\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(np.\u003cspan class=\"hljs-title function_\"\u003edot\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003edz\u003c/span\u003e[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'weights'\u003c/span\u003e].\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e) * self.\u003cspan class=\"hljs-title function_\"\u003esigmoid_derivative\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ea\u003c/span\u003e[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]))\n\n        self.\u003cspan class=\"hljs-property\"\u003edz\u003c/span\u003e = self.\u003cspan class=\"hljs-property\"\u003edz\u003c/span\u003e[::-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e)):\n            self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e[i][\u003cspan class=\"hljs-string\"\u003e'weights'\u003c/span\u003e] -= learning_rate * np.\u003cspan class=\"hljs-title function_\"\u003edot\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ea\u003c/span\u003e[i].\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003edz\u003c/span\u003e[i]) / m\n            self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e[i][\u003cspan class=\"hljs-string\"\u003e'biases'\u003c/span\u003e] -= learning_rate * np.\u003cspan class=\"hljs-title function_\"\u003esum\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003edz\u003c/span\u003e[i], axis=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, keepdims=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e) / m\n\n    def \u003cspan class=\"hljs-title function_\"\u003esigmoid\u003c/span\u003e(self, x):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e / (\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e + np.\u003cspan class=\"hljs-title function_\"\u003eexp\u003c/span\u003e(-x))\n\n    def \u003cspan class=\"hljs-title function_\"\u003esigmoid_derivative\u003c/span\u003e(self, x):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e x * (\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e - x)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 섹션에서는 신경망의 작동 방식에 중요한 조정을 가했으며, 임의의 층 수를 유연하게 지원하는 모델을 목표로했습니다. 변경된 사항은 다음과 같습니다:\u003c/p\u003e\n\u003cp\u003e먼저, 이전에 각 층의 노드 수를 정의했던 self.input, self.hidden, self.output 변수를 삭제했습니다. 이제 목표는 임의의 층 수를 관리할 수 있는 다목적 모델입니다. 예를 들어, 이전에 숫자 데이터셋에 사용했던 모델인 64개의 입력 노드, 64개의 은닉 노드 및 10개의 출력 노드를 사용하는 경우, 다음과 같이 설정할 수 있습니다:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003enn = \u003cspan class=\"hljs-title class_\"\u003eNeuralNetwork\u003c/span\u003e((layers = [\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e]));\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 코드가 각 레이어를 세 번씩 순환하며 다른 목적으로 사용됨을 알 수 있습니다:\u003c/p\u003e\n\u003cp\u003e초기화 과정 중에는 모든 레이어의 가중치와 편향이 설정됩니다. 이 단계는 학습 프로세스를 위해 초기 매개변수로 네트워크를 준비하는 데 중요합니다.\u003c/p\u003e\n\u003cp\u003e순방향 패스 동안 활성화 self.a는 리스트에 수집됩니다. 입력 레이어의 활성화(본질적으로 입력 데이터 X)로 시작합니다. 각 레이어에 대해, np.dot(self.a[-1], layer['weights']) + layer['biases']를 사용하여 가중치 합과 편향을 계산하고 시그모이드 활성화 함수를 적용하여 결과를 self.a에 첨부합니다. 네트워크의 결과는 self.a의 마지막 요소로, 최종 출력을 나타냅니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e역 전파 동안, 이 단계는 마지막 레이어의 활성화에 대한 손실의 도함수를 계산하고 출력 레이어의 오차 목록을 준비함으로써 시작합니다 (self.dz). 그런 다음 네트워크를 역방향으로 거슬러 올라가며 (reversed(range(len(self.layers) - 1))를 사용하여), 숨은 레이어에 대한 오차 항목을 계산합니다. 이 과정은 현재 오차 항목을 다음 레이어의 가중치와 점곱(역방향)하여 시그모이드 함수의 도함수로 비선형성을 처리하는 작업을 포함합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTrainer\u003c/span\u003e:\n    ...\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, X_train, y_train, X_test, y_test, epochs, learning_rate\u003c/span\u003e):\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e _ \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(epochs):\n            self.model.forward(X_train)\n            self.model.backward(X_train, y_train, learning_rate)\n            train_loss = self.calculate_loss(y_train, self.model.a[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n            self.train_loss.append(train_loss)\n\n            self.model.forward(X_test)\n            test_loss = self.calculate_loss(y_test, self.model.a[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n            self.test_loss.append(test_loss)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e마지막으로, NeuralNetwork 클래스의 변화에 따라 Trainer 클래스를 업데이트했습니다. 주요한 수정 사항은 특히 train 메서드에 있으며, 네트워크의 출력이 이제 self.model.a[-1]에서 가져온다는 점 때문에 훈련 및 테스트 손실을 다시 계산하는 방식입니다.\u003c/p\u003e\n\u003cp\u003e이러한 수정 사항은 우리의 신경망을 다양한 아키텍처에 적응할 수 있도록 할뿐만 아니라 데이터와 그래디언트의 흐름을 이해하는 중요성을 강조합니다. 구조를 간소화함으로써, 각종 작업에서 네트워크의 성능을 실험하고 최적화할 수 있는 능력을 향상시킵니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e3: 향상된 학습을 위한 최적화 기법\u003c/h1\u003e\n\u003cp\u003e신경망을 최적화하는 것은 그들이 배우는 능력을 향상시키고 효율적인 학습을 보장하며 최상의 버전으로 이끄는 데 중요합니다. 저희 모델이 얼마나 잘 수행되는지에 상당한 영향을 미치는 몇 가지 중요한 최적화 기술에 대해 알아보겠습니다.\u003c/p\u003e\n\u003ch2\u003e3.1: 학습률\u003c/h2\u003e\n\u003cp\u003e학습률은 손실 경사에 기반하여 네트워크의 가중치를 조정하는 제어 장치입니다. 이는 모델이 학습하는 속도를 결정하며 최적화 중에 취하는 단계가 얼마나 큰지 작은지를 결정합니다. 학습률을 적절하게 설정하면 모델이 빠르게 낮은 오차의 해결책을 찾을 수 있습니다. 그러나 올바르게 설정하지 않으면 모델이 수렴하는 데 시간이 오래 걸리거나 아예 좋은 해결책을 찾지 못할 수 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e너무 높은 학습률을 설정하면 모델이 최적해를 뛰어넘을 수 있어 불안정한 행동을 일으킬 수 있습니다. 이는 정확도나 손실이 훈련 중에 급격하게 변하는 것으로 나타날 수 있어요.\u003c/p\u003e\n\u003cp\u003e학습률이 너무 낮으면 훈련 과정이 지나치게 느리게 진행될 수 있어요. 이 경우, 훈련 손실이 시간이 지남에 따라 거의 변하지 않는 것을 볼 수 있어요.\u003c/p\u003e\n\u003cp\u003e관건은 훈련 및 검증 손실을 추적하면서 학습률이 어떻게 작동하는지에 대한 단서를 얻는 것이에요. 훈련 중에 일정 간격으로 이러한 손실을 기록하고 나중에 이를 플로팅하여 손실 landscape가 얼마나 매끄럽거나 불안정한지 보다 명확히 파악할 수 있어요. 우리의 코드에서는 이러한 메트릭을 추적하기 위해 Python의 logging 라이브러리를 사용하고 있어요. 이렇게 생겼답니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e logging\n\u003cspan class=\"hljs-comment\"\u003e# Logger 설정\u003c/span\u003e\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTrainer\u003c/span\u003e:\n    ...\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, X_train, y_train, X_val, y_val, epochs, learning_rate\u003c/span\u003e):\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e epoch \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(epochs):\n            ...\n            \u003cspan class=\"hljs-comment\"\u003e# 50 에폭마다 손실 및 검증 손실을 로그로 남깁니다\u003c/span\u003e\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e epoch % \u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e:\n                logger.info(\u003cspan class=\"hljs-string\"\u003ef'에폭 \u003cspan class=\"hljs-subst\"\u003e{epoch}\u003c/span\u003e: 손실 = \u003cspan class=\"hljs-subst\"\u003e{train_loss}\u003c/span\u003e, 검증 손실 = \u003cspan class=\"hljs-subst\"\u003e{val_loss}\u003c/span\u003e'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e시작할 때, 우리는 훈련 업데이트를 캡처하고 표시하기 위해 로거를 설정했습니다. 이 설정을 통해 우리는 훈련 및 검증 손실을 매 50번째 에포크마다 기록하여 모델의 진행 상황에 대한 안정적인 피드백을 받을 수 있습니다. 이 피드백을 통해 손실이 잘 감소하고 있는지, 아니면 너무 불규칙하게 변동하는지 파악할 수 있어서 학습률을 조정해야 할 필요가 있을지도 모릅니다.\u003c/p\u003e\n\u003cp\u003e위의 코드는 훈련 및 검증 손실을 그래프로 플로팅하여 훈련 중에 손실이 어떻게 변화하는지 더 잘 이해할 수 있도록 해줍니다. 많은 반복에서 약간의 잡음이 예상되므로 부드러운(스무딩) 효과를 추가했습니다. 잡음을 부드럽게 처리하여 그래프를 더 잘 분석할 수 있도록 도와줄 것입니다.\u003c/p\u003e\n\u003cp\u003e이러한 방식을 따르면 훈련을 시작하면 로그가 나타나면서 우리의 진행 상황을 한 눈에 볼 수 있고 조정할 수 있는 정보를 제공하여 우리가 방향을 수정하는 데 도움이 될 것입니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_1.png\"\u003e\n\u003cp\u003e그런 다음, 훈련이 끝난 후 손실을 그래프로 그려볼 수 있습니다:\u003c/p\u003e\n\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_2.png\"\u003e\n\u003cp\u003e훈련 및 검증 손실이 꾸준히 감소하는 것을 보는 것은 좋은 신호입니다. 이는 에포크 수를 늘리고 학습률 스텝 크기를 증가시킨다면 잘 작동할 수 있다는 신호일 수 있습니다. 그러나 반대로 손실이 감소한 후 급상승하는 것을 관찰하면, 학습률 스텝 크기를 줄이는 것이 명백한 신호입니다. 그렇지만 재미있는 점이 있습니다: 에포크 0부터 50까지 우리의 손실이 어떤 이상한 일이 일어나고 있습니다. 우리는 그 부분을 확인하기 위해 다시 살펴보겠습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e그 달콤한 학습률의 최적값을 찾기 위해서는 학습률 앤달링 또는 적응형 학습률 기법과 같은 방법이 정말 유용할 수 있어요. 이러한 방법들은 학습률을 실시간으로 세밀하게 조정하여 훈련 중에 최적의 속도를 유지하도록 도와줘요.\u003c/p\u003e\n\u003ch2\u003e3.2: 조기 중단 기법\u003c/h2\u003e\n\u003cp\u003e조기 중단은 마치 안전망 같아요 — 유효성 검사 세트에서 모델의 성능을 보고, 더 이상 성능이 개선되지 않을 때 훈련을 중지하는 것이에요. 이는 과적합에 대한 안전 장치이며, 우리 모델이 이전에 본 적 없는 데이터에서도 잘 작동하도록 보장해줘요.\u003c/p\u003e\n\u003cp\u003e여기에 이를 실행하는 방법이 있어요:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e검증 세트: 교육 데이터의 일부를 분리하여 검증 세트로 사용합니다. 이것은 중요합니다. 왜냐하면 이렇게 하면 멈춤 결정이 신선한 보이지 않는 데이터에 기반하기 때문입니다.\u003c/li\u003e\n\u003cli\u003e모니터링: 각 학습 에포크 후 모델이 검증 세트에서 어떻게 수행되는지 주시하세요. 성능이 향상되고 있나요, 아니면 정체되었나요?\u003c/li\u003e\n\u003cli\u003e멈춤 기준: 멈출 시점을 결정하세요. 일반적으로 \"연속적으로 50번의 에포크 동안 검증 손실이 향상되지 않음\"이 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이를 위한 코드를 살펴보죠:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eTrainer\u003c/span\u003e:\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003etrain\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, X_train, y_train, X_val, y_val, epochs, learning_rate,\n              early_stopping=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e, patience=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e\u003c/span\u003e):\n        best_loss = np.inf\n        epochs_no_improve = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e epoch \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(epochs):\n           ...\n\n            \u003cspan class=\"hljs-comment\"\u003e# 조기 중단\u003c/span\u003e\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e early_stopping:\n                \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e val_loss \u0026#x3C; best_loss:\n                    best_loss = val_loss\n                    best_weights = [layer[\u003cspan class=\"hljs-string\"\u003e'weights'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e layer \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.model.layers]\n                    epochs_no_improve = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n                \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n                    epochs_no_improve += \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n\n                \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e epochs_no_improve == patience:\n                    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'조기 중단!'\u003c/span\u003e)\n                    \u003cspan class=\"hljs-comment\"\u003e# 최적의 가중치로 복원\u003c/span\u003e\n                    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i, layer \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eenumerate\u003c/span\u003e(self.model.layers):\n                        layer[\u003cspan class=\"hljs-string\"\u003e'weights'\u003c/span\u003e] = best_weights[i]\n                    \u003cspan class=\"hljs-keyword\"\u003ebreak\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003etrain 메서드에서 두 가지 새로운 옵션을 소개했습니다:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003eearly_stopping: 이는 조기 중단을 켜거나 끄는 여부를 결정하는 이진 플래그입니다.\u003c/li\u003e\n\u003cli\u003epatience: 이는 훈련을 중단하기 전에 유효성 검사 손실이 향상되지 않은 라운드 수를 설정합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e우리는 가장 낮은 유효성 검사 손실을 현재까지 본 최저치로 설정하기 위해 best_loss를 무한대로 설정합니다. 한편, epochs_no_improve는 얼마 동안 유효성 검사 손실이 개선되지 않은 에포크 수를 기록합니다.\u003c/p\u003e\n\u003cp\u003e모델을 훈련하기 위해 각 에포크를 순회하는 동안에는 실제 훈련 단계(순방향 전파 및 역전파)가 여기에 자세히 나와 있지는 않지만 프로세스의 중요한 부분입니다.\u003c/p\u003e\n\u003cp\u003e매 에포크가 끝날 때마다 현재 에포크의 유효성 검사 손실(val_loss)이 best_loss보다 낮아졌다면, 이는 우리가 진전을 이루고 있다는 뜻입니다. 우리는 best_loss를 이 새로운 최솟값으로 업데이트하고, 또한 현재 모델 가중치를 best_weights로 저장합니다. 이렇게 하면 모델이 최상의 성능을 발휘한 시점의 스냅샷을 항상 가지게 됩니다. 그리고 우리는 방금 개선을 보았기 때문에 epochs_no_improve 카운트를 다시 0으로 재설정합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e만약 val_loss에 감소가 없다면, epochs_no_improve를 하나씩 증가시켜서 다른 epoch가 향상되지 않은 것으로 표시합니다.\u003c/p\u003e\n\u003cp\u003e만약 우리가 설정한 인내심 한계치에 epochs_no_improve 카운트가 달성하면, 모델이 더 나아질 가능성이 낮다는 신호로 조기 종료를 시작합니다. 메시지와 함께 알림을 표시하고, 모델의 가중치를 최적의 가중치인 best_weights로 되돌립니다. 그런 다음 학습 루프를 종료합니다.\u003c/p\u003e\n\u003cp\u003e이 접근 방식은 학습을 중단하는 균형있는 방법을 제공합니다. 모델에 학습의 공정한 기회를 제공하여 너무 일찍 중단하지 않으면서, 너무 늦지도 않아 시간을 낭비하거나 과적합의 위험을 가져올 수 있습니다.\u003c/p\u003e\n\u003ch2\u003e3.3: 초기화 방법\u003c/h2\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e신경망을 설정할 때, 가중치를 어떻게 시작하느냐에 따라 네트워크가 얼마나 잘, 그리고 얼마나 빨리 학습하는지가 달라질 수 있어요. 가중치를 초기화하는 몇 가지 다른 방법 – 랜덤, 영, Glorot(Xavier), 그리고 He 초기화 – 에 대해 알아봐요.\u003c/p\u003e\n\u003cp\u003e랜덤 초기화\n랜덤 방식을 선택하면 주로 균일하거나 정규 분포에서 숫자를 추출하여 초기 가중치를 설정하는 것을 의미해요. 이러한 무작위성은 모든 뉴런이 동일한 위치에서 시작하지 않도록하여 네트워크가 학습함에 따라 서로 다른 것을 배울 수 있도록 도와줘요. 핵심은 적절한 분산을 선택하는 것인데, 너무 많으면 기울기가 폭발할 위험이 있고, 너무 적으면 사라질 수도 있어요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eweights = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erandn\u003c/span\u003e(layers[i], layers[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 코드 라인은 표준 정규 분포에서 가중치를 추출하여 각 뉴런이 학습의 길로 나아갈 수 있도록 준비를 합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e장점: 뉴런이 서로 모방하는 것을 방지하는 간단한 방법입니다.\u003c/p\u003e\n\u003cp\u003e단점: 분산을 잘못 설정하면 학습 과정이 불안정해질 수 있습니다.\u003c/p\u003e\n\u003cp\u003e제로 초기화\n모든 가중치를 0으로 설정하는 방법은 매우 간단합니다. 그러나 이 방법에는 주요 단점이 있습니다: 이로 인해 층의 모든 뉴런이 사실상 동일해집니다. 이러한 동일성으로 인해 네트워크의 학습이 저해될 수 있으며, 모든 층의 뉴런이 학습 중에 동일하게 업데이트될 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eweights = np.\u003cspan class=\"hljs-title function_\"\u003ezeros\u003c/span\u003e((layers[i], layers[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]));\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e마지막으로, 우리는 모두 0으로 채워진 가중치 행렬을 얻습니다. 깔끔하고 정돈되어 있지만, 네트워크를 통해 나아가는 모든 경로가 처음에는 동일한 가중치를 갖게 되어 학습 다양성을 위한 좋지 않은 결과를 초래할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e장점: 구현이 매우 쉽습니다.\u003c/p\u003e\n\u003cp\u003e단점: 학습 과정을 제약시켜 네트워크의 성능이 보통 좋지 않게끔 만듭니다.\u003c/p\u003e\n\u003cp\u003eGlorot 초기화\n시그모이드 활성화 함수를 사용하는 네트워크를 위해 설계된 Glorot 초기화는 네트워크 내 입력 단위와 출력 단위의 수에 기반하여 가중치를 설정합니다. 이 초기화는 활성화와 역전파된 그래디언트의 분산을 유지하고 vanishing 또는 exploding 그래디언트 문제를 방지하기 위해 레이어를 통해 전달됩니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e글로럿 초기화에서의 가중치는 균일 분포나 정규 분포로 생성할 수 있습니다. 균일 분포를 사용하는 경우, 가중치는 [-a, a] 범위로 초기화됩니다. 여기서 a 값은:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_3.png\" alt=\"식\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eglorot_uniform\u003c/span\u003e(self, fan_in, fan_out):\n    limit = np.\u003cspan class=\"hljs-title function_\"\u003esqrt\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e / (fan_in + fan_out))\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003euniform\u003c/span\u003e(-limit, limit, (fan_in, fan_out))\n\nweights = \u003cspan class=\"hljs-title function_\"\u003eglorot_uniform\u003c/span\u003e(layers[i - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], layers[i])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 공식은 가중치가 균등하게 분포되고, 가져올 수 있으며, 좋은 기울기 흐름을 유지할 수 있도록 보장합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e정상 분포에 대한 정보입니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_4.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eglorot_normal\u003c/span\u003e(self, fan_in, fan_out):\n    stddev = np.\u003cspan class=\"hljs-title function_\"\u003esqrt\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e2.\u003c/span\u003e / (fan_in + fan_out))\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003enormal\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e, stddev, size=(fan_in, fan_out))\n\nweights = self.\u003cspan class=\"hljs-title function_\"\u003eglorot_normal\u003c/span\u003e(layers[i - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], layers[i])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 조정은 시그모이드 활성화 함수를 사용하는 네트워크에서 적절하게 가중치를 유지합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e장점: 합리적인 범위 내 그라디언트 변화를 유지하여 심층 신경망의 안정성을 향상시킵니다.\u003c/p\u003e\n\u003cp\u003e단점: ReLU(또는 변형) 활성화를 사용하는 레이어에는 신호 전파 특성이 다르기 때문에 최적이 아닐 수 있습니다.\u003c/p\u003e\n\u003cp\u003eHe 초기화\nHe 초기화는 ReLU 활성화 함수를 사용하는 레이어에 적합하게 설계되었으며, ReLU의 비선형 특성을 고려하여 가중치의 분산을 조정합니다. 이 전략은 특히 ReLU가 일반적으로 사용되는 깊은 신경망에서 그라디언트 흐름을 유지하는 데 도움이 됩니다.\u003c/p\u003e\n\u003cp\u003eGlorot 초기화와 마찬가지로, 가중치는 균등 분포 또는 정규 분포에서 선택할 수 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e균일 분포를 위해 가중치는 [-a, a] 범위를 사용하여 초기화됩니다. 여기서 a는 다음과 같이 계산됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_5.png\" alt=\"a 계산 공식\"\u003e\u003c/p\u003e\n\u003cp\u003e따라서 가중치 W는 균일 분포에서 추출됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_6.png\" alt=\"균일 분포에서 가중치 추출 공식\"\u003e\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003ehe_uniform\u003c/span\u003e(self, fan_in, fan_out):\n    limit = np.\u003cspan class=\"hljs-title function_\"\u003esqrt\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e / fan_in)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003euniform\u003c/span\u003e(-limit, limit, (fan_in, fan_out))\n\nweights = self.\u003cspan class=\"hljs-title function_\"\u003ehe_uniform\u003c/span\u003e(layers[i - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], layers[i])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e일반 분포를 사용할 때, 가중치는 다음과 같은 수식에 따라 초기화됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_7.png\" alt=\"수식\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서 W는 가중치를, N은 정규 분포를, 0은 분포의 평균을, 그리고 2/n은 분산을 나타냅니다. n-in은 레이어로 들어오는 입력 단위의 수를 나타냅니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ehe_normal\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, fan_in, fan_out\u003c/span\u003e):\n    stddev = np.sqrt(\u003cspan class=\"hljs-number\"\u003e2.\u003c/span\u003e / fan_in)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.random.normal(\u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e, stddev, size=(fan_in, fan_out))\n\nweights = self.he_normal(layers[i - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], layers[i])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e양쪽 경우 모두 초기화 전략은 ReLU 활성화 함수의 특성을 반영하려고 합니다. 이 함수는 양수 입력에 대해 비활성화된 뉴런을 가지기 때문에 초기 가중치의 분산 조정은 깊은 네트워크에서 발생할 수 있는 그래디언트의 소실 또는 폭발을 방지하고 더 안정적이고 효율적인 훈련 과정을 촉진합니다.\u003c/p\u003e\n\u003cp\u003e장점: ReLU 활성화 함수를 사용하는 네트워크에서 그래디언트 크기를 유지하여 깊은 학습 모델을 용이하게 학습시킵니다.\u003c/p\u003e\n\u003cp\u003e단점: 특히 ReLU에 최적화되어 있어 다른 활성화 함수만큼 효과적이지 않을 수 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이제 초기화를 소개한 후 NeuralNetwork 클래스가 어떻게 보이는지 살펴보겠습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e클래스 \u003cspan class=\"hljs-title class_\"\u003eNeuralNetwork\u003c/span\u003e:\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self,\n                 layers,\n                 init_method=\u003cspan class=\"hljs-string\"\u003e'glorot_uniform'\u003c/span\u003e, # \u003cspan class=\"hljs-string\"\u003e'zeros'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'random'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'glorot_uniform'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'glorot_normal'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'he_uniform'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'he_normal'\u003c/span\u003e\n                 loss_func=\u003cspan class=\"hljs-string\"\u003e'mse'\u003c/span\u003e,\n                 ):\n        ...\n\n        self.\u003cspan class=\"hljs-property\"\u003einit_method\u003c/span\u003e = init_method\n\n        # 레이어 초기화\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(layers) - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e):\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003einit_method\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'zeros'\u003c/span\u003e:\n                weights = np.\u003cspan class=\"hljs-title function_\"\u003ezeros\u003c/span\u003e((layers[i], layers[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]))\n            elif self.\u003cspan class=\"hljs-property\"\u003einit_method\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'random'\u003c/span\u003e:\n                weights = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erandn\u003c/span\u003e(layers[i], layers[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n            elif self.\u003cspan class=\"hljs-property\"\u003einit_method\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'glorot_uniform'\u003c/span\u003e:\n                weights = self.\u003cspan class=\"hljs-title function_\"\u003eglorot_uniform\u003c/span\u003e(layers[i], layers[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n            elif self.\u003cspan class=\"hljs-property\"\u003einit_method\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'glorot_normal'\u003c/span\u003e:\n                weights = self.\u003cspan class=\"hljs-title function_\"\u003eglorot_normal\u003c/span\u003e(layers[i], layers[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n            elif self.\u003cspan class=\"hljs-property\"\u003einit_method\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'he_uniform'\u003c/span\u003e:\n                weights = self.\u003cspan class=\"hljs-title function_\"\u003ehe_uniform\u003c/span\u003e(layers[i], layers[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n            elif self.\u003cspan class=\"hljs-property\"\u003einit_method\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'he_normal'\u003c/span\u003e:\n                weights = self.\u003cspan class=\"hljs-title function_\"\u003ehe_normal\u003c/span\u003e(layers[i], layers[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n\n            \u003cspan class=\"hljs-attr\"\u003eelse\u003c/span\u003e:\n                raise \u003cspan class=\"hljs-title class_\"\u003eValueError\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'알 수없는 초기화 방법 {self.init_method}'\u003c/span\u003e)\n\n            self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e({\n                \u003cspan class=\"hljs-string\"\u003e'weights'\u003c/span\u003e: weights,\n                \u003cspan class=\"hljs-string\"\u003e'biases'\u003c/span\u003e: np.\u003cspan class=\"hljs-title function_\"\u003ezeros\u003c/span\u003e((\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, layers[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]))\n            })\n\n        ...\n\n    ...\n\n    def \u003cspan class=\"hljs-title function_\"\u003eglorot_uniform\u003c/span\u003e(self, fan_in, fan_out):\n        limit = np.\u003cspan class=\"hljs-title function_\"\u003esqrt\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e / (fan_in + fan_out))\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003euniform\u003c/span\u003e(-limit, limit, (fan_in, fan_out))\n\n    def \u003cspan class=\"hljs-title function_\"\u003ehe_uniform\u003c/span\u003e(self, fan_in, fan_out):\n        limit = np.\u003cspan class=\"hljs-title function_\"\u003esqrt\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e / fan_in)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003euniform\u003c/span\u003e(-limit, limit, (fan_in, fan_out))\n\n    def \u003cspan class=\"hljs-title function_\"\u003eglorot_normal\u003c/span\u003e(self, fan_in, fan_out):\n        stddev = np.\u003cspan class=\"hljs-title function_\"\u003esqrt\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e2.\u003c/span\u003e / (fan_in + fan_out))\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003enormal\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e, stddev, size=(fan_in, fan_out))\n\n    def \u003cspan class=\"hljs-title function_\"\u003ehe_normal\u003c/span\u003e(self, fan_in, fan_out):\n        stddev = np.\u003cspan class=\"hljs-title function_\"\u003esqrt\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e2.\u003c/span\u003e / fan_in)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003enormal\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e, stddev, size=(fan_in, fan_out))\n\n    ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e적절한 가중치 초기화 전략을 선택하는 것은 효과적인 신경망 학습에 중요합니다. 무작위 및 영점 초기화는 기본적인 접근법을 제공하지만 항상 최적의 학습 동역학을 이끌어내지 않을 수 있습니다. 반면, Glorot/Xavier 및 He 초기화는 신경망 아키텍처 및 사용된 활성화 함수를 고려하여 딥 러닝 모델의 특정 요구 사항을 고려하는 더 소박한 솔루션을 제공합니다. 이러한 전략은 너무 빠른 학습과 너무 느린 학습 사이의 절충안을 균형있게 맞추어 더 신뢰할 수 있는 수렴 방향으로 학습 프로세스를 이끕니다.\u003c/p\u003e\n\u003ch2\u003e3.4: 드롭아웃\u003c/h2\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eDropout은 신경망에서 오버피팅을 방지하기 위해 설계된 정규화 기술로, 훈련 단계에서 네트워크에서 임시로 그리고 무작위로 유닛(뉴런)과 해당 연결을 제거함으로써 사용합니다. 이 방법은 Srivastava 및 그 동료들이 2014 년 논문에서 고안한 간단하면서도 효과적인 방법으로 견고한 신경망을 훈련하는 데 사용됩니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_8.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e각 훈련 반복에서 각 뉴런(입력 단위 포함되지만 보통 출력 단위는 제외)은 일시적으로 \"드랍아웃\"될 확률 p를 가집니다. 이는 해당 뉴런이 이 전방 및 역방향 패스 동안 완전히 무시된다는 것을 의미합니다. 이 확률 p은 \"드랍아웃 비율\"로 불리며 성능을 최적화하기 위해 조절할 수 있는 하이퍼파라미터입니다. 예를 들어, 0.5의 드랍아웃 비율은 각 뉴런이 각 훈련 패스에서 계산에서 제외될 확률이 50% 라는 것을 의미합니다.\u003c/p\u003e\n\u003cp\u003e이 과정의 효과는 네트워크가 개별 뉴런의 특정 가중치에 덜 민감해진다는 것입니다. 이것은 예측을 할 때 개별 뉴런의 출력에 의존할 수 없으므로 네트워크가 뉴런들 사이에 중요성을 분산시키도록 장려합니다. 이는 실제로 가중치를 공유하는 신경망의 의사앙상블을 훈련하며, 각 훈련 반복에서 네트워크의 다른 \"드랍아웃된\" 버전이 포함됩니다. 시험 시간에는 드랍아웃이 적용되지 않고, 대신 가중치는 일반적으로 드랍아웃 비율 p에 의해 조정되어 더 많은 유닛이 활성화되었다는 사실을 균형 있게 합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e올바른 드롭아웃 비율 선택하기\n드롭아웃 비율은 각 신경망 구조와 데이터셋에 대해 조정이 필요한 하이퍼파라미터입니다. 일반적으로, 숨겨진 유닛에 대해 시작점으로 0.5의 비율이 사용되며, 이는 원래 드롭아웃 논문에서 제안되었습니다.\u003c/p\u003e\n\u003cp\u003e높은 드롭아웃 비율 (1에 가까운 값)은 학습 중에 더 많은 뉴런이 제거되는 것을 의미합니다. 이는 네트워크가 데이터를 충분히 학습하지 못할 수 있어서, 훈련 데이터의 복잡성을 모델링하는 데 어려움을 겪어 과소적합을 초래할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e반대로, 낮은 드롭아웉 비율 (0에 가까운 값)은 더 적은 뉴런이 제거되어 드롭아웃의 정규화 효과가 줄어들 수 있으며, 이는 모델이 훈련 데이터에서 잘 수행되지만 보이지 않는 데이터에서 성능이 나빠질 수 있는 과적합을 초래할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e코드 구현\n우리 코드에서 어떻게 보이는지 살펴보겠습니다:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eNeuralNetwork\u003c/span\u003e:\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself,\n                 layers,\n                 init_method=\u003cspan class=\"hljs-string\"\u003e'glorot_uniform'\u003c/span\u003e, \u003cspan class=\"hljs-comment\"\u003e# 'zeros', 'random', 'glorot_uniform', 'glorot_normal', 'he_uniform', 'he_normal'\u003c/span\u003e\n                 loss_func=\u003cspan class=\"hljs-string\"\u003e'mse'\u003c/span\u003e,\n                 dropout_rate=\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e\n                 \u003c/span\u003e):\n        ...\n\n        self.dropout_rate = dropout_rate\n\n        ...\n\n    ...\n\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, X, is_training=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e\u003c/span\u003e):\n        self.a = [X]\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i, layer \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eenumerate\u003c/span\u003e(self.layers):\n            z = np.dot(self.a[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], layer[\u003cspan class=\"hljs-string\"\u003e'weights'\u003c/span\u003e]) + layer[\u003cspan class=\"hljs-string\"\u003e'biases'\u003c/span\u003e]\n            a = self.sigmoid(z)\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e is_training \u003cspan class=\"hljs-keyword\"\u003eand\u003c/span\u003e i \u0026#x3C; \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(self.layers) - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e:  \u003cspan class=\"hljs-comment\"\u003e# apply dropout to all layers except the output layer\u003c/span\u003e\n                dropout_mask = np.random.rand(*a.shape) \u003e self.dropout_rate\n                a *= dropout_mask\n            self.a.append(a)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self.a[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n\n    ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e저희 신경망 클래스는 새로운 초기화 매개변수와 드롭아웃 정규화를 포함한 새로운 순전파 메서드로 업그레이드되었습니다.\u003c/p\u003e\n\u003cp\u003edropout_rate : 이것은 훈련 중에 신경세포들이 네트워크에서 일시적으로 제거될 가능성을 결정하는 설정입니다. 오버피팅을 피하는 데 도움이 됩니다. 0.5로 설정함으로써 어떤 신경세포가 한 번의 훈련 라운드에서 \"제거\"될 확률이 50%라고 말하고 있습니다. 이 무작위성은 네트워크가 어떤 단일 신경세포에 너무 의존하지 않도록 보장하여 더 견고한 학습 과정을 촉진합니다.\u003c/p\u003e\n\u003cp\u003eis_training 부울 플래그는 네트워크가 현재 훈련되고 있는지를 알려줍니다. 이것은 훈련 중에만 드롭아웃이 발생해야 하므로 새 데이터에 대한 네트워크 성능을 평가할 때는 드롭아웃이 일어나서는 안 된다는 점이 중요합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e네트워크를 통해 데이터(X로 표시)가 전달되면, 네트워크는 들어오는 데이터와 레이어의 편향을 가중합(z)으로 계산합니다. 그런 다음 이 합계를 시그모이드 활성화 함수를 통해 활성화(a)로 변환하여 다음 레이어로 전달할 신호를 얻습니다.\u003c/p\u003e\n\u003cp\u003e하지만 훈련 중에 다음 레이어로 진행하기 전에 드롭아웃을 적용할 수 있습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eis_training이 true이고 출력 레이어를 다루고 있지 않다면, 각 뉴런에 대해 주사위를 굴려 떨어뜨릴지 여부를 확인합니다. 이를 위해 무작위 수가 드롭아웃 비율을 초과하는지 확인하여 드롭아웃 마스크(모양은 a와 같은 배열)를 생성합니다.\u003c/li\u003e\n\u003cli\u003e이 마스크를 사용하여 a의 일부 활성화를 0으로 만들어 네트워크에서 일시적으로 뉴런을 제거하는 것을 흉내냅니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e드롭아웃을 적용한 후(해당하는 경우), 생성된 활성화를 self.a에 추가하여 모든 레이어를 통해 활성화를 추적하는 리스트를 유지합니다. 이렇게 하면 신호를 그냥 한 레이어에서 다음 레이어로 무작정 이동시키는 것이 아니라, 네트워크가 더 견고하게 학습하도록 장려하는 기술을 적용하여 특정 경로의 뉴런에 지나치게 의존하지 않도록 합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e3.5: 그레이디언트 클리핑\u003c/h2\u003e\n\u003cp\u003e그레이디언트 클리핑은 깊은 신경망을 훈련할 때 중요한 기술로, 특히 폭주하는 그레이디언트 문제를 해결할 때 주로 사용됩니다. 폭주하는 그레이디언트는 신경망의 매개변수에 대한 손실 함수의 미분이나 그레이디언트가 층을 거치면서 지수적으로 증가하여 훈련 중에 가중치에 대해 매우 큰 업데이트를 유도할 때 발생합니다. 이는 학습 과정을 불안정하게 만들 수 있으며, 종종 가중치나 손실에서 NaN 값의 형태로 나타나 수치 오버플로우 때문에 발산하여 모델이 해결책으로 수렴하지 못하도록 방해할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e그레이디언트 클리핑은 값에 의한 클리핑과 법에 의한 클리핑 두 가지 주요 방법으로 구현할 수 있으며, 각각 폭주하는 그레이디언트 문제를 완화하는 전략을 가지고 있습니다.\u003c/p\u003e\n\u003cp\u003e값에 의한 클리핑\n이 방법은 미리 정의된 임계값을 설정하고, 각 그레이디언트 구성 요소를 해당 임계값을 초과하는 경우 지정된 범위 내로 직접 클리핑하는 접근 방식입니다. 예를 들어, 임계값이 1로 설정되면, 1보다 큰 모든 그레이디언트 구성 요소를 1로 설정하고, -1보다 작은 모든 구성 요소를 -1로 설정합니다. 이는 모든 그레이디언트가 [-1, 1] 범위 내에 유지되도록 보장하여 너무 커지는 그레이디언트를 효과적으로 방지합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_9.png\"\u003e\n\u003cp\u003egi는 기울기 벡터의 각 구성 요소를 나타냅니다.\u003c/p\u003e\n\u003cp\u003e노름에 의한 클리핑\n이 방법은 각 기울기 구성 요소를 개별적으로 클리핑하는 대신, 일정 임계값을 초과하는 경우 전체 기울기를 조절합니다. 이렇게 하면 기울기의 방향을 보존한 채 크기가 지정된 한도를 초과하지 않도록 할 수 있습니다. 이는 모든 매개변수를 통해 업데이트의 상대적 방향을 유지하는 데 특히 유용하며, 값에 의한 클리핑보다 학습 과정에 더 유익할 수 있습니다.\u003c/p\u003e\n\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_10.png\"\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e그래디언트 벡터를 나타내는 g이고 ∥g∥는 그 노름값입니다.\u003c/p\u003e\n\u003cp\u003e훈련에의 응용\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eNeuralNetwork\u003c/span\u003e:\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self,\n                 layers,\n                 init_method=\u003cspan class=\"hljs-string\"\u003e'glorot_uniform'\u003c/span\u003e, # \u003cspan class=\"hljs-string\"\u003e'zeros'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'random'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'glorot_uniform'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'glorot_normal'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'he_uniform'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'he_normal'\u003c/span\u003e\n                 loss_func=\u003cspan class=\"hljs-string\"\u003e'mse'\u003c/span\u003e,\n                 dropout_rate=\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e,\n                 clip_type=\u003cspan class=\"hljs-string\"\u003e'value'\u003c/span\u003e,\n                 grad_clip=\u003cspan class=\"hljs-number\"\u003e5.0\u003c/span\u003e\n                 ):\n        ...\n\n        self.\u003cspan class=\"hljs-property\"\u003eclip_type\u003c/span\u003e = clip_type\n        self.\u003cspan class=\"hljs-property\"\u003egrad_clip\u003c/span\u003e = grad_clip\n\n        ...\n\n    ...\n\n    def \u003cspan class=\"hljs-title function_\"\u003ebackward\u003c/span\u003e(self, X, y, learning_rate):\n        m = X.\u003cspan class=\"hljs-property\"\u003eshape\u003c/span\u003e[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n        self.\u003cspan class=\"hljs-property\"\u003edz\u003c/span\u003e = [self.\u003cspan class=\"hljs-property\"\u003ea\u003c/span\u003e[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e] - y]\n        self.\u003cspan class=\"hljs-property\"\u003egradient_norms\u003c/span\u003e = []  # 그래디언트 노름을 저장하는 리스트\n\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ereversed\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e) - \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)):\n            self.\u003cspan class=\"hljs-property\"\u003edz\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(np.\u003cspan class=\"hljs-title function_\"\u003edot\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003edz\u003c/span\u003e[-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'weights'\u003c/span\u003e].\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e) * self.\u003cspan class=\"hljs-title function_\"\u003esigmoid_derivative\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ea\u003c/span\u003e[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]))\n            self.\u003cspan class=\"hljs-property\"\u003egradient_norms\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(np.\u003cspan class=\"hljs-property\"\u003elinalg\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003enorm\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e[i + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e][\u003cspan class=\"hljs-string\"\u003e'weights'\u003c/span\u003e]))  # 그래디언트 노름을 계산하고 저장\n\n        self.\u003cspan class=\"hljs-property\"\u003edz\u003c/span\u003e = self.\u003cspan class=\"hljs-property\"\u003edz\u003c/span\u003e[::-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n        self.\u003cspan class=\"hljs-property\"\u003egradient_norms\u003c/span\u003e = self.\u003cspan class=\"hljs-property\"\u003egradient_norms\u003c/span\u003e[::-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]  # 리스트를 뒤집어서 레이어의 순서와 일치시킴\n\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e)):\n            grads_w = np.\u003cspan class=\"hljs-title function_\"\u003edot\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003ea\u003c/span\u003e[i].\u003cspan class=\"hljs-property\"\u003eT\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003edz\u003c/span\u003e[i]) / m\n            grads_b = np.\u003cspan class=\"hljs-title function_\"\u003esum\u003c/span\u003e(self.\u003cspan class=\"hljs-property\"\u003edz\u003c/span\u003e[i], axis=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, keepdims=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e) / m\n\n            # 그래디언트 클리핑\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003eclip_type\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'value'\u003c/span\u003e:\n                grads_w = np.\u003cspan class=\"hljs-title function_\"\u003eclip\u003c/span\u003e(grads_w, -self.\u003cspan class=\"hljs-property\"\u003egrad_clip\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003egrad_clip\u003c/span\u003e)\n                grads_b = np.\u003cspan class=\"hljs-title function_\"\u003eclip\u003c/span\u003e(grads_b, -self.\u003cspan class=\"hljs-property\"\u003egrad_clip\u003c/span\u003e, self.\u003cspan class=\"hljs-property\"\u003egrad_clip\u003c/span\u003e)\n            elif self.\u003cspan class=\"hljs-property\"\u003eclip_type\u003c/span\u003e == \u003cspan class=\"hljs-string\"\u003e'norm'\u003c/span\u003e:\n                grads_w = self.\u003cspan class=\"hljs-title function_\"\u003eclip_by_norm\u003c/span\u003e(grads_w, self.\u003cspan class=\"hljs-property\"\u003egrad_clip\u003c/span\u003e)\n                grads_b = self.\u003cspan class=\"hljs-title function_\"\u003eclip_by_norm\u003c/span\u003e(grads_b, self.\u003cspan class=\"hljs-property\"\u003egrad_clip\u003c/span\u003e)\n\n            self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e[i][\u003cspan class=\"hljs-string\"\u003e'weights'\u003c/span\u003e] -= learning_rate * grads_w\n            self.\u003cspan class=\"hljs-property\"\u003elayers\u003c/span\u003e[i][\u003cspan class=\"hljs-string\"\u003e'biases'\u003c/span\u003e] -= learning_rate * grads_b\n\n    def \u003cspan class=\"hljs-title function_\"\u003eclip_by_norm\u003c/span\u003e(self, grads, clip_norm):\n        l2_norm = np.\u003cspan class=\"hljs-property\"\u003elinalg\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003enorm\u003c/span\u003e(grads)\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e l2_norm \u003e \u003cspan class=\"hljs-attr\"\u003eclip_norm\u003c/span\u003e:\n            grads = grads / l2_norm * clip_norm\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e grads\n\n    ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e초기화 중에 이제 사용할 그래디언트 클리핑 유형(clip_type)과 그래디언트 클리핑 임계값(grad_clip)이 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003ccode\u003eclip_type\u003c/code\u003e은 그레디언트를 값으로 자르는 경우에는 \u003ccode\u003evalue\u003c/code\u003e, 또는 L2 노름에 의해 그레디언트를 자르는 경우에는 \u003ccode\u003enorm\u003c/code\u003e이 될 수 있습니다. grad_clip은 자르는 임계값이나 한계를 지정합니다.\u003c/p\u003e\n\u003cp\u003e그런 다음, 역전파 중에 함수는 네트워크의 각 레이어에 대한 그레디언트를 계산합니다. 가중치(grads_w)와 편향(grads_b)에 대한 손실의 미분 값을 각 레이어마다 계산합니다.\u003c/p\u003e\n\u003cp\u003e만약 \u003ccode\u003eclip_type\u003c/code\u003e이 \u003ccode\u003evalue\u003c/code\u003e인 경우, np.clip을 사용하여 그레디언트를 [-grad_clip, grad_clip] 범위로 자릅니다. 이렇게 하면 그레디언트 성분이 이 한계를 초과하지 않도록 합니다.\u003c/p\u003e\n\u003cp\u003e만약 \u003ccode\u003eclip_type\u003c/code\u003e이 \u003ccode\u003enorm\u003c/code\u003e인 경우, 그레디언트의 노름이 grad_clip을 초과하는 경우 이 방향을 유지하면서 그에 대한 크기를 제한하기 위해 clip_by_norm 메서드가 호출됩니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e클리핑 이후, 각 층의 가중치와 편향을 학습률에 의해 조정하는 데 그래디언트가 사용됩니다.\u003c/p\u003e\n\u003cp\u003e마지막으로, 그래디언트의 L2 노름이 지정된 clip_norm을 초과하는 경우 그래디언트를 스케일링하는 clip_by_norm 메서드를 만듭니다. 이 메서드는 그래디언트의 L2 노름을 계산하고, clip_norm보다 크면 그래디언트를 clip_norm까지 스케일 다운시키면서 방향을 유지합니다. 이는 그래디언트를 그들의 L2 노름으로 나누고 clip_norm을 곱해 달성됩니다.\u003c/p\u003e\n\u003cp\u003e그래디언트 클리핑의 장점\n모델의 가중치에 대한 지나치게 큰 업데이트를 방지함으로써, 그래디언트 클리핑은 더 안정적이고 신뢰할 수 있는 훈련 과정에 기여합니다. 그래디언트의 계산이 큰 업데이트로 인해 불안정성을 초래할 수 있는 경우에도 손실 함수를 최소화하여 옵티마이저가 일관된 진전을 이룰 수 있도록 합니다. 이는 훈련하는 동안 그래디언트의 스케일이 큰 문제로 인해 불안정성 문제에 직면하는 순환 신경망(RNNs) 훈련과 같은 과제에서 특히 유용한 도구로 작용합니다.\u003c/p\u003e\n\u003cp\u003e그래디언트 클리핑은 신경망 훈련의 안정성과 성능을 향상시키는 간단하면서도 강력한 기술입니다. 그래디언트가 지나치게 커지지 않도록 보장함으로써, 훈련 불안정성(과적합, 과소적합, 수렴 속도 저하 등)의 문제를 피하고, 신경망이 효과적이고 효율적으로 학습하기 쉽도록 돕습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e4: 층의 최적 개수 결정하기\u003c/h1\u003e\n\u003cp\u003e신경망을 설계하는 중요한 결정 중 하나는 올바른 층의 개수를 결정하는 것입니다. 이 측면은 네트워크의 데이터로부터 학습하고 새로운, 보지 못한 데이터를 일반화하는 능력에 상당한 영향을 미칩니다. 신경망의 깊이 - 얼마나 많은 층이 있는지 - 능력을 강화시키거나 과적합 또는 학습이 부족하다는 문제로 이어질 수 있습니다.\u003c/p\u003e\n\u003ch2\u003e4.1: 층의 깊이와 모델 성능\u003c/h2\u003e\n\u003cp\u003e신경망에 더 많은 층을 추가하면 학습 능력이 향상되어 데이터의 더 복잡한 패턴과 관계를 파악할 수 있습니다. 이는 추가적인 층이 입력 데이터의 보다 추상적인 표현을 만들 수 있기 때문에 단순한 기능에서 더 복잡한 조합으로 이동할 수 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e더 깊은 신경망은 복잡한 패턴을 모델링할 수 있지만, 추가적인 깊이가 오버피팅으로 이어지는 기묘한 지점이 있습니다. 오버피팅은 모델이 훈련 데이터를 너무 잘 학습하여 그 잡음까지 포함해 새로운 데이터에서 성능이 나빠지는 현상입니다.\u003c/p\u003e\n\u003cp\u003e궁극적인 목표는 훈련 데이터로부터 잘 학습하는 모델을 갖는 것뿐만 아니라 이 학습을 새로운 데이터에서도 정확하게 수행할 수 있는 범용성을 갖는 것입니다. 이를 위해서는 층의 깊이에 대한 적절한 균형을 찾는 것이 중요합니다. 너무 적은 층은 과소적합될 수 있고, 너무 많은 층은 오버피팅될 수 있습니다.\u003c/p\u003e\n\u003ch2\u003e4.2: 적절한 깊이를 테스트하고 선택하는 전략\u003c/h2\u003e\n\u003cp\u003e점진적인 접근 방식\n간단한 모델부터 시작하여 점진적으로 층을 추가하고 검증 성능이 크게 향상될 때까지 관찰합니다. 이 접근 방식은 각 층이 전체 성능에 어떤 기여를 하는지 이해하는 데 도움이 됩니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e모델의 성능을 판단하기 위한 기준으로 검증 세트(학습 중에 사용되지 않은 학습 데이터의 하위 집합)에서 모델이 일반화하는 능력을 향상시키는지 여부를 결정합니다.\u003c/p\u003e\n\u003cp\u003e정규화 기법\n더 많은 레이어를 추가할 때 드롭아웃 또는 L2 정규화와 같은 정규화 방법을 사용하세요. 이러한 기법은 오버피팅의 위험을 줄일 수 있어 추가된 레이어가 모델의 학습 능력에 어떤 가치를 더하는지를 공정하게 평가할 수 있게 해줍니다.\u003c/p\u003e\n\u003cp\u003e학습 동태 관찰\n더 많은 레이어를 추가할 때 학습과 검증 손실을 모니터링하세요. 이 두 지표 사이에 차이가 발생하는 경우 — 학습 손실이 감소하지만 검증 손실이 그렇지 않을 때 — 오버피팅을 나타낼 수 있으며, 현재 깊이가 지나칠 수 있다는 것을 시사할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_11.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이 두 그래프는 기계 학습 모델을 훈련하는 과정에서 발생할 수 있는 두 가지 시나리오를 나타냅니다.\u003c/p\u003e\n\u003cp\u003e첫 번째 그래프에서는 훈련 손실과 검증 손실이 모두 감소하여 비슷한 값으로 수렴합니다. 이것은 이상적인 시나리오로, 모델이 잘 학습하고 적절하게 일반화되고 있음을 나타냅니다. 모델의 성능이 훈련 데이터와 보지 않은 검증 데이터 모두에서 향상되고 있는 것을 의미합니다. 이는 모델이 데이터를 과소적합하거나 과적합하지 않고 있다는 것을 시사합니다.\u003c/p\u003e\n\u003cp\u003e두 번째 그래프에서는 훈련 손실은 감소하지만 검증 손실이 증가합니다. 이는 과적합의 전형적인 징후입니다. 모델이 훈련 데이터를 너무 잘 학습하여 노이즈와 이상점을 포함하고 있으며 보지 않은 데이터에 대한 일반화를 실패합니다. 결과적으로, 검증 데이터에서의 성능이 시간이 지남에 따라 악화됩니다. 이는 모델의 복잡성을 줄이거나 정규화나 드롭아웃과 같은 과적합 방지 기술을 적용해야 할 수도 있다는 것을 나타냅니다.\u003c/p\u003e\n\u003cp\u003e자동화 아키텍처 탐색\n신경망 아키텍처 탐색(NAS) 도구나 Optuna와 같은 하이퍼파라미터 최적화 프레임워크를 활용하여 서로 다른 아키텍처를 체계적으로 탐색하십시오. 이러한 도구는 다양한 구성을 평가하고 검증 지표에서 최상의 성능을 발휘하는 구성을 선택함으로써 최적의 레이어 수를 자동화할 수 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e신경망의 최적 레이어 수를 결정하는 것은 모델의 복잡성과 학습 및 일반화 능력을 균형있게 고려하는 세심한 프로세스입니다. 레이어 추가에 체계적인 방법론을 채택하고 교차 검증을 활용하며 정칙화 기법을 통합함으로써 특정 문제에 적합한 네트워크 깊이를 결정할 수 있습니다. 이를 통해 보이지 않는 데이터에 대한 모델 성능을 최적화할 수 있습니다.\u003c/p\u003e\n\u003ch1\u003e5: Optuna을 활용한 자동 미세 튜닝\u003c/h1\u003e\n\u003cp\u003e최적 성능을 달성하기 위해 신경망을 미세 조정하는 것은 다양한 하이퍼파라미터의 섬세한 균형을 찾는 과정으로, 종종 방대한 탐색 공간 속에서 바늘을 찾는 것처럼 느껴질 수 있습니다. 이때 Optuna와 같은 자동 하이퍼파라미터 최적화 도구가 필요합니다.\u003c/p\u003e\n\u003ch2\u003e5.1: Optuna 소개\u003c/h2\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e옵투나는 최적화 하이퍼파라미터 선택을 자동화하기 위해 설계된 오픈소스 최적화 프레임워크입니다. 이는 가장 효율적인 신경망 모델로 이어지는 매개변수 조합을 식별하는 복잡한 작업을 간단화합니다. 옵투나는 고급 알고리즘을 활용하여 최적화 하이퍼파라미터 공간을 보다 효과적으로 탐색하여, 필요한 계산 자원과 수렴 시간을 줄입니다.\u003c/p\u003e\n\u003ch2\u003e5.2: 옵투나를 활용한 신경망 최적화 통합\u003c/h2\u003e\n\u003cp\u003e옵투나는 베이지안 최적화, 트리 구조 파르젠 추정기, 진화 알고리즘 등 다양한 전략을 활용하여 하이퍼파라미터 공간을 지능적으로 탐색합니다. 이 접근 방식을 통해 옵투나는 가장 유망한 하이퍼파라미터를 빠르게 식별하여 최적화 과정을 크게 가속화할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e옵투나를 신경망 훈련 워크플로우에 통합하는 것은 옵투나가 최소화 또는 최대화하려는 목적 함수를 정의하는 과정을 포함합니다. 이 함수에는 일반적으로 모델 훈련 및 검증 과정이 포함되며, 목표는 검증 손실을 최소화하거나 검증 정확도를 최대화하는 것입니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e검색 공간 정의: 각 하이퍼파라미터 값 범위를 지정하여 Optuna가 탐색할 것입니다 (예: 레이어 수, 학습률, 드롭아웃 비율).\u003c/li\u003e\n\u003cli\u003e시험과 평가: Optuna는 모델을 훈련시키기 위해 매번 새로운 하이퍼파라미터 세트를 선택하는 시험을 진행합니다. 검증 세트에서 모델의 성능을 평가하고 이 정보를 사용하여 탐색을 안내합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e5.3: 실제 구현\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e optuna\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eobjective\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003etrial\u003c/span\u003e):\n    \u003cspan class=\"hljs-comment\"\u003e# 하이퍼파라미터 정의\u003c/span\u003e\n    n_layers = trial.suggest_int(\u003cspan class=\"hljs-string\"\u003e'n_layers'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e)\n    hidden_sizes = [trial.suggest_int(\u003cspan class=\"hljs-string\"\u003ef'hidden_size_\u003cspan class=\"hljs-subst\"\u003e{i}\u003c/span\u003e'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(n_layers)]\n    dropout_rate = trial.suggest_uniform(\u003cspan class=\"hljs-string\"\u003e'dropout_rate'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e)  \u003cspan class=\"hljs-comment\"\u003e# 모든 레이어에 대한 단일 드롭아웃 비율\u003c/span\u003e\n    learning_rate = trial.suggest_loguniform(\u003cspan class=\"hljs-string\"\u003e'learning_rate'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1e-3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1e-1\u003c/span\u003e)\n    init_method = trial.suggest_categorical(\u003cspan class=\"hljs-string\"\u003e'init_method'\u003c/span\u003e, [\u003cspan class=\"hljs-string\"\u003e'glorot_uniform'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'glorot_normal'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'he_uniform'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'he_normal'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'random'\u003c/span\u003e])\n    clip_type = trial.suggest_categorical(\u003cspan class=\"hljs-string\"\u003e'clip_type'\u003c/span\u003e, [\u003cspan class=\"hljs-string\"\u003e'value'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'norm'\u003c/span\u003e])\n    clip_value = trial.suggest_uniform(\u003cspan class=\"hljs-string\"\u003e'clip_value'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e)\n    epochs = \u003cspan class=\"hljs-number\"\u003e10000\u003c/span\u003e\n\n    layers = [input_size] + hidden_sizes + [output_size]\n\n    \u003cspan class=\"hljs-comment\"\u003e# 신경망 생성 및 훈련\u003c/span\u003e\n    nn = NeuralNetwork(layers=layers, loss_func=loss_func, dropout_rate=dropout_rate, init_method=init_method, clip_type=clip_type, grad_clip=clip_value)\n    trainer = Trainer(nn, loss_func)\n    trainer.train(X_train, y_train, X_test, y_test, epochs, learning_rate, early_stopping=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-comment\"\u003e# 신경망 성능 평가\u003c/span\u003e\n    predictions = np.argmax(nn.forward(X_test), axis=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n    accuracy = np.mean(predictions == y_test_labels)\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e accuracy\n\n\u003cspan class=\"hljs-comment\"\u003e# Study 객체 생성 및 목적 함수 최적화\u003c/span\u003e\nstudy = optuna.create_study(study_name=\u003cspan class=\"hljs-string\"\u003e'nn_study'\u003c/span\u003e, direction=\u003cspan class=\"hljs-string\"\u003e'maximize'\u003c/span\u003e)\nstudy.optimize(objective, n_trials=\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e)\n\n\u003cspan class=\"hljs-comment\"\u003e# 최적 하이퍼파라미터 출력\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"Best trial: \u003cspan class=\"hljs-subst\"\u003e{study.best_trial.params}\u003c/span\u003e\"\u003c/span\u003e)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"Best value: \u003cspan class=\"hljs-subst\"\u003e{study.best_trial.value:\u003cspan class=\"hljs-number\"\u003e.3\u003c/span\u003ef}\u003c/span\u003e\"\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOptuna 최적화 프로세스의 핵심은 목적 함수입니다. 이 함수는 시험 목표를 정의하고 각 시험에 대해 Optuna에 의해 호출됩니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cstrong\u003eHeren_layers\u003c/strong\u003e은 신경망의 은닉층의 수이며, 1에서 10 사이를 추천합니다. 층의 수를 변화시킴으로써 얕은 네트워크와 깊은 네트워크 아키텍처를 탐색할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ehidden_sizes\u003c/strong\u003e는 각 층의 크기(뉴런 수)를 저장하며, 32에서 128 사이의 숫자를 제안하여 모델이 다양한 용량을 탐색하게 합니다.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003edropout_rate\u003c/strong\u003e는 균일하게 0.0(드롭아웃 없음)에서 0.5 사이를 제안하여 시험을 통해 정규화 유연성을 가능케 합니다.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003elearning_rate\u003c/strong\u003e는 로그 스케일로 1e-3에서 1e-1 사이를 제안하여, 학습률 최적화에 대한 공통적인 민감도로 인해 크기의 범위를 포괄하는 넓은 탐색 공간을 보장합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e신경망 가중치의 init_method은 일련의 일반적인 전략 중에서 선택됩니다. 이 선택은 훈련의 시작점과 수렴 동작을 영향을 줍니다.\u003c/p\u003e\n\u003cp\u003eclip_type과 clip_value는 그래디언트 클리핑 전략과 값으로, 값이나 노름을 기준으로 클리핑하여 폭발하는 그래디언트를 방지하는 데 도움이 됩니다.\u003c/p\u003e\n\u003cp\u003e그런 다음, 정의된 하이퍼파라미터를 사용하여 NeuralNetwork 인스턴스가 생성되고 훈련됩니다. 각 시행이 일정한 에포크 수동안 실행될 수 있도록 조기 중지가 비활성화되며, 일관된 비교를 보장합니다. 성능은 테스트 세트에서 모델의 예측 정확도를 기반으로 평가됩니다.\u003c/p\u003e\n\u003cp\u003e목적 함수와 NeuralNetwork 인스턴스가 정의된 후 Optuna 스터디로 이동할 수 있습니다. Optuna 스터디 객체는 목적 함수를 최대화(\u003ccode\u003emaximize\u003c/code\u003e)하는 데 사용되며, 이 문맥에서는 신경망의 정확도가 목적 함수입니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e연구는 목적 함수를 여러 번 호출합니다(n_trials=100), 매번 옵튜나 내부 최적화 알고리즘에서 제안한 다른 하이퍼파라미터 세트로 호출합니다. 옵튜나는 시험 이력에 기반하여 지능적으로 제안을 조정하여 하이퍼파라미터 공간을 효율적으로 탐색합니다.\u003c/p\u003e\n\u003cp\u003e이 프로세스를 통해 모든 실험에서 찾은 가장 좋은 하이퍼파라미터 세트(study.best_trial.params)와 달성한 최고 정확도(study.best_trial.value)가 생성됩니다. 이 출력은 주어진 작업에 대한 신경망의 최적 구성에 대한 통찰을 제공합니다.\u003c/p\u003e\n\u003ch2\u003e5.4: 혜택 및 결과\u003c/h2\u003e\n\u003cp\u003e옵튜나를 통합함으로써, 개발자는 하이퍼파라미터 튜닝 프로세스를 자동화할뿐만 아니라 어떻게 다른 매개변수가 모델에 영향을 미치는지에 대한 깊은 통찰을 얻을 수 있습니다. 이를 통해 수동 실험을 통해 걸릴 시간의 일부로 최적화된, 더 견고하고 정확한 신경망이 생성됩니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e옵투나의 체계적인 파라미터 조정 접근법은 신경망 개발에 새로운 수준의 정밀성과 효율성을 제공하여 개발자들이 더 높은 성능 표준을 달성하고 모델이 이룰 수 있는 한계를 뛰어넘을 수 있도록 돕습니다.\u003c/p\u003e\n\u003ch2\u003e5.5: 한계\u003c/h2\u003e\n\u003cp\u003e옵투나는 하이퍼파라미터 최적화에 강력하고 유연한 접근 방식을 제공하지만, 기계 학습 워크플로에 통합할 때 고려해야 할 몇 가지 한계점과 주의 사항이 있습니다.\u003c/p\u003e\n\u003cp\u003e계산 리소스\n각 시도는 신경망을 처음부터 훈련해야 하므로, 특히 심층 신경망이나 대규모 데이터셋의 경우에는 계산 리소스가 많이 필요할 수 있습니다. 하이퍼파라미터 공간을 철저히 탐색하기 위해 수백 번이나 수천 번의 시도를 실행하는 것은 상당한 계산 리소스와 시간이 필요할 수 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e하이퍼파라미터 검색 공간\n옵투나의 검색 효과는 검색 공간이 어떻게 정의되는지에 매우 의존합니다. 하이퍼파라미터 값의 범위가 너무 넓거나 문제와 제대로 일치하지 않으면 옵투나가 비최적 영역을 탐색하는 데 시간을 낭비할 수 있습니다. 반대로 검색 공간이 너무 좁으면 최적의 구성을 놓칠 수 있습니다.\u003c/p\u003e\n\u003cp\u003e하이퍼파라미터 수가 증가함에 따라 검색 공간이 기하급수적으로 증가하는데, 이를 \"차원의 저주\"라고 합니다. 이로 인해 옵투나가 공간을 효율적으로 탐색하고 합리적인 횟수의 시도 내에서 최적의 하이퍼파라미터를 찾는 것이 어렵다는 도전이 생길 수 있습니다.\u003c/p\u003e\n\u003cp\u003e평가 지표\n목적 함수와 평가 지표의 선택은 최적화 결과에 상당한 영향을 미칠 수 있습니다. 모델의 성능이나 작업 목표를 적절히 포착하지 못하는 지표는 하이퍼파라미터 구성을 부적절하게 만들 수 있습니다.\u003c/p\u003e\n\u003cp\u003e모델의 성능 평가는 무작위 초기화, 데이터 섞기, 또는 데이터셋 내 잡음과 같은 요소로 인해 달라질 수 있습니다. 이러한 변동성은 최적화 과정에 잡음을 도입하여 결과의 신뢰성에 영향을 줄 수 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e알고리즘 제한사항\nOptuna은 검색 공간을 탐색하기 위해 정교한 알고리즘을 사용하지만, 이러한 알고리즘의 효율성과 효과는 문제에 따라 다를 수 있습니다. 경우에 따라 특정 알고리즘이 지역 최적점으로 수렴하거나 하이퍼파라미터 공간의 특정 특성에 더 잘 맞도록 설정을 조정해야 할 수도 있습니다.\u003c/p\u003e\n\u003ch1\u003e6: 결론\u003c/h1\u003e\n\u003cp\u003e신경망의 세밀한 조정에 대해 심층적으로 살펴본 후에 우리가 걸어온 길을 돌아보는 좋은 시기입니다. 우리는 신경망이 어떻게 작동하는지에 대한 기본 사항부터 시작하여 그들의 성능과 효율성을 높이는 더 정교한 기술로 점진적으로 발전해왔습니다.\u003c/p\u003e\n\u003ch2\u003e6.1: 다음 단계\u003c/h2\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e우리는 신경망 최적화에 많은 영역을 다루었지만, 명백히 우리는 겨우 표면만 긁은 것 뿐입니다. 신경망 최적화의 영역은 방대하며 지속적으로 진화하고 있으며, 아직 탐험하지 않은 기술과 전략으로 넘쳐납니다. 다가오는 기사에서 더 심층적으로 파고들어 복잡한 신경망 구조와 더 높은 성능과 효율성을 끌어올릴 수 있는 고급 기술을 탐구할 예정입니다.\u003c/p\u003e\n\u003cp\u003e저희가 파헤치고자 하는 최적화 기술과 개념의 다양한 범위에는 다음과 같은 것들이 포함됩니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e배치 정규화: 입력 레이어를 정규화해 활성화를 조정하고 스케일링하여 훈련 속도를 높이고 안정성을 향상시키는 방법입니다.\u003c/li\u003e\n\u003cli\u003e최적화 알고리즘: SGD 및 Adam을 포함한 최적화 알고리즘은 복잡한 손실 함수의 영역을 더 효과적으로 탐색할 수 있는 도구를 제공하여 더 효율적인 훈련 주기와 더 나은 모델 성능을 보장합니다.\u003c/li\u003e\n\u003cli\u003e전이 학습 및 파인 튜닝: 사전 훈련된 모델을 활용하여 새로운 작업에 적응시키면 훈련 시간을 크게 단축하고 데이터가 제한적인 작업에서 모델 정확도를 향상시킬 수 있습니다.\u003c/li\u003e\n\u003cli\u003e신경 아키텍처 탐색(NAS): 자동화를 사용하여 신경망을 위한 최상의 아키텍처를 발견함으로써 직관적이지 않은 효율적인 모델을 발견할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이러한 주제들은 단지 저희가 다루는 것 중 일부에 불과하며, 각각 고유한 이점과 도전을 제공합니다. 앞으로 나아가면서, 이러한 기술을 자세히 살펴보고, 언제 사용해야 하는지, 그들이 어떻게 작용하는지, 그리고 당신의 신경망 프로젝트에 미치는 영향에 대한 통찰력을 제공할 것을 목표로 합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e추가 자료\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e“Deep Learning” - Ian Goodfellow, Yoshua Bengio, Aaron Courville 저: 깊은 학습 기술과 원리에 대한 깊이 있는 개요를 제공하는 이 근본적인 문헌은 고급 신경망 구조 및 최적화 방법을 다룹니다.\u003c/li\u003e\n\u003cli\u003e“Neural Networks and Deep Learning: A Textbook” - Charu C. Aggarwal 저: 신경망에 대한 상세한 탐구를 제공하며, 깊은 학습과 그 응용에 중점을 둡니다. 신경망 디자인 및 최적화의 복잡한 개념을 이해하는 데 탁월한 자료입니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e여기까지 왔습니다. 축하해요! 이 기사를 즐기셨다면 좋아요를 누르고 팔로우해주시면 감사하겠습니다. 저는 정기적으로 유사한 기사를 게시할 예정이니 많은 관심 부탁드립니다. 제 목표는 가장 인기 있는 알고리즘을 다시 처음부터 만들어 머신 러닝을 모든 사람이 접근 가능하도록 하는 것입니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks"},"buildId":"FuXRqV9h16krA5Mvtd6Dn","assetPrefix":"/TIL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>