<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>선형 회귀 쉽게 이해하기  실생활 예시와 함께하는 초보자 가이드 | TIL</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://13akstjq.github.io/TIL//post/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="선형 회귀 쉽게 이해하기  실생활 예시와 함께하는 초보자 가이드 | TIL" data-gatsby-head="true"/><meta property="og:title" content="선형 회귀 쉽게 이해하기  실생활 예시와 함께하는 초보자 가이드 | TIL" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://TIL.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://13akstjq.github.io/TIL//post/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample" data-gatsby-head="true"/><meta name="twitter:title" content="선형 회귀 쉽게 이해하기  실생활 예시와 함께하는 초보자 가이드 | TIL" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | TIL" data-gatsby-head="true"/><meta name="article:published_time" content="2024-07-14 20:03" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/TIL/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/TIL/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/TIL/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/TIL/favicons/favicon-96x96.png"/><link rel="icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/TIL/favicons/browserconfig.xml"/><link rel="preload" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/TIL/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/TIL/_next/static/chunks/webpack-21ffe88bdca56cba.js" defer=""></script><script src="/TIL/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/TIL/_next/static/chunks/main-a5eeabb286676ce6.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/TIL/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/TIL/_next/static/chunks/348-02483b66b493dd81.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/post/%5Bslug%5D-5ecfd58aae5a7e3d.js" defer=""></script><script src="/TIL/_next/static/jKAIrnIuHBv4ZHjiQbX6i/_buildManifest.js" defer=""></script><script src="/TIL/_next/static/jKAIrnIuHBv4ZHjiQbX6i/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/TIL">TIL</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/TIL/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">선형 회귀 쉽게 이해하기  실생활 예시와 함께하는 초보자 가이드</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="선형 회귀 쉽게 이해하기  실생활 예시와 함께하는 초보자 가이드" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/TIL/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">TIL</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jul 14, 2024</span><span class="posts_reading_time__f7YPP">10<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>선형 회귀는 머신 러닝 세계에서 가장 기본적인 알고리즘 중 하나이며, 분석의 ABC와 같습니다.</p>
<p>하늘의 별무리를 통해 가장 적합한 선을 만든다고 생각해보세요. 여기서 각 별은 데이터 점을 나타냅니다.</p>
<p>오늘은 선형 의존성의 클래식 예시 중 하나인 키와 몸무게를 활용해 실제 세계 머신 러닝에 대해 알아보겠습니다 👇🏻</p>
<h1>기본 원리 이해하기</h1>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>간단한 선형 회귀는 가장 기본적인 머신러닝 알고리즘입니다. 데이터 모델링에 대한 여정을 시작하는 대부분의 경우입니다.</p>
<p>신장과 몸무게를 그래프로 그려보면, 신장이 증가함에 따라 몸무게도 증가하는 선이 대략적으로 나타날 것입니다.</p>
<p>그것이 선형 회귀의 핵심입니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>그러니까 이 작업을 어떻게 수행할 수 있는지 살펴보겠습니다...</p>
<h2>#1. 데이터 탐색 - 뛰기 전에 훔쳐보기</h2>
<p>분석에 들어가기 전에 데이터와 친해지는 것부터 시작해봐요.</p>
<p>세 개의 열이 있는 테이블을 상상해보세요: 성별, 키, 몸무게.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_1.png" alt="image"></p>
<p>파이썬의 df.info()로 간단히 살펴보면 수천 개의 항목이 있고 중요한 것은 결측값이 없다는 것입니다.</p>
<p>그리고 데이터의 분포는 어떤가요?</p>
<p>키에 대한 종모양 곡선 하나와 몸무게에 대한 종모양 곡선 하나를 생각해보세요. 두 곡선 모두 나비 날개만큼 대칭적인데요 — 바로 여기가 우리의 정규 분포입니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_2.png">
<h1>#2. 최적 선을 찾는 방법</h1>
<p>우리의 최적 선을 얻기 위해서는 여러 가지 방법이 있습니다. 오늘날 대다수의 사람들은 scikit-learn을 활용하여 미리 빌드된 선형 회귀 알고리즘을 적용할 것입니다.</p>
<p>그러나 이번에는 최초로 세 가지 다른 방법을 시도해 봅시다:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>2 DIY — 모두 스스로 알고리즘 생성.</li>
<li>1 최종 방법은 scikit-learn 활용하는 것.</li>
</ul>
<h2>접근 방법 #1: 최소 제곱법 (OLS)</h2>
<p>최소 제곱법(OLS)의 목적은 예측 오차의 제곱을 최소화하여 최적의 계수 A와 B를 결정하는 것입니다 — 이것이 바로 선형 회귀의 비용 함수인 MSE입니다.</p>
<p>미적분을 활용하여 비용 함수의 최소값을 찾기 위해 편미분의 성질을 이용합니다. 그리고 이러한 편미분이 0이 되는 지점이 최소값에 해당합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>위 수학 문제를 해결하여 A와 B에 대한 정확한 닫힌 수학 공식을 얻게 되면, 가장 정확한 선형 모델로 향하는 직접적인 경로를 얻을 수 있습니다.</p>
<p><img src="/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_3.png" alt="링크"></p>
<p>이것은 이러한 수학적인 닫힌 해결책을 찾기 위해 몇 줄의 코드를 정의하는 것으로 번역됩니다. 그래서 매우 직관적입니다.</p>
<h2>방법 #2: 경사하강법 — 산에서의 하이킹</h2>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>경사 하강은 비용 함수를 최소화하는 데 사용되는 중요한 최적화 알고리즘으로, 예측 모델의 가장 정확한 가중치 값을 찾는 데 도움을 줍니다.</p>
<p>언덕 꼭대기에 서 있는 것으로 상상해보세요. 여러분의 목표는 아래의 계곡에 있습니다. 이것은 비용 함수의 최솟값을 나타냅니다.</p>
<p>이를 달성하기 위해 가중치 A와 B에 대한 초기 추정치로 시작하여 이 추정치를 반복적으로 개선합니다.</p>
<p>이 과정은 언덕을 내려가는 것과 유사합니다. 각각의 단계에서 우리는 주변 환경을 평가하고, 각각의 다음 단계가 계곡 바닥에 더 가까워지도록 우리의 궤적을 조정합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이러한 단계는 학습 속도에 의해 안내됩니다 — 방정식에서 lr로 상징되는 중요한 하이퍼파라미터입니다. 이 학습 속도는 매개변수 A와 B에 대한 조정이나 단계의 크기를 제어하여 최소값을 초과하지 않도록 합니다.</p>
<p><img src="/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_4.png" alt="image"></p>
<p>각 단계에서 비용 함수의 A 및 B에 대한 편도함수인 dA와 dB를 계산합니다. 이 도함수는 우리에게 비용 함수가 가장 빨리 감소하는 방향을 가리키는데, 이것은 비유적인 언덕에서 가장 가파른 하강 경로를 찾는 것과 유사합니다.</p>
<p>각 반복에서 A와 B에 대한 업데이트된 방정식은 다음과 같습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>지침대로 테이블 태그를 마크다운 형식으로 변경하겠습니다.</p>
<p>This meticulous process is repeated until we reach a point where the cost function’s decrease is negligible, suggesting we’ve arrived at or near the global minimum — our destination where the predictive error is minimized, and our model’s accuracy is maximized.</p>
<p><img src="/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_5.png" alt="Image"></p>
<p>This translates into defining two main functions:</p>
<ul>
<li>The function to compute MSE</li>
</ul>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>A 및 B를 업데이트하는 기능</li>
</ul>
<p>우리는 코드를 다음과 같이 초기화합니다:</p>
<ul>
<li>A = 0</li>
<li>B = 0</li>
<li>학습률 0.0001(학습률은 알고리즘이 더 빠르게 또는 더 느리게 학습할 수 있도록 합니다).</li>
<li>최대 반복 횟수</li>
</ul>
<p>그래서 최종 코드는 다음과 같습니다:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>접근 방법 #3: Sci-Kit Learn — 파이썬의 강력한 무브</h2>
<p>파이썬을 좋아하는 사람들을 위해, Sci-Kit Learn은 머신 러닝을 위한 스위스 아미 나이프입니다. 회귀, 분류, 클러스터링 등을 위한 다양한 도구로 가득합니다.</p>
<p>우리가 해야 할 일은 LinearRegression 라이브러리를 가져와서 객체를 만들고, 데이터로 훈련시키는 것뿐입니다.</p>
<p>몇 줄의 코드로 이를 구현할 수 있습니다:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>왔어요!</p>
<p>우리의 모델이 준비되었습니다.</p>
<h1>3. 최종 결과</h1>
<p>우리의 기술을 적용한 후, 공식이 생성되었습니다:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_6.png" alt="image"></p>
<p>With the data we crunched, A turned out to be around 7.17, and B was approximately -350.73.</p>
<p>What does this mean?</p>
<p>For every inch of height, the weight increases by about 7.17 units, minus our intercept value.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>가정 게임</h1>
<p>어떤 모델도 완벽하지 않으며, 선형 회귀는 특정 가정에 의존합니다:</p>
<ul>
<li>선형성: 우리의 데이터는 선이 되어야 합니다. 기억해주세요. 이미 간단한 산점도로 이 선형 패턴을 첫 번째 분석에서 확인했습니다.</li>
<li>독립성: 입력 변수는 서로 독립적이어야 합니다.</li>
<li>잔차의 정규 분포: 관측값과 예측값의 차이는 종 모양의 곡선을 형성해야 합니다.</li>
</ul>
<p><img src="/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_7.png" alt="이미지"></p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>잔차의 등분산성: 오차의 퍼짐은 독립 변수의 모든 값에서 일관되어야 합니다.</li>
</ul>
<p><img src="/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_8.png" alt="image"></p>
<h1>마무리의 생각</h1>
<p>선형 회귀는 간단한 것처럼 보일 수 있지만, 데이터 과학 무기함에 속한 강력한 도구입니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>데이터 분석의 매력은 그 간단함에 있습니다.</p>
<p>복잡한 모델을 사용하는 것이 중요한 게 아니라, 올바른 모델을 올바른 상황에 사용하는 것이 중요한 거죠.</p>
<p>이 개념을 소화하면, 당신은 데이터 속에 숨겨진 이야기를 발견하는 길에 여러분이 잘 나아가고 있습니다.</p>
<p>호기심을 가져라, 데이터 우주를 계속 탐험해 보세요! 🤓</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>다음의 GitHub 저장소에서 코드를 확인할 수 있어요.</p>
<p>ForCode’Sake를 팔로우하면 이와 유사한 기사를 더 많이 볼 수 있어요! ✨</p>
<p>MLBasics 이슈가 마음에 드셨나요? 그렇다면 DataBites 뉴스레터에 가입하여 최신 내용을 이메일로 받아보세요!</p>
<p>유니크한 콘텐츠를 약속합니다!</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>아래 Markdown 형식의 표를 확인하세요.</p>
<p><img src="/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_9.png" alt="이미지"></p>
<p>데이터 과학자 또는 데이터 엔지니어가 되기 위한 훌륭한 데이터 과학 로드맵을 확인해보세요! 🤓</p>
<p>또한 ML, SQL, Python 및 DataViz에 관한 매일 게시하는 치트 시트를 확인할 수 있는 X, Threads 및 LinkedIn에서 저를 만날 수도 있습니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"선형 회귀 쉽게 이해하기  실생활 예시와 함께하는 초보자 가이드","description":"","date":"2024-07-14 20:03","slug":"2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample","content":"\n\n선형 회귀는 머신 러닝 세계에서 가장 기본적인 알고리즘 중 하나이며, 분석의 ABC와 같습니다.\n\n하늘의 별무리를 통해 가장 적합한 선을 만든다고 생각해보세요. 여기서 각 별은 데이터 점을 나타냅니다.\n\n오늘은 선형 의존성의 클래식 예시 중 하나인 키와 몸무게를 활용해 실제 세계 머신 러닝에 대해 알아보겠습니다 👇🏻\n\n# 기본 원리 이해하기\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n간단한 선형 회귀는 가장 기본적인 머신러닝 알고리즘입니다. 데이터 모델링에 대한 여정을 시작하는 대부분의 경우입니다.\n\n신장과 몸무게를 그래프로 그려보면, 신장이 증가함에 따라 몸무게도 증가하는 선이 대략적으로 나타날 것입니다.\n\n그것이 선형 회귀의 핵심입니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그러니까 이 작업을 어떻게 수행할 수 있는지 살펴보겠습니다...\n\n## #1. 데이터 탐색 - 뛰기 전에 훔쳐보기\n\n분석에 들어가기 전에 데이터와 친해지는 것부터 시작해봐요.\n\n세 개의 열이 있는 테이블을 상상해보세요: 성별, 키, 몸무게.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n![image](/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_1.png)\n\n파이썬의 df.info()로 간단히 살펴보면 수천 개의 항목이 있고 중요한 것은 결측값이 없다는 것입니다.\n\n그리고 데이터의 분포는 어떤가요?\n\n키에 대한 종모양 곡선 하나와 몸무게에 대한 종모양 곡선 하나를 생각해보세요. 두 곡선 모두 나비 날개만큼 대칭적인데요 — 바로 여기가 우리의 정규 분포입니다.\n\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_2.png\" /\u003e\n\n# #2. 최적 선을 찾는 방법\n\n우리의 최적 선을 얻기 위해서는 여러 가지 방법이 있습니다. 오늘날 대다수의 사람들은 scikit-learn을 활용하여 미리 빌드된 선형 회귀 알고리즘을 적용할 것입니다.\n\n그러나 이번에는 최초로 세 가지 다른 방법을 시도해 봅시다:\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 2 DIY — 모두 스스로 알고리즘 생성.\n- 1 최종 방법은 scikit-learn 활용하는 것.\n\n## 접근 방법 #1: 최소 제곱법 (OLS)\n\n최소 제곱법(OLS)의 목적은 예측 오차의 제곱을 최소화하여 최적의 계수 A와 B를 결정하는 것입니다 — 이것이 바로 선형 회귀의 비용 함수인 MSE입니다.\n\n미적분을 활용하여 비용 함수의 최소값을 찾기 위해 편미분의 성질을 이용합니다. 그리고 이러한 편미분이 0이 되는 지점이 최소값에 해당합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위 수학 문제를 해결하여 A와 B에 대한 정확한 닫힌 수학 공식을 얻게 되면, 가장 정확한 선형 모델로 향하는 직접적인 경로를 얻을 수 있습니다.\n\n![링크](/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_3.png)\n\n이것은 이러한 수학적인 닫힌 해결책을 찾기 위해 몇 줄의 코드를 정의하는 것으로 번역됩니다. 그래서 매우 직관적입니다.\n\n## 방법 #2: 경사하강법 — 산에서의 하이킹\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n경사 하강은 비용 함수를 최소화하는 데 사용되는 중요한 최적화 알고리즘으로, 예측 모델의 가장 정확한 가중치 값을 찾는 데 도움을 줍니다.\n\n언덕 꼭대기에 서 있는 것으로 상상해보세요. 여러분의 목표는 아래의 계곡에 있습니다. 이것은 비용 함수의 최솟값을 나타냅니다.\n\n이를 달성하기 위해 가중치 A와 B에 대한 초기 추정치로 시작하여 이 추정치를 반복적으로 개선합니다.\n\n이 과정은 언덕을 내려가는 것과 유사합니다. 각각의 단계에서 우리는 주변 환경을 평가하고, 각각의 다음 단계가 계곡 바닥에 더 가까워지도록 우리의 궤적을 조정합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이러한 단계는 학습 속도에 의해 안내됩니다 — 방정식에서 lr로 상징되는 중요한 하이퍼파라미터입니다. 이 학습 속도는 매개변수 A와 B에 대한 조정이나 단계의 크기를 제어하여 최소값을 초과하지 않도록 합니다.\n\n![image](/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_4.png)\n\n각 단계에서 비용 함수의 A 및 B에 대한 편도함수인 dA와 dB를 계산합니다. 이 도함수는 우리에게 비용 함수가 가장 빨리 감소하는 방향을 가리키는데, 이것은 비유적인 언덕에서 가장 가파른 하강 경로를 찾는 것과 유사합니다.\n\n각 반복에서 A와 B에 대한 업데이트된 방정식은 다음과 같습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n지침대로 테이블 태그를 마크다운 형식으로 변경하겠습니다.\n\n\nThis meticulous process is repeated until we reach a point where the cost function’s decrease is negligible, suggesting we’ve arrived at or near the global minimum — our destination where the predictive error is minimized, and our model’s accuracy is maximized.\n\n![Image](/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_5.png)\n\nThis translates into defining two main functions:\n\n- The function to compute MSE\n\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- A 및 B를 업데이트하는 기능\n\n우리는 코드를 다음과 같이 초기화합니다:\n\n- A = 0\n- B = 0\n- 학습률 0.0001(학습률은 알고리즘이 더 빠르게 또는 더 느리게 학습할 수 있도록 합니다).\n- 최대 반복 횟수\n\n그래서 최종 코드는 다음과 같습니다:\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 접근 방법 #3: Sci-Kit Learn — 파이썬의 강력한 무브\n\n파이썬을 좋아하는 사람들을 위해, Sci-Kit Learn은 머신 러닝을 위한 스위스 아미 나이프입니다. 회귀, 분류, 클러스터링 등을 위한 다양한 도구로 가득합니다.\n\n우리가 해야 할 일은 LinearRegression 라이브러리를 가져와서 객체를 만들고, 데이터로 훈련시키는 것뿐입니다.\n\n몇 줄의 코드로 이를 구현할 수 있습니다:\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n왔어요!\n\n우리의 모델이 준비되었습니다.\n\n# 3. 최종 결과\n\n우리의 기술을 적용한 후, 공식이 생성되었습니다:\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n![image](/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_6.png)\n\nWith the data we crunched, A turned out to be around 7.17, and B was approximately -350.73.\n\nWhat does this mean?\n\nFor every inch of height, the weight increases by about 7.17 units, minus our intercept value.\n\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 가정 게임\n\n어떤 모델도 완벽하지 않으며, 선형 회귀는 특정 가정에 의존합니다:\n\n- 선형성: 우리의 데이터는 선이 되어야 합니다. 기억해주세요. 이미 간단한 산점도로 이 선형 패턴을 첫 번째 분석에서 확인했습니다.\n- 독립성: 입력 변수는 서로 독립적이어야 합니다.\n- 잔차의 정규 분포: 관측값과 예측값의 차이는 종 모양의 곡선을 형성해야 합니다.\n\n![이미지](/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_7.png)\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 잔차의 등분산성: 오차의 퍼짐은 독립 변수의 모든 값에서 일관되어야 합니다.\n\n![image](/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_8.png)\n\n# 마무리의 생각\n\n선형 회귀는 간단한 것처럼 보일 수 있지만, 데이터 과학 무기함에 속한 강력한 도구입니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n데이터 분석의 매력은 그 간단함에 있습니다.\n\n복잡한 모델을 사용하는 것이 중요한 게 아니라, 올바른 모델을 올바른 상황에 사용하는 것이 중요한 거죠.\n\n이 개념을 소화하면, 당신은 데이터 속에 숨겨진 이야기를 발견하는 길에 여러분이 잘 나아가고 있습니다.\n\n호기심을 가져라, 데이터 우주를 계속 탐험해 보세요! 🤓\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음의 GitHub 저장소에서 코드를 확인할 수 있어요.\n\nForCode’Sake를 팔로우하면 이와 유사한 기사를 더 많이 볼 수 있어요! ✨\n\nMLBasics 이슈가 마음에 드셨나요? 그렇다면 DataBites 뉴스레터에 가입하여 최신 내용을 이메일로 받아보세요!\n\n유니크한 콘텐츠를 약속합니다!\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아래 Markdown 형식의 표를 확인하세요.\n\n![이미지](/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_9.png)\n\n데이터 과학자 또는 데이터 엔지니어가 되기 위한 훌륭한 데이터 과학 로드맵을 확인해보세요! 🤓\n\n또한 ML, SQL, Python 및 DataViz에 관한 매일 게시하는 치트 시트를 확인할 수 있는 X, Threads 및 LinkedIn에서 저를 만날 수도 있습니다.","ogImage":{"url":"/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_0.png"},"coverImage":"/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_0.png","tag":["Tech"],"readingTime":10},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e선형 회귀는 머신 러닝 세계에서 가장 기본적인 알고리즘 중 하나이며, 분석의 ABC와 같습니다.\u003c/p\u003e\n\u003cp\u003e하늘의 별무리를 통해 가장 적합한 선을 만든다고 생각해보세요. 여기서 각 별은 데이터 점을 나타냅니다.\u003c/p\u003e\n\u003cp\u003e오늘은 선형 의존성의 클래식 예시 중 하나인 키와 몸무게를 활용해 실제 세계 머신 러닝에 대해 알아보겠습니다 👇🏻\u003c/p\u003e\n\u003ch1\u003e기본 원리 이해하기\u003c/h1\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e간단한 선형 회귀는 가장 기본적인 머신러닝 알고리즘입니다. 데이터 모델링에 대한 여정을 시작하는 대부분의 경우입니다.\u003c/p\u003e\n\u003cp\u003e신장과 몸무게를 그래프로 그려보면, 신장이 증가함에 따라 몸무게도 증가하는 선이 대략적으로 나타날 것입니다.\u003c/p\u003e\n\u003cp\u003e그것이 선형 회귀의 핵심입니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e그러니까 이 작업을 어떻게 수행할 수 있는지 살펴보겠습니다...\u003c/p\u003e\n\u003ch2\u003e#1. 데이터 탐색 - 뛰기 전에 훔쳐보기\u003c/h2\u003e\n\u003cp\u003e분석에 들어가기 전에 데이터와 친해지는 것부터 시작해봐요.\u003c/p\u003e\n\u003cp\u003e세 개의 열이 있는 테이블을 상상해보세요: 성별, 키, 몸무게.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_1.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e파이썬의 df.info()로 간단히 살펴보면 수천 개의 항목이 있고 중요한 것은 결측값이 없다는 것입니다.\u003c/p\u003e\n\u003cp\u003e그리고 데이터의 분포는 어떤가요?\u003c/p\u003e\n\u003cp\u003e키에 대한 종모양 곡선 하나와 몸무게에 대한 종모양 곡선 하나를 생각해보세요. 두 곡선 모두 나비 날개만큼 대칭적인데요 — 바로 여기가 우리의 정규 분포입니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_2.png\"\u003e\n\u003ch1\u003e#2. 최적 선을 찾는 방법\u003c/h1\u003e\n\u003cp\u003e우리의 최적 선을 얻기 위해서는 여러 가지 방법이 있습니다. 오늘날 대다수의 사람들은 scikit-learn을 활용하여 미리 빌드된 선형 회귀 알고리즘을 적용할 것입니다.\u003c/p\u003e\n\u003cp\u003e그러나 이번에는 최초로 세 가지 다른 방법을 시도해 봅시다:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e2 DIY — 모두 스스로 알고리즘 생성.\u003c/li\u003e\n\u003cli\u003e1 최종 방법은 scikit-learn 활용하는 것.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e접근 방법 #1: 최소 제곱법 (OLS)\u003c/h2\u003e\n\u003cp\u003e최소 제곱법(OLS)의 목적은 예측 오차의 제곱을 최소화하여 최적의 계수 A와 B를 결정하는 것입니다 — 이것이 바로 선형 회귀의 비용 함수인 MSE입니다.\u003c/p\u003e\n\u003cp\u003e미적분을 활용하여 비용 함수의 최소값을 찾기 위해 편미분의 성질을 이용합니다. 그리고 이러한 편미분이 0이 되는 지점이 최소값에 해당합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e위 수학 문제를 해결하여 A와 B에 대한 정확한 닫힌 수학 공식을 얻게 되면, 가장 정확한 선형 모델로 향하는 직접적인 경로를 얻을 수 있습니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_3.png\" alt=\"링크\"\u003e\u003c/p\u003e\n\u003cp\u003e이것은 이러한 수학적인 닫힌 해결책을 찾기 위해 몇 줄의 코드를 정의하는 것으로 번역됩니다. 그래서 매우 직관적입니다.\u003c/p\u003e\n\u003ch2\u003e방법 #2: 경사하강법 — 산에서의 하이킹\u003c/h2\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e경사 하강은 비용 함수를 최소화하는 데 사용되는 중요한 최적화 알고리즘으로, 예측 모델의 가장 정확한 가중치 값을 찾는 데 도움을 줍니다.\u003c/p\u003e\n\u003cp\u003e언덕 꼭대기에 서 있는 것으로 상상해보세요. 여러분의 목표는 아래의 계곡에 있습니다. 이것은 비용 함수의 최솟값을 나타냅니다.\u003c/p\u003e\n\u003cp\u003e이를 달성하기 위해 가중치 A와 B에 대한 초기 추정치로 시작하여 이 추정치를 반복적으로 개선합니다.\u003c/p\u003e\n\u003cp\u003e이 과정은 언덕을 내려가는 것과 유사합니다. 각각의 단계에서 우리는 주변 환경을 평가하고, 각각의 다음 단계가 계곡 바닥에 더 가까워지도록 우리의 궤적을 조정합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이러한 단계는 학습 속도에 의해 안내됩니다 — 방정식에서 lr로 상징되는 중요한 하이퍼파라미터입니다. 이 학습 속도는 매개변수 A와 B에 대한 조정이나 단계의 크기를 제어하여 최소값을 초과하지 않도록 합니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_4.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e각 단계에서 비용 함수의 A 및 B에 대한 편도함수인 dA와 dB를 계산합니다. 이 도함수는 우리에게 비용 함수가 가장 빨리 감소하는 방향을 가리키는데, 이것은 비유적인 언덕에서 가장 가파른 하강 경로를 찾는 것과 유사합니다.\u003c/p\u003e\n\u003cp\u003e각 반복에서 A와 B에 대한 업데이트된 방정식은 다음과 같습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e지침대로 테이블 태그를 마크다운 형식으로 변경하겠습니다.\u003c/p\u003e\n\u003cp\u003eThis meticulous process is repeated until we reach a point where the cost function’s decrease is negligible, suggesting we’ve arrived at or near the global minimum — our destination where the predictive error is minimized, and our model’s accuracy is maximized.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_5.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003eThis translates into defining two main functions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe function to compute MSE\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003eA 및 B를 업데이트하는 기능\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e우리는 코드를 다음과 같이 초기화합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA = 0\u003c/li\u003e\n\u003cli\u003eB = 0\u003c/li\u003e\n\u003cli\u003e학습률 0.0001(학습률은 알고리즘이 더 빠르게 또는 더 느리게 학습할 수 있도록 합니다).\u003c/li\u003e\n\u003cli\u003e최대 반복 횟수\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e그래서 최종 코드는 다음과 같습니다:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e접근 방법 #3: Sci-Kit Learn — 파이썬의 강력한 무브\u003c/h2\u003e\n\u003cp\u003e파이썬을 좋아하는 사람들을 위해, Sci-Kit Learn은 머신 러닝을 위한 스위스 아미 나이프입니다. 회귀, 분류, 클러스터링 등을 위한 다양한 도구로 가득합니다.\u003c/p\u003e\n\u003cp\u003e우리가 해야 할 일은 LinearRegression 라이브러리를 가져와서 객체를 만들고, 데이터로 훈련시키는 것뿐입니다.\u003c/p\u003e\n\u003cp\u003e몇 줄의 코드로 이를 구현할 수 있습니다:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e왔어요!\u003c/p\u003e\n\u003cp\u003e우리의 모델이 준비되었습니다.\u003c/p\u003e\n\u003ch1\u003e3. 최종 결과\u003c/h1\u003e\n\u003cp\u003e우리의 기술을 적용한 후, 공식이 생성되었습니다:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_6.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eWith the data we crunched, A turned out to be around 7.17, and B was approximately -350.73.\u003c/p\u003e\n\u003cp\u003eWhat does this mean?\u003c/p\u003e\n\u003cp\u003eFor every inch of height, the weight increases by about 7.17 units, minus our intercept value.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e가정 게임\u003c/h1\u003e\n\u003cp\u003e어떤 모델도 완벽하지 않으며, 선형 회귀는 특정 가정에 의존합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e선형성: 우리의 데이터는 선이 되어야 합니다. 기억해주세요. 이미 간단한 산점도로 이 선형 패턴을 첫 번째 분석에서 확인했습니다.\u003c/li\u003e\n\u003cli\u003e독립성: 입력 변수는 서로 독립적이어야 합니다.\u003c/li\u003e\n\u003cli\u003e잔차의 정규 분포: 관측값과 예측값의 차이는 종 모양의 곡선을 형성해야 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_7.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e잔차의 등분산성: 오차의 퍼짐은 독립 변수의 모든 값에서 일관되어야 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_8.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch1\u003e마무리의 생각\u003c/h1\u003e\n\u003cp\u003e선형 회귀는 간단한 것처럼 보일 수 있지만, 데이터 과학 무기함에 속한 강력한 도구입니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e데이터 분석의 매력은 그 간단함에 있습니다.\u003c/p\u003e\n\u003cp\u003e복잡한 모델을 사용하는 것이 중요한 게 아니라, 올바른 모델을 올바른 상황에 사용하는 것이 중요한 거죠.\u003c/p\u003e\n\u003cp\u003e이 개념을 소화하면, 당신은 데이터 속에 숨겨진 이야기를 발견하는 길에 여러분이 잘 나아가고 있습니다.\u003c/p\u003e\n\u003cp\u003e호기심을 가져라, 데이터 우주를 계속 탐험해 보세요! 🤓\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e다음의 GitHub 저장소에서 코드를 확인할 수 있어요.\u003c/p\u003e\n\u003cp\u003eForCode’Sake를 팔로우하면 이와 유사한 기사를 더 많이 볼 수 있어요! ✨\u003c/p\u003e\n\u003cp\u003eMLBasics 이슈가 마음에 드셨나요? 그렇다면 DataBites 뉴스레터에 가입하여 최신 내용을 이메일로 받아보세요!\u003c/p\u003e\n\u003cp\u003e유니크한 콘텐츠를 약속합니다!\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e아래 Markdown 형식의 표를 확인하세요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample_9.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cp\u003e데이터 과학자 또는 데이터 엔지니어가 되기 위한 훌륭한 데이터 과학 로드맵을 확인해보세요! 🤓\u003c/p\u003e\n\u003cp\u003e또한 ML, SQL, Python 및 DataViz에 관한 매일 게시하는 치트 시트를 확인할 수 있는 X, Threads 및 LinkedIn에서 저를 만날 수도 있습니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-07-14-SimplifyingLinearRegressionABeginnersGuidewithareal-worldPracticalExample"},"buildId":"jKAIrnIuHBv4ZHjiQbX6i","assetPrefix":"/TIL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>