<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>로컬에서 LLM 실행하는 방법 | TIL</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://13akstjq.github.io/TIL//post/2024-07-07-RunningLLMLocally" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="로컬에서 LLM 실행하는 방법 | TIL" data-gatsby-head="true"/><meta property="og:title" content="로컬에서 LLM 실행하는 방법 | TIL" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-07-07-RunningLLMLocally_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://TIL.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://13akstjq.github.io/TIL//post/2024-07-07-RunningLLMLocally" data-gatsby-head="true"/><meta name="twitter:title" content="로컬에서 LLM 실행하는 방법 | TIL" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-07-07-RunningLLMLocally_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | TIL" data-gatsby-head="true"/><meta name="article:published_time" content="2024-07-07 02:31" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/TIL/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/TIL/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/TIL/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/TIL/favicons/favicon-96x96.png"/><link rel="icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/TIL/favicons/browserconfig.xml"/><link rel="preload" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/TIL/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/TIL/_next/static/chunks/webpack-21ffe88bdca56cba.js" defer=""></script><script src="/TIL/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/TIL/_next/static/chunks/main-a5eeabb286676ce6.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/TIL/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/TIL/_next/static/chunks/463-925361deb4cec4b1.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/post/%5Bslug%5D-9d7ebbd29b9e08ce.js" defer=""></script><script src="/TIL/_next/static/oImVlPW2Twka9RFRTdE1r/_buildManifest.js" defer=""></script><script src="/TIL/_next/static/oImVlPW2Twka9RFRTdE1r/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/TIL">TIL</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/TIL/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">로컬에서 LLM 실행하는 방법</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="로컬에서 LLM 실행하는 방법" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/TIL/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">TIL</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jul 7, 2024</span><span class="posts_reading_time__f7YPP">5<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-07-07-RunningLLMLocally&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>Transformer 아키텍처의 소개로 자연어 처리에서 중요한 전환점이 시작되었습니다. 많은 주요 언어 모델이 지금은 이를 주요 아키텍처로 사용하고 있어요. 우리가 알다시피 큰 언어 모델(LLM)은 우리 삶을 더 쉽게 만들어줍니다.</p>
<p>LLM 모델의 크기를 생각해보세요. GPT-3은 1750억 개의 파라미터를 가지고 있고, 각각 16비트(2바이트)의 크기를 가지고 있어요. 약 350GB의 저장 공간이 필요합니다. 다른 모델들 중에는 1조를 초과하는 파라미터 크기가 있는 것도 많아요. 그래서 계산량이 매우 많습니다. 로컬에서 실행하는 것을 상상하기도 어렵죠.</p>
<p>하지만, 이 기사는 모두 Ollama 프레임워크에 관한 것입니다. 이를 사용하여 대규모 언어 모델을 로컬에서 실행할 수 있어요.</p>
<p>Ollama는 Llama 3, Mistral 등 다양한 대규모 언어 모델을 운영할 수 있게 해주는 오픈 소스 플랫폼입니다. 또한 애플리케이션에 맞게 모델을 사용자 정의하고, 쉽게 제품에 배포하는 것까지 가능하도록 해줘요.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/assets/img/2024-07-07-RunningLLMLocally_0.png">
<p>여기서는 다른 모델들을 실행하는 유사한 프로세스를 따라 우리의 로컬 시스템에서 Llama3 모델 실행에 초점을 맞춥니다.</p>
<p>Ollama는 Windows, Linux 및 macOS와 호환됩니다. 다운로드하기 위해 아래 링크를 사용하십시오.</p>
<p>Mac: <a href="https://ollama.com/download/mac" rel="nofollow" target="_blank">https://ollama.com/download/mac</a></p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Linux: <a href="https://ollama.com/download/linux" rel="nofollow" target="_blank">https://ollama.com/download/linux</a></p>
<p>Windows: <a href="https://ollama.com/download/windows" rel="nofollow" target="_blank">https://ollama.com/download/windows</a></p>
<p>Let’s try LLama 3:</p>
<ul>
<li>Refer to the article link: <a href="https://ollama.com/library/llama3:8b" rel="nofollow" target="_blank">https://ollama.com/library/llama3:8b</a>. It has all the information about the model size, variants, benchmark, and API information.</li>
</ul>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>먼저 컴퓨터의 명령 프롬프트에 "ollama --version"을 입력하여 설정이 올바르게 되어 있는지 확인하십시오.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:996/1*WXzD_p2lBl1vK4wMStW8Ug.gif" alt="image"></p>
<p>Llama 3 다운로드 :</p>
<p>채팅/대화 사례 모델을 다운로드하려면 "ollama pull llama3" 명령을 사용하십시오.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>다른 모델 목록은 여기에서 확인할 수 있어요: <a href="https://ollama.com/library" rel="nofollow" target="_blank">https://ollama.com/library</a>. 동일한 명령어를 사용하여 다른 모델을 가져와서 사용할 수 있어요.</p>
<p><img src="/assets/img/2024-07-07-RunningLLMLocally_1.png" alt="RunningLLMLocally_1"></p>
<p>Llama 3 실행:</p>
<p>“llama run llama3” 명령을 사용하여 모델을 직접 실행할 수 있어요.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/assets/img/2024-07-07-RunningLLMLocally_2.png">
<p>API 사용 방법:</p>
<p>만약 동일한 모델을 위해 API를 사용하고 싶다면, 몇 가지 간단한 단계를 따라주세요.</p>
<ul>
<li>"ollama serve"를 사용하여 로컬 서버를 시작하세요.</li>
</ul>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/assets/img/2024-07-07-RunningLLMLocally_3.png" alt="이미지"></p>
<ul>
<li>이제 "curl"이나 다른 코드를 사용하여 요청을 보낼 수 있습니다. 여기서는 Postman 도구를 사용하여 llama3 모델에 요청했습니다.</li>
</ul>
<p><img src="/assets/img/2024-07-07-RunningLLMLocally_4.png" alt="이미지"></p>
<ul>
<li>이 요청과 함께 전달할 수 있는 다른 매개변수도 있습니다. 자세한 API를 탐색할 수 있는 링크는 아래에 있습니다.</li>
</ul>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>API 문서: <a href="https://github.com/ollama/ollama/blob/main/docs/api.md" rel="nofollow" target="_blank">https://github.com/ollama/ollama/blob/main/docs/api.md</a></p>
<p>유용한 링크:</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"로컬에서 LLM 실행하는 방법","description":"","date":"2024-07-07 02:31","slug":"2024-07-07-RunningLLMLocally","content":"\n\nTransformer 아키텍처의 소개로 자연어 처리에서 중요한 전환점이 시작되었습니다. 많은 주요 언어 모델이 지금은 이를 주요 아키텍처로 사용하고 있어요. 우리가 알다시피 큰 언어 모델(LLM)은 우리 삶을 더 쉽게 만들어줍니다.\n\nLLM 모델의 크기를 생각해보세요. GPT-3은 1750억 개의 파라미터를 가지고 있고, 각각 16비트(2바이트)의 크기를 가지고 있어요. 약 350GB의 저장 공간이 필요합니다. 다른 모델들 중에는 1조를 초과하는 파라미터 크기가 있는 것도 많아요. 그래서 계산량이 매우 많습니다. 로컬에서 실행하는 것을 상상하기도 어렵죠.\n\n하지만, 이 기사는 모두 Ollama 프레임워크에 관한 것입니다. 이를 사용하여 대규모 언어 모델을 로컬에서 실행할 수 있어요.\n\nOllama는 Llama 3, Mistral 등 다양한 대규모 언어 모델을 운영할 수 있게 해주는 오픈 소스 플랫폼입니다. 또한 애플리케이션에 맞게 모델을 사용자 정의하고, 쉽게 제품에 배포하는 것까지 가능하도록 해줘요.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-07-07-RunningLLMLocally_0.png\" /\u003e\n\n여기서는 다른 모델들을 실행하는 유사한 프로세스를 따라 우리의 로컬 시스템에서 Llama3 모델 실행에 초점을 맞춥니다.\n\nOllama는 Windows, Linux 및 macOS와 호환됩니다. 다운로드하기 위해 아래 링크를 사용하십시오.\n\nMac: https://ollama.com/download/mac\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nLinux: [https://ollama.com/download/linux](https://ollama.com/download/linux)\n\nWindows: [https://ollama.com/download/windows](https://ollama.com/download/windows)\n\nLet’s try LLama 3:\n\n- Refer to the article link: [https://ollama.com/library/llama3:8b](https://ollama.com/library/llama3:8b). It has all the information about the model size, variants, benchmark, and API information.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n먼저 컴퓨터의 명령 프롬프트에 \"ollama --version\"을 입력하여 설정이 올바르게 되어 있는지 확인하십시오.\n\n\n![image](https://miro.medium.com/v2/resize:fit:996/1*WXzD_p2lBl1vK4wMStW8Ug.gif)\n\n\nLlama 3 다운로드 :\n\n채팅/대화 사례 모델을 다운로드하려면 \"ollama pull llama3\" 명령을 사용하십시오.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다른 모델 목록은 여기에서 확인할 수 있어요: https://ollama.com/library. 동일한 명령어를 사용하여 다른 모델을 가져와서 사용할 수 있어요.\n\n![RunningLLMLocally_1](/assets/img/2024-07-07-RunningLLMLocally_1.png)\n\nLlama 3 실행:\n\n“llama run llama3” 명령을 사용하여 모델을 직접 실행할 수 있어요.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/assets/img/2024-07-07-RunningLLMLocally_2.png\" /\u003e\n\nAPI 사용 방법:\n\n만약 동일한 모델을 위해 API를 사용하고 싶다면, 몇 가지 간단한 단계를 따라주세요.\n\n- \"ollama serve\"를 사용하여 로컬 서버를 시작하세요.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n![이미지](/assets/img/2024-07-07-RunningLLMLocally_3.png)\n\n- 이제 \"curl\"이나 다른 코드를 사용하여 요청을 보낼 수 있습니다. 여기서는 Postman 도구를 사용하여 llama3 모델에 요청했습니다.\n\n![이미지](/assets/img/2024-07-07-RunningLLMLocally_4.png)\n\n- 이 요청과 함께 전달할 수 있는 다른 매개변수도 있습니다. 자세한 API를 탐색할 수 있는 링크는 아래에 있습니다.\n\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nAPI 문서: [https://github.com/ollama/ollama/blob/main/docs/api.md](https://github.com/ollama/ollama/blob/main/docs/api.md)\n\n유용한 링크:","ogImage":{"url":"/assets/img/2024-07-07-RunningLLMLocally_0.png"},"coverImage":"/assets/img/2024-07-07-RunningLLMLocally_0.png","tag":["Tech"],"readingTime":5},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003eTransformer 아키텍처의 소개로 자연어 처리에서 중요한 전환점이 시작되었습니다. 많은 주요 언어 모델이 지금은 이를 주요 아키텍처로 사용하고 있어요. 우리가 알다시피 큰 언어 모델(LLM)은 우리 삶을 더 쉽게 만들어줍니다.\u003c/p\u003e\n\u003cp\u003eLLM 모델의 크기를 생각해보세요. GPT-3은 1750억 개의 파라미터를 가지고 있고, 각각 16비트(2바이트)의 크기를 가지고 있어요. 약 350GB의 저장 공간이 필요합니다. 다른 모델들 중에는 1조를 초과하는 파라미터 크기가 있는 것도 많아요. 그래서 계산량이 매우 많습니다. 로컬에서 실행하는 것을 상상하기도 어렵죠.\u003c/p\u003e\n\u003cp\u003e하지만, 이 기사는 모두 Ollama 프레임워크에 관한 것입니다. 이를 사용하여 대규모 언어 모델을 로컬에서 실행할 수 있어요.\u003c/p\u003e\n\u003cp\u003eOllama는 Llama 3, Mistral 등 다양한 대규모 언어 모델을 운영할 수 있게 해주는 오픈 소스 플랫폼입니다. 또한 애플리케이션에 맞게 모델을 사용자 정의하고, 쉽게 제품에 배포하는 것까지 가능하도록 해줘요.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/assets/img/2024-07-07-RunningLLMLocally_0.png\"\u003e\n\u003cp\u003e여기서는 다른 모델들을 실행하는 유사한 프로세스를 따라 우리의 로컬 시스템에서 Llama3 모델 실행에 초점을 맞춥니다.\u003c/p\u003e\n\u003cp\u003eOllama는 Windows, Linux 및 macOS와 호환됩니다. 다운로드하기 위해 아래 링크를 사용하십시오.\u003c/p\u003e\n\u003cp\u003eMac: \u003ca href=\"https://ollama.com/download/mac\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/download/mac\u003c/a\u003e\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eLinux: \u003ca href=\"https://ollama.com/download/linux\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/download/linux\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWindows: \u003ca href=\"https://ollama.com/download/windows\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/download/windows\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eLet’s try LLama 3:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRefer to the article link: \u003ca href=\"https://ollama.com/library/llama3:8b\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/library/llama3:8b\u003c/a\u003e. It has all the information about the model size, variants, benchmark, and API information.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e먼저 컴퓨터의 명령 프롬프트에 \"ollama --version\"을 입력하여 설정이 올바르게 되어 있는지 확인하십시오.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://miro.medium.com/v2/resize:fit:996/1*WXzD_p2lBl1vK4wMStW8Ug.gif\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003eLlama 3 다운로드 :\u003c/p\u003e\n\u003cp\u003e채팅/대화 사례 모델을 다운로드하려면 \"ollama pull llama3\" 명령을 사용하십시오.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e다른 모델 목록은 여기에서 확인할 수 있어요: \u003ca href=\"https://ollama.com/library\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://ollama.com/library\u003c/a\u003e. 동일한 명령어를 사용하여 다른 모델을 가져와서 사용할 수 있어요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-07-07-RunningLLMLocally_1.png\" alt=\"RunningLLMLocally_1\"\u003e\u003c/p\u003e\n\u003cp\u003eLlama 3 실행:\u003c/p\u003e\n\u003cp\u003e“llama run llama3” 명령을 사용하여 모델을 직접 실행할 수 있어요.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/assets/img/2024-07-07-RunningLLMLocally_2.png\"\u003e\n\u003cp\u003eAPI 사용 방법:\u003c/p\u003e\n\u003cp\u003e만약 동일한 모델을 위해 API를 사용하고 싶다면, 몇 가지 간단한 단계를 따라주세요.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\"ollama serve\"를 사용하여 로컬 서버를 시작하세요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-07-07-RunningLLMLocally_3.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e이제 \"curl\"이나 다른 코드를 사용하여 요청을 보낼 수 있습니다. 여기서는 Postman 도구를 사용하여 llama3 모델에 요청했습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/assets/img/2024-07-07-RunningLLMLocally_4.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e이 요청과 함께 전달할 수 있는 다른 매개변수도 있습니다. 자세한 API를 탐색할 수 있는 링크는 아래에 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eAPI 문서: \u003ca href=\"https://github.com/ollama/ollama/blob/main/docs/api.md\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://github.com/ollama/ollama/blob/main/docs/api.md\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e유용한 링크:\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-07-07-RunningLLMLocally"},"buildId":"oImVlPW2Twka9RFRTdE1r","assetPrefix":"/TIL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>