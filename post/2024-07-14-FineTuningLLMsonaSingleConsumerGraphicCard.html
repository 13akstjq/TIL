<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>소비자용 그래픽 카드 하나로 LLMs 미세 조정하는 방법 | TIL</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://13akstjq.github.io/TIL//post/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="소비자용 그래픽 카드 하나로 LLMs 미세 조정하는 방법 | TIL" data-gatsby-head="true"/><meta property="og:title" content="소비자용 그래픽 카드 하나로 LLMs 미세 조정하는 방법 | TIL" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/TIL/assets/img/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://TIL.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://13akstjq.github.io/TIL//post/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard" data-gatsby-head="true"/><meta name="twitter:title" content="소비자용 그래픽 카드 하나로 LLMs 미세 조정하는 방법 | TIL" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/TIL/assets/img/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | TIL" data-gatsby-head="true"/><meta name="article:published_time" content="2024-07-14 19:45" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/TIL/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/TIL/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/TIL/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/TIL/favicons/favicon-96x96.png"/><link rel="icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/TIL/favicons/browserconfig.xml"/><link rel="preload" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/TIL/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/TIL/_next/static/chunks/webpack-21ffe88bdca56cba.js" defer=""></script><script src="/TIL/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/TIL/_next/static/chunks/main-a5eeabb286676ce6.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/TIL/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/TIL/_next/static/chunks/348-02483b66b493dd81.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/post/%5Bslug%5D-8ded8b979ba73586.js" defer=""></script><script src="/TIL/_next/static/o6AmBAY_j9v9JmbaRA39X/_buildManifest.js" defer=""></script><script src="/TIL/_next/static/o6AmBAY_j9v9JmbaRA39X/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/TIL">TIL</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/TIL/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">소비자용 그래픽 카드 하나로 LLMs 미세 조정하는 방법</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="소비자용 그래픽 카드 하나로 LLMs 미세 조정하는 방법" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/TIL/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">TIL</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jul 14, 2024</span><span class="posts_reading_time__f7YPP">15<!-- --> min read</span></span></div></div></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h2>생성적 AI</h2>
<p><img src="/TIL/assets/img/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard_0.png" alt="이미지"></p>
<h1>배경</h1>
<p>대형 언어 모델 또는 다른 생성 모델을 생각할 때, 먼저 떠오르는 하드웨어는 GPU입니다. GPU 없이는 생성적 AI, 기계 학습, 심층 학습, 데이터 과학 등의 많은 발전이 불가능했을 것입니다. 15년 전, 게이머들이 최신 GPU 기술에 열광했다면, 오늘날 데이터 과학자와 기계 학습 엔지니어들도 이 분야의 소식을 따라가며 함께 관심을 가지고 있습니다. 보통 게이머들과 기계 학습 사용자는 서로 다른 종류의 GPU와 그래픽 카드를 사용한다고 볼 수 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>게임 사용자들은 일반적으로 소비자용 그래픽 카드(예: NVIDIA GeForce RTX 시리즈 GPU)를 사용하고, ML 및 AI 개발자들은 주로 데이터 센터 및 클라우드 컴퓨팅 GPU(예: V100, A100 또는 H100)에 대한 뉴스를 따릅니다. 게임 그래픽 카드는 일반적으로 GPU 메모리가 훨씬 적습니다(2024년 1월 기준 최대 24GB). 반면 데이터 센터 GPU는 일반적으로 40GB에서 80GB 정도의 범위에 있습니다. 또한 가격도 다른 중요한 차이점입니다. 대부분의 소비자용 그래픽 카드의 가격이 최대 3000달러가 될 수 있는 반면, 대부분의 데이터 센터 그래픽 카드는 그 가격부터 시작하여 수십만 달러까지 쉽게 올라갈 수 있습니다.</p>
<p>저를 포함한 많은 사람들이 그래픽 카드를 게임이나 일상적인 용도로 사용할 수 있기 때문에, 같은 그래픽 카드를 사용하여 LLM 모델의 학습, 미세 조정 또는 추론에 사용할 수 있는지 궁금할 수 있습니다. 2020년에 저는 소비자용 그래픽 카드를 데이터 과학 프로젝트에 사용할 수 있는지에 대해 포괄적인 기사를 썼습니다. 당시에는 대부분 작은 ML이나 딥 러닝 모델이었고, 6GB 메모리를 가진 그래픽 카드라도 많은 학습 프로젝트를 처리할 수 있었습니다. 그러나 본 기사에서는 수십억 개의 매개변수를 가진 대형 언어 모델에 이러한 그래픽 카드를 사용할 것입니다.</p>
<p>본 기사에서는 24GB GPU 메모리를 가진 GeForce 3090 RTX 카드를 사용했습니다. 참고로, A100 및 H100과 같은 데이터 센터 그래픽 카드는 각각 40GB 및 80GB의 메모리를 가지고 있습니다. 또한 전형적인 AWS EC2 p4d.24xlarge 인스턴스는 총 320GB의 GPU 메모리를 가진 8개의 GPU(V100)를 가지고 있습니다. 간단한 소비자용 GPU와 전형적인 클라우드 ML 인스턴스 간의 차이가 상당히 크다는 것을 보실 수 있습니다. 그러나 질문은, 우리가 단일 소비자용 그래픽 카드에서 대형 모델을 학습할 수 있는지 여부인데요? 가능하다면, 팁과 교훈은 무엇인가요? 이 기사의 나머지 부분을 읽어보세요.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>아무런 LLM 모델이나 교육 데이터 집합을로드하기 전에 그러한 프로세스에 필요한 하드웨어 및 소프트웨어를 찾아야합니다.</p>
<p>언급한 바와 같이, 나는 소비자 GPU 중에서도 가장 높은 메모리(24GB) 중 하나를 갖고 있는 NVIDIA GeForce RTX 3090 GPU를 사용했습니다(참고로, 4090 모델도 동일한 메모리 크기를 가지고 있습니다). 이 GPU는 유명한 A100 GPU에 있는 것과 동일한 Ampere 아키텍처를 기반으로 하고 있습니다. GeForce RTX 3090 GPU 사양에 대해 더 자세히 알아볼 수 있습니다.</p>
<p>모든 테스트를 거친 후, 24GB가 10억 개의 매개변수를 갖는 LLM과 작업을 수행하는 데 필요한 최소한의 GPU 메모리라고 생각합니다.</p>
<img src="/TIL/assets/img/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard_1.png">
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>그래픽 카드 외에도 PC의 좋은 환기 시스템이 필요합니다. 세밀한 조정 중에 GPU의 온도가 쉽게 올라가고 팬으로는 충분히 식힐 수 없는 경우가 있습니다. 높은 GPU 온도는 GPU 성능을 낮출 수 있고, 처리 시간이 더 오래 걸릴 수 있습니다.</p>
<p>하드웨어 외에도 여기서 언급해야 할 몇 가지 소프트웨어 고려 사항이 있습니다. 먼저, Windows 사용자라면 안타깝게도 나쁜 소식이 있어요. 일부 라이브러리와 도구는 Linux에서만 작동합니다. 특히, 모델 양자화에 자주 사용되는 bitsandbytes는 Windows 친화적이지 않습니다. 어떤 사람들은 Windows용 래퍼를 만들었지만 (예를 들어 여기), 그들은 장단점이 있어요. 그래서 제 추천은 WSL에 Linux를 설치하거나 저와 같이 듀얼 부팅 시스템을 사용하여 LLM 작업 중에 완전히 Linux로 전환하는 것입니다.</p>
<p>또한, PyTorch와 호환되는 CUDA 버전을 설치해야 합니다. 제 추천은 CUDA 12.3을 설치하는 것입니다 (링크). 그런 다음 이 페이지로 이동하여 시스템, CUDA 버전 및 패키지 관리자 시스템에 따라 올바른 PyTorch를 다운로드하고 설치해야 합니다 (<a href="https://pytorch.org/" rel="nofollow" target="_blank">https://pytorch.org/</a>).</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">export</span> <span class="hljs-variable constant_">BNB_CUDA_VERSION</span>=<span class="hljs-number">123</span>
<span class="hljs-keyword">export</span> <span class="hljs-variable constant_">LD_LIBRARY_PATH</span>=<span class="hljs-attr">$LD_LIBRARY_PATH</span>:<span class="hljs-regexp">/home/</span>&#x3C;<span class="hljs-variable constant_">YOUR</span>-<span class="hljs-variable constant_">USER</span>-<span class="hljs-variable constant_">DIR</span>><span class="hljs-regexp">/local/</span>cuda-<span class="hljs-number">12.3</span>
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>마지막으로 시스템에 다음 패키지를 설치해야 합니다. 충돌을 피하기 위해 시스템에 이미 설치된 다른 패키지와 충돌을 피하기 위해 새 가상 환경(venv)을 만드는 것을 권장합니다. 또한, 아래는 제가 성공적으로 사용한 패키지 버전들입니다:</p>
<pre><code class="hljs language-js">torch==<span class="hljs-number">2.1</span><span class="hljs-number">.2</span>
transformers==<span class="hljs-number">4.36</span><span class="hljs-number">.2</span>
datasests==<span class="hljs-number">2.16</span><span class="hljs-number">.1</span>
bitsandbytes==<span class="hljs-number">0.42</span><span class="hljs-number">.0</span>
peft==<span class="hljs-number">0.7</span><span class="hljs-number">.1</span>
</code></pre>
<h1>기술적 배경</h1>
<p>이제 시스템에서 LLMs를 사용하기 위해 모든 하드웨어와 소프트웨어를 준비했으니, 다음 섹션에서 마주할 기술적 개념에 대해 매우 간단히 검토하는 것이 좋습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>대형 언어 모델은 수백만 또는 수십억 개의 매개변수로 구성됩니다. 일반적으로 수십억 또는 때로는 수조의 토큰으로 훈련된 사전 훈련된 모델을 사용하는데 굉장히 긴 훈련 과정과 수백만 달러가 들었습니다. 이러한 모델 매개변수 각각은 32비트(4바이트)의 메모리를 차지하고 로드하기 위해 필요합니다. 일반적으로 10억 개의 매개변수당 약 4GB의 메모리가 필요하다고 생각할 수 있습니다. 로드(및 이후 추론 또는 후속 모델 훈련)를 위해 메모리 사용량을 줄이는 한 가지 기술은 "양자화"입니다. 이 기술에서는 모델 가중치의 정밀도를 32비트의 완전 정밀도에서 16비트(fp16 또는 bfloat16), 8비트(int8) 또는 그 이하로 줄입니다.</p>
<p>모델 가중치의 정밀도를 줄이면 제한적인 메모리에 더 큰 모델을 로드할 수 있지만 모델 성능을 희생해야 합니다. 그러나 일부 연구에서는 fp32와 bfloat16 간의 모델 성능 차이가 중요하지 않다고 제안하며, 많은 유명한 모델(Llama2 포함)이 bfloat16로 사전 훈련되었습니다.</p>
<p>양자화는 단일 GPU에서 메모리가 24GB인 대형 언어 모델을 세밀하게 조정하거나 추론할 때 반드시 사용해야 하는 기술입니다. 나중에 볼 것처럼 bitsandbytes 라이브러리를 사용하여 모델 양자화를 구현할 수 있습니다.</p>
<p>가장 엄격한 양자화 기술을 사용하더라도 수십억 개의 매개변수를 가진 작은 크기의 LLM 모델을 사전 훈련할 수 없습니다. 크리스 프레글리 등은 최근 발표된 'AWS에서의 생성적 AI' 도서에서 모델 훈련에 필요한 메모리에 대한 좋은 규칙을 설명했습니다. 그들은 모델의 10억 개의 매개변수당 16비트 반 정밀도에서 6GB의 메모리가 필요하다고 설명했죠.</p>
<p>기억해야 할 것은 메모리 크기가 훈련 이야기의 일부일 뿐이라는 점입니다. 사전 훈련을 완료하는 데 필요한 시간도 또 하나의 중요한 측면입니다. 예를 들어 가장 작은 Llama2 모델인 Llama2 7B는 70억 개의 매개변수를 가지고 훈련을 완료하는 데 184320 GPU 시간이 걸렸습니다.</p>
<p>그래서 대부분의 사람들(상당한 하드웨어 자원과 예산을 갖춘 사람들도)는 특정 사용 사례에 맞게 사전 훈련된 모델을 사용하고 세밀하게 조정하려는 경향이 있습니다. 그러나 한정된 자원(예: 단일 GPU)으로 완전한 세밀 조정을 수행하는 것은 다소 어려울 수 있습니다. 이에 따라 모델 매개변수의 한정된 부분만 업데이트하는 "효율적인 매개 조정" (PEFT) 이 보다 현실적으로 보입니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>다양한 PEFT 기술 중에서, LoRA (Low Ranking Adaption)는 그 계산 효율성으로 매우 인기가 높습니다. 이 기술에서는 원래 모델의 가중치를 모두 고정시키고 대신 Transformer 아키텍처의 특정 레이어에 추가할 수 있는 저랭크 행렬을 학습합니다. LLM을 세세히 조정할 때 LoRA를 사용하는 경우, 모델 가중치의 0.5%를 업데이트합니다.</p>
<p>QLoRA는 저랭크 행렬 LoRA에 우리가 설명한 양자화 개념을 결합한 변형입니다. 특히, QLoRA 구현에서는 모델을 세세히 조정하기 위해 nf4 또는 Normal Float 4를 사용할 것입니다. QLoRA는 단일 소비자 GPU로 대규모 모델을 세세히 조정하는 경우 연구 사례에서 매우 유용합니다.</p>
<h1>코딩 타임</h1>
<p>마지막으로, 이제 코딩할 시간입니다!</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>내 GitHub 리포지토리에서 작동하는 주피터 노트북을 찾을 수 있어요. 이 코드의 많은 부분은 Mathieu Busquet의 깔끔한 글에서 영감을 받아 따랐어요.</p>
<p>코드를 한 줄씩 설명하지는 않겠지만, 단일 GPU에서 대규모 모델을 세밀하게 조정하는 데 중요한 부분을 강조할 거에요.</p>
<h2>트랜스포머 모델</h2>
<p>우선, 이 테스트에 Mistral 7B 모델(mistralai/Mistral-7B-v0.1)을 선택했어요. Mistral AI가 개발한 Mistral 7B 모델은 2023년 9월에 공개된 오픈 소스 LLM이에요 (논문 링크). 많은 측면에서 이 모델은 Llama2와 같은 유명한 모델들을 능가해요 (다음 차트를 참고하세요).</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<img src="/TIL/assets/img/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard_2.png">
<h2>데이터셋</h2>
<p>또한, 저는 fine-tuning을 위해 Databricks databricks-dolly-15k dataset를 사용했어요 (CC BY-SA 3.0 라이선스하에 제공됨). fine-tuning 시간을 줄이기 위해 이 데이터의 작은 부분(1000행)을 사용했고 컨셉을 증명했어요.</p>
<h2>구성 요소</h2>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>모델 로딩 시, GPU 메모리 제한을 극복하기 위해 다음과 같은 양자화 구성을 사용했어요.</p>
<pre><code class="hljs language-js">quantization_config = <span class="hljs-title class_">BitsAndBytesConfig</span>(
    load_in_4bit=<span class="hljs-title class_">True</span>,
    bnb_4bit_use_double_quant=<span class="hljs-title class_">True</span>,
    bnb_4bit_quant_type=<span class="hljs-string">"nf4"</span>,
    bnb_4bit_compute_dtype=torch.<span class="hljs-property">bfloat16</span>,
)
</code></pre>
<p>이 양자화 구성은 bfloat16 계산 데이터 유형과 nf4(4비트 Normal Float)인 저정밀 스토리지 데이터 유형을 가지고 있기 때문에 단일 GPU에서 모델 세밀 조정에 매우 중요해요. 실제로는 QLORA 가중치 텐서가 사용될 때, 텐서를 bfloat16로 비양자화하고 16비트에서 행렬 곱셈을 수행하게 됩니다(자세한 내용은 원본 논문 참조).</p>
<p>또한 이전에 언급한 대로, 양자화와 함께 LoRA를 사용하여 메모리 제한을 극복하기 위한 QLoRA를 사용 중이에요. 여기 LoRA를 위한 제 설정입니다:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">lora_config = <span class="hljs-title class_">LoraConfig</span>(
    r=<span class="hljs-number">16</span>,
    lora_alpha=<span class="hljs-number">64</span>,
    target_modules=[<span class="hljs-string">"q_proj"</span>, <span class="hljs-string">"k_proj"</span>, <span class="hljs-string">"v_proj"</span>, <span class="hljs-string">"o_proj"</span>, <span class="hljs-string">"gate_proj"</span>],
    bias=<span class="hljs-string">"none"</span>,
    lora_dropout=<span class="hljs-number">0.05</span>,
    task_type=<span class="hljs-string">"CAUSAL_LM"</span>,
)
</code></pre>
<p>제 LoRA 구성에는 랭크로 16을 사용했어요. 4부터 16 사이의 랭크로 설정하는 것이 학습 가능한 매개변수의 수를 줄이고 모델 성능 사이의 적절한 균형을 얻기 위해 권장됩니다. 마지막으로, Mistral 7B 트랜스포머 모델의 일부 선형 계층에 LoRA를 적용했습니다.</p>
<h1>학습 및 모니터링</h1>
<p>제 개인적인 그래픽 카드를 사용하여 4 에포크 (1000 단계)의 학습을 완료할 수 있었어요. 지역 GPU에서 LLM을 학습하는 이러한 테스트 중 하나의 목적은 어떠한 제한 없이 하드웨어 리소스를 모니터링하는 것이에요. 학습 중 GPU를 모니터링하는 가장 간단한 도구 중 하나는 Nvidia 시스템 관리 인터페이스 (SMI)입니다. 단순히 터미널을 열고 명령줄에 다음을 입력하세요:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">nvidia-smi
</code></pre>
<p>또는 지속적인 모니터링과 업데이트를 위해 다음을 사용하세요 (1초마다 새로고침):</p>
<pre><code class="hljs language-js">nvidia-smi -l <span class="hljs-number">1</span>
</code></pre>
<p>이렇게 하면 GPU에서 각 프로세스의 메모리 사용량을 확인할 수 있습니다. 다음 SMI 보기에서 저는 모델을 불러왔고 약 5GB의 메모리를 사용했습니다 (양자화 덕분에). 또한 Anaconda3 Python (Jupyter 노트북) 프로세스로 모델이 불러와진 것을 확인할 수 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">+---------------------------------------------------------------------------------------+
| <span class="hljs-variable constant_">NVIDIA</span>-<span class="hljs-variable constant_">SMI</span> <span class="hljs-number">545.23</span><span class="hljs-number">.08</span>              <span class="hljs-title class_">Driver</span> <span class="hljs-title class_">Version</span>: <span class="hljs-number">545.23</span><span class="hljs-number">.08</span>    <span class="hljs-variable constant_">CUDA</span> <span class="hljs-title class_">Version</span>: <span class="hljs-number">12.3</span>     |
|-----------------------------------------+----------------------+----------------------+
| <span class="hljs-variable constant_">GPU</span>  <span class="hljs-title class_">Name</span>                 <span class="hljs-title class_">Persistence</span>-M | <span class="hljs-title class_">Bus</span>-<span class="hljs-title class_">Id</span>        <span class="hljs-title class_">Disp</span>.<span class="hljs-property">A</span> | <span class="hljs-title class_">Volatile</span> <span class="hljs-title class_">Uncorr</span>. <span class="hljs-variable constant_">ECC</span> |
| <span class="hljs-title class_">Fan</span>  <span class="hljs-title class_">Temp</span>   <span class="hljs-title class_">Perf</span>          <span class="hljs-title class_">Pwr</span>:<span class="hljs-title class_">Usage</span>/<span class="hljs-title class_">Cap</span> |         <span class="hljs-title class_">Memory</span>-<span class="hljs-title class_">Usage</span> | <span class="hljs-variable constant_">GPU</span>-<span class="hljs-title class_">Util</span>  <span class="hljs-title class_">Compute</span> M. |
|                                         |                      |               <span class="hljs-variable constant_">MIG</span> M. |
|=========================================+======================+======================|
|   <span class="hljs-number">0</span>  <span class="hljs-variable constant_">NVIDIA</span> <span class="hljs-title class_">GeForce</span> <span class="hljs-variable constant_">RTX</span> <span class="hljs-number">3090</span>        <span class="hljs-title class_">On</span>  | <span class="hljs-number">00000000</span>:<span class="hljs-number">29</span>:<span class="hljs-number">00</span><span class="hljs-number">.0</span>  <span class="hljs-title class_">On</span> |                  N/A |
| <span class="hljs-number">30</span>%   37C    <span class="hljs-variable constant_">P8</span>              33W / 350W |   5346MiB / 24576MiB |      <span class="hljs-number">5</span>%      <span class="hljs-title class_">Default</span> |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| <span class="hljs-title class_">Processes</span>:                                                                            |
|  <span class="hljs-variable constant_">GPU</span>   <span class="hljs-variable constant_">GI</span>   <span class="hljs-variable constant_">CI</span>        <span class="hljs-variable constant_">PID</span>   <span class="hljs-title class_">Type</span>   <span class="hljs-title class_">Process</span> name                            <span class="hljs-variable constant_">GPU</span> <span class="hljs-title class_">Memory</span> |
|        <span class="hljs-variable constant_">ID</span>   <span class="hljs-variable constant_">ID</span>                                                             <span class="hljs-title class_">Usage</span>      |
|=======================================================================================|
|    <span class="hljs-number">0</span>   N/A  N/A      <span class="hljs-number">1610</span>      G   /usr/lib/xorg/<span class="hljs-title class_">Xorg</span>                          179MiB |
|    <span class="hljs-number">0</span>   N/A  N/A      <span class="hljs-number">1820</span>      G   /usr/bin/gnome-shell                         41MiB |
|    <span class="hljs-number">0</span>   N/A  N/A    <span class="hljs-number">108004</span>      G   ...<span class="hljs-number">2023.3</span><span class="hljs-number">.3</span>/host-linux-x64/nsys-ui.<span class="hljs-property">bin</span>        8MiB |
|    <span class="hljs-number">0</span>   N/A  N/A    <span class="hljs-number">168032</span>      G   ...seed-version=<span class="hljs-number">20240110</span>-<span class="hljs-number">180219.406000</span>      117MiB |
|    <span class="hljs-number">0</span>   N/A  N/A    <span class="hljs-number">327503</span>      C   /home<span class="hljs-comment">/***/</span>anaconda3/bin/python             4880MiB |
+---------------------------------------------------------------------------------------+
</code></pre>
<p>그리고 이곳은 훈련 과정 중 약 30단계 이후의 메모리 상태입니다. 보시다시피, 사용 중인 GPU 메모리는 현재 약 15GB입니다.</p>
<pre><code class="hljs language-js">+---------------------------------------------------------------------------------------+
| <span class="hljs-variable constant_">NVIDIA</span>-<span class="hljs-variable constant_">SMI</span> <span class="hljs-number">545.23</span><span class="hljs-number">.08</span>              <span class="hljs-title class_">Driver</span> <span class="hljs-title class_">Version</span>: <span class="hljs-number">545.23</span><span class="hljs-number">.08</span>    <span class="hljs-variable constant_">CUDA</span> <span class="hljs-title class_">Version</span>: <span class="hljs-number">12.3</span>     |
|-----------------------------------------+----------------------+----------------------+
| <span class="hljs-variable constant_">GPU</span>  <span class="hljs-title class_">Name</span>                 <span class="hljs-title class_">Persistence</span>-M | <span class="hljs-title class_">Bus</span>-<span class="hljs-title class_">Id</span>        <span class="hljs-title class_">Disp</span>.<span class="hljs-property">A</span> | <span class="hljs-title class_">Volatile</span> <span class="hljs-title class_">Uncorr</span>. <span class="hljs-variable constant_">ECC</span> |
| <span class="hljs-title class_">Fan</span>  <span class="hljs-title class_">Temp</span>   <span class="hljs-title class_">Perf</span>          <span class="hljs-title class_">Pwr</span>:<span class="hljs-title class_">Usage</span>/<span class="hljs-title class_">Cap</span> |         <span class="hljs-title class_">Memory</span>-<span class="hljs-title class_">Usage</span> | <span class="hljs-variable constant_">GPU</span>-<span class="hljs-title class_">Util</span>  <span class="hljs-title class_">Compute</span> M. |
|                                         |                      |               <span class="hljs-variable constant_">MIG</span> M. |
|=========================================+======================+======================|
|   <span class="hljs-number">0</span>  <span class="hljs-variable constant_">NVIDIA</span> <span class="hljs-title class_">GeForce</span> <span class="hljs-variable constant_">RTX</span> <span class="hljs-number">3090</span>        <span class="hljs-title class_">On</span>  | <span class="hljs-number">00000000</span>:<span class="hljs-number">29</span>:<span class="hljs-number">00</span><span class="hljs-number">.0</span>  <span class="hljs-title class_">On</span> |                  N/A |
| <span class="hljs-number">30</span>%   57C    <span class="hljs-variable constant_">P2</span>             341W / 350W |  15054MiB / 24576MiB |    <span class="hljs-number">100</span>%      <span class="hljs-title class_">Default</span> |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| <span class="hljs-title class_">Processes</span>:                                                                            |
|  <span class="hljs-variable constant_">GPU</span>   <span class="hljs-variable constant_">GI</span>   <span class="hljs-variable constant_">CI</span>        <span class="hljs-variable constant_">PID</span>   <span class="hljs-title class_">Type</span>   <span class="hljs-title class_">Process</span> name                            <span class="hljs-variable constant_">GPU</span> <span class="hljs-title class_">Memory</span> |
|        <span class="hljs-variable constant_">ID</span>   <span class="hljs-variable constant_">ID</span>                                                             <span class="hljs-title class_">Usage</span>      |
|=======================================================================================|
|    <span class="hljs-number">0</span>   N/A  N/A      <span class="hljs-number">1610</span>      G   /usr/lib/xorg/<span class="hljs-title class_">Xorg</span>                          179MiB |
|    <span class="hljs-number">0</span>   N/A  N/A      <span class="hljs-number">1820</span>      G   /usr/bin/gnome-shell                         40MiB |
|    <span class="hljs-number">0</span>   N/A  N/A    <span class="hljs-number">108004</span>      G   ...<span class="hljs-number">2023.3</span><span class="hljs-number">.3</span>/host-linux-x64/nsys-ui.<span class="hljs-property">bin</span>        8MiB |
|    <span class="hljs-number">0</span>   N/A  N/A    <span class="hljs-number">168032</span>      G   ...seed-version=<span class="hljs-number">20240110</span>-<span class="hljs-number">180219.406000</span>      182MiB |
|    <span class="hljs-number">0</span>   N/A  N/A    <span class="hljs-number">327503</span>      C   /home<span class="hljs-comment">/***/</span>anaconda3/bin/python            14524MiB |
+---------------------------------------------------------------------------------------+
</code></pre>
<p>SMI는 GPU 메모리 사용량을 모니터링하는 간단한 도구이지만, 더 자세한 정보를 제공하는 고급 모니터링 도구들도 몇 가지 있습니다. 그 중 하나가 PyTorch Memory Snapshot인데, 이에 관해 더 읽어볼 수 있는 흥미로운 기사가 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>개요</h1>
<p>본 문서에서는 Mistral 7B와 같은 대규모 언어 모델을 단일 24GB GPU(예: NVIDIA GeForce RTX 3090 GPU)에서 세밀하게 조정할 수 있는 것을 보여드렸습니다. 그러나 자세히 설명한 대로 QLoRA와 같은 특별한 PEFT 기술이 필요합니다. 또한 모델의 배치 크기가 중요하며, 한정된 자원 때문에 보다 오랜 시간의 훈련이 필요할 수 있습니다.</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"소비자용 그래픽 카드 하나로 LLMs 미세 조정하는 방법","description":"","date":"2024-07-14 19:45","slug":"2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard","content":"\n\n## 생성적 AI\n\n![이미지](/TIL/assets/img/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard_0.png)\n\n# 배경\n\n대형 언어 모델 또는 다른 생성 모델을 생각할 때, 먼저 떠오르는 하드웨어는 GPU입니다. GPU 없이는 생성적 AI, 기계 학습, 심층 학습, 데이터 과학 등의 많은 발전이 불가능했을 것입니다. 15년 전, 게이머들이 최신 GPU 기술에 열광했다면, 오늘날 데이터 과학자와 기계 학습 엔지니어들도 이 분야의 소식을 따라가며 함께 관심을 가지고 있습니다. 보통 게이머들과 기계 학습 사용자는 서로 다른 종류의 GPU와 그래픽 카드를 사용한다고 볼 수 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n게임 사용자들은 일반적으로 소비자용 그래픽 카드(예: NVIDIA GeForce RTX 시리즈 GPU)를 사용하고, ML 및 AI 개발자들은 주로 데이터 센터 및 클라우드 컴퓨팅 GPU(예: V100, A100 또는 H100)에 대한 뉴스를 따릅니다. 게임 그래픽 카드는 일반적으로 GPU 메모리가 훨씬 적습니다(2024년 1월 기준 최대 24GB). 반면 데이터 센터 GPU는 일반적으로 40GB에서 80GB 정도의 범위에 있습니다. 또한 가격도 다른 중요한 차이점입니다. 대부분의 소비자용 그래픽 카드의 가격이 최대 3000달러가 될 수 있는 반면, 대부분의 데이터 센터 그래픽 카드는 그 가격부터 시작하여 수십만 달러까지 쉽게 올라갈 수 있습니다.\n\n저를 포함한 많은 사람들이 그래픽 카드를 게임이나 일상적인 용도로 사용할 수 있기 때문에, 같은 그래픽 카드를 사용하여 LLM 모델의 학습, 미세 조정 또는 추론에 사용할 수 있는지 궁금할 수 있습니다. 2020년에 저는 소비자용 그래픽 카드를 데이터 과학 프로젝트에 사용할 수 있는지에 대해 포괄적인 기사를 썼습니다. 당시에는 대부분 작은 ML이나 딥 러닝 모델이었고, 6GB 메모리를 가진 그래픽 카드라도 많은 학습 프로젝트를 처리할 수 있었습니다. 그러나 본 기사에서는 수십억 개의 매개변수를 가진 대형 언어 모델에 이러한 그래픽 카드를 사용할 것입니다.\n\n본 기사에서는 24GB GPU 메모리를 가진 GeForce 3090 RTX 카드를 사용했습니다. 참고로, A100 및 H100과 같은 데이터 센터 그래픽 카드는 각각 40GB 및 80GB의 메모리를 가지고 있습니다. 또한 전형적인 AWS EC2 p4d.24xlarge 인스턴스는 총 320GB의 GPU 메모리를 가진 8개의 GPU(V100)를 가지고 있습니다. 간단한 소비자용 GPU와 전형적인 클라우드 ML 인스턴스 간의 차이가 상당히 크다는 것을 보실 수 있습니다. 그러나 질문은, 우리가 단일 소비자용 그래픽 카드에서 대형 모델을 학습할 수 있는지 여부인데요? 가능하다면, 팁과 교훈은 무엇인가요? 이 기사의 나머지 부분을 읽어보세요.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n아무런 LLM 모델이나 교육 데이터 집합을로드하기 전에 그러한 프로세스에 필요한 하드웨어 및 소프트웨어를 찾아야합니다.\n\n언급한 바와 같이, 나는 소비자 GPU 중에서도 가장 높은 메모리(24GB) 중 하나를 갖고 있는 NVIDIA GeForce RTX 3090 GPU를 사용했습니다(참고로, 4090 모델도 동일한 메모리 크기를 가지고 있습니다). 이 GPU는 유명한 A100 GPU에 있는 것과 동일한 Ampere 아키텍처를 기반으로 하고 있습니다. GeForce RTX 3090 GPU 사양에 대해 더 자세히 알아볼 수 있습니다.\n\n모든 테스트를 거친 후, 24GB가 10억 개의 매개변수를 갖는 LLM과 작업을 수행하는 데 필요한 최소한의 GPU 메모리라고 생각합니다.\n\n\u003cimg src=\"/TIL/assets/img/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard_1.png\" /\u003e\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그래픽 카드 외에도 PC의 좋은 환기 시스템이 필요합니다. 세밀한 조정 중에 GPU의 온도가 쉽게 올라가고 팬으로는 충분히 식힐 수 없는 경우가 있습니다. 높은 GPU 온도는 GPU 성능을 낮출 수 있고, 처리 시간이 더 오래 걸릴 수 있습니다.\n\n하드웨어 외에도 여기서 언급해야 할 몇 가지 소프트웨어 고려 사항이 있습니다. 먼저, Windows 사용자라면 안타깝게도 나쁜 소식이 있어요. 일부 라이브러리와 도구는 Linux에서만 작동합니다. 특히, 모델 양자화에 자주 사용되는 bitsandbytes는 Windows 친화적이지 않습니다. 어떤 사람들은 Windows용 래퍼를 만들었지만 (예를 들어 여기), 그들은 장단점이 있어요. 그래서 제 추천은 WSL에 Linux를 설치하거나 저와 같이 듀얼 부팅 시스템을 사용하여 LLM 작업 중에 완전히 Linux로 전환하는 것입니다.\n\n또한, PyTorch와 호환되는 CUDA 버전을 설치해야 합니다. 제 추천은 CUDA 12.3을 설치하는 것입니다 (링크). 그런 다음 이 페이지로 이동하여 시스템, CUDA 버전 및 패키지 관리자 시스템에 따라 올바른 PyTorch를 다운로드하고 설치해야 합니다 (https://pytorch.org/).\n\n```js\nexport BNB_CUDA_VERSION=123\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/\u003cYOUR-USER-DIR\u003e/local/cuda-12.3\n```\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n마지막으로 시스템에 다음 패키지를 설치해야 합니다. 충돌을 피하기 위해 시스템에 이미 설치된 다른 패키지와 충돌을 피하기 위해 새 가상 환경(venv)을 만드는 것을 권장합니다. 또한, 아래는 제가 성공적으로 사용한 패키지 버전들입니다:\n\n```js\ntorch==2.1.2\ntransformers==4.36.2\ndatasests==2.16.1\nbitsandbytes==0.42.0\npeft==0.7.1\n```\n\n# 기술적 배경\n\n이제 시스템에서 LLMs를 사용하기 위해 모든 하드웨어와 소프트웨어를 준비했으니, 다음 섹션에서 마주할 기술적 개념에 대해 매우 간단히 검토하는 것이 좋습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n대형 언어 모델은 수백만 또는 수십억 개의 매개변수로 구성됩니다. 일반적으로 수십억 또는 때로는 수조의 토큰으로 훈련된 사전 훈련된 모델을 사용하는데 굉장히 긴 훈련 과정과 수백만 달러가 들었습니다. 이러한 모델 매개변수 각각은 32비트(4바이트)의 메모리를 차지하고 로드하기 위해 필요합니다. 일반적으로 10억 개의 매개변수당 약 4GB의 메모리가 필요하다고 생각할 수 있습니다. 로드(및 이후 추론 또는 후속 모델 훈련)를 위해 메모리 사용량을 줄이는 한 가지 기술은 \"양자화\"입니다. 이 기술에서는 모델 가중치의 정밀도를 32비트의 완전 정밀도에서 16비트(fp16 또는 bfloat16), 8비트(int8) 또는 그 이하로 줄입니다.\n\n모델 가중치의 정밀도를 줄이면 제한적인 메모리에 더 큰 모델을 로드할 수 있지만 모델 성능을 희생해야 합니다. 그러나 일부 연구에서는 fp32와 bfloat16 간의 모델 성능 차이가 중요하지 않다고 제안하며, 많은 유명한 모델(Llama2 포함)이 bfloat16로 사전 훈련되었습니다.\n\n양자화는 단일 GPU에서 메모리가 24GB인 대형 언어 모델을 세밀하게 조정하거나 추론할 때 반드시 사용해야 하는 기술입니다. 나중에 볼 것처럼 bitsandbytes 라이브러리를 사용하여 모델 양자화를 구현할 수 있습니다.\n\n가장 엄격한 양자화 기술을 사용하더라도 수십억 개의 매개변수를 가진 작은 크기의 LLM 모델을 사전 훈련할 수 없습니다. 크리스 프레글리 등은 최근 발표된 'AWS에서의 생성적 AI' 도서에서 모델 훈련에 필요한 메모리에 대한 좋은 규칙을 설명했습니다. 그들은 모델의 10억 개의 매개변수당 16비트 반 정밀도에서 6GB의 메모리가 필요하다고 설명했죠.\n\n기억해야 할 것은 메모리 크기가 훈련 이야기의 일부일 뿐이라는 점입니다. 사전 훈련을 완료하는 데 필요한 시간도 또 하나의 중요한 측면입니다. 예를 들어 가장 작은 Llama2 모델인 Llama2 7B는 70억 개의 매개변수를 가지고 훈련을 완료하는 데 184320 GPU 시간이 걸렸습니다.\n\n그래서 대부분의 사람들(상당한 하드웨어 자원과 예산을 갖춘 사람들도)는 특정 사용 사례에 맞게 사전 훈련된 모델을 사용하고 세밀하게 조정하려는 경향이 있습니다. 그러나 한정된 자원(예: 단일 GPU)으로 완전한 세밀 조정을 수행하는 것은 다소 어려울 수 있습니다. 이에 따라 모델 매개변수의 한정된 부분만 업데이트하는 \"효율적인 매개 조정\" (PEFT) 이 보다 현실적으로 보입니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다양한 PEFT 기술 중에서, LoRA (Low Ranking Adaption)는 그 계산 효율성으로 매우 인기가 높습니다. 이 기술에서는 원래 모델의 가중치를 모두 고정시키고 대신 Transformer 아키텍처의 특정 레이어에 추가할 수 있는 저랭크 행렬을 학습합니다. LLM을 세세히 조정할 때 LoRA를 사용하는 경우, 모델 가중치의 0.5%를 업데이트합니다.\n\nQLoRA는 저랭크 행렬 LoRA에 우리가 설명한 양자화 개념을 결합한 변형입니다. 특히, QLoRA 구현에서는 모델을 세세히 조정하기 위해 nf4 또는 Normal Float 4를 사용할 것입니다. QLoRA는 단일 소비자 GPU로 대규모 모델을 세세히 조정하는 경우 연구 사례에서 매우 유용합니다.\n\n# 코딩 타임\n\n마지막으로, 이제 코딩할 시간입니다!\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n내 GitHub 리포지토리에서 작동하는 주피터 노트북을 찾을 수 있어요. 이 코드의 많은 부분은 Mathieu Busquet의 깔끔한 글에서 영감을 받아 따랐어요.\n\n코드를 한 줄씩 설명하지는 않겠지만, 단일 GPU에서 대규모 모델을 세밀하게 조정하는 데 중요한 부분을 강조할 거에요.\n\n## 트랜스포머 모델\n\n우선, 이 테스트에 Mistral 7B 모델(mistralai/Mistral-7B-v0.1)을 선택했어요. Mistral AI가 개발한 Mistral 7B 모델은 2023년 9월에 공개된 오픈 소스 LLM이에요 (논문 링크). 많은 측면에서 이 모델은 Llama2와 같은 유명한 모델들을 능가해요 (다음 차트를 참고하세요).\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\u003cimg src=\"/TIL/assets/img/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard_2.png\" /\u003e\n\n## 데이터셋\n\n또한, 저는 fine-tuning을 위해 Databricks databricks-dolly-15k dataset를 사용했어요 (CC BY-SA 3.0 라이선스하에 제공됨). fine-tuning 시간을 줄이기 위해 이 데이터의 작은 부분(1000행)을 사용했고 컨셉을 증명했어요.\n\n## 구성 요소\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모델 로딩 시, GPU 메모리 제한을 극복하기 위해 다음과 같은 양자화 구성을 사용했어요.\n\n```js\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n```\n\n이 양자화 구성은 bfloat16 계산 데이터 유형과 nf4(4비트 Normal Float)인 저정밀 스토리지 데이터 유형을 가지고 있기 때문에 단일 GPU에서 모델 세밀 조정에 매우 중요해요. 실제로는 QLORA 가중치 텐서가 사용될 때, 텐서를 bfloat16로 비양자화하고 16비트에서 행렬 곱셈을 수행하게 됩니다(자세한 내용은 원본 논문 참조).\n\n또한 이전에 언급한 대로, 양자화와 함께 LoRA를 사용하여 메모리 제한을 극복하기 위한 QLoRA를 사용 중이에요. 여기 LoRA를 위한 제 설정입니다:\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=64,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"],\n    bias=\"none\",\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\",\n)\n```\n\n제 LoRA 구성에는 랭크로 16을 사용했어요. 4부터 16 사이의 랭크로 설정하는 것이 학습 가능한 매개변수의 수를 줄이고 모델 성능 사이의 적절한 균형을 얻기 위해 권장됩니다. 마지막으로, Mistral 7B 트랜스포머 모델의 일부 선형 계층에 LoRA를 적용했습니다.\n\n# 학습 및 모니터링\n\n제 개인적인 그래픽 카드를 사용하여 4 에포크 (1000 단계)의 학습을 완료할 수 있었어요. 지역 GPU에서 LLM을 학습하는 이러한 테스트 중 하나의 목적은 어떠한 제한 없이 하드웨어 리소스를 모니터링하는 것이에요. 학습 중 GPU를 모니터링하는 가장 간단한 도구 중 하나는 Nvidia 시스템 관리 인터페이스 (SMI)입니다. 단순히 터미널을 열고 명령줄에 다음을 입력하세요:\n\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nnvidia-smi\n```\n\n또는 지속적인 모니터링과 업데이트를 위해 다음을 사용하세요 (1초마다 새로고침):\n\n```js\nnvidia-smi -l 1\n```\n\n이렇게 하면 GPU에서 각 프로세스의 메모리 사용량을 확인할 수 있습니다. 다음 SMI 보기에서 저는 모델을 불러왔고 약 5GB의 메모리를 사용했습니다 (양자화 덕분에). 또한 Anaconda3 Python (Jupyter 노트북) 프로세스로 모델이 불러와진 것을 확인할 수 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 3090        On  | 00000000:29:00.0  On |                  N/A |\n| 30%   37C    P8              33W / 350W |   5346MiB / 24576MiB |      5%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      1610      G   /usr/lib/xorg/Xorg                          179MiB |\n|    0   N/A  N/A      1820      G   /usr/bin/gnome-shell                         41MiB |\n|    0   N/A  N/A    108004      G   ...2023.3.3/host-linux-x64/nsys-ui.bin        8MiB |\n|    0   N/A  N/A    168032      G   ...seed-version=20240110-180219.406000      117MiB |\n|    0   N/A  N/A    327503      C   /home/***/anaconda3/bin/python             4880MiB |\n+---------------------------------------------------------------------------------------+\n```\n\n그리고 이곳은 훈련 과정 중 약 30단계 이후의 메모리 상태입니다. 보시다시피, 사용 중인 GPU 메모리는 현재 약 15GB입니다.\n\n```js\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce RTX 3090        On  | 00000000:29:00.0  On |                  N/A |\n| 30%   57C    P2             341W / 350W |  15054MiB / 24576MiB |    100%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      1610      G   /usr/lib/xorg/Xorg                          179MiB |\n|    0   N/A  N/A      1820      G   /usr/bin/gnome-shell                         40MiB |\n|    0   N/A  N/A    108004      G   ...2023.3.3/host-linux-x64/nsys-ui.bin        8MiB |\n|    0   N/A  N/A    168032      G   ...seed-version=20240110-180219.406000      182MiB |\n|    0   N/A  N/A    327503      C   /home/***/anaconda3/bin/python            14524MiB |\n+---------------------------------------------------------------------------------------+\n```\n\nSMI는 GPU 메모리 사용량을 모니터링하는 간단한 도구이지만, 더 자세한 정보를 제공하는 고급 모니터링 도구들도 몇 가지 있습니다. 그 중 하나가 PyTorch Memory Snapshot인데, 이에 관해 더 읽어볼 수 있는 흥미로운 기사가 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 개요\n\n본 문서에서는 Mistral 7B와 같은 대규모 언어 모델을 단일 24GB GPU(예: NVIDIA GeForce RTX 3090 GPU)에서 세밀하게 조정할 수 있는 것을 보여드렸습니다. 그러나 자세히 설명한 대로 QLoRA와 같은 특별한 PEFT 기술이 필요합니다. 또한 모델의 배치 크기가 중요하며, 한정된 자원 때문에 보다 오랜 시간의 훈련이 필요할 수 있습니다.","ogImage":{"url":"/TIL/assets/img/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard_0.png"},"coverImage":"/TIL/assets/img/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard_0.png","tag":["Tech"],"readingTime":15},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch2\u003e생성적 AI\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch1\u003e배경\u003c/h1\u003e\n\u003cp\u003e대형 언어 모델 또는 다른 생성 모델을 생각할 때, 먼저 떠오르는 하드웨어는 GPU입니다. GPU 없이는 생성적 AI, 기계 학습, 심층 학습, 데이터 과학 등의 많은 발전이 불가능했을 것입니다. 15년 전, 게이머들이 최신 GPU 기술에 열광했다면, 오늘날 데이터 과학자와 기계 학습 엔지니어들도 이 분야의 소식을 따라가며 함께 관심을 가지고 있습니다. 보통 게이머들과 기계 학습 사용자는 서로 다른 종류의 GPU와 그래픽 카드를 사용한다고 볼 수 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e게임 사용자들은 일반적으로 소비자용 그래픽 카드(예: NVIDIA GeForce RTX 시리즈 GPU)를 사용하고, ML 및 AI 개발자들은 주로 데이터 센터 및 클라우드 컴퓨팅 GPU(예: V100, A100 또는 H100)에 대한 뉴스를 따릅니다. 게임 그래픽 카드는 일반적으로 GPU 메모리가 훨씬 적습니다(2024년 1월 기준 최대 24GB). 반면 데이터 센터 GPU는 일반적으로 40GB에서 80GB 정도의 범위에 있습니다. 또한 가격도 다른 중요한 차이점입니다. 대부분의 소비자용 그래픽 카드의 가격이 최대 3000달러가 될 수 있는 반면, 대부분의 데이터 센터 그래픽 카드는 그 가격부터 시작하여 수십만 달러까지 쉽게 올라갈 수 있습니다.\u003c/p\u003e\n\u003cp\u003e저를 포함한 많은 사람들이 그래픽 카드를 게임이나 일상적인 용도로 사용할 수 있기 때문에, 같은 그래픽 카드를 사용하여 LLM 모델의 학습, 미세 조정 또는 추론에 사용할 수 있는지 궁금할 수 있습니다. 2020년에 저는 소비자용 그래픽 카드를 데이터 과학 프로젝트에 사용할 수 있는지에 대해 포괄적인 기사를 썼습니다. 당시에는 대부분 작은 ML이나 딥 러닝 모델이었고, 6GB 메모리를 가진 그래픽 카드라도 많은 학습 프로젝트를 처리할 수 있었습니다. 그러나 본 기사에서는 수십억 개의 매개변수를 가진 대형 언어 모델에 이러한 그래픽 카드를 사용할 것입니다.\u003c/p\u003e\n\u003cp\u003e본 기사에서는 24GB GPU 메모리를 가진 GeForce 3090 RTX 카드를 사용했습니다. 참고로, A100 및 H100과 같은 데이터 센터 그래픽 카드는 각각 40GB 및 80GB의 메모리를 가지고 있습니다. 또한 전형적인 AWS EC2 p4d.24xlarge 인스턴스는 총 320GB의 GPU 메모리를 가진 8개의 GPU(V100)를 가지고 있습니다. 간단한 소비자용 GPU와 전형적인 클라우드 ML 인스턴스 간의 차이가 상당히 크다는 것을 보실 수 있습니다. 그러나 질문은, 우리가 단일 소비자용 그래픽 카드에서 대형 모델을 학습할 수 있는지 여부인데요? 가능하다면, 팁과 교훈은 무엇인가요? 이 기사의 나머지 부분을 읽어보세요.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e아무런 LLM 모델이나 교육 데이터 집합을로드하기 전에 그러한 프로세스에 필요한 하드웨어 및 소프트웨어를 찾아야합니다.\u003c/p\u003e\n\u003cp\u003e언급한 바와 같이, 나는 소비자 GPU 중에서도 가장 높은 메모리(24GB) 중 하나를 갖고 있는 NVIDIA GeForce RTX 3090 GPU를 사용했습니다(참고로, 4090 모델도 동일한 메모리 크기를 가지고 있습니다). 이 GPU는 유명한 A100 GPU에 있는 것과 동일한 Ampere 아키텍처를 기반으로 하고 있습니다. GeForce RTX 3090 GPU 사양에 대해 더 자세히 알아볼 수 있습니다.\u003c/p\u003e\n\u003cp\u003e모든 테스트를 거친 후, 24GB가 10억 개의 매개변수를 갖는 LLM과 작업을 수행하는 데 필요한 최소한의 GPU 메모리라고 생각합니다.\u003c/p\u003e\n\u003cimg src=\"/TIL/assets/img/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard_1.png\"\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e그래픽 카드 외에도 PC의 좋은 환기 시스템이 필요합니다. 세밀한 조정 중에 GPU의 온도가 쉽게 올라가고 팬으로는 충분히 식힐 수 없는 경우가 있습니다. 높은 GPU 온도는 GPU 성능을 낮출 수 있고, 처리 시간이 더 오래 걸릴 수 있습니다.\u003c/p\u003e\n\u003cp\u003e하드웨어 외에도 여기서 언급해야 할 몇 가지 소프트웨어 고려 사항이 있습니다. 먼저, Windows 사용자라면 안타깝게도 나쁜 소식이 있어요. 일부 라이브러리와 도구는 Linux에서만 작동합니다. 특히, 모델 양자화에 자주 사용되는 bitsandbytes는 Windows 친화적이지 않습니다. 어떤 사람들은 Windows용 래퍼를 만들었지만 (예를 들어 여기), 그들은 장단점이 있어요. 그래서 제 추천은 WSL에 Linux를 설치하거나 저와 같이 듀얼 부팅 시스템을 사용하여 LLM 작업 중에 완전히 Linux로 전환하는 것입니다.\u003c/p\u003e\n\u003cp\u003e또한, PyTorch와 호환되는 CUDA 버전을 설치해야 합니다. 제 추천은 CUDA 12.3을 설치하는 것입니다 (링크). 그런 다음 이 페이지로 이동하여 시스템, CUDA 버전 및 패키지 관리자 시스템에 따라 올바른 PyTorch를 다운로드하고 설치해야 합니다 (\u003ca href=\"https://pytorch.org/\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://pytorch.org/\u003c/a\u003e).\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eexport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eBNB_CUDA_VERSION\u003c/span\u003e=\u003cspan class=\"hljs-number\"\u003e123\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eexport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eLD_LIBRARY_PATH\u003c/span\u003e=\u003cspan class=\"hljs-attr\"\u003e$LD_LIBRARY_PATH\u003c/span\u003e:\u003cspan class=\"hljs-regexp\"\u003e/home/\u003c/span\u003e\u0026#x3C;\u003cspan class=\"hljs-variable constant_\"\u003eYOUR\u003c/span\u003e-\u003cspan class=\"hljs-variable constant_\"\u003eUSER\u003c/span\u003e-\u003cspan class=\"hljs-variable constant_\"\u003eDIR\u003c/span\u003e\u003e\u003cspan class=\"hljs-regexp\"\u003e/local/\u003c/span\u003ecuda-\u003cspan class=\"hljs-number\"\u003e12.3\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e마지막으로 시스템에 다음 패키지를 설치해야 합니다. 충돌을 피하기 위해 시스템에 이미 설치된 다른 패키지와 충돌을 피하기 위해 새 가상 환경(venv)을 만드는 것을 권장합니다. 또한, 아래는 제가 성공적으로 사용한 패키지 버전들입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003etorch==\u003cspan class=\"hljs-number\"\u003e2.1\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.2\u003c/span\u003e\ntransformers==\u003cspan class=\"hljs-number\"\u003e4.36\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.2\u003c/span\u003e\ndatasests==\u003cspan class=\"hljs-number\"\u003e2.16\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.1\u003c/span\u003e\nbitsandbytes==\u003cspan class=\"hljs-number\"\u003e0.42\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.0\u003c/span\u003e\npeft==\u003cspan class=\"hljs-number\"\u003e0.7\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.1\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e기술적 배경\u003c/h1\u003e\n\u003cp\u003e이제 시스템에서 LLMs를 사용하기 위해 모든 하드웨어와 소프트웨어를 준비했으니, 다음 섹션에서 마주할 기술적 개념에 대해 매우 간단히 검토하는 것이 좋습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e대형 언어 모델은 수백만 또는 수십억 개의 매개변수로 구성됩니다. 일반적으로 수십억 또는 때로는 수조의 토큰으로 훈련된 사전 훈련된 모델을 사용하는데 굉장히 긴 훈련 과정과 수백만 달러가 들었습니다. 이러한 모델 매개변수 각각은 32비트(4바이트)의 메모리를 차지하고 로드하기 위해 필요합니다. 일반적으로 10억 개의 매개변수당 약 4GB의 메모리가 필요하다고 생각할 수 있습니다. 로드(및 이후 추론 또는 후속 모델 훈련)를 위해 메모리 사용량을 줄이는 한 가지 기술은 \"양자화\"입니다. 이 기술에서는 모델 가중치의 정밀도를 32비트의 완전 정밀도에서 16비트(fp16 또는 bfloat16), 8비트(int8) 또는 그 이하로 줄입니다.\u003c/p\u003e\n\u003cp\u003e모델 가중치의 정밀도를 줄이면 제한적인 메모리에 더 큰 모델을 로드할 수 있지만 모델 성능을 희생해야 합니다. 그러나 일부 연구에서는 fp32와 bfloat16 간의 모델 성능 차이가 중요하지 않다고 제안하며, 많은 유명한 모델(Llama2 포함)이 bfloat16로 사전 훈련되었습니다.\u003c/p\u003e\n\u003cp\u003e양자화는 단일 GPU에서 메모리가 24GB인 대형 언어 모델을 세밀하게 조정하거나 추론할 때 반드시 사용해야 하는 기술입니다. 나중에 볼 것처럼 bitsandbytes 라이브러리를 사용하여 모델 양자화를 구현할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e가장 엄격한 양자화 기술을 사용하더라도 수십억 개의 매개변수를 가진 작은 크기의 LLM 모델을 사전 훈련할 수 없습니다. 크리스 프레글리 등은 최근 발표된 'AWS에서의 생성적 AI' 도서에서 모델 훈련에 필요한 메모리에 대한 좋은 규칙을 설명했습니다. 그들은 모델의 10억 개의 매개변수당 16비트 반 정밀도에서 6GB의 메모리가 필요하다고 설명했죠.\u003c/p\u003e\n\u003cp\u003e기억해야 할 것은 메모리 크기가 훈련 이야기의 일부일 뿐이라는 점입니다. 사전 훈련을 완료하는 데 필요한 시간도 또 하나의 중요한 측면입니다. 예를 들어 가장 작은 Llama2 모델인 Llama2 7B는 70억 개의 매개변수를 가지고 훈련을 완료하는 데 184320 GPU 시간이 걸렸습니다.\u003c/p\u003e\n\u003cp\u003e그래서 대부분의 사람들(상당한 하드웨어 자원과 예산을 갖춘 사람들도)는 특정 사용 사례에 맞게 사전 훈련된 모델을 사용하고 세밀하게 조정하려는 경향이 있습니다. 그러나 한정된 자원(예: 단일 GPU)으로 완전한 세밀 조정을 수행하는 것은 다소 어려울 수 있습니다. 이에 따라 모델 매개변수의 한정된 부분만 업데이트하는 \"효율적인 매개 조정\" (PEFT) 이 보다 현실적으로 보입니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e다양한 PEFT 기술 중에서, LoRA (Low Ranking Adaption)는 그 계산 효율성으로 매우 인기가 높습니다. 이 기술에서는 원래 모델의 가중치를 모두 고정시키고 대신 Transformer 아키텍처의 특정 레이어에 추가할 수 있는 저랭크 행렬을 학습합니다. LLM을 세세히 조정할 때 LoRA를 사용하는 경우, 모델 가중치의 0.5%를 업데이트합니다.\u003c/p\u003e\n\u003cp\u003eQLoRA는 저랭크 행렬 LoRA에 우리가 설명한 양자화 개념을 결합한 변형입니다. 특히, QLoRA 구현에서는 모델을 세세히 조정하기 위해 nf4 또는 Normal Float 4를 사용할 것입니다. QLoRA는 단일 소비자 GPU로 대규모 모델을 세세히 조정하는 경우 연구 사례에서 매우 유용합니다.\u003c/p\u003e\n\u003ch1\u003e코딩 타임\u003c/h1\u003e\n\u003cp\u003e마지막으로, 이제 코딩할 시간입니다!\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e내 GitHub 리포지토리에서 작동하는 주피터 노트북을 찾을 수 있어요. 이 코드의 많은 부분은 Mathieu Busquet의 깔끔한 글에서 영감을 받아 따랐어요.\u003c/p\u003e\n\u003cp\u003e코드를 한 줄씩 설명하지는 않겠지만, 단일 GPU에서 대규모 모델을 세밀하게 조정하는 데 중요한 부분을 강조할 거에요.\u003c/p\u003e\n\u003ch2\u003e트랜스포머 모델\u003c/h2\u003e\n\u003cp\u003e우선, 이 테스트에 Mistral 7B 모델(mistralai/Mistral-7B-v0.1)을 선택했어요. Mistral AI가 개발한 Mistral 7B 모델은 2023년 9월에 공개된 오픈 소스 LLM이에요 (논문 링크). 많은 측면에서 이 모델은 Llama2와 같은 유명한 모델들을 능가해요 (다음 차트를 참고하세요).\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cimg src=\"/TIL/assets/img/2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard_2.png\"\u003e\n\u003ch2\u003e데이터셋\u003c/h2\u003e\n\u003cp\u003e또한, 저는 fine-tuning을 위해 Databricks databricks-dolly-15k dataset를 사용했어요 (CC BY-SA 3.0 라이선스하에 제공됨). fine-tuning 시간을 줄이기 위해 이 데이터의 작은 부분(1000행)을 사용했고 컨셉을 증명했어요.\u003c/p\u003e\n\u003ch2\u003e구성 요소\u003c/h2\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e모델 로딩 시, GPU 메모리 제한을 극복하기 위해 다음과 같은 양자화 구성을 사용했어요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003equantization_config = \u003cspan class=\"hljs-title class_\"\u003eBitsAndBytesConfig\u003c/span\u003e(\n    load_in_4bit=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n    bnb_4bit_use_double_quant=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e,\n    bnb_4bit_quant_type=\u003cspan class=\"hljs-string\"\u003e\"nf4\"\u003c/span\u003e,\n    bnb_4bit_compute_dtype=torch.\u003cspan class=\"hljs-property\"\u003ebfloat16\u003c/span\u003e,\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 양자화 구성은 bfloat16 계산 데이터 유형과 nf4(4비트 Normal Float)인 저정밀 스토리지 데이터 유형을 가지고 있기 때문에 단일 GPU에서 모델 세밀 조정에 매우 중요해요. 실제로는 QLORA 가중치 텐서가 사용될 때, 텐서를 bfloat16로 비양자화하고 16비트에서 행렬 곱셈을 수행하게 됩니다(자세한 내용은 원본 논문 참조).\u003c/p\u003e\n\u003cp\u003e또한 이전에 언급한 대로, 양자화와 함께 LoRA를 사용하여 메모리 제한을 극복하기 위한 QLoRA를 사용 중이에요. 여기 LoRA를 위한 제 설정입니다:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003elora_config = \u003cspan class=\"hljs-title class_\"\u003eLoraConfig\u003c/span\u003e(\n    r=\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e,\n    lora_alpha=\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e,\n    target_modules=[\u003cspan class=\"hljs-string\"\u003e\"q_proj\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"k_proj\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"v_proj\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"o_proj\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"gate_proj\"\u003c/span\u003e],\n    bias=\u003cspan class=\"hljs-string\"\u003e\"none\"\u003c/span\u003e,\n    lora_dropout=\u003cspan class=\"hljs-number\"\u003e0.05\u003c/span\u003e,\n    task_type=\u003cspan class=\"hljs-string\"\u003e\"CAUSAL_LM\"\u003c/span\u003e,\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e제 LoRA 구성에는 랭크로 16을 사용했어요. 4부터 16 사이의 랭크로 설정하는 것이 학습 가능한 매개변수의 수를 줄이고 모델 성능 사이의 적절한 균형을 얻기 위해 권장됩니다. 마지막으로, Mistral 7B 트랜스포머 모델의 일부 선형 계층에 LoRA를 적용했습니다.\u003c/p\u003e\n\u003ch1\u003e학습 및 모니터링\u003c/h1\u003e\n\u003cp\u003e제 개인적인 그래픽 카드를 사용하여 4 에포크 (1000 단계)의 학습을 완료할 수 있었어요. 지역 GPU에서 LLM을 학습하는 이러한 테스트 중 하나의 목적은 어떠한 제한 없이 하드웨어 리소스를 모니터링하는 것이에요. 학습 중 GPU를 모니터링하는 가장 간단한 도구 중 하나는 Nvidia 시스템 관리 인터페이스 (SMI)입니다. 단순히 터미널을 열고 명령줄에 다음을 입력하세요:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003envidia-smi\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e또는 지속적인 모니터링과 업데이트를 위해 다음을 사용하세요 (1초마다 새로고침):\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003envidia-smi -l \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이렇게 하면 GPU에서 각 프로세스의 메모리 사용량을 확인할 수 있습니다. 다음 SMI 보기에서 저는 모델을 불러왔고 약 5GB의 메모리를 사용했습니다 (양자화 덕분에). 또한 Anaconda3 Python (Jupyter 노트북) 프로세스로 모델이 불러와진 것을 확인할 수 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e+---------------------------------------------------------------------------------------+\n| \u003cspan class=\"hljs-variable constant_\"\u003eNVIDIA\u003c/span\u003e-\u003cspan class=\"hljs-variable constant_\"\u003eSMI\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e545.23\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.08\u003c/span\u003e              \u003cspan class=\"hljs-title class_\"\u003eDriver\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eVersion\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e545.23\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.08\u003c/span\u003e    \u003cspan class=\"hljs-variable constant_\"\u003eCUDA\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eVersion\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e12.3\u003c/span\u003e     |\n|-----------------------------------------+----------------------+----------------------+\n| \u003cspan class=\"hljs-variable constant_\"\u003eGPU\u003c/span\u003e  \u003cspan class=\"hljs-title class_\"\u003eName\u003c/span\u003e                 \u003cspan class=\"hljs-title class_\"\u003ePersistence\u003c/span\u003e-M | \u003cspan class=\"hljs-title class_\"\u003eBus\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eId\u003c/span\u003e        \u003cspan class=\"hljs-title class_\"\u003eDisp\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eA\u003c/span\u003e | \u003cspan class=\"hljs-title class_\"\u003eVolatile\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eUncorr\u003c/span\u003e. \u003cspan class=\"hljs-variable constant_\"\u003eECC\u003c/span\u003e |\n| \u003cspan class=\"hljs-title class_\"\u003eFan\u003c/span\u003e  \u003cspan class=\"hljs-title class_\"\u003eTemp\u003c/span\u003e   \u003cspan class=\"hljs-title class_\"\u003ePerf\u003c/span\u003e          \u003cspan class=\"hljs-title class_\"\u003ePwr\u003c/span\u003e:\u003cspan class=\"hljs-title class_\"\u003eUsage\u003c/span\u003e/\u003cspan class=\"hljs-title class_\"\u003eCap\u003c/span\u003e |         \u003cspan class=\"hljs-title class_\"\u003eMemory\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eUsage\u003c/span\u003e | \u003cspan class=\"hljs-variable constant_\"\u003eGPU\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eUtil\u003c/span\u003e  \u003cspan class=\"hljs-title class_\"\u003eCompute\u003c/span\u003e M. |\n|                                         |                      |               \u003cspan class=\"hljs-variable constant_\"\u003eMIG\u003c/span\u003e M. |\n|=========================================+======================+======================|\n|   \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e  \u003cspan class=\"hljs-variable constant_\"\u003eNVIDIA\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eGeForce\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eRTX\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e3090\u003c/span\u003e        \u003cspan class=\"hljs-title class_\"\u003eOn\u003c/span\u003e  | \u003cspan class=\"hljs-number\"\u003e00000000\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e29\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e00\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.0\u003c/span\u003e  \u003cspan class=\"hljs-title class_\"\u003eOn\u003c/span\u003e |                  N/A |\n| \u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e%   37C    \u003cspan class=\"hljs-variable constant_\"\u003eP8\u003c/span\u003e              33W / 350W |   5346MiB / 24576MiB |      \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e%      \u003cspan class=\"hljs-title class_\"\u003eDefault\u003c/span\u003e |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| \u003cspan class=\"hljs-title class_\"\u003eProcesses\u003c/span\u003e:                                                                            |\n|  \u003cspan class=\"hljs-variable constant_\"\u003eGPU\u003c/span\u003e   \u003cspan class=\"hljs-variable constant_\"\u003eGI\u003c/span\u003e   \u003cspan class=\"hljs-variable constant_\"\u003eCI\u003c/span\u003e        \u003cspan class=\"hljs-variable constant_\"\u003ePID\u003c/span\u003e   \u003cspan class=\"hljs-title class_\"\u003eType\u003c/span\u003e   \u003cspan class=\"hljs-title class_\"\u003eProcess\u003c/span\u003e name                            \u003cspan class=\"hljs-variable constant_\"\u003eGPU\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eMemory\u003c/span\u003e |\n|        \u003cspan class=\"hljs-variable constant_\"\u003eID\u003c/span\u003e   \u003cspan class=\"hljs-variable constant_\"\u003eID\u003c/span\u003e                                                             \u003cspan class=\"hljs-title class_\"\u003eUsage\u003c/span\u003e      |\n|=======================================================================================|\n|    \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e   N/A  N/A      \u003cspan class=\"hljs-number\"\u003e1610\u003c/span\u003e      G   /usr/lib/xorg/\u003cspan class=\"hljs-title class_\"\u003eXorg\u003c/span\u003e                          179MiB |\n|    \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e   N/A  N/A      \u003cspan class=\"hljs-number\"\u003e1820\u003c/span\u003e      G   /usr/bin/gnome-shell                         41MiB |\n|    \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e   N/A  N/A    \u003cspan class=\"hljs-number\"\u003e108004\u003c/span\u003e      G   ...\u003cspan class=\"hljs-number\"\u003e2023.3\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.3\u003c/span\u003e/host-linux-x64/nsys-ui.\u003cspan class=\"hljs-property\"\u003ebin\u003c/span\u003e        8MiB |\n|    \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e   N/A  N/A    \u003cspan class=\"hljs-number\"\u003e168032\u003c/span\u003e      G   ...seed-version=\u003cspan class=\"hljs-number\"\u003e20240110\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e180219.406000\u003c/span\u003e      117MiB |\n|    \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e   N/A  N/A    \u003cspan class=\"hljs-number\"\u003e327503\u003c/span\u003e      C   /home\u003cspan class=\"hljs-comment\"\u003e/***/\u003c/span\u003eanaconda3/bin/python             4880MiB |\n+---------------------------------------------------------------------------------------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e그리고 이곳은 훈련 과정 중 약 30단계 이후의 메모리 상태입니다. 보시다시피, 사용 중인 GPU 메모리는 현재 약 15GB입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e+---------------------------------------------------------------------------------------+\n| \u003cspan class=\"hljs-variable constant_\"\u003eNVIDIA\u003c/span\u003e-\u003cspan class=\"hljs-variable constant_\"\u003eSMI\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e545.23\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.08\u003c/span\u003e              \u003cspan class=\"hljs-title class_\"\u003eDriver\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eVersion\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e545.23\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.08\u003c/span\u003e    \u003cspan class=\"hljs-variable constant_\"\u003eCUDA\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eVersion\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e12.3\u003c/span\u003e     |\n|-----------------------------------------+----------------------+----------------------+\n| \u003cspan class=\"hljs-variable constant_\"\u003eGPU\u003c/span\u003e  \u003cspan class=\"hljs-title class_\"\u003eName\u003c/span\u003e                 \u003cspan class=\"hljs-title class_\"\u003ePersistence\u003c/span\u003e-M | \u003cspan class=\"hljs-title class_\"\u003eBus\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eId\u003c/span\u003e        \u003cspan class=\"hljs-title class_\"\u003eDisp\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eA\u003c/span\u003e | \u003cspan class=\"hljs-title class_\"\u003eVolatile\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eUncorr\u003c/span\u003e. \u003cspan class=\"hljs-variable constant_\"\u003eECC\u003c/span\u003e |\n| \u003cspan class=\"hljs-title class_\"\u003eFan\u003c/span\u003e  \u003cspan class=\"hljs-title class_\"\u003eTemp\u003c/span\u003e   \u003cspan class=\"hljs-title class_\"\u003ePerf\u003c/span\u003e          \u003cspan class=\"hljs-title class_\"\u003ePwr\u003c/span\u003e:\u003cspan class=\"hljs-title class_\"\u003eUsage\u003c/span\u003e/\u003cspan class=\"hljs-title class_\"\u003eCap\u003c/span\u003e |         \u003cspan class=\"hljs-title class_\"\u003eMemory\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eUsage\u003c/span\u003e | \u003cspan class=\"hljs-variable constant_\"\u003eGPU\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eUtil\u003c/span\u003e  \u003cspan class=\"hljs-title class_\"\u003eCompute\u003c/span\u003e M. |\n|                                         |                      |               \u003cspan class=\"hljs-variable constant_\"\u003eMIG\u003c/span\u003e M. |\n|=========================================+======================+======================|\n|   \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e  \u003cspan class=\"hljs-variable constant_\"\u003eNVIDIA\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eGeForce\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eRTX\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e3090\u003c/span\u003e        \u003cspan class=\"hljs-title class_\"\u003eOn\u003c/span\u003e  | \u003cspan class=\"hljs-number\"\u003e00000000\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e29\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e00\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.0\u003c/span\u003e  \u003cspan class=\"hljs-title class_\"\u003eOn\u003c/span\u003e |                  N/A |\n| \u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e%   57C    \u003cspan class=\"hljs-variable constant_\"\u003eP2\u003c/span\u003e             341W / 350W |  15054MiB / 24576MiB |    \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e%      \u003cspan class=\"hljs-title class_\"\u003eDefault\u003c/span\u003e |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| \u003cspan class=\"hljs-title class_\"\u003eProcesses\u003c/span\u003e:                                                                            |\n|  \u003cspan class=\"hljs-variable constant_\"\u003eGPU\u003c/span\u003e   \u003cspan class=\"hljs-variable constant_\"\u003eGI\u003c/span\u003e   \u003cspan class=\"hljs-variable constant_\"\u003eCI\u003c/span\u003e        \u003cspan class=\"hljs-variable constant_\"\u003ePID\u003c/span\u003e   \u003cspan class=\"hljs-title class_\"\u003eType\u003c/span\u003e   \u003cspan class=\"hljs-title class_\"\u003eProcess\u003c/span\u003e name                            \u003cspan class=\"hljs-variable constant_\"\u003eGPU\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eMemory\u003c/span\u003e |\n|        \u003cspan class=\"hljs-variable constant_\"\u003eID\u003c/span\u003e   \u003cspan class=\"hljs-variable constant_\"\u003eID\u003c/span\u003e                                                             \u003cspan class=\"hljs-title class_\"\u003eUsage\u003c/span\u003e      |\n|=======================================================================================|\n|    \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e   N/A  N/A      \u003cspan class=\"hljs-number\"\u003e1610\u003c/span\u003e      G   /usr/lib/xorg/\u003cspan class=\"hljs-title class_\"\u003eXorg\u003c/span\u003e                          179MiB |\n|    \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e   N/A  N/A      \u003cspan class=\"hljs-number\"\u003e1820\u003c/span\u003e      G   /usr/bin/gnome-shell                         40MiB |\n|    \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e   N/A  N/A    \u003cspan class=\"hljs-number\"\u003e108004\u003c/span\u003e      G   ...\u003cspan class=\"hljs-number\"\u003e2023.3\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.3\u003c/span\u003e/host-linux-x64/nsys-ui.\u003cspan class=\"hljs-property\"\u003ebin\u003c/span\u003e        8MiB |\n|    \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e   N/A  N/A    \u003cspan class=\"hljs-number\"\u003e168032\u003c/span\u003e      G   ...seed-version=\u003cspan class=\"hljs-number\"\u003e20240110\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e180219.406000\u003c/span\u003e      182MiB |\n|    \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e   N/A  N/A    \u003cspan class=\"hljs-number\"\u003e327503\u003c/span\u003e      C   /home\u003cspan class=\"hljs-comment\"\u003e/***/\u003c/span\u003eanaconda3/bin/python            14524MiB |\n+---------------------------------------------------------------------------------------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSMI는 GPU 메모리 사용량을 모니터링하는 간단한 도구이지만, 더 자세한 정보를 제공하는 고급 모니터링 도구들도 몇 가지 있습니다. 그 중 하나가 PyTorch Memory Snapshot인데, 이에 관해 더 읽어볼 수 있는 흥미로운 기사가 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e개요\u003c/h1\u003e\n\u003cp\u003e본 문서에서는 Mistral 7B와 같은 대규모 언어 모델을 단일 24GB GPU(예: NVIDIA GeForce RTX 3090 GPU)에서 세밀하게 조정할 수 있는 것을 보여드렸습니다. 그러나 자세히 설명한 대로 QLoRA와 같은 특별한 PEFT 기술이 필요합니다. 또한 모델의 배치 크기가 중요하며, 한정된 자원 때문에 보다 오랜 시간의 훈련이 필요할 수 있습니다.\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-07-14-FineTuningLLMsonaSingleConsumerGraphicCard"},"buildId":"o6AmBAY_j9v9JmbaRA39X","assetPrefix":"/TIL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>