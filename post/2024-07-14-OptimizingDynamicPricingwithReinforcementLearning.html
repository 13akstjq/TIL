<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>강화 학습을 이용한 동적 가격 책정 최적화 방법 | TIL</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://13akstjq.github.io/TIL//post/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="강화 학습을 이용한 동적 가격 책정 최적화 방법 | TIL" data-gatsby-head="true"/><meta property="og:title" content="강화 학습을 이용한 동적 가격 책정 최적화 방법 | TIL" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://TIL.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://13akstjq.github.io/TIL//post/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning" data-gatsby-head="true"/><meta name="twitter:title" content="강화 학습을 이용한 동적 가격 책정 최적화 방법 | TIL" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | TIL" data-gatsby-head="true"/><meta name="article:published_time" content="2024-07-14 20:13" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/TIL/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/TIL/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/TIL/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/TIL/favicons/favicon-96x96.png"/><link rel="icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/TIL/favicons/browserconfig.xml"/><link rel="preload" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/TIL/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/TIL/_next/static/chunks/webpack-21ffe88bdca56cba.js" defer=""></script><script src="/TIL/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/TIL/_next/static/chunks/main-a5eeabb286676ce6.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/TIL/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/TIL/_next/static/chunks/348-02483b66b493dd81.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/post/%5Bslug%5D-8ded8b979ba73586.js" defer=""></script><script src="/TIL/_next/static/N1mNhRlQaHCliEGDvPEpG/_buildManifest.js" defer=""></script><script src="/TIL/_next/static/N1mNhRlQaHCliEGDvPEpG/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/TIL">TIL</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/TIL/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">강화 학습을 이용한 동적 가격 책정 최적화 방법</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="강화 학습을 이용한 동적 가격 책정 최적화 방법" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/TIL/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">TIL</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jul 14, 2024</span><span class="posts_reading_time__f7YPP">16<!-- --> min read</span></span></div></div></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<img src="/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_0.png">
<h1>1. 소개</h1>
<p>소매 가격 전략은 매출과 이익을 최적화하는 데 중요합니다. 효과적인 가격 책정은 수요, 시장 상황 및 경쟁을 고려하여 소비자 행동을 영향을 주고 매출을 극대화합니다. 예를 들어 소매업체는 가격을 전략적으로 조정하고 할인을 적용하여 매출을 촉진하고 수익을 증가시킬 수 있습니다.</p>
<p>본 논문은 Deep Deterministic Policy Gradient (DDPG) 알고리즘을 사용한 강화 학습 접근 방식을 통해 가격 전략을 최적화하는 것을 탐구합니다. 가격과 할인을 동적으로 조정함으로써 가격 결정을 개선할 수 있습니다. 또한 SHAP (Shapley Additive Explanations) 값은 모델의 결정에 미치는 가격, 할인 및 매출의 영향에 대한 통찰을 제공합니다. 이러한 복합 접근 방식은 실시간 분석 및 설명 가능한 인공지능 기술을 통합하여 전통적인 가격 모델을 향상시킵니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2. 소매 업계의 가격 정책 모델링</h1>
<p>소매 업계의 가격 정책은 수익과 이윤을 최적화하기 위해 수학적으로 모델링될 수 있습니다. 매출 기능은 다음과 같이 작성할 수 있습니다:</p>
<p><img src="/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_1.png" alt="매출 기능"></p>
<p>이는 매출이 주로 가격과 할인과 같은 여러 요소에 의존한다는 것을 의미합니다. 일반적으로 가격이 증가하면 매출이 감소하고 그 반대도 마찬가지입니다. 최적의 가격을 찾아 매출이나 이윤을 극대화하는 것이 목표입니다. 예를 들어, 매출 기능이 이차 함수를 따른다면:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_2.png" alt="optimization"></p>
<p>상수 a와 b가 있는 경우, 최적화 기법으로는 이차 또는 선형 프로그래밍을 사용하여 최적 가격을 찾을 수 있습니다.</p>
<p>하지만 전통적인 최적화 방법에는 한계가 있습니다. 실시간 적응성이 부족해 즉각적인 시장 변화에 기초한 효율적인 가격 조정이 어려울 수 있습니다. 또한 판매에 영향을 미치는 요소에 대한 사전지식이 필요한데, 동적인 시장에서는 항상 실행 가능하지 않을 수 있습니다.</p>
<p>실시간 데이터 및 강화 학습과 같은 고급 머신러닝 모델은 이러한 도전에 대한 해결책을 제공합니다. 이러한 모델은 가격 전략을 동적으로 조정하고 다양한 요소의 영향을 분석하여 소매 환경에서 더 효과적이고 반응력있는 가격 결정을 지원할 수 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3. Pricing Strategies를 위한 강화 학습</h1>
<p>강화 학습 (RL)은 환경과 상호 작용하여 누적 보상을 극대화하기 위해 최적의 조치를 학습하는 기계 학습 기술입니다. 우리의 가격 전략에서는:</p>
<ul>
<li>환경: 소매 시장</li>
<li>에이전트: 가격 모델</li>
<li>목표: 가격과 할인을 동적으로 조정하여 매출과 이윤을 최적화</li>
</ul>
<p>우리는 실시간 의사 결정에 이상적인 정책 기반 및 가치 기반 학습을 결합한 Deep Deterministic Policy Gradient (DDPG) 알고리즘을 활용합니다. DDPG의 작동 방식은 다음과 같습니다:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>정책 기반 학습: 강화 학습의 정책 함수인 액터-네트워크를 사용합니다:</p>
<p><img src="/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_3.png" alt="Policy-Based Learning"></p>
<p>상태 s가 주어진 경우 동작 a를 선택합니다. θ^π는 정책 네트워크의 매개변수입니다.</p>
<p>가치 기반 학습: 비평가 네트워크(Q 함수)를 사용합니다:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_4.png" alt="image"></p>
<p>행동-가치 함수를 평가하기 위해.</p>
<p>학습 과정:</p>
<ul>
<li>Actor-Critic 아키텍처: Actor는 기대값 반환의 그래디언트를 따라 정책을 업데이트하며, Critic은 벨만 방정식을 사용하여 가치 추정을 업데이트합니다.</li>
<li>Experience Replay: 과거 경험 (s,a,r,s′)을 재생 버퍼에 저장하여 상관 관계를 끊고 학습을 안정화합니다.</li>
<li>Target Networks: 학습을 안정화하도록 학습된 네트워크를 천천히 추적하기 위해 목표 네트워크 세트 θ^π′ 및 θ^Q′를 유지합니다.</li>
</ul>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>DDPG의 장점은 다음과 같습니다:</p>
<ul>
<li>적응성: DDPG는 최신 시장 데이터를 기반으로 실시간으로 조정을 제공합니다.</li>
<li>세밀한 결정: 연속적인 액션 공간은 정확한 가격 조정을 가능하게 합니다.</li>
<li>데이터 기반 통찰력: 다양한 요소(가격, 할인 등)가 매출에 미치는 영향을 이해하는 데 도움이 되어 보다 효과적인 가격 전략을 도와줍니다.</li>
</ul>
<h1>4. 코딩 및 데이터 실험</h1>
<p>이제 강화 학습(RL) 프레임워크 내에서 딥 디터미니스틱 정책 그라디언트(DDPG) 알고리즘을 구현하여 소매 가격 전략을 최적화해봅니다. 이 접근 방식은 매출과 이익을 극대화하기 위해 가격과 할인을 동적으로 조정합니다. 게다가, SHAP(Shapley Additive Explanations) 분석을 사용하여 모델 결정에 각 기능이 미치는 영향을 이해하여 RL 기반 가격 모델의 해석 가능성을 향상시킵니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>강화학습 환경 설정:</p>
<ul>
<li>환경 초기화: 우리는 맞춤 gym 환경인 SalesPredictionEnv를 정의합니다. 이 환경은 소매 시장을 시뮬레이션합니다. 환경은 초기 가격과 할인을 입력으로 받고 진짜 판매 기능을 사용하여 판매를 시뮬레이션합니다. 액션 공간은 가격과 할인을 연속적으로 조정할 수 있으며 관찰 공간에는 현재 가격, 할인 및 예측된 판매가 포함됩니다.</li>
</ul>
<pre><code class="hljs language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SalesPredictionEnv</span>(gym.Env):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, initial_price, initial_discount, true_sales_function</span>):
        <span class="hljs-built_in">super</span>(SalesPredictionEnv, self).__init__()
        self.initial_price = initial_price
        self.initial_discount = initial_discount
        self.true_sales_function = true_sales_function

        self.action_space = spaces.Box(low=-<span class="hljs-number">0.1</span>, high=<span class="hljs-number">0.1</span>, shape=(<span class="hljs-number">2</span>,), dtype=np.float32)
        self.observation_space = spaces.Box(low=<span class="hljs-number">0</span>, high=np.inf, shape=(<span class="hljs-number">3</span>,), dtype=np.float32)

        self.price = self.initial_price
        self.discount = self.initial_discount
        self.sales = self.true_sales_function(self.price, self.discount)
        self.done = <span class="hljs-literal">False</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self, seed=<span class="hljs-literal">None</span>, options=<span class="hljs-literal">None</span></span>):
        <span class="hljs-built_in">super</span>().reset(seed=seed)
        self.price = self.initial_price
        self.discount = self.initial_discount
        self.sales = self.true_sales_function(self.price, self.discount)
        <span class="hljs-keyword">return</span> np.array([self.price, self.discount, self.sales], dtype=np.float32), {}

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self, action</span>):
        self.price += action[<span class="hljs-number">0</span>]
        self.discount += action[<span class="hljs-number">1</span>]
        new_sales = self.true_sales_function(self.price, self.discount)

        reward = -<span class="hljs-built_in">abs</span>(self.sales - new_sales)
        self.sales = new_sales
        self.done = <span class="hljs-literal">False</span>

        <span class="hljs-keyword">return</span> np.array([self.price, self.discount, self.sales], dtype=np.float32), reward, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>, {}

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">render</span>(<span class="hljs-params">self, mode=<span class="hljs-string">'human'</span></span>):
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f'Price: <span class="hljs-subst">{self.price}</span>, Discount: <span class="hljs-subst">{self.discount}</span>, Sales: <span class="hljs-subst">{self.sales}</span>'</span>)
</code></pre>
<p>진짜 판매 함수: 그런 다음 가격, 할인 및 판매 간의 관계를 모델링하는 판매 함수를 정의합니다. 이 함수는 강화학습(RL) 구현에서 소매 환경을 시뮬레이션할 수 있습니다. RL 에이전트가 다양한 가격과 할인 수준이 판매에 어떤 영향을 미치는지 이해할 수 있도록 합니다. 이 함수는 다음과 같이 공식화됩니다:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">def <span class="hljs-title function_">true_sales_function</span>(가격, 할인):
    <span class="hljs-keyword">return</span> -<span class="hljs-number">0.5</span> * 가격 ** <span class="hljs-number">2</span> + 가격 + <span class="hljs-number">11</span> + <span class="hljs-number">2</span> * 할인
</code></pre>
<p>실제 세계의 강화학습 구현에서는 이러한 함수들이 종종 과거 판매 데이터, 경험적 연구 또는 도메인 전문 지식을 기반으로 실제 시장 행위를 모방하는 데 사용됩니다. 이 이차 함수 형태는 중간 가격 상승이 판매를 촉진할 수 있지만, 과도한 가격이나 할인은 전반적인 판매에 부정적인 영향을 미칠 수 있다.</p>
<p>환경 및 모델 설정: check_env를 사용하여 환경을 초기화합니다. 그런 다음 환경에 DDPG 에이전트를 설정합니다.</p>
<pre><code class="hljs language-js">env = <span class="hljs-title class_">SalesPredictionEnv</span>(initial_price=<span class="hljs-number">5.0</span>, initial_discount=<span class="hljs-number">1.0</span>, true_sales_function=true_sales_function)
<span class="hljs-title function_">check_env</span>(env)
model = <span class="hljs-title function_">DDPG</span>(<span class="hljs-string">'MlpPolicy'</span>, env, verbose=<span class="hljs-number">1</span>)
model.<span class="hljs-title function_">learn</span>(total_timesteps=<span class="hljs-number">10000</span>)
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>SHAP 분석:</p>
<p>SHAP (Shapley Additive Explanations)은 각 특징이 예측에 미치는 영향을 양적으로 설명하여 모델을 해석 가능하게 합니다. RL 설정에서 SHAP를 구현하는 과정은 다음과 같습니다:</p>
<ul>
<li>SHAP를 위한 데이터 수집: 환경을 재설정하고 SHAP 분석을 위해 상태와 행동을 수집합니다.</li>
</ul>
<pre><code class="hljs language-js">obs, _ = env.<span class="hljs-title function_">reset</span>()
states = []
actions = []
<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-number">10</span>):
    action, _states = model.<span class="hljs-title function_">predict</span>(obs)
    obs, rewards, terminated, truncated, _ = env.<span class="hljs-title function_">step</span>(action)
    env.<span class="hljs-title function_">render</span>()
    states.<span class="hljs-title function_">append</span>(obs)
    actions.<span class="hljs-title function_">append</span>(action)

states = np.<span class="hljs-title function_">array</span>(states)
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>SHAP 예측 래퍼: SHAP의 올바른 출력 형식을 보장하기 위해 래퍼 함수를 정의합니다.</p>
<pre><code class="hljs language-js">def <span class="hljs-title function_">predict_wrapper</span>(observations):
    predictions = []
    <span class="hljs-keyword">for</span> obs <span class="hljs-keyword">in</span> <span class="hljs-attr">observations</span>:
        action, _states = model.<span class="hljs-title function_">predict</span>(obs)
        predictions.<span class="hljs-title function_">append</span>(action.<span class="hljs-title function_">flatten</span>())
    <span class="hljs-keyword">return</span> np.<span class="hljs-title function_">array</span>(predictions)
</code></pre>
<p>예측 DataFrame: 예측을 저장할 DataFrame을 생성하고 추가 분석을 위해 Excel 파일에 저장합니다.</p>
<pre><code class="hljs language-js">predictions = {
    <span class="hljs-string">'ID'</span>: <span class="hljs-title function_">list</span>(<span class="hljs-title function_">range</span>(<span class="hljs-title function_">len</span>(states))),
    <span class="hljs-string">'price'</span>: states[:, <span class="hljs-number">0</span>],
    <span class="hljs-string">'discount'</span>: states[:, <span class="hljs-number">1</span>],
    <span class="hljs-string">'sales'</span>: states[:, <span class="hljs-number">2</span>],
    <span class="hljs-string">'predicted_action_0'</span>: [<span class="hljs-title class_">None</span>] * <span class="hljs-title function_">len</span>(states),
    <span class="hljs-string">'predicted_action_1'</span>: [<span class="hljs-title class_">None</span>] * <span class="hljs-title function_">len</span>(states)
}

<span class="hljs-keyword">for</span> idx, state <span class="hljs-keyword">in</span> <span class="hljs-title function_">enumerate</span>(states):
    action, _states = model.<span class="hljs-title function_">predict</span>(state)
    predictions[<span class="hljs-string">'predicted_action_0'</span>][idx] = action[<span class="hljs-number">0</span>]
    predictions[<span class="hljs-string">'predicted_action_1'</span>][idx] = action[<span class="hljs-number">1</span>]

predictions_df = pd.<span class="hljs-title class_">DataFrame</span>(predictions)
predictions_df.<span class="hljs-title function_">to_excel</span>(<span class="hljs-string">"reinforcement_learning_predictions.xlsx"</span>, index=<span class="hljs-title class_">False</span>)
<span class="hljs-title function_">print</span>(predictions_df.<span class="hljs-title function_">head</span>(<span class="hljs-number">10</span>))
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>SHAP Explainer 및 Visualization: 우리는 SHAP를 사용하여 모델의 의사결정에 미치는 다른 feature들의 영향을 분석하고 결과를 시각화합니다.</p>
<pre><code class="hljs language-js">explainer = shap.<span class="hljs-title class_">Explainer</span>(predict_wrapper, states)
shap_values = <span class="hljs-title function_">explainer</span>(states)

shap_values_price = shap_values[..., <span class="hljs-number">0</span>]

shap.<span class="hljs-property">plots</span>.<span class="hljs-title function_">beeswarm</span>(shap_values_price)
shap.<span class="hljs-property">plots</span>.<span class="hljs-title function_">bar</span>(shap_values_price[<span class="hljs-number">0</span>])
</code></pre>
<p>상위 영향력 있는 feature들: 각 state의 상위 영향력 있는 feature들을 추출하여 DataFrame에 저장하여 쉬운 분석을 할 수 있습니다.</p>
<pre><code class="hljs language-js">data = {
    <span class="hljs-string">'ID'</span>: <span class="hljs-title function_">list</span>(<span class="hljs-title function_">range</span>(<span class="hljs-title function_">len</span>(states))),
    <span class="hljs-string">'price'</span>: states[:, <span class="hljs-number">0</span>],
    <span class="hljs-string">'discount'</span>: states[:, <span class="hljs-number">1</span>],
    <span class="hljs-string">'sales'</span>: states[:, <span class="hljs-number">2</span>],
    <span class="hljs-string">'top_feature1'</span>: [<span class="hljs-title class_">None</span>] * <span class="hljs-title function_">len</span>(states),
    <span class="hljs-string">'top_feature2'</span>: [<span class="hljs-title class_">None</span>] * <span class="hljs-title function_">len</span>(states),
    <span class="hljs-string">'importance1'</span>: [<span class="hljs-title class_">None</span>] * <span class="hljs-title function_">len</span>(states),
    <span class="hljs-string">'importance2'</span>: [<span class="hljs-title class_">None</span>] * <span class="hljs-title function_">len</span>(states)
}

features = [<span class="hljs-string">'price'</span>, <span class="hljs-string">'discount'</span>, <span class="hljs-string">'sales'</span>]
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-title function_">range</span>(<span class="hljs-title function_">len</span>(states)):
    sorted_indices = np.<span class="hljs-title function_">argsort</span>(-np.<span class="hljs-title function_">abs</span>(shap_values.<span class="hljs-property">values</span>[i][:, <span class="hljs-number">0</span>]))
    data[<span class="hljs-string">'top_feature1'</span>][i] = features[sorted_indices[<span class="hljs-number">0</span>]]
    data[<span class="hljs-string">'importance1'</span>][i] = shap_values.<span class="hljs-property">values</span>[i][sorted_indices[<span class="hljs-number">0</span>], <span class="hljs-number">0</span>]

    <span class="hljs-keyword">if</span> <span class="hljs-title function_">len</span>(sorted_indices) > <span class="hljs-number">1</span>:
        data[<span class="hljs-string">'top_feature2'</span>][i] = features[sorted_indices[<span class="hljs-number">1</span>]]
        data[<span class="hljs-string">'importance2'</span>][i] = shap_values.<span class="hljs-property">values</span>[i][sorted_indices[<span class="hljs-number">1</span>], <span class="hljs-number">0</span>]

reason_df = pd.<span class="hljs-title class_">DataFrame</span>(data)
<span class="hljs-title function_">print</span>(reason_df.<span class="hljs-title function_">head</span>(<span class="hljs-number">10</span>))
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5. 분석과 인사이트</h1>
<p>다음의 SHAP 막대 그래프는 Price(가격), Discount(할인) 및 Sales(판매)가 특정 인스턴스에 대한 모델의 가격 결정에 미치는 영향을 보여줍니다:</p>
<ul>
<li>SHAP 막대 그래프는 Price(가격), Discount(할인) 및 Sales(판매)가 특정 인스턴스에 대한 모델의 가격 결정에 미치는 영향을 보여줍니다:</li>
<li>Sales(판매): 가장 높은 긍정적 영향을 나타내며, 높은 판매량이 모델이 가격과 할인을 유지하거나 높이는 데 강력한 영향을 미친다는 것을 시사합니다.</li>
<li>Discount(할인): 높은 할인은 결과에 부정적인 영향을 미치며, 과도한 할인을 피하기 위해 할인액을 줄이는 것을 권장하는 모델의 결론으로 이어집니다.</li>
<li>Price(가격): 약간의 긍정적 영향을 나타내며, 결과를 향상시키기 위해 가격을 약간 올리는 것을 선호하는 모델이고 판매량에 큰 영향을 미치지 않는다는 것을 나타냅니다.</li>
</ul>
<p><img src="/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_5.png" alt="이미지"></p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>모델은 가격 책정 전략을 안내하기 위해 판매를 우선시하며, 수익을 극대화하기 위해 신중한 할인 관리와 약간의 가격 인상을 권장합니다. 막대 도표는 모델의 특정 사례에 대한 가격 결정에 영향을 미치는 Sales, Price 및 Discount가 어떻게 변하는지를 강조합니다.</p>
<p>다음은 SHAP beeswarm 도표로, 여러 사례에 걸쳐 Price, Discount 및 Sales가 모델의 가격 결정에 미치는 영향을 보여줍니다:</p>
<ul>
<li>Sales (Feature 2): 높은 값(빨강)은 모델의 출력을 증가시키고, 낮은 값(파랑)은 감소시킵니다.</li>
<li>Price (Feature 0): 낮은 값(파랑)은 부정적인 영향을 미치며, 더 높은 값(빨강)은 긍정적인 영향을 미칩니다.</li>
<li>Discount (Feature 1): 높은 값(빨강)은 모델의 출력을 감소시키고, 낮은 값(파랑)은 긍정적인 영향을 미칩니다.</li>
</ul>
<p><img src="/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_6.png" alt="도표 이미지"></p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>벌집 그림은 매출, 가격 및 할인이 모델의 결정에 미치는 영향이 여러 인스턴스에 걸쳐 어떻게 변하는지를 제공하여 그들이 모델의 결정에 미치는 중요성과 일관성을 강조합니다.</p>
<p>예측된 작업 테이블은 다른 기능에 대한 모델의 예측을 제시합니다:</p>
<p><img src="/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_7.png" alt="Predicted Actions Table"></p>
<ul>
<li>가격 조정: 가격에 대한 예측된 작업은 약간 부정적이며(예측 작업 0), 가격이 감소함에 따라 약간의 인하를 시사합니다.</li>
<li>할인 조정: 할인에 대한 예측된 작업(예측 작업 1)도 약간 부정적이며, 소폭의 감소를 나타냅니다. 모델은 수익성을 유지하기 위해 신중한 할인을 권장하는 일관된 경향을 보입니다.</li>
<li>매출 영향: 가격과 할인이 감소함에 따라 매출이 증가하며, 전형적인 시장 행동을 반영합니다. 모델이 가격과 할인을 약간 감소시킨 것은 수익성을 유지하면서 매출을 최적화할 수 있습니다.</li>
</ul>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>Feature Importance Table은 각 인스턴스에 대해 모델 결정에 영향을 줄인 최상위 두 가지 기능과 그 중요도 값을 식별합니다:</p>
<ul>
<li>판매량: 모든 인스턴스에서 가장 중요한 기능(top_feature1)으로 일관되게 나타납니다.</li>
<li>가격 및 할인: 교체 가능 기능(top_feature2)은 다양한 중요도 값을 가지며, 판매량에 높은 중요도 값이 해당 기능이 모델 예측에 미치는 강력한 영향을 나타냅니다.</li>
</ul>
<p><img src="/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_8.png" alt="image"></p>
<p>요약하면, 판매가격은 모델의 가격 결정에서 우세한 요소이며, 가격 및 할인은 보조이지만 중요한 역할을 합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6. 결론</h1>
<p>본 연구는 소매 가격 전략을 최적화하기 위해 Deep Deterministic Policy Gradient (DDPG) 알고리즘을 활용합니다. 강화 학습 (RL)과 SHAP (Shapley Additive Explanations)을 결합하여 가격 및 할인을 조정하여 매출과 이익을 극대화할 수 있습니다.</p>
<p>장점:</p>
<ul>
<li>적응성: 전통적인 가격 모델과 달리 RL은 실시간 데이터로부터 계속 학습하여 시장 변화에 즉각 대응할 수 있습니다.</li>
<li>정밀성: DDPG의 연속적인 행동 공간은 섬세한 가격 결정을 가능하게 합니다.</li>
<li>통찰력: SHAP 값은 다양한 요인의 영향에 대한 설명 가능한 통찰력을 제공하여 의사 결정 투명성을 향상시킵니다.</li>
</ul>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>단점:</p>
<ul>
<li>복잡성: 강화 학습 모델을 구현하는 데는 상당한 컴퓨팅 자원과 전문 지식이 필요합니다.</li>
<li>데이터 의존성: 강화 학습의 효과는 사용 가능한 데이터의 품질과 양에 매우 의존합니다.</li>
<li>안정성: 동적 환경에서 안정적인 학습을 보장하는 것은 어려울 수 있으며 하이퍼파라미터를 세심히 조정해야 합니다.</li>
</ul>
<p>개선 제안:</p>
<ul>
<li>혼합 모델: 강화 학습과 전통적 최적화 방법을 결합함으로써 안전성과 성능을 향상시킬 수 있습니다.</li>
<li>향상된 데이터 통합: 고객 피드백 및 경쟁사 가격과 같은 다양한 데이터 원본을 통합하여 모델의 정확성을 향상시킬 수 있습니다.</li>
<li>확장성: 확장 가능한 강화 학습 프레임워크를 개발하여 소매 세그먼트와 시장 전반에 이러한 방법을 도움이 될 수 있습니다.</li>
<li>지속적인 모니터링: 비즈니스 목표 및 시장 조건과 일치하는 모델의 결정을 보장하기 위해 모니터링 및 검증 프로세스를 구현합니다.</li>
</ul>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>GitHub의 내 저장소에 있는 Python 스크립트는 다음과 같습니다: <a href="https://github.com/datalev001/Reinforcement_price" rel="nofollow" target="_blank">datalev001/Reinforcement_price</a></p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"강화 학습을 이용한 동적 가격 책정 최적화 방법","description":"","date":"2024-07-14 20:13","slug":"2024-07-14-OptimizingDynamicPricingwithReinforcementLearning","content":"\n\n\u003cimg src=\"/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_0.png\" /\u003e\n\n# 1. 소개\n\n소매 가격 전략은 매출과 이익을 최적화하는 데 중요합니다. 효과적인 가격 책정은 수요, 시장 상황 및 경쟁을 고려하여 소비자 행동을 영향을 주고 매출을 극대화합니다. 예를 들어 소매업체는 가격을 전략적으로 조정하고 할인을 적용하여 매출을 촉진하고 수익을 증가시킬 수 있습니다.\n\n본 논문은 Deep Deterministic Policy Gradient (DDPG) 알고리즘을 사용한 강화 학습 접근 방식을 통해 가격 전략을 최적화하는 것을 탐구합니다. 가격과 할인을 동적으로 조정함으로써 가격 결정을 개선할 수 있습니다. 또한 SHAP (Shapley Additive Explanations) 값은 모델의 결정에 미치는 가격, 할인 및 매출의 영향에 대한 통찰을 제공합니다. 이러한 복합 접근 방식은 실시간 분석 및 설명 가능한 인공지능 기술을 통합하여 전통적인 가격 모델을 향상시킵니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 2. 소매 업계의 가격 정책 모델링\n\n소매 업계의 가격 정책은 수익과 이윤을 최적화하기 위해 수학적으로 모델링될 수 있습니다. 매출 기능은 다음과 같이 작성할 수 있습니다:\n\n![매출 기능](/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_1.png)\n\n이는 매출이 주로 가격과 할인과 같은 여러 요소에 의존한다는 것을 의미합니다. 일반적으로 가격이 증가하면 매출이 감소하고 그 반대도 마찬가지입니다. 최적의 가격을 찾아 매출이나 이윤을 극대화하는 것이 목표입니다. 예를 들어, 매출 기능이 이차 함수를 따른다면:\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n![optimization](/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_2.png)\n\n상수 a와 b가 있는 경우, 최적화 기법으로는 이차 또는 선형 프로그래밍을 사용하여 최적 가격을 찾을 수 있습니다.\n\n하지만 전통적인 최적화 방법에는 한계가 있습니다. 실시간 적응성이 부족해 즉각적인 시장 변화에 기초한 효율적인 가격 조정이 어려울 수 있습니다. 또한 판매에 영향을 미치는 요소에 대한 사전지식이 필요한데, 동적인 시장에서는 항상 실행 가능하지 않을 수 있습니다.\n\n실시간 데이터 및 강화 학습과 같은 고급 머신러닝 모델은 이러한 도전에 대한 해결책을 제공합니다. 이러한 모델은 가격 전략을 동적으로 조정하고 다양한 요소의 영향을 분석하여 소매 환경에서 더 효과적이고 반응력있는 가격 결정을 지원할 수 있습니다.\n\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 3. Pricing Strategies를 위한 강화 학습\n\n강화 학습 (RL)은 환경과 상호 작용하여 누적 보상을 극대화하기 위해 최적의 조치를 학습하는 기계 학습 기술입니다. 우리의 가격 전략에서는:\n\n- 환경: 소매 시장\n- 에이전트: 가격 모델\n- 목표: 가격과 할인을 동적으로 조정하여 매출과 이윤을 최적화\n\n우리는 실시간 의사 결정에 이상적인 정책 기반 및 가치 기반 학습을 결합한 Deep Deterministic Policy Gradient (DDPG) 알고리즘을 활용합니다. DDPG의 작동 방식은 다음과 같습니다:\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n정책 기반 학습: 강화 학습의 정책 함수인 액터-네트워크를 사용합니다:\n\n![Policy-Based Learning](/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_3.png)\n\n상태 s가 주어진 경우 동작 a를 선택합니다. θ^π는 정책 네트워크의 매개변수입니다.\n\n가치 기반 학습: 비평가 네트워크(Q 함수)를 사용합니다:\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n![image](/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_4.png)\n\n행동-가치 함수를 평가하기 위해.\n\n학습 과정:\n\n- Actor-Critic 아키텍처: Actor는 기대값 반환의 그래디언트를 따라 정책을 업데이트하며, Critic은 벨만 방정식을 사용하여 가치 추정을 업데이트합니다.\n- Experience Replay: 과거 경험 (s,a,r,s′)을 재생 버퍼에 저장하여 상관 관계를 끊고 학습을 안정화합니다.\n- Target Networks: 학습을 안정화하도록 학습된 네트워크를 천천히 추적하기 위해 목표 네트워크 세트 θ^π′ 및 θ^Q′를 유지합니다.\n\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nDDPG의 장점은 다음과 같습니다:\n\n- 적응성: DDPG는 최신 시장 데이터를 기반으로 실시간으로 조정을 제공합니다.\n- 세밀한 결정: 연속적인 액션 공간은 정확한 가격 조정을 가능하게 합니다.\n- 데이터 기반 통찰력: 다양한 요소(가격, 할인 등)가 매출에 미치는 영향을 이해하는 데 도움이 되어 보다 효과적인 가격 전략을 도와줍니다.\n\n# 4. 코딩 및 데이터 실험\n\n이제 강화 학습(RL) 프레임워크 내에서 딥 디터미니스틱 정책 그라디언트(DDPG) 알고리즘을 구현하여 소매 가격 전략을 최적화해봅니다. 이 접근 방식은 매출과 이익을 극대화하기 위해 가격과 할인을 동적으로 조정합니다. 게다가, SHAP(Shapley Additive Explanations) 분석을 사용하여 모델 결정에 각 기능이 미치는 영향을 이해하여 RL 기반 가격 모델의 해석 가능성을 향상시킵니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n강화학습 환경 설정:\n\n- 환경 초기화: 우리는 맞춤 gym 환경인 SalesPredictionEnv를 정의합니다. 이 환경은 소매 시장을 시뮬레이션합니다. 환경은 초기 가격과 할인을 입력으로 받고 진짜 판매 기능을 사용하여 판매를 시뮬레이션합니다. 액션 공간은 가격과 할인을 연속적으로 조정할 수 있으며 관찰 공간에는 현재 가격, 할인 및 예측된 판매가 포함됩니다.\n\n```python\nclass SalesPredictionEnv(gym.Env):\n    def __init__(self, initial_price, initial_discount, true_sales_function):\n        super(SalesPredictionEnv, self).__init__()\n        self.initial_price = initial_price\n        self.initial_discount = initial_discount\n        self.true_sales_function = true_sales_function\n\n        self.action_space = spaces.Box(low=-0.1, high=0.1, shape=(2,), dtype=np.float32)\n        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(3,), dtype=np.float32)\n\n        self.price = self.initial_price\n        self.discount = self.initial_discount\n        self.sales = self.true_sales_function(self.price, self.discount)\n        self.done = False\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        self.price = self.initial_price\n        self.discount = self.initial_discount\n        self.sales = self.true_sales_function(self.price, self.discount)\n        return np.array([self.price, self.discount, self.sales], dtype=np.float32), {}\n\n    def step(self, action):\n        self.price += action[0]\n        self.discount += action[1]\n        new_sales = self.true_sales_function(self.price, self.discount)\n\n        reward = -abs(self.sales - new_sales)\n        self.sales = new_sales\n        self.done = False\n\n        return np.array([self.price, self.discount, self.sales], dtype=np.float32), reward, False, False, {}\n\n    def render(self, mode='human'):\n        print(f'Price: {self.price}, Discount: {self.discount}, Sales: {self.sales}')\n```\n\n진짜 판매 함수: 그런 다음 가격, 할인 및 판매 간의 관계를 모델링하는 판매 함수를 정의합니다. 이 함수는 강화학습(RL) 구현에서 소매 환경을 시뮬레이션할 수 있습니다. RL 에이전트가 다양한 가격과 할인 수준이 판매에 어떤 영향을 미치는지 이해할 수 있도록 합니다. 이 함수는 다음과 같이 공식화됩니다:\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ndef true_sales_function(가격, 할인):\n    return -0.5 * 가격 ** 2 + 가격 + 11 + 2 * 할인\n```\n\n실제 세계의 강화학습 구현에서는 이러한 함수들이 종종 과거 판매 데이터, 경험적 연구 또는 도메인 전문 지식을 기반으로 실제 시장 행위를 모방하는 데 사용됩니다. 이 이차 함수 형태는 중간 가격 상승이 판매를 촉진할 수 있지만, 과도한 가격이나 할인은 전반적인 판매에 부정적인 영향을 미칠 수 있다.\n\n환경 및 모델 설정: check_env를 사용하여 환경을 초기화합니다. 그런 다음 환경에 DDPG 에이전트를 설정합니다.\n\n```js\nenv = SalesPredictionEnv(initial_price=5.0, initial_discount=1.0, true_sales_function=true_sales_function)\ncheck_env(env)\nmodel = DDPG('MlpPolicy', env, verbose=1)\nmodel.learn(total_timesteps=10000)\n```\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nSHAP 분석:\n\nSHAP (Shapley Additive Explanations)은 각 특징이 예측에 미치는 영향을 양적으로 설명하여 모델을 해석 가능하게 합니다. RL 설정에서 SHAP를 구현하는 과정은 다음과 같습니다:\n\n- SHAP를 위한 데이터 수집: 환경을 재설정하고 SHAP 분석을 위해 상태와 행동을 수집합니다.\n\n```js\nobs, _ = env.reset()\nstates = []\nactions = []\nfor _ in range(10):\n    action, _states = model.predict(obs)\n    obs, rewards, terminated, truncated, _ = env.step(action)\n    env.render()\n    states.append(obs)\n    actions.append(action)\n\nstates = np.array(states)\n```\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nSHAP 예측 래퍼: SHAP의 올바른 출력 형식을 보장하기 위해 래퍼 함수를 정의합니다.\n\n```js\ndef predict_wrapper(observations):\n    predictions = []\n    for obs in observations:\n        action, _states = model.predict(obs)\n        predictions.append(action.flatten())\n    return np.array(predictions)\n```\n\n예측 DataFrame: 예측을 저장할 DataFrame을 생성하고 추가 분석을 위해 Excel 파일에 저장합니다.\n\n```js\npredictions = {\n    'ID': list(range(len(states))),\n    'price': states[:, 0],\n    'discount': states[:, 1],\n    'sales': states[:, 2],\n    'predicted_action_0': [None] * len(states),\n    'predicted_action_1': [None] * len(states)\n}\n\nfor idx, state in enumerate(states):\n    action, _states = model.predict(state)\n    predictions['predicted_action_0'][idx] = action[0]\n    predictions['predicted_action_1'][idx] = action[1]\n\npredictions_df = pd.DataFrame(predictions)\npredictions_df.to_excel(\"reinforcement_learning_predictions.xlsx\", index=False)\nprint(predictions_df.head(10))\n```\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nSHAP Explainer 및 Visualization: 우리는 SHAP를 사용하여 모델의 의사결정에 미치는 다른 feature들의 영향을 분석하고 결과를 시각화합니다.\n\n```js\nexplainer = shap.Explainer(predict_wrapper, states)\nshap_values = explainer(states)\n\nshap_values_price = shap_values[..., 0]\n\nshap.plots.beeswarm(shap_values_price)\nshap.plots.bar(shap_values_price[0])\n```\n\n상위 영향력 있는 feature들: 각 state의 상위 영향력 있는 feature들을 추출하여 DataFrame에 저장하여 쉬운 분석을 할 수 있습니다.\n\n```js\ndata = {\n    'ID': list(range(len(states))),\n    'price': states[:, 0],\n    'discount': states[:, 1],\n    'sales': states[:, 2],\n    'top_feature1': [None] * len(states),\n    'top_feature2': [None] * len(states),\n    'importance1': [None] * len(states),\n    'importance2': [None] * len(states)\n}\n\nfeatures = ['price', 'discount', 'sales']\nfor i in range(len(states)):\n    sorted_indices = np.argsort(-np.abs(shap_values.values[i][:, 0]))\n    data['top_feature1'][i] = features[sorted_indices[0]]\n    data['importance1'][i] = shap_values.values[i][sorted_indices[0], 0]\n\n    if len(sorted_indices) \u003e 1:\n        data['top_feature2'][i] = features[sorted_indices[1]]\n        data['importance2'][i] = shap_values.values[i][sorted_indices[1], 0]\n\nreason_df = pd.DataFrame(data)\nprint(reason_df.head(10))\n```\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 5. 분석과 인사이트\n\n다음의 SHAP 막대 그래프는 Price(가격), Discount(할인) 및 Sales(판매)가 특정 인스턴스에 대한 모델의 가격 결정에 미치는 영향을 보여줍니다:\n\n- SHAP 막대 그래프는 Price(가격), Discount(할인) 및 Sales(판매)가 특정 인스턴스에 대한 모델의 가격 결정에 미치는 영향을 보여줍니다:\n- Sales(판매): 가장 높은 긍정적 영향을 나타내며, 높은 판매량이 모델이 가격과 할인을 유지하거나 높이는 데 강력한 영향을 미친다는 것을 시사합니다.\n- Discount(할인): 높은 할인은 결과에 부정적인 영향을 미치며, 과도한 할인을 피하기 위해 할인액을 줄이는 것을 권장하는 모델의 결론으로 이어집니다.\n- Price(가격): 약간의 긍정적 영향을 나타내며, 결과를 향상시키기 위해 가격을 약간 올리는 것을 선호하는 모델이고 판매량에 큰 영향을 미치지 않는다는 것을 나타냅니다.\n\n![이미지](/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_5.png)\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모델은 가격 책정 전략을 안내하기 위해 판매를 우선시하며, 수익을 극대화하기 위해 신중한 할인 관리와 약간의 가격 인상을 권장합니다. 막대 도표는 모델의 특정 사례에 대한 가격 결정에 영향을 미치는 Sales, Price 및 Discount가 어떻게 변하는지를 강조합니다.\n\n다음은 SHAP beeswarm 도표로, 여러 사례에 걸쳐 Price, Discount 및 Sales가 모델의 가격 결정에 미치는 영향을 보여줍니다:\n\n- Sales (Feature 2): 높은 값(빨강)은 모델의 출력을 증가시키고, 낮은 값(파랑)은 감소시킵니다.\n- Price (Feature 0): 낮은 값(파랑)은 부정적인 영향을 미치며, 더 높은 값(빨강)은 긍정적인 영향을 미칩니다.\n- Discount (Feature 1): 높은 값(빨강)은 모델의 출력을 감소시키고, 낮은 값(파랑)은 긍정적인 영향을 미칩니다.\n\n![도표 이미지](/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_6.png)\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n벌집 그림은 매출, 가격 및 할인이 모델의 결정에 미치는 영향이 여러 인스턴스에 걸쳐 어떻게 변하는지를 제공하여 그들이 모델의 결정에 미치는 중요성과 일관성을 강조합니다.\n\n예측된 작업 테이블은 다른 기능에 대한 모델의 예측을 제시합니다:\n\n![Predicted Actions Table](/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_7.png)\n\n- 가격 조정: 가격에 대한 예측된 작업은 약간 부정적이며(예측 작업 0), 가격이 감소함에 따라 약간의 인하를 시사합니다.\n- 할인 조정: 할인에 대한 예측된 작업(예측 작업 1)도 약간 부정적이며, 소폭의 감소를 나타냅니다. 모델은 수익성을 유지하기 위해 신중한 할인을 권장하는 일관된 경향을 보입니다.\n- 매출 영향: 가격과 할인이 감소함에 따라 매출이 증가하며, 전형적인 시장 행동을 반영합니다. 모델이 가격과 할인을 약간 감소시킨 것은 수익성을 유지하면서 매출을 최적화할 수 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nFeature Importance Table은 각 인스턴스에 대해 모델 결정에 영향을 줄인 최상위 두 가지 기능과 그 중요도 값을 식별합니다:\n\n- 판매량: 모든 인스턴스에서 가장 중요한 기능(top_feature1)으로 일관되게 나타납니다.\n- 가격 및 할인: 교체 가능 기능(top_feature2)은 다양한 중요도 값을 가지며, 판매량에 높은 중요도 값이 해당 기능이 모델 예측에 미치는 강력한 영향을 나타냅니다.\n\n![image](/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_8.png)\n\n요약하면, 판매가격은 모델의 가격 결정에서 우세한 요소이며, 가격 및 할인은 보조이지만 중요한 역할을 합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 6. 결론\n\n본 연구는 소매 가격 전략을 최적화하기 위해 Deep Deterministic Policy Gradient (DDPG) 알고리즘을 활용합니다. 강화 학습 (RL)과 SHAP (Shapley Additive Explanations)을 결합하여 가격 및 할인을 조정하여 매출과 이익을 극대화할 수 있습니다.\n\n장점:\n\n- 적응성: 전통적인 가격 모델과 달리 RL은 실시간 데이터로부터 계속 학습하여 시장 변화에 즉각 대응할 수 있습니다.\n- 정밀성: DDPG의 연속적인 행동 공간은 섬세한 가격 결정을 가능하게 합니다.\n- 통찰력: SHAP 값은 다양한 요인의 영향에 대한 설명 가능한 통찰력을 제공하여 의사 결정 투명성을 향상시킵니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n단점:\n\n- 복잡성: 강화 학습 모델을 구현하는 데는 상당한 컴퓨팅 자원과 전문 지식이 필요합니다.\n- 데이터 의존성: 강화 학습의 효과는 사용 가능한 데이터의 품질과 양에 매우 의존합니다.\n- 안정성: 동적 환경에서 안정적인 학습을 보장하는 것은 어려울 수 있으며 하이퍼파라미터를 세심히 조정해야 합니다.\n\n개선 제안:\n\n- 혼합 모델: 강화 학습과 전통적 최적화 방법을 결합함으로써 안전성과 성능을 향상시킬 수 있습니다.\n- 향상된 데이터 통합: 고객 피드백 및 경쟁사 가격과 같은 다양한 데이터 원본을 통합하여 모델의 정확성을 향상시킬 수 있습니다.\n- 확장성: 확장 가능한 강화 학습 프레임워크를 개발하여 소매 세그먼트와 시장 전반에 이러한 방법을 도움이 될 수 있습니다.\n- 지속적인 모니터링: 비즈니스 목표 및 시장 조건과 일치하는 모델의 결정을 보장하기 위해 모니터링 및 검증 프로세스를 구현합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nGitHub의 내 저장소에 있는 Python 스크립트는 다음과 같습니다: [datalev001/Reinforcement_price](https://github.com/datalev001/Reinforcement_price)","ogImage":{"url":"/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_0.png"},"coverImage":"/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_0.png","tag":["Tech"],"readingTime":16},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cimg src=\"/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_0.png\"\u003e\n\u003ch1\u003e1. 소개\u003c/h1\u003e\n\u003cp\u003e소매 가격 전략은 매출과 이익을 최적화하는 데 중요합니다. 효과적인 가격 책정은 수요, 시장 상황 및 경쟁을 고려하여 소비자 행동을 영향을 주고 매출을 극대화합니다. 예를 들어 소매업체는 가격을 전략적으로 조정하고 할인을 적용하여 매출을 촉진하고 수익을 증가시킬 수 있습니다.\u003c/p\u003e\n\u003cp\u003e본 논문은 Deep Deterministic Policy Gradient (DDPG) 알고리즘을 사용한 강화 학습 접근 방식을 통해 가격 전략을 최적화하는 것을 탐구합니다. 가격과 할인을 동적으로 조정함으로써 가격 결정을 개선할 수 있습니다. 또한 SHAP (Shapley Additive Explanations) 값은 모델의 결정에 미치는 가격, 할인 및 매출의 영향에 대한 통찰을 제공합니다. 이러한 복합 접근 방식은 실시간 분석 및 설명 가능한 인공지능 기술을 통합하여 전통적인 가격 모델을 향상시킵니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e2. 소매 업계의 가격 정책 모델링\u003c/h1\u003e\n\u003cp\u003e소매 업계의 가격 정책은 수익과 이윤을 최적화하기 위해 수학적으로 모델링될 수 있습니다. 매출 기능은 다음과 같이 작성할 수 있습니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_1.png\" alt=\"매출 기능\"\u003e\u003c/p\u003e\n\u003cp\u003e이는 매출이 주로 가격과 할인과 같은 여러 요소에 의존한다는 것을 의미합니다. 일반적으로 가격이 증가하면 매출이 감소하고 그 반대도 마찬가지입니다. 최적의 가격을 찾아 매출이나 이윤을 극대화하는 것이 목표입니다. 예를 들어, 매출 기능이 이차 함수를 따른다면:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_2.png\" alt=\"optimization\"\u003e\u003c/p\u003e\n\u003cp\u003e상수 a와 b가 있는 경우, 최적화 기법으로는 이차 또는 선형 프로그래밍을 사용하여 최적 가격을 찾을 수 있습니다.\u003c/p\u003e\n\u003cp\u003e하지만 전통적인 최적화 방법에는 한계가 있습니다. 실시간 적응성이 부족해 즉각적인 시장 변화에 기초한 효율적인 가격 조정이 어려울 수 있습니다. 또한 판매에 영향을 미치는 요소에 대한 사전지식이 필요한데, 동적인 시장에서는 항상 실행 가능하지 않을 수 있습니다.\u003c/p\u003e\n\u003cp\u003e실시간 데이터 및 강화 학습과 같은 고급 머신러닝 모델은 이러한 도전에 대한 해결책을 제공합니다. 이러한 모델은 가격 전략을 동적으로 조정하고 다양한 요소의 영향을 분석하여 소매 환경에서 더 효과적이고 반응력있는 가격 결정을 지원할 수 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e3. Pricing Strategies를 위한 강화 학습\u003c/h1\u003e\n\u003cp\u003e강화 학습 (RL)은 환경과 상호 작용하여 누적 보상을 극대화하기 위해 최적의 조치를 학습하는 기계 학습 기술입니다. 우리의 가격 전략에서는:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e환경: 소매 시장\u003c/li\u003e\n\u003cli\u003e에이전트: 가격 모델\u003c/li\u003e\n\u003cli\u003e목표: 가격과 할인을 동적으로 조정하여 매출과 이윤을 최적화\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e우리는 실시간 의사 결정에 이상적인 정책 기반 및 가치 기반 학습을 결합한 Deep Deterministic Policy Gradient (DDPG) 알고리즘을 활용합니다. DDPG의 작동 방식은 다음과 같습니다:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e정책 기반 학습: 강화 학습의 정책 함수인 액터-네트워크를 사용합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_3.png\" alt=\"Policy-Based Learning\"\u003e\u003c/p\u003e\n\u003cp\u003e상태 s가 주어진 경우 동작 a를 선택합니다. θ^π는 정책 네트워크의 매개변수입니다.\u003c/p\u003e\n\u003cp\u003e가치 기반 학습: 비평가 네트워크(Q 함수)를 사용합니다:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_4.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e행동-가치 함수를 평가하기 위해.\u003c/p\u003e\n\u003cp\u003e학습 과정:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eActor-Critic 아키텍처: Actor는 기대값 반환의 그래디언트를 따라 정책을 업데이트하며, Critic은 벨만 방정식을 사용하여 가치 추정을 업데이트합니다.\u003c/li\u003e\n\u003cli\u003eExperience Replay: 과거 경험 (s,a,r,s′)을 재생 버퍼에 저장하여 상관 관계를 끊고 학습을 안정화합니다.\u003c/li\u003e\n\u003cli\u003eTarget Networks: 학습을 안정화하도록 학습된 네트워크를 천천히 추적하기 위해 목표 네트워크 세트 θ^π′ 및 θ^Q′를 유지합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eDDPG의 장점은 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e적응성: DDPG는 최신 시장 데이터를 기반으로 실시간으로 조정을 제공합니다.\u003c/li\u003e\n\u003cli\u003e세밀한 결정: 연속적인 액션 공간은 정확한 가격 조정을 가능하게 합니다.\u003c/li\u003e\n\u003cli\u003e데이터 기반 통찰력: 다양한 요소(가격, 할인 등)가 매출에 미치는 영향을 이해하는 데 도움이 되어 보다 효과적인 가격 전략을 도와줍니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e4. 코딩 및 데이터 실험\u003c/h1\u003e\n\u003cp\u003e이제 강화 학습(RL) 프레임워크 내에서 딥 디터미니스틱 정책 그라디언트(DDPG) 알고리즘을 구현하여 소매 가격 전략을 최적화해봅니다. 이 접근 방식은 매출과 이익을 극대화하기 위해 가격과 할인을 동적으로 조정합니다. 게다가, SHAP(Shapley Additive Explanations) 분석을 사용하여 모델 결정에 각 기능이 미치는 영향을 이해하여 RL 기반 가격 모델의 해석 가능성을 향상시킵니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e강화학습 환경 설정:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e환경 초기화: 우리는 맞춤 gym 환경인 SalesPredictionEnv를 정의합니다. 이 환경은 소매 시장을 시뮬레이션합니다. 환경은 초기 가격과 할인을 입력으로 받고 진짜 판매 기능을 사용하여 판매를 시뮬레이션합니다. 액션 공간은 가격과 할인을 연속적으로 조정할 수 있으며 관찰 공간에는 현재 가격, 할인 및 예측된 판매가 포함됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSalesPredictionEnv\u003c/span\u003e(gym.Env):\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, initial_price, initial_discount, true_sales_function\u003c/span\u003e):\n        \u003cspan class=\"hljs-built_in\"\u003esuper\u003c/span\u003e(SalesPredictionEnv, self).__init__()\n        self.initial_price = initial_price\n        self.initial_discount = initial_discount\n        self.true_sales_function = true_sales_function\n\n        self.action_space = spaces.Box(low=-\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, high=\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, shape=(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,), dtype=np.float32)\n        self.observation_space = spaces.Box(low=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, high=np.inf, shape=(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e,), dtype=np.float32)\n\n        self.price = self.initial_price\n        self.discount = self.initial_discount\n        self.sales = self.true_sales_function(self.price, self.discount)\n        self.done = \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ereset\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, seed=\u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e, options=\u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\u003c/span\u003e):\n        \u003cspan class=\"hljs-built_in\"\u003esuper\u003c/span\u003e().reset(seed=seed)\n        self.price = self.initial_price\n        self.discount = self.initial_discount\n        self.sales = self.true_sales_function(self.price, self.discount)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.array([self.price, self.discount, self.sales], dtype=np.float32), {}\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003estep\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, action\u003c/span\u003e):\n        self.price += action[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n        self.discount += action[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n        new_sales = self.true_sales_function(self.price, self.discount)\n\n        reward = -\u003cspan class=\"hljs-built_in\"\u003eabs\u003c/span\u003e(self.sales - new_sales)\n        self.sales = new_sales\n        self.done = \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e\n\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.array([self.price, self.discount, self.sales], dtype=np.float32), reward, \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e, \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e, {}\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erender\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, mode=\u003cspan class=\"hljs-string\"\u003e'human'\u003c/span\u003e\u003c/span\u003e):\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef'Price: \u003cspan class=\"hljs-subst\"\u003e{self.price}\u003c/span\u003e, Discount: \u003cspan class=\"hljs-subst\"\u003e{self.discount}\u003c/span\u003e, Sales: \u003cspan class=\"hljs-subst\"\u003e{self.sales}\u003c/span\u003e'\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e진짜 판매 함수: 그런 다음 가격, 할인 및 판매 간의 관계를 모델링하는 판매 함수를 정의합니다. 이 함수는 강화학습(RL) 구현에서 소매 환경을 시뮬레이션할 수 있습니다. RL 에이전트가 다양한 가격과 할인 수준이 판매에 어떤 영향을 미치는지 이해할 수 있도록 합니다. 이 함수는 다음과 같이 공식화됩니다:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003etrue_sales_function\u003c/span\u003e(가격, 할인):\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e -\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e * 가격 ** \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e + 가격 + \u003cspan class=\"hljs-number\"\u003e11\u003c/span\u003e + \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e * 할인\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e실제 세계의 강화학습 구현에서는 이러한 함수들이 종종 과거 판매 데이터, 경험적 연구 또는 도메인 전문 지식을 기반으로 실제 시장 행위를 모방하는 데 사용됩니다. 이 이차 함수 형태는 중간 가격 상승이 판매를 촉진할 수 있지만, 과도한 가격이나 할인은 전반적인 판매에 부정적인 영향을 미칠 수 있다.\u003c/p\u003e\n\u003cp\u003e환경 및 모델 설정: check_env를 사용하여 환경을 초기화합니다. 그런 다음 환경에 DDPG 에이전트를 설정합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eenv = \u003cspan class=\"hljs-title class_\"\u003eSalesPredictionEnv\u003c/span\u003e(initial_price=\u003cspan class=\"hljs-number\"\u003e5.0\u003c/span\u003e, initial_discount=\u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, true_sales_function=true_sales_function)\n\u003cspan class=\"hljs-title function_\"\u003echeck_env\u003c/span\u003e(env)\nmodel = \u003cspan class=\"hljs-title function_\"\u003eDDPG\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'MlpPolicy'\u003c/span\u003e, env, verbose=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\nmodel.\u003cspan class=\"hljs-title function_\"\u003elearn\u003c/span\u003e(total_timesteps=\u003cspan class=\"hljs-number\"\u003e10000\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eSHAP 분석:\u003c/p\u003e\n\u003cp\u003eSHAP (Shapley Additive Explanations)은 각 특징이 예측에 미치는 영향을 양적으로 설명하여 모델을 해석 가능하게 합니다. RL 설정에서 SHAP를 구현하는 과정은 다음과 같습니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSHAP를 위한 데이터 수집: 환경을 재설정하고 SHAP 분석을 위해 상태와 행동을 수집합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eobs, _ = env.\u003cspan class=\"hljs-title function_\"\u003ereset\u003c/span\u003e()\nstates = []\nactions = []\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e _ \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e):\n    action, _states = model.\u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(obs)\n    obs, rewards, terminated, truncated, _ = env.\u003cspan class=\"hljs-title function_\"\u003estep\u003c/span\u003e(action)\n    env.\u003cspan class=\"hljs-title function_\"\u003erender\u003c/span\u003e()\n    states.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(obs)\n    actions.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(action)\n\nstates = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(states)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eSHAP 예측 래퍼: SHAP의 올바른 출력 형식을 보장하기 위해 래퍼 함수를 정의합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003epredict_wrapper\u003c/span\u003e(observations):\n    predictions = []\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e obs \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eobservations\u003c/span\u003e:\n        action, _states = model.\u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(obs)\n        predictions.\u003cspan class=\"hljs-title function_\"\u003eappend\u003c/span\u003e(action.\u003cspan class=\"hljs-title function_\"\u003eflatten\u003c/span\u003e())\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e(predictions)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e예측 DataFrame: 예측을 저장할 DataFrame을 생성하고 추가 분석을 위해 Excel 파일에 저장합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epredictions = {\n    \u003cspan class=\"hljs-string\"\u003e'ID'\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003elist\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(states))),\n    \u003cspan class=\"hljs-string\"\u003e'price'\u003c/span\u003e: states[:, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e],\n    \u003cspan class=\"hljs-string\"\u003e'discount'\u003c/span\u003e: states[:, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e],\n    \u003cspan class=\"hljs-string\"\u003e'sales'\u003c/span\u003e: states[:, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e],\n    \u003cspan class=\"hljs-string\"\u003e'predicted_action_0'\u003c/span\u003e: [\u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e] * \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(states),\n    \u003cspan class=\"hljs-string\"\u003e'predicted_action_1'\u003c/span\u003e: [\u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e] * \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(states)\n}\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e idx, state \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eenumerate\u003c/span\u003e(states):\n    action, _states = model.\u003cspan class=\"hljs-title function_\"\u003epredict\u003c/span\u003e(state)\n    predictions[\u003cspan class=\"hljs-string\"\u003e'predicted_action_0'\u003c/span\u003e][idx] = action[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n    predictions[\u003cspan class=\"hljs-string\"\u003e'predicted_action_1'\u003c/span\u003e][idx] = action[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n\npredictions_df = pd.\u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e(predictions)\npredictions_df.\u003cspan class=\"hljs-title function_\"\u003eto_excel\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"reinforcement_learning_predictions.xlsx\"\u003c/span\u003e, index=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(predictions_df.\u003cspan class=\"hljs-title function_\"\u003ehead\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e))\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eSHAP Explainer 및 Visualization: 우리는 SHAP를 사용하여 모델의 의사결정에 미치는 다른 feature들의 영향을 분석하고 결과를 시각화합니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eexplainer = shap.\u003cspan class=\"hljs-title class_\"\u003eExplainer\u003c/span\u003e(predict_wrapper, states)\nshap_values = \u003cspan class=\"hljs-title function_\"\u003eexplainer\u003c/span\u003e(states)\n\nshap_values_price = shap_values[..., \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n\nshap.\u003cspan class=\"hljs-property\"\u003eplots\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ebeeswarm\u003c/span\u003e(shap_values_price)\nshap.\u003cspan class=\"hljs-property\"\u003eplots\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003ebar\u003c/span\u003e(shap_values_price[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e상위 영향력 있는 feature들: 각 state의 상위 영향력 있는 feature들을 추출하여 DataFrame에 저장하여 쉬운 분석을 할 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edata = {\n    \u003cspan class=\"hljs-string\"\u003e'ID'\u003c/span\u003e: \u003cspan class=\"hljs-title function_\"\u003elist\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(states))),\n    \u003cspan class=\"hljs-string\"\u003e'price'\u003c/span\u003e: states[:, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e],\n    \u003cspan class=\"hljs-string\"\u003e'discount'\u003c/span\u003e: states[:, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e],\n    \u003cspan class=\"hljs-string\"\u003e'sales'\u003c/span\u003e: states[:, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e],\n    \u003cspan class=\"hljs-string\"\u003e'top_feature1'\u003c/span\u003e: [\u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e] * \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(states),\n    \u003cspan class=\"hljs-string\"\u003e'top_feature2'\u003c/span\u003e: [\u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e] * \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(states),\n    \u003cspan class=\"hljs-string\"\u003e'importance1'\u003c/span\u003e: [\u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e] * \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(states),\n    \u003cspan class=\"hljs-string\"\u003e'importance2'\u003c/span\u003e: [\u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e] * \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(states)\n}\n\nfeatures = [\u003cspan class=\"hljs-string\"\u003e'price'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'discount'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'sales'\u003c/span\u003e]\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(states)):\n    sorted_indices = np.\u003cspan class=\"hljs-title function_\"\u003eargsort\u003c/span\u003e(-np.\u003cspan class=\"hljs-title function_\"\u003eabs\u003c/span\u003e(shap_values.\u003cspan class=\"hljs-property\"\u003evalues\u003c/span\u003e[i][:, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]))\n    data[\u003cspan class=\"hljs-string\"\u003e'top_feature1'\u003c/span\u003e][i] = features[sorted_indices[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]]\n    data[\u003cspan class=\"hljs-string\"\u003e'importance1'\u003c/span\u003e][i] = shap_values.\u003cspan class=\"hljs-property\"\u003evalues\u003c/span\u003e[i][sorted_indices[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e(sorted_indices) \u003e \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e:\n        data[\u003cspan class=\"hljs-string\"\u003e'top_feature2'\u003c/span\u003e][i] = features[sorted_indices[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]]\n        data[\u003cspan class=\"hljs-string\"\u003e'importance2'\u003c/span\u003e][i] = shap_values.\u003cspan class=\"hljs-property\"\u003evalues\u003c/span\u003e[i][sorted_indices[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n\nreason_df = pd.\u003cspan class=\"hljs-title class_\"\u003eDataFrame\u003c/span\u003e(data)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(reason_df.\u003cspan class=\"hljs-title function_\"\u003ehead\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e))\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e5. 분석과 인사이트\u003c/h1\u003e\n\u003cp\u003e다음의 SHAP 막대 그래프는 Price(가격), Discount(할인) 및 Sales(판매)가 특정 인스턴스에 대한 모델의 가격 결정에 미치는 영향을 보여줍니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSHAP 막대 그래프는 Price(가격), Discount(할인) 및 Sales(판매)가 특정 인스턴스에 대한 모델의 가격 결정에 미치는 영향을 보여줍니다:\u003c/li\u003e\n\u003cli\u003eSales(판매): 가장 높은 긍정적 영향을 나타내며, 높은 판매량이 모델이 가격과 할인을 유지하거나 높이는 데 강력한 영향을 미친다는 것을 시사합니다.\u003c/li\u003e\n\u003cli\u003eDiscount(할인): 높은 할인은 결과에 부정적인 영향을 미치며, 과도한 할인을 피하기 위해 할인액을 줄이는 것을 권장하는 모델의 결론으로 이어집니다.\u003c/li\u003e\n\u003cli\u003ePrice(가격): 약간의 긍정적 영향을 나타내며, 결과를 향상시키기 위해 가격을 약간 올리는 것을 선호하는 모델이고 판매량에 큰 영향을 미치지 않는다는 것을 나타냅니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_5.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e모델은 가격 책정 전략을 안내하기 위해 판매를 우선시하며, 수익을 극대화하기 위해 신중한 할인 관리와 약간의 가격 인상을 권장합니다. 막대 도표는 모델의 특정 사례에 대한 가격 결정에 영향을 미치는 Sales, Price 및 Discount가 어떻게 변하는지를 강조합니다.\u003c/p\u003e\n\u003cp\u003e다음은 SHAP beeswarm 도표로, 여러 사례에 걸쳐 Price, Discount 및 Sales가 모델의 가격 결정에 미치는 영향을 보여줍니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSales (Feature 2): 높은 값(빨강)은 모델의 출력을 증가시키고, 낮은 값(파랑)은 감소시킵니다.\u003c/li\u003e\n\u003cli\u003ePrice (Feature 0): 낮은 값(파랑)은 부정적인 영향을 미치며, 더 높은 값(빨강)은 긍정적인 영향을 미칩니다.\u003c/li\u003e\n\u003cli\u003eDiscount (Feature 1): 높은 값(빨강)은 모델의 출력을 감소시키고, 낮은 값(파랑)은 긍정적인 영향을 미칩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_6.png\" alt=\"도표 이미지\"\u003e\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e벌집 그림은 매출, 가격 및 할인이 모델의 결정에 미치는 영향이 여러 인스턴스에 걸쳐 어떻게 변하는지를 제공하여 그들이 모델의 결정에 미치는 중요성과 일관성을 강조합니다.\u003c/p\u003e\n\u003cp\u003e예측된 작업 테이블은 다른 기능에 대한 모델의 예측을 제시합니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_7.png\" alt=\"Predicted Actions Table\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e가격 조정: 가격에 대한 예측된 작업은 약간 부정적이며(예측 작업 0), 가격이 감소함에 따라 약간의 인하를 시사합니다.\u003c/li\u003e\n\u003cli\u003e할인 조정: 할인에 대한 예측된 작업(예측 작업 1)도 약간 부정적이며, 소폭의 감소를 나타냅니다. 모델은 수익성을 유지하기 위해 신중한 할인을 권장하는 일관된 경향을 보입니다.\u003c/li\u003e\n\u003cli\u003e매출 영향: 가격과 할인이 감소함에 따라 매출이 증가하며, 전형적인 시장 행동을 반영합니다. 모델이 가격과 할인을 약간 감소시킨 것은 수익성을 유지하면서 매출을 최적화할 수 있습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eFeature Importance Table은 각 인스턴스에 대해 모델 결정에 영향을 줄인 최상위 두 가지 기능과 그 중요도 값을 식별합니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e판매량: 모든 인스턴스에서 가장 중요한 기능(top_feature1)으로 일관되게 나타납니다.\u003c/li\u003e\n\u003cli\u003e가격 및 할인: 교체 가능 기능(top_feature2)은 다양한 중요도 값을 가지며, 판매량에 높은 중요도 값이 해당 기능이 모델 예측에 미치는 강력한 영향을 나타냅니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-OptimizingDynamicPricingwithReinforcementLearning_8.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003cp\u003e요약하면, 판매가격은 모델의 가격 결정에서 우세한 요소이며, 가격 및 할인은 보조이지만 중요한 역할을 합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e6. 결론\u003c/h1\u003e\n\u003cp\u003e본 연구는 소매 가격 전략을 최적화하기 위해 Deep Deterministic Policy Gradient (DDPG) 알고리즘을 활용합니다. 강화 학습 (RL)과 SHAP (Shapley Additive Explanations)을 결합하여 가격 및 할인을 조정하여 매출과 이익을 극대화할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e장점:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e적응성: 전통적인 가격 모델과 달리 RL은 실시간 데이터로부터 계속 학습하여 시장 변화에 즉각 대응할 수 있습니다.\u003c/li\u003e\n\u003cli\u003e정밀성: DDPG의 연속적인 행동 공간은 섬세한 가격 결정을 가능하게 합니다.\u003c/li\u003e\n\u003cli\u003e통찰력: SHAP 값은 다양한 요인의 영향에 대한 설명 가능한 통찰력을 제공하여 의사 결정 투명성을 향상시킵니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e단점:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e복잡성: 강화 학습 모델을 구현하는 데는 상당한 컴퓨팅 자원과 전문 지식이 필요합니다.\u003c/li\u003e\n\u003cli\u003e데이터 의존성: 강화 학습의 효과는 사용 가능한 데이터의 품질과 양에 매우 의존합니다.\u003c/li\u003e\n\u003cli\u003e안정성: 동적 환경에서 안정적인 학습을 보장하는 것은 어려울 수 있으며 하이퍼파라미터를 세심히 조정해야 합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e개선 제안:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e혼합 모델: 강화 학습과 전통적 최적화 방법을 결합함으로써 안전성과 성능을 향상시킬 수 있습니다.\u003c/li\u003e\n\u003cli\u003e향상된 데이터 통합: 고객 피드백 및 경쟁사 가격과 같은 다양한 데이터 원본을 통합하여 모델의 정확성을 향상시킬 수 있습니다.\u003c/li\u003e\n\u003cli\u003e확장성: 확장 가능한 강화 학습 프레임워크를 개발하여 소매 세그먼트와 시장 전반에 이러한 방법을 도움이 될 수 있습니다.\u003c/li\u003e\n\u003cli\u003e지속적인 모니터링: 비즈니스 목표 및 시장 조건과 일치하는 모델의 결정을 보장하기 위해 모니터링 및 검증 프로세스를 구현합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eGitHub의 내 저장소에 있는 Python 스크립트는 다음과 같습니다: \u003ca href=\"https://github.com/datalev001/Reinforcement_price\" rel=\"nofollow\" target=\"_blank\"\u003edatalev001/Reinforcement_price\u003c/a\u003e\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-07-14-OptimizingDynamicPricingwithReinforcementLearning"},"buildId":"N1mNhRlQaHCliEGDvPEpG","assetPrefix":"/TIL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>