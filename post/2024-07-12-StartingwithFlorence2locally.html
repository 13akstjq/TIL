<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>로컬에서 Florence 2 시작하는 방법 | TIL</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://13akstjq.github.io/TIL//post/2024-07-12-StartingwithFlorence2locally" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="로컬에서 Florence 2 시작하는 방법 | TIL" data-gatsby-head="true"/><meta property="og:title" content="로컬에서 Florence 2 시작하는 방법 | TIL" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/TIL/assets/img/2024-07-12-StartingwithFlorence2locally_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://TIL.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://13akstjq.github.io/TIL//post/2024-07-12-StartingwithFlorence2locally" data-gatsby-head="true"/><meta name="twitter:title" content="로컬에서 Florence 2 시작하는 방법 | TIL" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/TIL/assets/img/2024-07-12-StartingwithFlorence2locally_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | TIL" data-gatsby-head="true"/><meta name="article:published_time" content="2024-07-12 20:11" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/TIL/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/TIL/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/TIL/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/TIL/favicons/favicon-96x96.png"/><link rel="icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/TIL/favicons/browserconfig.xml"/><link rel="preload" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/TIL/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/TIL/_next/static/chunks/webpack-21ffe88bdca56cba.js" defer=""></script><script src="/TIL/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/TIL/_next/static/chunks/main-a5eeabb286676ce6.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/TIL/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/TIL/_next/static/chunks/348-02483b66b493dd81.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/post/%5Bslug%5D-5ecfd58aae5a7e3d.js" defer=""></script><script src="/TIL/_next/static/jKAIrnIuHBv4ZHjiQbX6i/_buildManifest.js" defer=""></script><script src="/TIL/_next/static/jKAIrnIuHBv4ZHjiQbX6i/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/TIL">TIL</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/TIL/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">로컬에서 Florence 2 시작하는 방법</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="로컬에서 Florence 2 시작하는 방법" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/TIL/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">TIL</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jul 12, 2024</span><span class="posts_reading_time__f7YPP">7<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-07-12-StartingwithFlorence2locally&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p>플로렌스-2는 Microsoft의 고급 비전 기반 모델로, 프롬프트 기반 방식을 사용하여 다양한 비전 및 비전-언어 작업을 처리하기 위해 설계되었습니다. 로컬에서 플로렌스-2를 설정하고 실행하는 데 도움이 되는 시작 스크립트가 여기 있어요.</p>
<p><img src="/TIL/assets/img/2024-07-12-StartingwithFlorence2locally_0.png" alt="이미지"></p>
<h1>시작 스크립트</h1>
<p>플로렌스-2를 실행하는 데 사용할 수 있는 시작 스크립트입니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModelForCausalLM  
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> unittest.mock <span class="hljs-keyword">import</span> patch
<span class="hljs-keyword">from</span> transformers.dynamic_module_utils <span class="hljs-keyword">import</span> get_imports

<span class="hljs-keyword">def</span> <span class="hljs-title function_">run_example</span>(<span class="hljs-params">task_prompt, text_input=<span class="hljs-literal">None</span></span>):
    <span class="hljs-keyword">if</span> text_input <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
        prompt = task_prompt
    <span class="hljs-keyword">else</span>:
        prompt = task_prompt + text_input
    inputs = processor(text=prompt, images=image, return_tensors=<span class="hljs-string">"pt"</span>)
    generated_ids = model.generate(
        input_ids=inputs[<span class="hljs-string">"input_ids"</span>].cuda(),
        pixel_values=inputs[<span class="hljs-string">"pixel_values"</span>].cuda(),
        max_new_tokens=<span class="hljs-number">1024</span>,
        early_stopping=<span class="hljs-literal">False</span>,
        do_sample=<span class="hljs-literal">False</span>,
        num_beams=<span class="hljs-number">3</span>,
    )
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]
    parsed_answer = processor.post_process_generation(
        generated_text, 
        task=task_prompt, 
        image_size=(image.width, image.height),
    )
    <span class="hljs-keyword">return</span> parsed_answer

<span class="hljs-comment"># Example usage</span>
fn = <span class="hljs-string">'ray_ban_meta.jpeg'</span>
image = Image.<span class="hljs-built_in">open</span>(fn)
task_prompt = <span class="hljs-string">'&#x3C;MORE_DETAILED_CAPTION>'</span>
ret = run_example(task_prompt)
<span class="hljs-built_in">print</span>(ret)
</code></pre>
<h2>너무 괴롭히지 마세요</h2>
<p>만약 지역에서 실행하는 모든 시도 끝에도 이 예외를 받게 된다면: <code>pip install flash_attn</code>를 실행해 보세요.</p>
<pre><code class="hljs language-python">File <span class="hljs-string">"C:\Users\alex_\aichat\florence2_vision\myenv\lib\site-packages\transformers\dynamic_module_utils.py"</span>, line <span class="hljs-number">182</span>, <span class="hljs-keyword">in</span> check_imports
    <span class="hljs-keyword">raise</span> ImportError(
ImportError: 이 모델링 파일은 환경에 없는 다음 패키지가 필요합니다: flash_attn. `pip install flash_attn`을 실행해 보세요.
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>상태 코드를 Markdown 형식으로 변경하십시오.</p>
<pre><code class="hljs language-js"><span class="hljs-title class_">File</span> <span class="hljs-string">"C:\Users\alex_\aichat\florence2_vision\myenv\lib\site-packages\flash_attn\flash_attn_interface.py"</span>, line <span class="hljs-number">10</span>, <span class="hljs-keyword">in</span> &#x3C;<span class="hljs-variable language_">module</span>>
    <span class="hljs-keyword">import</span> flash_attn_2_cuda <span class="hljs-keyword">as</span> flash_attn_cuda
<span class="hljs-title class_">ImportError</span>: <span class="hljs-variable constant_">DLL</span> load failed <span class="hljs-keyword">while</span> importing <span class="hljs-attr">flash_attn_2_cuda</span>: <span class="hljs-title class_">The</span> specified procedure could not be found.
</code></pre>
<p>청소한 상태에서 Miniconda를 사용하여 시작해 보세요.</p>
<h1>깔끔한 환경 설정</h1>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>다음과 같이 새로운 conda 환경을 만들어 보세요:</p>
<pre><code class="hljs language-js">conda create -n florence2 python=<span class="hljs-number">3.11</span> -y
conda activate florence2
</code></pre>
<p>CUDA 설치 여부 확인:</p>
<p>CUDA가 설치되어 있는지 확인해주세요:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">nvcc --version

<span class="hljs-attr">nvcc</span>: <span class="hljs-variable constant_">NVIDIA</span> (R) <span class="hljs-title class_">Cuda</span> 컴파일러 드라이버
<span class="hljs-title class_">Copyright</span> (c) <span class="hljs-number">2005</span>-<span class="hljs-number">2024</span> <span class="hljs-variable constant_">NVIDIA</span> <span class="hljs-title class_">Corporation</span>
<span class="hljs-number">2024</span>년 <span class="hljs-number">4</span>월 <span class="hljs-number">17</span>일 수요일에 빌드됨
<span class="hljs-title class_">Cuda</span> 컴파일 도구, 릴리즈 <span class="hljs-number">12.5</span>, <span class="hljs-variable constant_">V12</span><span class="hljs-number">.5</span><span class="hljs-number">.40</span>
빌드 cuda_12<span class="hljs-number">.5</span>.<span class="hljs-property">r12</span><span class="hljs-number">.5</span>/compiler<span class="hljs-number">.34177558_0</span>
</code></pre>
<p>CUDA 경로 설정:</p>
<pre><code class="hljs language-js">set <span class="hljs-string">"CUDA_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.5"</span>
set <span class="hljs-string">"CUDA_HOME=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.5"</span>
</code></pre>
<p>PyTorch 및 종속성 설치하기:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">pip install torch torchvision torchaudio --index-url <span class="hljs-attr">https</span>:<span class="hljs-comment">//download.pytorch.org/whl/cu121</span>
pip install transformers einops timm
</code></pre>
<h1>Flash 어텐션 불필요</h1>
<p>transformers 의존성에서 요구되지도 않고 알려진 문제입니다.</p>
<p>이것이 해결책이에요:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js"># <span class="hljs-title class_">With</span> <span class="hljs-title class_">Python</span> <span class="hljs-number">3.11</span><span class="hljs-number">.7</span>, transformers==<span class="hljs-number">4.36</span><span class="hljs-number">.2</span>
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">from</span> unittest.<span class="hljs-property">mock</span> <span class="hljs-keyword">import</span> patch

<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> <span class="hljs-title class_">AutoModelForCausalLM</span>
<span class="hljs-keyword">from</span> transformers.<span class="hljs-property">dynamic_module_utils</span> <span class="hljs-keyword">import</span> get_imports


def <span class="hljs-title function_">fixed_get_imports</span>(<span class="hljs-attr">filename</span>: str | os.<span class="hljs-property">PathLike</span>) -> list[str]:
    <span class="hljs-string">""</span><span class="hljs-string">"https://huggingface.co/microsoft/phi-1_5/discussions/72을 위한 해결책."</span><span class="hljs-string">""</span>
    <span class="hljs-keyword">if</span> not <span class="hljs-title function_">str</span>(filename).<span class="hljs-title function_">endswith</span>(<span class="hljs-string">"/modeling_phi.py"</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-title function_">get_imports</span>(filename)
    imports = <span class="hljs-title function_">get_imports</span>(filename)
    imports.<span class="hljs-title function_">remove</span>(<span class="hljs-string">"flash_attn"</span>)
    <span class="hljs-keyword">return</span> imports


<span class="hljs-keyword">with</span> <span class="hljs-title function_">patch</span>(<span class="hljs-string">"transformers.dynamic_module_utils.get_imports"</span>, fixed_get_imports):
    model = <span class="hljs-title class_">AutoModelForCausalLM</span>.<span class="hljs-title function_">from_pretrained</span>(<span class="hljs-string">"microsoft/phi-1_5"</span>, trust_remote_code=<span class="hljs-title class_">True</span>)
</code></pre>
<h1>OCR 테스트</h1>
<h2>입력 이미지</h2>
<img src="/TIL/assets/img/2024-07-12-StartingwithFlorence2locally_1.png">
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>다음은 스크립트를 실행한 후의 업데이트 내용입니다:</p>
<pre><code class="hljs language-js"><span class="hljs-attr">D</span>:\<span class="hljs-variable constant_">DEV</span>\<span class="hljs-variable constant_">MODELS</span>\modules\transformers_modules\microsoft\<span class="hljs-title class_">Florence</span>-<span class="hljs-number">2</span>-large-ft\3112cd2e25c969cfdcb600a01489c56737d943d3\modeling_florence2.<span class="hljs-property">py</span>:<span class="hljs-number">1209</span>: <span class="hljs-title class_">UserWarning</span>: 1Torch was not compiled <span class="hljs-keyword">with</span> flash attention. (<span class="hljs-title class_">Triggered</span> internally at ..\aten\src\<span class="hljs-title class_">ATen</span>\native\transformers\cuda\sdp_utils.<span class="hljs-property">cpp</span>:<span class="hljs-number">455.</span>)
  attn_output = torch.<span class="hljs-property">nn</span>.<span class="hljs-property">functional</span>.<span class="hljs-title function_">scaled_dot_product_attention</span>(
{<span class="hljs-string">'&#x3C;OCR>'</span>: <span class="hljs-string">'2310Z8MOOWN - RW4008 6015IC 26243-4003RCID 2AYOA-403'</span>}
</code></pre>
<p>잘 작동합니다! 좀 그렇지만요. 일부 숫자가 누락되었지만, 그건 다음에 다시 이야기할 주제입니다.</p>
<h1>마지막으로</h1>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이 단계를 따르면 어려움 없이 Florence-2를 로컬에서 실행할 수 있을 것입니다. 다른 문제가 발생하면 모든 의존성이 올바르게 설치되어 있는지, 환경이 올바르게 구성되어 있는지 확인해주세요.</p>
<h1>Hugging Face Spaces</h1>
<p>환경을 로컬로 설정하기를 원치 않는다면 Hugging Face Spaces를 사용하여 Florence-2를 실행할 수도 있습니다. 이는 로컬 구성이 필요 없이 모델에 액세스할 수 있는 클라우드 기반 솔루션을 제공합니다. Hugging Face의 Florence-2 스페이스를 확인해보세요: Hugging Face Spaces</p>
<h1>출처</h1>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"로컬에서 Florence 2 시작하는 방법","description":"","date":"2024-07-12 20:11","slug":"2024-07-12-StartingwithFlorence2locally","content":"\n\n플로렌스-2는 Microsoft의 고급 비전 기반 모델로, 프롬프트 기반 방식을 사용하여 다양한 비전 및 비전-언어 작업을 처리하기 위해 설계되었습니다. 로컬에서 플로렌스-2를 설정하고 실행하는 데 도움이 되는 시작 스크립트가 여기 있어요.\n\n![이미지](/TIL/assets/img/2024-07-12-StartingwithFlorence2locally_0.png)\n\n# 시작 스크립트\n\n플로렌스-2를 실행하는 데 사용할 수 있는 시작 스크립트입니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```python\nimport os\nfrom transformers import AutoProcessor, AutoModelForCausalLM  \nfrom PIL import Image\nimport requests\nfrom unittest.mock import patch\nfrom transformers.dynamic_module_utils import get_imports\n\ndef run_example(task_prompt, text_input=None):\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n    generated_ids = model.generate(\n        input_ids=inputs[\"input_ids\"].cuda(),\n        pixel_values=inputs[\"pixel_values\"].cuda(),\n        max_new_tokens=1024,\n        early_stopping=False,\n        do_sample=False,\n        num_beams=3,\n    )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n    parsed_answer = processor.post_process_generation(\n        generated_text, \n        task=task_prompt, \n        image_size=(image.width, image.height),\n    )\n    return parsed_answer\n\n# Example usage\nfn = 'ray_ban_meta.jpeg'\nimage = Image.open(fn)\ntask_prompt = '\u003cMORE_DETAILED_CAPTION\u003e'\nret = run_example(task_prompt)\nprint(ret)\n```\n\n## 너무 괴롭히지 마세요\n\n만약 지역에서 실행하는 모든 시도 끝에도 이 예외를 받게 된다면: `pip install flash_attn`를 실행해 보세요.\n\n```python\nFile \"C:\\Users\\alex_\\aichat\\florence2_vision\\myenv\\lib\\site-packages\\transformers\\dynamic_module_utils.py\", line 182, in check_imports\n    raise ImportError(\nImportError: 이 모델링 파일은 환경에 없는 다음 패키지가 필요합니다: flash_attn. `pip install flash_attn`을 실행해 보세요.\n```\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n상태 코드를 Markdown 형식으로 변경하십시오.\n\n```js\nFile \"C:\\Users\\alex_\\aichat\\florence2_vision\\myenv\\lib\\site-packages\\flash_attn\\flash_attn_interface.py\", line 10, in \u003cmodule\u003e\n    import flash_attn_2_cuda as flash_attn_cuda\nImportError: DLL load failed while importing flash_attn_2_cuda: The specified procedure could not be found.\n```\n\n청소한 상태에서 Miniconda를 사용하여 시작해 보세요.\n\n# 깔끔한 환경 설정\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음과 같이 새로운 conda 환경을 만들어 보세요:\n\n```js\nconda create -n florence2 python=3.11 -y\nconda activate florence2\n```\n\nCUDA 설치 여부 확인:\n\nCUDA가 설치되어 있는지 확인해주세요:\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\r\nnvcc --version\n\nnvcc: NVIDIA (R) Cuda 컴파일러 드라이버\nCopyright (c) 2005-2024 NVIDIA Corporation\n2024년 4월 17일 수요일에 빌드됨\nCuda 컴파일 도구, 릴리즈 12.5, V12.5.40\n빌드 cuda_12.5.r12.5/compiler.34177558_0\r\n```\n\nCUDA 경로 설정:\n\n```js\r\nset \"CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\"\nset \"CUDA_HOME=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\"\r\n```\n\nPyTorch 및 종속성 설치하기:\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\npip install transformers einops timm\n```\n\n# Flash 어텐션 불필요\n\ntransformers 의존성에서 요구되지도 않고 알려진 문제입니다.\n\n이것이 해결책이에요:\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n# With Python 3.11.7, transformers==4.36.2\nimport os\nfrom unittest.mock import patch\n\nfrom transformers import AutoModelForCausalLM\nfrom transformers.dynamic_module_utils import get_imports\n\n\ndef fixed_get_imports(filename: str | os.PathLike) -\u003e list[str]:\n    \"\"\"https://huggingface.co/microsoft/phi-1_5/discussions/72을 위한 해결책.\"\"\"\n    if not str(filename).endswith(\"/modeling_phi.py\"):\n        return get_imports(filename)\n    imports = get_imports(filename)\n    imports.remove(\"flash_attn\")\n    return imports\n\n\nwith patch(\"transformers.dynamic_module_utils.get_imports\", fixed_get_imports):\n    model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\r\n```\n\n# OCR 테스트\n\n## 입력 이미지\n\n\u003cimg src=\"/TIL/assets/img/2024-07-12-StartingwithFlorence2locally_1.png\" /\u003e\n\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다음은 스크립트를 실행한 후의 업데이트 내용입니다:\n\n```js\nD:\\DEV\\MODELS\\modules\\transformers_modules\\microsoft\\Florence-2-large-ft\\3112cd2e25c969cfdcb600a01489c56737d943d3\\modeling_florence2.py:1209: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n  attn_output = torch.nn.functional.scaled_dot_product_attention(\n{'\u003cOCR\u003e': '2310Z8MOOWN - RW4008 6015IC 26243-4003RCID 2AYOA-403'}\n```\n\n잘 작동합니다! 좀 그렇지만요. 일부 숫자가 누락되었지만, 그건 다음에 다시 이야기할 주제입니다.\n\n# 마지막으로\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 단계를 따르면 어려움 없이 Florence-2를 로컬에서 실행할 수 있을 것입니다. 다른 문제가 발생하면 모든 의존성이 올바르게 설치되어 있는지, 환경이 올바르게 구성되어 있는지 확인해주세요.\n\n# Hugging Face Spaces\n\n환경을 로컬로 설정하기를 원치 않는다면 Hugging Face Spaces를 사용하여 Florence-2를 실행할 수도 있습니다. 이는 로컬 구성이 필요 없이 모델에 액세스할 수 있는 클라우드 기반 솔루션을 제공합니다. Hugging Face의 Florence-2 스페이스를 확인해보세요: Hugging Face Spaces\n\n# 출처","ogImage":{"url":"/TIL/assets/img/2024-07-12-StartingwithFlorence2locally_0.png"},"coverImage":"/TIL/assets/img/2024-07-12-StartingwithFlorence2locally_0.png","tag":["Tech"],"readingTime":7},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e플로렌스-2는 Microsoft의 고급 비전 기반 모델로, 프롬프트 기반 방식을 사용하여 다양한 비전 및 비전-언어 작업을 처리하기 위해 설계되었습니다. 로컬에서 플로렌스-2를 설정하고 실행하는 데 도움이 되는 시작 스크립트가 여기 있어요.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-12-StartingwithFlorence2locally_0.png\" alt=\"이미지\"\u003e\u003c/p\u003e\n\u003ch1\u003e시작 스크립트\u003c/h1\u003e\n\u003cp\u003e플로렌스-2를 실행하는 데 사용할 수 있는 시작 스크립트입니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e AutoProcessor, AutoModelForCausalLM  \n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e PIL \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Image\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e requests\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e unittest.mock \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e patch\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers.dynamic_module_utils \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e get_imports\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erun_example\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003etask_prompt, text_input=\u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e text_input \u003cspan class=\"hljs-keyword\"\u003eis\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e:\n        prompt = task_prompt\n    \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n        prompt = task_prompt + text_input\n    inputs = processor(text=prompt, images=image, return_tensors=\u003cspan class=\"hljs-string\"\u003e\"pt\"\u003c/span\u003e)\n    generated_ids = model.generate(\n        input_ids=inputs[\u003cspan class=\"hljs-string\"\u003e\"input_ids\"\u003c/span\u003e].cuda(),\n        pixel_values=inputs[\u003cspan class=\"hljs-string\"\u003e\"pixel_values\"\u003c/span\u003e].cuda(),\n        max_new_tokens=\u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e,\n        early_stopping=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e,\n        do_sample=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e,\n        num_beams=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e,\n    )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e)[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n    parsed_answer = processor.post_process_generation(\n        generated_text, \n        task=task_prompt, \n        image_size=(image.width, image.height),\n    )\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e parsed_answer\n\n\u003cspan class=\"hljs-comment\"\u003e# Example usage\u003c/span\u003e\nfn = \u003cspan class=\"hljs-string\"\u003e'ray_ban_meta.jpeg'\u003c/span\u003e\nimage = Image.\u003cspan class=\"hljs-built_in\"\u003eopen\u003c/span\u003e(fn)\ntask_prompt = \u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;MORE_DETAILED_CAPTION\u003e'\u003c/span\u003e\nret = run_example(task_prompt)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(ret)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e너무 괴롭히지 마세요\u003c/h2\u003e\n\u003cp\u003e만약 지역에서 실행하는 모든 시도 끝에도 이 예외를 받게 된다면: \u003ccode\u003epip install flash_attn\u003c/code\u003e를 실행해 보세요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003eFile \u003cspan class=\"hljs-string\"\u003e\"C:\\Users\\alex_\\aichat\\florence2_vision\\myenv\\lib\\site-packages\\transformers\\dynamic_module_utils.py\"\u003c/span\u003e, line \u003cspan class=\"hljs-number\"\u003e182\u003c/span\u003e, \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e check_imports\n    \u003cspan class=\"hljs-keyword\"\u003eraise\u003c/span\u003e ImportError(\nImportError: 이 모델링 파일은 환경에 없는 다음 패키지가 필요합니다: flash_attn. `pip install flash_attn`을 실행해 보세요.\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e상태 코드를 Markdown 형식으로 변경하십시오.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title class_\"\u003eFile\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"C:\\Users\\alex_\\aichat\\florence2_vision\\myenv\\lib\\site-packages\\flash_attn\\flash_attn_interface.py\"\u003c/span\u003e, line \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u0026#x3C;\u003cspan class=\"hljs-variable language_\"\u003emodule\u003c/span\u003e\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e flash_attn_2_cuda \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e flash_attn_cuda\n\u003cspan class=\"hljs-title class_\"\u003eImportError\u003c/span\u003e: \u003cspan class=\"hljs-variable constant_\"\u003eDLL\u003c/span\u003e load failed \u003cspan class=\"hljs-keyword\"\u003ewhile\u003c/span\u003e importing \u003cspan class=\"hljs-attr\"\u003eflash_attn_2_cuda\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eThe\u003c/span\u003e specified procedure could not be found.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e청소한 상태에서 Miniconda를 사용하여 시작해 보세요.\u003c/p\u003e\n\u003ch1\u003e깔끔한 환경 설정\u003c/h1\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e다음과 같이 새로운 conda 환경을 만들어 보세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003econda create -n florence2 python=\u003cspan class=\"hljs-number\"\u003e3.11\u003c/span\u003e -y\nconda activate florence2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCUDA 설치 여부 확인:\u003c/p\u003e\n\u003cp\u003eCUDA가 설치되어 있는지 확인해주세요:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003envcc --version\n\n\u003cspan class=\"hljs-attr\"\u003envcc\u003c/span\u003e: \u003cspan class=\"hljs-variable constant_\"\u003eNVIDIA\u003c/span\u003e (R) \u003cspan class=\"hljs-title class_\"\u003eCuda\u003c/span\u003e 컴파일러 드라이버\n\u003cspan class=\"hljs-title class_\"\u003eCopyright\u003c/span\u003e (c) \u003cspan class=\"hljs-number\"\u003e2005\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e2024\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eNVIDIA\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eCorporation\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e2024\u003c/span\u003e년 \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e월 \u003cspan class=\"hljs-number\"\u003e17\u003c/span\u003e일 수요일에 빌드됨\n\u003cspan class=\"hljs-title class_\"\u003eCuda\u003c/span\u003e 컴파일 도구, 릴리즈 \u003cspan class=\"hljs-number\"\u003e12.5\u003c/span\u003e, \u003cspan class=\"hljs-variable constant_\"\u003eV12\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.5\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.40\u003c/span\u003e\n빌드 cuda_12\u003cspan class=\"hljs-number\"\u003e.5\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003er12\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.5\u003c/span\u003e/compiler\u003cspan class=\"hljs-number\"\u003e.34177558_0\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCUDA 경로 설정:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003eset \u003cspan class=\"hljs-string\"\u003e\"CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\"\u003c/span\u003e\nset \u003cspan class=\"hljs-string\"\u003e\"CUDA_HOME=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePyTorch 및 종속성 설치하기:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epip install torch torchvision torchaudio --index-url \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//download.pytorch.org/whl/cu121\u003c/span\u003e\npip install transformers einops timm\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003eFlash 어텐션 불필요\u003c/h1\u003e\n\u003cp\u003etransformers 의존성에서 요구되지도 않고 알려진 문제입니다.\u003c/p\u003e\n\u003cp\u003e이것이 해결책이에요:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# \u003cspan class=\"hljs-title class_\"\u003eWith\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003ePython\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e3.11\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.7\u003c/span\u003e, transformers==\u003cspan class=\"hljs-number\"\u003e4.36\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.2\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e unittest.\u003cspan class=\"hljs-property\"\u003emock\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e patch\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAutoModelForCausalLM\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers.\u003cspan class=\"hljs-property\"\u003edynamic_module_utils\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e get_imports\n\n\ndef \u003cspan class=\"hljs-title function_\"\u003efixed_get_imports\u003c/span\u003e(\u003cspan class=\"hljs-attr\"\u003efilename\u003c/span\u003e: str | os.\u003cspan class=\"hljs-property\"\u003ePathLike\u003c/span\u003e) -\u003e list[str]:\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"https://huggingface.co/microsoft/phi-1_5/discussions/72을 위한 해결책.\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e not \u003cspan class=\"hljs-title function_\"\u003estr\u003c/span\u003e(filename).\u003cspan class=\"hljs-title function_\"\u003eendswith\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"/modeling_phi.py\"\u003c/span\u003e):\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eget_imports\u003c/span\u003e(filename)\n    imports = \u003cspan class=\"hljs-title function_\"\u003eget_imports\u003c/span\u003e(filename)\n    imports.\u003cspan class=\"hljs-title function_\"\u003eremove\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"flash_attn\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e imports\n\n\n\u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003epatch\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"transformers.dynamic_module_utils.get_imports\"\u003c/span\u003e, fixed_get_imports):\n    model = \u003cspan class=\"hljs-title class_\"\u003eAutoModelForCausalLM\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003efrom_pretrained\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"microsoft/phi-1_5\"\u003c/span\u003e, trust_remote_code=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003eOCR 테스트\u003c/h1\u003e\n\u003ch2\u003e입력 이미지\u003c/h2\u003e\n\u003cimg src=\"/TIL/assets/img/2024-07-12-StartingwithFlorence2locally_1.png\"\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e다음은 스크립트를 실행한 후의 업데이트 내용입니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-attr\"\u003eD\u003c/span\u003e:\\\u003cspan class=\"hljs-variable constant_\"\u003eDEV\u003c/span\u003e\\\u003cspan class=\"hljs-variable constant_\"\u003eMODELS\u003c/span\u003e\\modules\\transformers_modules\\microsoft\\\u003cspan class=\"hljs-title class_\"\u003eFlorence\u003c/span\u003e-\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e-large-ft\\3112cd2e25c969cfdcb600a01489c56737d943d3\\modeling_florence2.\u003cspan class=\"hljs-property\"\u003epy\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e1209\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eUserWarning\u003c/span\u003e: 1Torch was not compiled \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e flash attention. (\u003cspan class=\"hljs-title class_\"\u003eTriggered\u003c/span\u003e internally at ..\\aten\\src\\\u003cspan class=\"hljs-title class_\"\u003eATen\u003c/span\u003e\\native\\transformers\\cuda\\sdp_utils.\u003cspan class=\"hljs-property\"\u003ecpp\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e455.\u003c/span\u003e)\n  attn_output = torch.\u003cspan class=\"hljs-property\"\u003enn\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003efunctional\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003escaled_dot_product_attention\u003c/span\u003e(\n{\u003cspan class=\"hljs-string\"\u003e'\u0026#x3C;OCR\u003e'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'2310Z8MOOWN - RW4008 6015IC 26243-4003RCID 2AYOA-403'\u003c/span\u003e}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e잘 작동합니다! 좀 그렇지만요. 일부 숫자가 누락되었지만, 그건 다음에 다시 이야기할 주제입니다.\u003c/p\u003e\n\u003ch1\u003e마지막으로\u003c/h1\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이 단계를 따르면 어려움 없이 Florence-2를 로컬에서 실행할 수 있을 것입니다. 다른 문제가 발생하면 모든 의존성이 올바르게 설치되어 있는지, 환경이 올바르게 구성되어 있는지 확인해주세요.\u003c/p\u003e\n\u003ch1\u003eHugging Face Spaces\u003c/h1\u003e\n\u003cp\u003e환경을 로컬로 설정하기를 원치 않는다면 Hugging Face Spaces를 사용하여 Florence-2를 실행할 수도 있습니다. 이는 로컬 구성이 필요 없이 모델에 액세스할 수 있는 클라우드 기반 솔루션을 제공합니다. Hugging Face의 Florence-2 스페이스를 확인해보세요: Hugging Face Spaces\u003c/p\u003e\n\u003ch1\u003e출처\u003c/h1\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-07-12-StartingwithFlorence2locally"},"buildId":"jKAIrnIuHBv4ZHjiQbX6i","assetPrefix":"/TIL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>