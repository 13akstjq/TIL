<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>iTransformer 최신 시계열 예측 기법 대공개 | TIL</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://13akstjq.github.io/TIL//post/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="iTransformer 최신 시계열 예측 기법 대공개 | TIL" data-gatsby-head="true"/><meta property="og:title" content="iTransformer 최신 시계열 예측 기법 대공개 | TIL" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://TIL.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://13akstjq.github.io/TIL//post/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting" data-gatsby-head="true"/><meta name="twitter:title" content="iTransformer 최신 시계열 예측 기법 대공개 | TIL" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | TIL" data-gatsby-head="true"/><meta name="article:published_time" content="2024-07-09 19:21" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/TIL/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/TIL/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/TIL/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/TIL/favicons/favicon-96x96.png"/><link rel="icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/TIL/favicons/browserconfig.xml"/><link rel="preload" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/TIL/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/TIL/_next/static/chunks/webpack-21ffe88bdca56cba.js" defer=""></script><script src="/TIL/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/TIL/_next/static/chunks/main-a5eeabb286676ce6.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/TIL/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/TIL/_next/static/chunks/348-02483b66b493dd81.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/post/%5Bslug%5D-5ecfd58aae5a7e3d.js" defer=""></script><script src="/TIL/_next/static/Suu-uTE6tpVjS7rqQHkw3/_buildManifest.js" defer=""></script><script src="/TIL/_next/static/Suu-uTE6tpVjS7rqQHkw3/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/TIL">TIL</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/TIL/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">iTransformer 최신 시계열 예측 기법 대공개</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="iTransformer 최신 시계열 예측 기법 대공개" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/TIL/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">TIL</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jul 9, 2024</span><span class="posts_reading_time__f7YPP">19<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_0.png" alt="2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting"></p>
<p>예측 분야에서는 Lag-LLaMA, Time-LLM, Chronos, Moirai와 같은 모델들이 2024년 초부터 제안되어 기초 모델 분야에서 많은 활동을 보이고 있습니다.</p>
<p>그러나 이러한 모델들의 성능은 조금 아쉬운 면이 있습니다 (<a href="%EC%97%AC%EA%B8%B0">재현 가능한 벤치마크를 보려면 여기를 참조하십시오</a>) 그리고 저는 데이터 특화 모델이 여전히 현재 최적의 해결책이라고 믿습니다.</p>
<p>이에 따라 Transformer 아키텍처가 다양한 형태로 시계열 예측에 적용되어왔으며, PatchTST는 장기 예측에서 최고 수준의 성능을 달성하였습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>도전적인 PatchTST에 이어 2024년 3월에 제안된 iTransformer 모델이 등장했습니다. 논문 "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting"에서 소개되었습니다.</p>
<p>이 기사에서는 iTransformer의 놀라운 간단한 개념을 발견하고 그 아키텍처를 탐구합니다. 그런 다음 해당 모델을 소규모 실험에 적용하고 그 성능을 TSMixer, N-HiTS 및 PatchTST와 비교합니다.</p>
<p>더 자세한 내용은 원본 논문을 읽어보세요.</p>
<p>시작해봅시다!</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>iTransformer 탐색</h1>
<p>iTransformer의 아이디어는 바닐라 Transformer 모델이 시간 토큰을 사용한다는 깨달음에서 나왔어요.</p>
<p>이것은 모델이 단일 시간 단계에서 모든 특징을 살펴본다는 것을 의미합니다. 그래서 모델이 한 번에 한 시간 단계씩 살펴볼 때 시간 의존성을 학습하는 것이 어려울 수 있어요.</p>
<p>그 문제에 대한 해결책은 PatchTST 모델과 함께 제안된 패칭이에요. 패칭을 사용하면 토큰화하고 임베딩하기 전에 시간 지점을 단순히 그룹화할 수 있어요. 아래에서 보여준 것처럼요.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p><img src="/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_1.png" alt="iTransformer image 1"></p>
<p>In iTransformer, we push patching to the extreme by simply applying the model on the inverted dimensions.</p>
<p><img src="/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_2.png" alt="iTransformer image 2"></p>
<p>In the figure above, we can see how the iTransformer differs from the vanilla Transformer. Instead of looking at all features at one time step, it looks at one feature across many time steps. This is done simply by inverting the shape of the input.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이렇게하면 어텐션 레이어가 다변량 상관 관계를 학습하고 피드포워드 네트워크가 전체 입력 시퀀스의 표현을 인코딩합니다.</p>
<p>iTransformer의 일반 아이디어를 이해했으니, 이제 더 자세히 살펴보겠습니다.</p>
<h2>iTransformer의 아키텍처</h2>
<p>iTransformer는 2017년에 Attention Is All You Need에서 처음으로 제안된 임베딩, 프로젝션 및 트랜스포머 블록을 사용한 바닐라 인코더-디코더 아키텍처를 채택합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>위의 그림에서 건물 블록들은 동일하지만 기능은 완전히 다르다는 것을 볼 수 있습니다. 좀 더 자세히 살펴보겠습니다.</p>
<p>임베딩 레이어</p>
<p>먼저, 입력 시리즈는 독립적으로 토큰으로 임베딩됩니다. 다시 말해서, 이는 입력의 서브시퀀스를 토큰화하는 대신, 모델이 전체 입력 시퀀스를 토큰화하는 극단적인 경우와 같습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>다변량 주의력</p>
<p>그런 다음, 임베딩은 주의층으로 전송되어 다변량 상관 맵을 학습할 것입니다.</p>
<p>이는 역전 모델이 각 특징을 독립된 프로세스로 간주하기 때문에 가능합니다. 이러한 결과로 주의 메커니즘은 특징들 사이의 상관 관계를 학습하게 되며, 이로써 iTransformer는 특히 다변량 예측 작업에 적합합니다.</p>
<p>층 정규화</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>어텐션 레이어의 출력은 정규화 레이어로 전송됩니다.</p>
<p>전통적인 트랜스포머 아키텍처에서는 정규화가 모든 특성에 대해 고정된 타임스탬프에서 이루어집니다. 이는 모델이 쓸모없는 관계를 학습하게 될 수 있는 상호작용 소음을 도입할 수 있습니다. 또한, 지나치게 매끄러운 신호를 초래할 수 있습니다.</p>
<p>반면, iTransformer는 차원을 뒤집으므로 정규화가 타임스탬프를 횡단하여 이루어집니다. 이는 모델이 비정상적인 시계열에 대처하도록 도와주며, 시계열의 소음을 줄여줍니다.</p>
<p>피드포워드 네트워크</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>마지막으로, 피드 포워드 네트워크(FFN)는 들어오는 토큰의 깊은 표현을 학습합니다.</p>
<p>다시 말해서, 모양이 반전되어 있기 때문에 다층 퍼셉트론(MLP)은 주기성이나 진폭과 같은 다른 시계열 속성을 학습할 수 있습니다. 이는 MLP 기반 모델(N-BEATS, N-HiTS, TSMixer 등)의 능력을 모방합니다.</p>
<p>프로젝션</p>
<p>여기서 간단히 많은 블록을 쌓는 것으로 이루어진 단계입니다:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<ul>
<li>주의 층</li>
<li>계층 정규화</li>
<li>피드포워드 네트워크</li>
<li>계층 정규화</li>
</ul>
<p>각 블록은 입력 시리즈의 다른 표현을 학습합니다. 그런 다음, 블록 스택의 출력은 최종 예측을 얻기 위해 선형 투사 단계를 거쳐 전송됩니다.</p>
<p>요약하자면, iTransformer는 새로운 아키텍처가 아니며 Transformer를 새롭게 만들어내지는 않습니다. 단순히 입력의 역된 차원에 Transformer를 적용하여 모델이 다변량 상관 관계를 학습하고 시간적 특성을 포착할 수 있도록 합니다.</p>
<p>이제 iTransformer 모델에 대한 깊은 이해를 갖고 작은 예측 실험에서 적용해 보겠습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>iTransformer를 사용한 예측</h1>
<p>이 작은 실험에서는 Creative Commons 라이선스로 공개된 전기 변압기 데이터셋에 iTransformer 모델을 적용합니다.</p>
<p>중국 한 성의 두 지역에서 전기 변압기의 오일 온도를 추적하는 인기 있는 벤치마크 데이터셋입니다. 두 지역 모두 1시간마다 샘플링된 데이터셋을 가지고 있으며, 15분마다 샘플링된 데이터셋이 있어 총 네 개의 데이터셋이 있습니다.</p>
<p>iTransformer는 근본적으로 다변량 모델이지만, 우리는 96개의 시간 단계에 걸친 일변량 예측 능력을 테스트합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이 실험에 대한 코드는 GitHub에서 확인할 수 있어요.</p>
<p>자, 시작해봅시다!</p>
<p>초기 설정</p>
<p>이 실험에서는 neuralforecast라는 라이브러리를 사용하는데, 이 라이브러리가 딥러닝 방법의 가장 빠르고 직관적인 사용 가능한 구현을 제공한다고 믿습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.<span class="hljs-property">pyplot</span> <span class="hljs-keyword">as</span> plt

<span class="hljs-keyword">from</span> datasetsforecast.<span class="hljs-property">long_horizon</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">LongHorizon</span>

<span class="hljs-keyword">from</span> neuralforecast.<span class="hljs-property">core</span> <span class="hljs-keyword">import</span> <span class="hljs-title class_">NeuralForecast</span>
<span class="hljs-keyword">from</span> neuralforecast.<span class="hljs-property">models</span> <span class="hljs-keyword">import</span> <span class="hljs-variable constant_">NHITS</span>, <span class="hljs-title class_">PatchTST</span>, iTransformer, <span class="hljs-title class_">TSMixer</span>
</code></pre>
<p>본 글을 작성하는 시점에서 iTransformer가 아직 neuralforecast의 공개 릴리스에 포함되지 않았음을 참고하세요. 즉시 해당 모델에 액세스하려면 다음을 실행하세요:</p>
<pre><code class="hljs language-js">pip install git+<span class="hljs-attr">https</span>:<span class="hljs-comment">//github.com/Nixtla/neuralforecast.git</span>
</code></pre>
<p>이제 ETT 데이터셋을로드하고, 검증 크기, 테스트 크기, 그리고 주기를 포함하는 함수를 작성해봅시다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">def <span class="hljs-title function_">load_data</span>(name):
    <span class="hljs-keyword">if</span> name == <span class="hljs-string">"ettm1"</span>:
        Y_df, *_ = <span class="hljs-title class_">LongHorizon</span>.<span class="hljs-title function_">load</span>(directory=<span class="hljs-string">'./'</span>, group=<span class="hljs-string">'ETTm1'</span>)
        Y_df = Y_df[Y_df[<span class="hljs-string">'unique_id'</span>] == <span class="hljs-string">'OT'</span>]
        Y_df[<span class="hljs-string">'ds'</span>] = pd.<span class="hljs-title function_">to_datetime</span>(Y_df[<span class="hljs-string">'ds'</span>])
        val_size = <span class="hljs-number">11520</span>
        test_size = <span class="hljs-number">11520</span>
        freq = <span class="hljs-string">'15T'</span>
    elif name == <span class="hljs-string">"ettm2"</span>:
        Y_df, *_ = <span class="hljs-title class_">LongHorizon</span>.<span class="hljs-title function_">load</span>(directory=<span class="hljs-string">'./'</span>, group=<span class="hljs-string">'ETTm2'</span>)
        Y_df = Y_df[Y_df[<span class="hljs-string">'unique_id'</span>] == <span class="hljs-string">'OT'</span>]
        Y_df[<span class="hljs-string">'ds'</span>] = pd.<span class="hljs-title function_">to_datetime</span>(Y_df[<span class="hljs-string">'ds'</span>])
        val_size = <span class="hljs-number">11520</span>
        test_size = <span class="hljs-number">11520</span>
        freq = <span class="hljs-string">'15T'</span>
    elif name == <span class="hljs-string">'etth1'</span>:
        Y_df, *_ = <span class="hljs-title class_">LongHorizon</span>.<span class="hljs-title function_">load</span>(directory=<span class="hljs-string">'./'</span>, group=<span class="hljs-string">'ETTh1'</span>)
        Y_df[<span class="hljs-string">'ds'</span>] = pd.<span class="hljs-title function_">to_datetime</span>(Y_df[<span class="hljs-string">'ds'</span>])
        val_size = <span class="hljs-number">2880</span>
        test_size = <span class="hljs-number">2880</span>
        freq = <span class="hljs-string">'H'</span>
    elif name == <span class="hljs-string">"etth2"</span>:
        Y_df, *_ = <span class="hljs-title class_">LongHorizon</span>.<span class="hljs-title function_">load</span>(directory=<span class="hljs-string">'./'</span>, group=<span class="hljs-string">'ETTh2'</span>)
        Y_df[<span class="hljs-string">'ds'</span>] = pd.<span class="hljs-title function_">to_datetime</span>(Y_df[<span class="hljs-string">'ds'</span>])
        val_size = <span class="hljs-number">2880</span>
        test_size = <span class="hljs-number">2880</span>
        freq = <span class="hljs-string">'H'</span>

    <span class="hljs-keyword">return</span> Y_df, val_size, test_size, freq
</code></pre>
<p>The above function conveniently loads the data in the required format for neuralforecast. It includes a unique_id column to identify time series, a ds column for timestamps, and a y column for series values.</p>
<p>Please note that the validation and test sizes align with standards in the scientific community for publishing research papers.</p>
<p>We are all set to start training the models.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>훈련 및 예측</h2>
<p>iTransformer 모델을 훈련시키기 위해서는 단순히 다음을 지정해주면 됩니다:</p>
<ul>
<li>예측 기간</li>
<li>입력 크기</li>
<li>시리즈 수</li>
</ul>
<p>iTransformer가 본질적으로 다변량 모델이기 때문에 모델을 적합할 때 시리즈 수를 지정해야 합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>단변량 시나리오이므로 n_series=1입니다.</p>
<pre><code class="hljs language-js"><span class="hljs-title function_">iTransformer</span>(
  (h = horizon),
  (input_size = <span class="hljs-number">3</span> * horizon),
  (n_series = <span class="hljs-number">1</span>),
  (max_steps = <span class="hljs-number">1000</span>),
  (early_stop_patience_steps = <span class="hljs-number">3</span>)
);
</code></pre>
<p>위의 코드 블록에서는 최대 학습 단계 수를 지정하고, 과적합을 방지하기 위해 조기 중지를 3번 반복으로 설정합니다.</p>
<p>나머지 모델들에 대해 같은 작업을 수행한 후, 리스트에 넣어줍니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">horizon = <span class="hljs-number">96</span>;

models = [
  <span class="hljs-title function_">iTransformer</span>(
    (h = horizon),
    (input_size = <span class="hljs-number">3</span> * horizon),
    (n_series = <span class="hljs-number">1</span>),
    (max_steps = <span class="hljs-number">1000</span>),
    (early_stop_patience_steps = <span class="hljs-number">3</span>)
  ),
  <span class="hljs-title class_">TSMixer</span>(
    (h = horizon),
    (input_size = <span class="hljs-number">3</span> * horizon),
    (n_series = <span class="hljs-number">1</span>),
    (max_steps = <span class="hljs-number">1000</span>),
    (early_stop_patience_steps = <span class="hljs-number">3</span>)
  ),
  <span class="hljs-title function_">NHITS</span>((h = horizon), (input_size = <span class="hljs-number">3</span> * horizon), (max_steps = <span class="hljs-number">1000</span>), (early_stop_patience_steps = <span class="hljs-number">3</span>)),
  <span class="hljs-title class_">PatchTST</span>((h = horizon), (input_size = <span class="hljs-number">3</span> * horizon), (max_steps = <span class="hljs-number">1000</span>), (early_stop_patience_steps = <span class="hljs-number">3</span>)),
];
</code></pre>
<p>좋아요! 이제 우리는 단순히 NeuralForecast 객체를 초기화하면 되는데, 이 객체는 학습, 교차 검증 및 예측을 위한 메서드에 액세스할 수 있게 해줍니다.</p>
<pre><code class="hljs language-js">nf = <span class="hljs-title class_">NeuralForecast</span>((models = models), (freq = freq));
nf_preds = nf.<span class="hljs-title function_">cross_validation</span>((df = Y_df), (val_size = val_size), (test_size = test_size), (n_windows = <span class="hljs-title class_">None</span>));
</code></pre>
<p>마지막으로, 우리는 각 모델의 성능을 utilsforecast 라이브러리를 사용하여 평가합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> utilsforecast.<span class="hljs-property">losses</span> <span class="hljs-keyword">import</span> mae, mse
<span class="hljs-keyword">from</span> utilsforecast.<span class="hljs-property">evaluation</span> <span class="hljs-keyword">import</span> evaluate

ettm1_evaluation = evaluate(df=nf_preds, metrics=[mae, mse], models=[<span class="hljs-string">'iTransformer'</span>, <span class="hljs-string">'TSMixer'</span>, <span class="hljs-string">'NHITS'</span>, <span class="hljs-string">'PatchTST'</span>])
ettm1_evaluation.<span class="hljs-title function_">to_csv</span>(<span class="hljs-string">'ettm1_results.csv'</span>, index=<span class="hljs-title class_">False</span>, header=<span class="hljs-title class_">True</span>)
</code></pre>
<p>이 단계는 모든 데이터셋에 대해 반복됩니다. 이 실험을 실행하는 완전한 함수는 아래에 표시됩니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">from</span> utilsforecast.<span class="hljs-property">losses</span> <span class="hljs-keyword">import</span> mae, mse
<span class="hljs-keyword">from</span> utilsforecast.<span class="hljs-property">evaluation</span> <span class="hljs-keyword">import</span> evaluate

datasets = [<span class="hljs-string">'ettm1'</span>, <span class="hljs-string">'ettm2'</span>, <span class="hljs-string">'etth1'</span>, <span class="hljs-string">'etth2'</span>]

<span class="hljs-keyword">for</span> dataset <span class="hljs-keyword">in</span> <span class="hljs-attr">datasets</span>:

    Y_df, val_size, test_size, freq = <span class="hljs-title function_">load_data</span>(dataset)

    horizon = <span class="hljs-number">96</span>

    models = [
        <span class="hljs-title function_">iTransformer</span>(h=horizon, input_size=<span class="hljs-number">3</span>*horizon, n_series=<span class="hljs-number">1</span>, max_steps=<span class="hljs-number">1000</span>, early_stop_patience_steps=<span class="hljs-number">3</span>),
        <span class="hljs-title class_">TSMixer</span>(h=horizon, input_size=<span class="hljs-number">3</span>*horizon, n_series=<span class="hljs-number">1</span>, max_steps=<span class="hljs-number">1000</span>, early_stop_patience_steps=<span class="hljs-number">3</span>),
        <span class="hljs-title function_">NHITS</span>(h=horizon, input_size=<span class="hljs-number">3</span>*horizon, max_steps=<span class="hljs-number">1000</span>, early_stop_patience_steps=<span class="hljs-number">3</span>),
        <span class="hljs-title class_">PatchTST</span>(h=horizon, input_size=<span class="hljs-number">3</span>*horizon, max_steps=<span class="hljs-number">1000</span>, early_stop_patience_steps=<span class="hljs-number">3</span>)
    ]

    nf = <span class="hljs-title class_">NeuralForecast</span>(models=models, freq=freq)
    nf_preds = nf.<span class="hljs-title function_">cross_validation</span>(df=Y_df, val_size=val_size, test_size=test_size, n_windows=<span class="hljs-title class_">None</span>)
    nf_preds = nf_preds.<span class="hljs-title function_">reset_index</span>()

    evaluation = evaluate(df=nf_preds, metrics=[mae, mse], models=[<span class="hljs-string">'iTransformer'</span>, <span class="hljs-string">'TSMixer'</span>, <span class="hljs-string">'NHITS'</span>, <span class="hljs-string">'PatchTST'</span>])
    evaluation.<span class="hljs-title function_">to_csv</span>(f<span class="hljs-string">'{dataset}_results.csv'</span>, index=<span class="hljs-title class_">False</span>, header=<span class="hljs-title class_">True</span>)
</code></pre>
<p>이 작업을 완료하면 모든 데이터셋에 대해 모든 모델의 예측이 있게 됩니다. 그런 다음 평가로 넘어갈 수 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>성능 평가</h2>
<p>성능 지표를 모두 CSV 파일에 저장했으므로, pandas를 사용하여 이를 읽고 각 모델의 각 데이터셋에 대한 성능을 그릴 수 있습니다.</p>
<pre><code class="hljs language-python">files = [<span class="hljs-string">'etth1_results.csv'</span>, <span class="hljs-string">'etth2_results.csv'</span>, <span class="hljs-string">'ettm1_results.csv'</span>, <span class="hljs-string">'ettm2_results.csv'</span>]
datasets = [<span class="hljs-string">'etth1'</span>, <span class="hljs-string">'etth2'</span>, <span class="hljs-string">'ettm1'</span>, <span class="hljs-string">'ettm2'</span>]

dataframes = []

<span class="hljs-keyword">for</span> file, dataset <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(files, datasets):
    df = pd.read_csv(file)
    df[<span class="hljs-string">'dataset'</span>] = dataset

    dataframes.append(df)

full_df = pd.concat(dataframes, ignore_index=<span class="hljs-literal">True</span>)
full_df = full_df.drop([<span class="hljs-string">'unique_id'</span>], axis=<span class="hljs-number">1</span>)
</code></pre>
<p>이후, 지표를 그래프로 그리려면:</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

dataset_names = full_df[<span class="hljs-string">'dataset'</span>].unique()
model_names = [<span class="hljs-string">'iTransformer'</span>, <span class="hljs-string">'TSMixer'</span>, <span class="hljs-string">'NHITS'</span>, <span class="hljs-string">'PatchTST'</span>]

fig, axs = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">15</span>))
bar_width = <span class="hljs-number">0.35</span>

axs = axs.flatten()

<span class="hljs-keyword">for</span> i, dataset_name <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataset_names):
    df_subset = full_df[(full_df[<span class="hljs-string">'dataset'</span>] == dataset_name) &#x26; (full_df[<span class="hljs-string">'metric'</span>] == <span class="hljs-string">'mae'</span>)]
    mae_vals = df_subset[model_names].values.flatten()
    df_subset = full_df[(full_df[<span class="hljs-string">'dataset'</span>] == dataset_name) &#x26; (full_df[<span class="hljs-string">'metric'</span>] == <span class="hljs-string">'mse'</span>)]
    mse_vals = df_subset[model_names].values.flatten()

    indices = np.arange(<span class="hljs-built_in">len</span>(model_names))

    bars_mae = axs[i].bar(indices - bar_width / <span class="hljs-number">2</span>, mae_vals, bar_width, color=<span class="hljs-string">'skyblue'</span>, label=<span class="hljs-string">'MAE'</span>)
    bars_mse = axs[i].bar(indices + bar_width / <span class="hljs-number">2</span>, mse_vals, bar_width, color=<span class="hljs-string">'orange'</span>, label=<span class="hljs-string">'MSE'</span>)

    <span class="hljs-keyword">for</span> bars <span class="hljs-keyword">in</span> [bars_mae, bars_mse]:
        <span class="hljs-keyword">for</span> bar <span class="hljs-keyword">in</span> bars:
            height = bar.get_height()
            axs[i].annotate(<span class="hljs-string">f'<span class="hljs-subst">{height:<span class="hljs-number">.2</span>f}</span>'</span>,
                            xy=(bar.get_x() + bar.get_width() / <span class="hljs-number">2</span>, height),
                            xytext=(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>),
                            textcoords=<span class="hljs-string">"offset points"</span>,
                            ha=<span class="hljs-string">'center'</span>, va=<span class="hljs-string">'bottom'</span>)

    axs[i].set_xticks(indices)
    axs[i].set_xticklabels(model_names, rotation=<span class="hljs-number">45</span>)
    axs[i].set_title(dataset_name)
    axs[i].legend(loc=<span class="hljs-string">'best'</span>)

plt.tight_layout()
</code></pre>
<p><img src="/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_4.png" alt="Image"></p>
<p>From the figure above, we can see that the iTransformer performs fairly well on all datasets, but TSMixer is overall slightly better than iTransformer, and PatchTST is the overall champion model in this experiment.</p>
<p>Of course, keep in mind that we did not leverage the multivariate capabilities of iTransformer, and we only tested on a single forecast horizon. Therefore, it is not a complete assessment of the iTransformer’s performance.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>그럼에도 불구하고, 모델이 PatchTST와 매우 유사하게 수행되는 것을 볼 때, Transformer를 사용한 시계열 예측에서 새로운 성능에 도달하는 데 그룹화 시간 단계를 토큰화하기 전에 묶는 아이디어를 더 지원하는 점이 흥미로운 부분입니다.</p>
<h1>결론</h1>
<p>iTransformer는 베이닐라 Transformer 아키텍처를 적용한 뒤 입력 시리즈의 역방향 모양으로 그냥 적용합니다.</p>
<p>이렇게 하면 전체 시리즈가 토큰화되고 PatchTST에서 제안한 것과 같이 극단적인 케이스를 모방합니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>모델이 주의 매커니즘을 사용하여 다변량 상관 관계를 학습하고, 피드포워드 네트워크가 시계열의 시간적 특성을 학습합니다.</p>
<p>iTransformer는 많은 벤치마크 데이터셋에서 장기 예측에 대한 최신 기술을 보여주었으며, 우리의 한정된 실험에서는 PatchTST가 전반적으로 가장 우수한 성과를 보였습니다.</p>
<p>모든 문제는 고유한 해결책이 필요하다고 단언합니다. 이제 iTransformer를 도구 상자에 추가하고 여러분의 프로젝트에 적용할 수 있습니다.</p>
<p>읽어 주셔서 감사합니다! 즐겁게 읽으셨기를 바라며 새로운 지식을 얻으셨기를 기대합니다!</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>만나서 반가워요 🌟</p>
<h1>저를 지원해주세요</h1>
<p>제 작업을 즐기고 계신가요? Buy me a coffee로 제게 지원을 표현해주세요. 여러분의 응원을 받으면 저는 커피 한 잔을 즐길 수 있어요! 만약 그렇게 느끼신다면, 아래 버튼을 클릭해주세요 👇</p>
<p><img src="/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_5.png" alt="Image"></p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>참고 자료</h1>
<p>iTransformer: Inverted Transformers Are Effective for Time Series Forecasting by Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, Mingsheng Long</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"iTransformer 최신 시계열 예측 기법 대공개","description":"","date":"2024-07-09 19:21","slug":"2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting","content":"\n![2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting](/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_0.png)\n\n예측 분야에서는 Lag-LLaMA, Time-LLM, Chronos, Moirai와 같은 모델들이 2024년 초부터 제안되어 기초 모델 분야에서 많은 활동을 보이고 있습니다.\n\n그러나 이러한 모델들의 성능은 조금 아쉬운 면이 있습니다 ([재현 가능한 벤치마크를 보려면 여기를 참조하십시오](여기)) 그리고 저는 데이터 특화 모델이 여전히 현재 최적의 해결책이라고 믿습니다.\n\n이에 따라 Transformer 아키텍처가 다양한 형태로 시계열 예측에 적용되어왔으며, PatchTST는 장기 예측에서 최고 수준의 성능을 달성하였습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n도전적인 PatchTST에 이어 2024년 3월에 제안된 iTransformer 모델이 등장했습니다. 논문 \"iTransformer: Inverted Transformers Are Effective for Time Series Forecasting\"에서 소개되었습니다.\n\n이 기사에서는 iTransformer의 놀라운 간단한 개념을 발견하고 그 아키텍처를 탐구합니다. 그런 다음 해당 모델을 소규모 실험에 적용하고 그 성능을 TSMixer, N-HiTS 및 PatchTST와 비교합니다.\n\n더 자세한 내용은 원본 논문을 읽어보세요.\n\n시작해봅시다!\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# iTransformer 탐색\n\niTransformer의 아이디어는 바닐라 Transformer 모델이 시간 토큰을 사용한다는 깨달음에서 나왔어요.\n\n이것은 모델이 단일 시간 단계에서 모든 특징을 살펴본다는 것을 의미합니다. 그래서 모델이 한 번에 한 시간 단계씩 살펴볼 때 시간 의존성을 학습하는 것이 어려울 수 있어요.\n\n그 문제에 대한 해결책은 PatchTST 모델과 함께 제안된 패칭이에요. 패칭을 사용하면 토큰화하고 임베딩하기 전에 시간 지점을 단순히 그룹화할 수 있어요. 아래에서 보여준 것처럼요.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n![iTransformer image 1](/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_1.png)\n\nIn iTransformer, we push patching to the extreme by simply applying the model on the inverted dimensions.\n\n![iTransformer image 2](/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_2.png)\n\nIn the figure above, we can see how the iTransformer differs from the vanilla Transformer. Instead of looking at all features at one time step, it looks at one feature across many time steps. This is done simply by inverting the shape of the input.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이렇게하면 어텐션 레이어가 다변량 상관 관계를 학습하고 피드포워드 네트워크가 전체 입력 시퀀스의 표현을 인코딩합니다.\n\niTransformer의 일반 아이디어를 이해했으니, 이제 더 자세히 살펴보겠습니다.\n\n## iTransformer의 아키텍처\n\niTransformer는 2017년에 Attention Is All You Need에서 처음으로 제안된 임베딩, 프로젝션 및 트랜스포머 블록을 사용한 바닐라 인코더-디코더 아키텍처를 채택합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n위의 그림에서 건물 블록들은 동일하지만 기능은 완전히 다르다는 것을 볼 수 있습니다. 좀 더 자세히 살펴보겠습니다.\n\n임베딩 레이어\n\n먼저, 입력 시리즈는 독립적으로 토큰으로 임베딩됩니다. 다시 말해서, 이는 입력의 서브시퀀스를 토큰화하는 대신, 모델이 전체 입력 시퀀스를 토큰화하는 극단적인 경우와 같습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n다변량 주의력\n\n그런 다음, 임베딩은 주의층으로 전송되어 다변량 상관 맵을 학습할 것입니다.\n\n이는 역전 모델이 각 특징을 독립된 프로세스로 간주하기 때문에 가능합니다. 이러한 결과로 주의 메커니즘은 특징들 사이의 상관 관계를 학습하게 되며, 이로써 iTransformer는 특히 다변량 예측 작업에 적합합니다.\n\n층 정규화\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n어텐션 레이어의 출력은 정규화 레이어로 전송됩니다.\n\n전통적인 트랜스포머 아키텍처에서는 정규화가 모든 특성에 대해 고정된 타임스탬프에서 이루어집니다. 이는 모델이 쓸모없는 관계를 학습하게 될 수 있는 상호작용 소음을 도입할 수 있습니다. 또한, 지나치게 매끄러운 신호를 초래할 수 있습니다.\n\n반면, iTransformer는 차원을 뒤집으므로 정규화가 타임스탬프를 횡단하여 이루어집니다. 이는 모델이 비정상적인 시계열에 대처하도록 도와주며, 시계열의 소음을 줄여줍니다.\n\n피드포워드 네트워크\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n마지막으로, 피드 포워드 네트워크(FFN)는 들어오는 토큰의 깊은 표현을 학습합니다.\n\n다시 말해서, 모양이 반전되어 있기 때문에 다층 퍼셉트론(MLP)은 주기성이나 진폭과 같은 다른 시계열 속성을 학습할 수 있습니다. 이는 MLP 기반 모델(N-BEATS, N-HiTS, TSMixer 등)의 능력을 모방합니다.\n\n프로젝션\n\n여기서 간단히 많은 블록을 쌓는 것으로 이루어진 단계입니다:\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n- 주의 층\n- 계층 정규화\n- 피드포워드 네트워크\n- 계층 정규화\n\n각 블록은 입력 시리즈의 다른 표현을 학습합니다. 그런 다음, 블록 스택의 출력은 최종 예측을 얻기 위해 선형 투사 단계를 거쳐 전송됩니다.\n\n요약하자면, iTransformer는 새로운 아키텍처가 아니며 Transformer를 새롭게 만들어내지는 않습니다. 단순히 입력의 역된 차원에 Transformer를 적용하여 모델이 다변량 상관 관계를 학습하고 시간적 특성을 포착할 수 있도록 합니다.\n\n이제 iTransformer 모델에 대한 깊은 이해를 갖고 작은 예측 실험에서 적용해 보겠습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# iTransformer를 사용한 예측\n\n이 작은 실험에서는 Creative Commons 라이선스로 공개된 전기 변압기 데이터셋에 iTransformer 모델을 적용합니다.\n\n중국 한 성의 두 지역에서 전기 변압기의 오일 온도를 추적하는 인기 있는 벤치마크 데이터셋입니다. 두 지역 모두 1시간마다 샘플링된 데이터셋을 가지고 있으며, 15분마다 샘플링된 데이터셋이 있어 총 네 개의 데이터셋이 있습니다.\n\niTransformer는 근본적으로 다변량 모델이지만, 우리는 96개의 시간 단계에 걸친 일변량 예측 능력을 테스트합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이 실험에 대한 코드는 GitHub에서 확인할 수 있어요.\n\n자, 시작해봅시다!\n\n초기 설정\n\n이 실험에서는 neuralforecast라는 라이브러리를 사용하는데, 이 라이브러리가 딥러닝 방법의 가장 빠르고 직관적인 사용 가능한 구현을 제공한다고 믿습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom datasetsforecast.long_horizon import LongHorizon\n\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.models import NHITS, PatchTST, iTransformer, TSMixer\n```\n\n본 글을 작성하는 시점에서 iTransformer가 아직 neuralforecast의 공개 릴리스에 포함되지 않았음을 참고하세요. 즉시 해당 모델에 액세스하려면 다음을 실행하세요:\n\n```js\npip install git+https://github.com/Nixtla/neuralforecast.git\n```\n\n이제 ETT 데이터셋을로드하고, 검증 크기, 테스트 크기, 그리고 주기를 포함하는 함수를 작성해봅시다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\ndef load_data(name):\n    if name == \"ettm1\":\n        Y_df, *_ = LongHorizon.load(directory='./', group='ETTm1')\n        Y_df = Y_df[Y_df['unique_id'] == 'OT']\n        Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n        val_size = 11520\n        test_size = 11520\n        freq = '15T'\n    elif name == \"ettm2\":\n        Y_df, *_ = LongHorizon.load(directory='./', group='ETTm2')\n        Y_df = Y_df[Y_df['unique_id'] == 'OT']\n        Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n        val_size = 11520\n        test_size = 11520\n        freq = '15T'\n    elif name == 'etth1':\n        Y_df, *_ = LongHorizon.load(directory='./', group='ETTh1')\n        Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n        val_size = 2880\n        test_size = 2880\n        freq = 'H'\n    elif name == \"etth2\":\n        Y_df, *_ = LongHorizon.load(directory='./', group='ETTh2')\n        Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n        val_size = 2880\n        test_size = 2880\n        freq = 'H'\n\n    return Y_df, val_size, test_size, freq\n```\n\nThe above function conveniently loads the data in the required format for neuralforecast. It includes a unique_id column to identify time series, a ds column for timestamps, and a y column for series values.\n\nPlease note that the validation and test sizes align with standards in the scientific community for publishing research papers.\n\nWe are all set to start training the models.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 훈련 및 예측\n\niTransformer 모델을 훈련시키기 위해서는 단순히 다음을 지정해주면 됩니다:\n\n- 예측 기간\n- 입력 크기\n- 시리즈 수\n\niTransformer가 본질적으로 다변량 모델이기 때문에 모델을 적합할 때 시리즈 수를 지정해야 합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n단변량 시나리오이므로 n_series=1입니다.\n\n```js\niTransformer(\n  (h = horizon),\n  (input_size = 3 * horizon),\n  (n_series = 1),\n  (max_steps = 1000),\n  (early_stop_patience_steps = 3)\n);\n```\n\n위의 코드 블록에서는 최대 학습 단계 수를 지정하고, 과적합을 방지하기 위해 조기 중지를 3번 반복으로 설정합니다.\n\n나머지 모델들에 대해 같은 작업을 수행한 후, 리스트에 넣어줍니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nhorizon = 96;\n\nmodels = [\n  iTransformer(\n    (h = horizon),\n    (input_size = 3 * horizon),\n    (n_series = 1),\n    (max_steps = 1000),\n    (early_stop_patience_steps = 3)\n  ),\n  TSMixer(\n    (h = horizon),\n    (input_size = 3 * horizon),\n    (n_series = 1),\n    (max_steps = 1000),\n    (early_stop_patience_steps = 3)\n  ),\n  NHITS((h = horizon), (input_size = 3 * horizon), (max_steps = 1000), (early_stop_patience_steps = 3)),\n  PatchTST((h = horizon), (input_size = 3 * horizon), (max_steps = 1000), (early_stop_patience_steps = 3)),\n];\n```\n\n좋아요! 이제 우리는 단순히 NeuralForecast 객체를 초기화하면 되는데, 이 객체는 학습, 교차 검증 및 예측을 위한 메서드에 액세스할 수 있게 해줍니다.\n\n```js\nnf = NeuralForecast((models = models), (freq = freq));\nnf_preds = nf.cross_validation((df = Y_df), (val_size = val_size), (test_size = test_size), (n_windows = None));\n```\n\n마지막으로, 우리는 각 모델의 성능을 utilsforecast 라이브러리를 사용하여 평가합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\nfrom utilsforecast.losses import mae, mse\nfrom utilsforecast.evaluation import evaluate\n\nettm1_evaluation = evaluate(df=nf_preds, metrics=[mae, mse], models=['iTransformer', 'TSMixer', 'NHITS', 'PatchTST'])\nettm1_evaluation.to_csv('ettm1_results.csv', index=False, header=True)\n```\n\n이 단계는 모든 데이터셋에 대해 반복됩니다. 이 실험을 실행하는 완전한 함수는 아래에 표시됩니다.\n\n```js\nfrom utilsforecast.losses import mae, mse\nfrom utilsforecast.evaluation import evaluate\n\ndatasets = ['ettm1', 'ettm2', 'etth1', 'etth2']\n\nfor dataset in datasets:\n\n    Y_df, val_size, test_size, freq = load_data(dataset)\n\n    horizon = 96\n\n    models = [\n        iTransformer(h=horizon, input_size=3*horizon, n_series=1, max_steps=1000, early_stop_patience_steps=3),\n        TSMixer(h=horizon, input_size=3*horizon, n_series=1, max_steps=1000, early_stop_patience_steps=3),\n        NHITS(h=horizon, input_size=3*horizon, max_steps=1000, early_stop_patience_steps=3),\n        PatchTST(h=horizon, input_size=3*horizon, max_steps=1000, early_stop_patience_steps=3)\n    ]\n\n    nf = NeuralForecast(models=models, freq=freq)\n    nf_preds = nf.cross_validation(df=Y_df, val_size=val_size, test_size=test_size, n_windows=None)\n    nf_preds = nf_preds.reset_index()\n\n    evaluation = evaluate(df=nf_preds, metrics=[mae, mse], models=['iTransformer', 'TSMixer', 'NHITS', 'PatchTST'])\n    evaluation.to_csv(f'{dataset}_results.csv', index=False, header=True)\n```\n\n이 작업을 완료하면 모든 데이터셋에 대해 모든 모델의 예측이 있게 됩니다. 그런 다음 평가로 넘어갈 수 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 성능 평가\n\n성능 지표를 모두 CSV 파일에 저장했으므로, pandas를 사용하여 이를 읽고 각 모델의 각 데이터셋에 대한 성능을 그릴 수 있습니다.\n\n```python\nfiles = ['etth1_results.csv', 'etth2_results.csv', 'ettm1_results.csv', 'ettm2_results.csv']\ndatasets = ['etth1', 'etth2', 'ettm1', 'ettm2']\n\ndataframes = []\n\nfor file, dataset in zip(files, datasets):\n    df = pd.read_csv(file)\n    df['dataset'] = dataset\n\n    dataframes.append(df)\n\nfull_df = pd.concat(dataframes, ignore_index=True)\nfull_df = full_df.drop(['unique_id'], axis=1)\n```\n\n이후, 지표를 그래프로 그리려면:\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndataset_names = full_df['dataset'].unique()\nmodel_names = ['iTransformer', 'TSMixer', 'NHITS', 'PatchTST']\n\nfig, axs = plt.subplots(2, 2, figsize=(15, 15))\nbar_width = 0.35\n\naxs = axs.flatten()\n\nfor i, dataset_name in enumerate(dataset_names):\n    df_subset = full_df[(full_df['dataset'] == dataset_name) \u0026 (full_df['metric'] == 'mae')]\n    mae_vals = df_subset[model_names].values.flatten()\n    df_subset = full_df[(full_df['dataset'] == dataset_name) \u0026 (full_df['metric'] == 'mse')]\n    mse_vals = df_subset[model_names].values.flatten()\n\n    indices = np.arange(len(model_names))\n\n    bars_mae = axs[i].bar(indices - bar_width / 2, mae_vals, bar_width, color='skyblue', label='MAE')\n    bars_mse = axs[i].bar(indices + bar_width / 2, mse_vals, bar_width, color='orange', label='MSE')\n\n    for bars in [bars_mae, bars_mse]:\n        for bar in bars:\n            height = bar.get_height()\n            axs[i].annotate(f'{height:.2f}',\n                            xy=(bar.get_x() + bar.get_width() / 2, height),\n                            xytext=(0, 3),\n                            textcoords=\"offset points\",\n                            ha='center', va='bottom')\n\n    axs[i].set_xticks(indices)\n    axs[i].set_xticklabels(model_names, rotation=45)\n    axs[i].set_title(dataset_name)\n    axs[i].legend(loc='best')\n\nplt.tight_layout()\n```\n\n![Image](/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_4.png)\n\nFrom the figure above, we can see that the iTransformer performs fairly well on all datasets, but TSMixer is overall slightly better than iTransformer, and PatchTST is the overall champion model in this experiment.\n\nOf course, keep in mind that we did not leverage the multivariate capabilities of iTransformer, and we only tested on a single forecast horizon. Therefore, it is not a complete assessment of the iTransformer’s performance.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n그럼에도 불구하고, 모델이 PatchTST와 매우 유사하게 수행되는 것을 볼 때, Transformer를 사용한 시계열 예측에서 새로운 성능에 도달하는 데 그룹화 시간 단계를 토큰화하기 전에 묶는 아이디어를 더 지원하는 점이 흥미로운 부분입니다.\n\n# 결론\n\niTransformer는 베이닐라 Transformer 아키텍처를 적용한 뒤 입력 시리즈의 역방향 모양으로 그냥 적용합니다.\n\n이렇게 하면 전체 시리즈가 토큰화되고 PatchTST에서 제안한 것과 같이 극단적인 케이스를 모방합니다.\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n모델이 주의 매커니즘을 사용하여 다변량 상관 관계를 학습하고, 피드포워드 네트워크가 시계열의 시간적 특성을 학습합니다.\n\niTransformer는 많은 벤치마크 데이터셋에서 장기 예측에 대한 최신 기술을 보여주었으며, 우리의 한정된 실험에서는 PatchTST가 전반적으로 가장 우수한 성과를 보였습니다.\n\n모든 문제는 고유한 해결책이 필요하다고 단언합니다. 이제 iTransformer를 도구 상자에 추가하고 여러분의 프로젝트에 적용할 수 있습니다.\n\n읽어 주셔서 감사합니다! 즐겁게 읽으셨기를 바라며 새로운 지식을 얻으셨기를 기대합니다!\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n만나서 반가워요 🌟\n\n# 저를 지원해주세요\n\n제 작업을 즐기고 계신가요? Buy me a coffee로 제게 지원을 표현해주세요. 여러분의 응원을 받으면 저는 커피 한 잔을 즐길 수 있어요! 만약 그렇게 느끼신다면, 아래 버튼을 클릭해주세요 👇\n\n![Image](/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_5.png)\n\n\u003c!-- TIL 수평 --\u003e\n\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 참고 자료\n\niTransformer: Inverted Transformers Are Effective for Time Series Forecasting by Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, Mingsheng Long\n","ogImage":{"url":"/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_0.png","tag":["Tech"],"readingTime":19},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_0.png\" alt=\"2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting\"\u003e\u003c/p\u003e\n\u003cp\u003e예측 분야에서는 Lag-LLaMA, Time-LLM, Chronos, Moirai와 같은 모델들이 2024년 초부터 제안되어 기초 모델 분야에서 많은 활동을 보이고 있습니다.\u003c/p\u003e\n\u003cp\u003e그러나 이러한 모델들의 성능은 조금 아쉬운 면이 있습니다 (\u003ca href=\"%EC%97%AC%EA%B8%B0\"\u003e재현 가능한 벤치마크를 보려면 여기를 참조하십시오\u003c/a\u003e) 그리고 저는 데이터 특화 모델이 여전히 현재 최적의 해결책이라고 믿습니다.\u003c/p\u003e\n\u003cp\u003e이에 따라 Transformer 아키텍처가 다양한 형태로 시계열 예측에 적용되어왔으며, PatchTST는 장기 예측에서 최고 수준의 성능을 달성하였습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e도전적인 PatchTST에 이어 2024년 3월에 제안된 iTransformer 모델이 등장했습니다. 논문 \"iTransformer: Inverted Transformers Are Effective for Time Series Forecasting\"에서 소개되었습니다.\u003c/p\u003e\n\u003cp\u003e이 기사에서는 iTransformer의 놀라운 간단한 개념을 발견하고 그 아키텍처를 탐구합니다. 그런 다음 해당 모델을 소규모 실험에 적용하고 그 성능을 TSMixer, N-HiTS 및 PatchTST와 비교합니다.\u003c/p\u003e\n\u003cp\u003e더 자세한 내용은 원본 논문을 읽어보세요.\u003c/p\u003e\n\u003cp\u003e시작해봅시다!\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003eiTransformer 탐색\u003c/h1\u003e\n\u003cp\u003eiTransformer의 아이디어는 바닐라 Transformer 모델이 시간 토큰을 사용한다는 깨달음에서 나왔어요.\u003c/p\u003e\n\u003cp\u003e이것은 모델이 단일 시간 단계에서 모든 특징을 살펴본다는 것을 의미합니다. 그래서 모델이 한 번에 한 시간 단계씩 살펴볼 때 시간 의존성을 학습하는 것이 어려울 수 있어요.\u003c/p\u003e\n\u003cp\u003e그 문제에 대한 해결책은 PatchTST 모델과 함께 제안된 패칭이에요. 패칭을 사용하면 토큰화하고 임베딩하기 전에 시간 지점을 단순히 그룹화할 수 있어요. 아래에서 보여준 것처럼요.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_1.png\" alt=\"iTransformer image 1\"\u003e\u003c/p\u003e\n\u003cp\u003eIn iTransformer, we push patching to the extreme by simply applying the model on the inverted dimensions.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_2.png\" alt=\"iTransformer image 2\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the figure above, we can see how the iTransformer differs from the vanilla Transformer. Instead of looking at all features at one time step, it looks at one feature across many time steps. This is done simply by inverting the shape of the input.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이렇게하면 어텐션 레이어가 다변량 상관 관계를 학습하고 피드포워드 네트워크가 전체 입력 시퀀스의 표현을 인코딩합니다.\u003c/p\u003e\n\u003cp\u003eiTransformer의 일반 아이디어를 이해했으니, 이제 더 자세히 살펴보겠습니다.\u003c/p\u003e\n\u003ch2\u003eiTransformer의 아키텍처\u003c/h2\u003e\n\u003cp\u003eiTransformer는 2017년에 Attention Is All You Need에서 처음으로 제안된 임베딩, 프로젝션 및 트랜스포머 블록을 사용한 바닐라 인코더-디코더 아키텍처를 채택합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e위의 그림에서 건물 블록들은 동일하지만 기능은 완전히 다르다는 것을 볼 수 있습니다. 좀 더 자세히 살펴보겠습니다.\u003c/p\u003e\n\u003cp\u003e임베딩 레이어\u003c/p\u003e\n\u003cp\u003e먼저, 입력 시리즈는 독립적으로 토큰으로 임베딩됩니다. 다시 말해서, 이는 입력의 서브시퀀스를 토큰화하는 대신, 모델이 전체 입력 시퀀스를 토큰화하는 극단적인 경우와 같습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e다변량 주의력\u003c/p\u003e\n\u003cp\u003e그런 다음, 임베딩은 주의층으로 전송되어 다변량 상관 맵을 학습할 것입니다.\u003c/p\u003e\n\u003cp\u003e이는 역전 모델이 각 특징을 독립된 프로세스로 간주하기 때문에 가능합니다. 이러한 결과로 주의 메커니즘은 특징들 사이의 상관 관계를 학습하게 되며, 이로써 iTransformer는 특히 다변량 예측 작업에 적합합니다.\u003c/p\u003e\n\u003cp\u003e층 정규화\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e어텐션 레이어의 출력은 정규화 레이어로 전송됩니다.\u003c/p\u003e\n\u003cp\u003e전통적인 트랜스포머 아키텍처에서는 정규화가 모든 특성에 대해 고정된 타임스탬프에서 이루어집니다. 이는 모델이 쓸모없는 관계를 학습하게 될 수 있는 상호작용 소음을 도입할 수 있습니다. 또한, 지나치게 매끄러운 신호를 초래할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e반면, iTransformer는 차원을 뒤집으므로 정규화가 타임스탬프를 횡단하여 이루어집니다. 이는 모델이 비정상적인 시계열에 대처하도록 도와주며, 시계열의 소음을 줄여줍니다.\u003c/p\u003e\n\u003cp\u003e피드포워드 네트워크\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e마지막으로, 피드 포워드 네트워크(FFN)는 들어오는 토큰의 깊은 표현을 학습합니다.\u003c/p\u003e\n\u003cp\u003e다시 말해서, 모양이 반전되어 있기 때문에 다층 퍼셉트론(MLP)은 주기성이나 진폭과 같은 다른 시계열 속성을 학습할 수 있습니다. 이는 MLP 기반 모델(N-BEATS, N-HiTS, TSMixer 등)의 능력을 모방합니다.\u003c/p\u003e\n\u003cp\u003e프로젝션\u003c/p\u003e\n\u003cp\u003e여기서 간단히 많은 블록을 쌓는 것으로 이루어진 단계입니다:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cul\u003e\n\u003cli\u003e주의 층\u003c/li\u003e\n\u003cli\u003e계층 정규화\u003c/li\u003e\n\u003cli\u003e피드포워드 네트워크\u003c/li\u003e\n\u003cli\u003e계층 정규화\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e각 블록은 입력 시리즈의 다른 표현을 학습합니다. 그런 다음, 블록 스택의 출력은 최종 예측을 얻기 위해 선형 투사 단계를 거쳐 전송됩니다.\u003c/p\u003e\n\u003cp\u003e요약하자면, iTransformer는 새로운 아키텍처가 아니며 Transformer를 새롭게 만들어내지는 않습니다. 단순히 입력의 역된 차원에 Transformer를 적용하여 모델이 다변량 상관 관계를 학습하고 시간적 특성을 포착할 수 있도록 합니다.\u003c/p\u003e\n\u003cp\u003e이제 iTransformer 모델에 대한 깊은 이해를 갖고 작은 예측 실험에서 적용해 보겠습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003eiTransformer를 사용한 예측\u003c/h1\u003e\n\u003cp\u003e이 작은 실험에서는 Creative Commons 라이선스로 공개된 전기 변압기 데이터셋에 iTransformer 모델을 적용합니다.\u003c/p\u003e\n\u003cp\u003e중국 한 성의 두 지역에서 전기 변압기의 오일 온도를 추적하는 인기 있는 벤치마크 데이터셋입니다. 두 지역 모두 1시간마다 샘플링된 데이터셋을 가지고 있으며, 15분마다 샘플링된 데이터셋이 있어 총 네 개의 데이터셋이 있습니다.\u003c/p\u003e\n\u003cp\u003eiTransformer는 근본적으로 다변량 모델이지만, 우리는 96개의 시간 단계에 걸친 일변량 예측 능력을 테스트합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이 실험에 대한 코드는 GitHub에서 확인할 수 있어요.\u003c/p\u003e\n\u003cp\u003e자, 시작해봅시다!\u003c/p\u003e\n\u003cp\u003e초기 설정\u003c/p\u003e\n\u003cp\u003e이 실험에서는 neuralforecast라는 라이브러리를 사용하는데, 이 라이브러리가 딥러닝 방법의 가장 빠르고 직관적인 사용 가능한 구현을 제공한다고 믿습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e pd\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.\u003cspan class=\"hljs-property\"\u003epyplot\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e datasetsforecast.\u003cspan class=\"hljs-property\"\u003elong_horizon\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eLongHorizon\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e neuralforecast.\u003cspan class=\"hljs-property\"\u003ecore\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eNeuralForecast\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e neuralforecast.\u003cspan class=\"hljs-property\"\u003emodels\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-variable constant_\"\u003eNHITS\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003ePatchTST\u003c/span\u003e, iTransformer, \u003cspan class=\"hljs-title class_\"\u003eTSMixer\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e본 글을 작성하는 시점에서 iTransformer가 아직 neuralforecast의 공개 릴리스에 포함되지 않았음을 참고하세요. 즉시 해당 모델에 액세스하려면 다음을 실행하세요:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003epip install git+\u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//github.com/Nixtla/neuralforecast.git\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이제 ETT 데이터셋을로드하고, 검증 크기, 테스트 크기, 그리고 주기를 포함하는 함수를 작성해봅시다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eload_data\u003c/span\u003e(name):\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e name == \u003cspan class=\"hljs-string\"\u003e\"ettm1\"\u003c/span\u003e:\n        Y_df, *_ = \u003cspan class=\"hljs-title class_\"\u003eLongHorizon\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eload\u003c/span\u003e(directory=\u003cspan class=\"hljs-string\"\u003e'./'\u003c/span\u003e, group=\u003cspan class=\"hljs-string\"\u003e'ETTm1'\u003c/span\u003e)\n        Y_df = Y_df[Y_df[\u003cspan class=\"hljs-string\"\u003e'unique_id'\u003c/span\u003e] == \u003cspan class=\"hljs-string\"\u003e'OT'\u003c/span\u003e]\n        Y_df[\u003cspan class=\"hljs-string\"\u003e'ds'\u003c/span\u003e] = pd.\u003cspan class=\"hljs-title function_\"\u003eto_datetime\u003c/span\u003e(Y_df[\u003cspan class=\"hljs-string\"\u003e'ds'\u003c/span\u003e])\n        val_size = \u003cspan class=\"hljs-number\"\u003e11520\u003c/span\u003e\n        test_size = \u003cspan class=\"hljs-number\"\u003e11520\u003c/span\u003e\n        freq = \u003cspan class=\"hljs-string\"\u003e'15T'\u003c/span\u003e\n    elif name == \u003cspan class=\"hljs-string\"\u003e\"ettm2\"\u003c/span\u003e:\n        Y_df, *_ = \u003cspan class=\"hljs-title class_\"\u003eLongHorizon\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eload\u003c/span\u003e(directory=\u003cspan class=\"hljs-string\"\u003e'./'\u003c/span\u003e, group=\u003cspan class=\"hljs-string\"\u003e'ETTm2'\u003c/span\u003e)\n        Y_df = Y_df[Y_df[\u003cspan class=\"hljs-string\"\u003e'unique_id'\u003c/span\u003e] == \u003cspan class=\"hljs-string\"\u003e'OT'\u003c/span\u003e]\n        Y_df[\u003cspan class=\"hljs-string\"\u003e'ds'\u003c/span\u003e] = pd.\u003cspan class=\"hljs-title function_\"\u003eto_datetime\u003c/span\u003e(Y_df[\u003cspan class=\"hljs-string\"\u003e'ds'\u003c/span\u003e])\n        val_size = \u003cspan class=\"hljs-number\"\u003e11520\u003c/span\u003e\n        test_size = \u003cspan class=\"hljs-number\"\u003e11520\u003c/span\u003e\n        freq = \u003cspan class=\"hljs-string\"\u003e'15T'\u003c/span\u003e\n    elif name == \u003cspan class=\"hljs-string\"\u003e'etth1'\u003c/span\u003e:\n        Y_df, *_ = \u003cspan class=\"hljs-title class_\"\u003eLongHorizon\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eload\u003c/span\u003e(directory=\u003cspan class=\"hljs-string\"\u003e'./'\u003c/span\u003e, group=\u003cspan class=\"hljs-string\"\u003e'ETTh1'\u003c/span\u003e)\n        Y_df[\u003cspan class=\"hljs-string\"\u003e'ds'\u003c/span\u003e] = pd.\u003cspan class=\"hljs-title function_\"\u003eto_datetime\u003c/span\u003e(Y_df[\u003cspan class=\"hljs-string\"\u003e'ds'\u003c/span\u003e])\n        val_size = \u003cspan class=\"hljs-number\"\u003e2880\u003c/span\u003e\n        test_size = \u003cspan class=\"hljs-number\"\u003e2880\u003c/span\u003e\n        freq = \u003cspan class=\"hljs-string\"\u003e'H'\u003c/span\u003e\n    elif name == \u003cspan class=\"hljs-string\"\u003e\"etth2\"\u003c/span\u003e:\n        Y_df, *_ = \u003cspan class=\"hljs-title class_\"\u003eLongHorizon\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eload\u003c/span\u003e(directory=\u003cspan class=\"hljs-string\"\u003e'./'\u003c/span\u003e, group=\u003cspan class=\"hljs-string\"\u003e'ETTh2'\u003c/span\u003e)\n        Y_df[\u003cspan class=\"hljs-string\"\u003e'ds'\u003c/span\u003e] = pd.\u003cspan class=\"hljs-title function_\"\u003eto_datetime\u003c/span\u003e(Y_df[\u003cspan class=\"hljs-string\"\u003e'ds'\u003c/span\u003e])\n        val_size = \u003cspan class=\"hljs-number\"\u003e2880\u003c/span\u003e\n        test_size = \u003cspan class=\"hljs-number\"\u003e2880\u003c/span\u003e\n        freq = \u003cspan class=\"hljs-string\"\u003e'H'\u003c/span\u003e\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e Y_df, val_size, test_size, freq\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe above function conveniently loads the data in the required format for neuralforecast. It includes a unique_id column to identify time series, a ds column for timestamps, and a y column for series values.\u003c/p\u003e\n\u003cp\u003ePlease note that the validation and test sizes align with standards in the scientific community for publishing research papers.\u003c/p\u003e\n\u003cp\u003eWe are all set to start training the models.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e훈련 및 예측\u003c/h2\u003e\n\u003cp\u003eiTransformer 모델을 훈련시키기 위해서는 단순히 다음을 지정해주면 됩니다:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e예측 기간\u003c/li\u003e\n\u003cli\u003e입력 크기\u003c/li\u003e\n\u003cli\u003e시리즈 수\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eiTransformer가 본질적으로 다변량 모델이기 때문에 모델을 적합할 때 시리즈 수를 지정해야 합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e단변량 시나리오이므로 n_series=1입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title function_\"\u003eiTransformer\u003c/span\u003e(\n  (h = horizon),\n  (input_size = \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e * horizon),\n  (n_series = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e),\n  (max_steps = \u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e),\n  (early_stop_patience_steps = \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)\n);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e위의 코드 블록에서는 최대 학습 단계 수를 지정하고, 과적합을 방지하기 위해 조기 중지를 3번 반복으로 설정합니다.\u003c/p\u003e\n\u003cp\u003e나머지 모델들에 대해 같은 작업을 수행한 후, 리스트에 넣어줍니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003ehorizon = \u003cspan class=\"hljs-number\"\u003e96\u003c/span\u003e;\n\nmodels = [\n  \u003cspan class=\"hljs-title function_\"\u003eiTransformer\u003c/span\u003e(\n    (h = horizon),\n    (input_size = \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e * horizon),\n    (n_series = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e),\n    (max_steps = \u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e),\n    (early_stop_patience_steps = \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)\n  ),\n  \u003cspan class=\"hljs-title class_\"\u003eTSMixer\u003c/span\u003e(\n    (h = horizon),\n    (input_size = \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e * horizon),\n    (n_series = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e),\n    (max_steps = \u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e),\n    (early_stop_patience_steps = \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)\n  ),\n  \u003cspan class=\"hljs-title function_\"\u003eNHITS\u003c/span\u003e((h = horizon), (input_size = \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e * horizon), (max_steps = \u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e), (early_stop_patience_steps = \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)),\n  \u003cspan class=\"hljs-title class_\"\u003ePatchTST\u003c/span\u003e((h = horizon), (input_size = \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e * horizon), (max_steps = \u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e), (early_stop_patience_steps = \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)),\n];\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e좋아요! 이제 우리는 단순히 NeuralForecast 객체를 초기화하면 되는데, 이 객체는 학습, 교차 검증 및 예측을 위한 메서드에 액세스할 수 있게 해줍니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003enf = \u003cspan class=\"hljs-title class_\"\u003eNeuralForecast\u003c/span\u003e((models = models), (freq = freq));\nnf_preds = nf.\u003cspan class=\"hljs-title function_\"\u003ecross_validation\u003c/span\u003e((df = Y_df), (val_size = val_size), (test_size = test_size), (n_windows = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e));\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e마지막으로, 우리는 각 모델의 성능을 utilsforecast 라이브러리를 사용하여 평가합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e utilsforecast.\u003cspan class=\"hljs-property\"\u003elosses\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e mae, mse\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e utilsforecast.\u003cspan class=\"hljs-property\"\u003eevaluation\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e evaluate\n\nettm1_evaluation = evaluate(df=nf_preds, metrics=[mae, mse], models=[\u003cspan class=\"hljs-string\"\u003e'iTransformer'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'TSMixer'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'NHITS'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'PatchTST'\u003c/span\u003e])\nettm1_evaluation.\u003cspan class=\"hljs-title function_\"\u003eto_csv\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'ettm1_results.csv'\u003c/span\u003e, index=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e, header=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 단계는 모든 데이터셋에 대해 반복됩니다. 이 실험을 실행하는 완전한 함수는 아래에 표시됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e utilsforecast.\u003cspan class=\"hljs-property\"\u003elosses\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e mae, mse\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e utilsforecast.\u003cspan class=\"hljs-property\"\u003eevaluation\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e evaluate\n\ndatasets = [\u003cspan class=\"hljs-string\"\u003e'ettm1'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'ettm2'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'etth1'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'etth2'\u003c/span\u003e]\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e dataset \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003edatasets\u003c/span\u003e:\n\n    Y_df, val_size, test_size, freq = \u003cspan class=\"hljs-title function_\"\u003eload_data\u003c/span\u003e(dataset)\n\n    horizon = \u003cspan class=\"hljs-number\"\u003e96\u003c/span\u003e\n\n    models = [\n        \u003cspan class=\"hljs-title function_\"\u003eiTransformer\u003c/span\u003e(h=horizon, input_size=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e*horizon, n_series=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, max_steps=\u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e, early_stop_patience_steps=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e),\n        \u003cspan class=\"hljs-title class_\"\u003eTSMixer\u003c/span\u003e(h=horizon, input_size=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e*horizon, n_series=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, max_steps=\u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e, early_stop_patience_steps=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e),\n        \u003cspan class=\"hljs-title function_\"\u003eNHITS\u003c/span\u003e(h=horizon, input_size=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e*horizon, max_steps=\u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e, early_stop_patience_steps=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e),\n        \u003cspan class=\"hljs-title class_\"\u003ePatchTST\u003c/span\u003e(h=horizon, input_size=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e*horizon, max_steps=\u003cspan class=\"hljs-number\"\u003e1000\u003c/span\u003e, early_stop_patience_steps=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)\n    ]\n\n    nf = \u003cspan class=\"hljs-title class_\"\u003eNeuralForecast\u003c/span\u003e(models=models, freq=freq)\n    nf_preds = nf.\u003cspan class=\"hljs-title function_\"\u003ecross_validation\u003c/span\u003e(df=Y_df, val_size=val_size, test_size=test_size, n_windows=\u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e)\n    nf_preds = nf_preds.\u003cspan class=\"hljs-title function_\"\u003ereset_index\u003c/span\u003e()\n\n    evaluation = evaluate(df=nf_preds, metrics=[mae, mse], models=[\u003cspan class=\"hljs-string\"\u003e'iTransformer'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'TSMixer'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'NHITS'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'PatchTST'\u003c/span\u003e])\n    evaluation.\u003cspan class=\"hljs-title function_\"\u003eto_csv\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e'{dataset}_results.csv'\u003c/span\u003e, index=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e, header=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 작업을 완료하면 모든 데이터셋에 대해 모든 모델의 예측이 있게 됩니다. 그런 다음 평가로 넘어갈 수 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e성능 평가\u003c/h2\u003e\n\u003cp\u003e성능 지표를 모두 CSV 파일에 저장했으므로, pandas를 사용하여 이를 읽고 각 모델의 각 데이터셋에 대한 성능을 그릴 수 있습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003efiles = [\u003cspan class=\"hljs-string\"\u003e'etth1_results.csv'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'etth2_results.csv'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'ettm1_results.csv'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'ettm2_results.csv'\u003c/span\u003e]\ndatasets = [\u003cspan class=\"hljs-string\"\u003e'etth1'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'etth2'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'ettm1'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'ettm2'\u003c/span\u003e]\n\ndataframes = []\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e file, dataset \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003ezip\u003c/span\u003e(files, datasets):\n    df = pd.read_csv(file)\n    df[\u003cspan class=\"hljs-string\"\u003e'dataset'\u003c/span\u003e] = dataset\n\n    dataframes.append(df)\n\nfull_df = pd.concat(dataframes, ignore_index=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\nfull_df = full_df.drop([\u003cspan class=\"hljs-string\"\u003e'unique_id'\u003c/span\u003e], axis=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이후, 지표를 그래프로 그리려면:\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\ndataset_names = full_df[\u003cspan class=\"hljs-string\"\u003e'dataset'\u003c/span\u003e].unique()\nmodel_names = [\u003cspan class=\"hljs-string\"\u003e'iTransformer'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'TSMixer'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'NHITS'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'PatchTST'\u003c/span\u003e]\n\nfig, axs = plt.subplots(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e))\nbar_width = \u003cspan class=\"hljs-number\"\u003e0.35\u003c/span\u003e\n\naxs = axs.flatten()\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i, dataset_name \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eenumerate\u003c/span\u003e(dataset_names):\n    df_subset = full_df[(full_df[\u003cspan class=\"hljs-string\"\u003e'dataset'\u003c/span\u003e] == dataset_name) \u0026#x26; (full_df[\u003cspan class=\"hljs-string\"\u003e'metric'\u003c/span\u003e] == \u003cspan class=\"hljs-string\"\u003e'mae'\u003c/span\u003e)]\n    mae_vals = df_subset[model_names].values.flatten()\n    df_subset = full_df[(full_df[\u003cspan class=\"hljs-string\"\u003e'dataset'\u003c/span\u003e] == dataset_name) \u0026#x26; (full_df[\u003cspan class=\"hljs-string\"\u003e'metric'\u003c/span\u003e] == \u003cspan class=\"hljs-string\"\u003e'mse'\u003c/span\u003e)]\n    mse_vals = df_subset[model_names].values.flatten()\n\n    indices = np.arange(\u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(model_names))\n\n    bars_mae = axs[i].bar(indices - bar_width / \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, mae_vals, bar_width, color=\u003cspan class=\"hljs-string\"\u003e'skyblue'\u003c/span\u003e, label=\u003cspan class=\"hljs-string\"\u003e'MAE'\u003c/span\u003e)\n    bars_mse = axs[i].bar(indices + bar_width / \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, mse_vals, bar_width, color=\u003cspan class=\"hljs-string\"\u003e'orange'\u003c/span\u003e, label=\u003cspan class=\"hljs-string\"\u003e'MSE'\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e bars \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e [bars_mae, bars_mse]:\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e bar \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e bars:\n            height = bar.get_height()\n            axs[i].annotate(\u003cspan class=\"hljs-string\"\u003ef'\u003cspan class=\"hljs-subst\"\u003e{height:\u003cspan class=\"hljs-number\"\u003e.2\u003c/span\u003ef}\u003c/span\u003e'\u003c/span\u003e,\n                            xy=(bar.get_x() + bar.get_width() / \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, height),\n                            xytext=(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e),\n                            textcoords=\u003cspan class=\"hljs-string\"\u003e\"offset points\"\u003c/span\u003e,\n                            ha=\u003cspan class=\"hljs-string\"\u003e'center'\u003c/span\u003e, va=\u003cspan class=\"hljs-string\"\u003e'bottom'\u003c/span\u003e)\n\n    axs[i].set_xticks(indices)\n    axs[i].set_xticklabels(model_names, rotation=\u003cspan class=\"hljs-number\"\u003e45\u003c/span\u003e)\n    axs[i].set_title(dataset_name)\n    axs[i].legend(loc=\u003cspan class=\"hljs-string\"\u003e'best'\u003c/span\u003e)\n\nplt.tight_layout()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_4.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003cp\u003eFrom the figure above, we can see that the iTransformer performs fairly well on all datasets, but TSMixer is overall slightly better than iTransformer, and PatchTST is the overall champion model in this experiment.\u003c/p\u003e\n\u003cp\u003eOf course, keep in mind that we did not leverage the multivariate capabilities of iTransformer, and we only tested on a single forecast horizon. Therefore, it is not a complete assessment of the iTransformer’s performance.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e그럼에도 불구하고, 모델이 PatchTST와 매우 유사하게 수행되는 것을 볼 때, Transformer를 사용한 시계열 예측에서 새로운 성능에 도달하는 데 그룹화 시간 단계를 토큰화하기 전에 묶는 아이디어를 더 지원하는 점이 흥미로운 부분입니다.\u003c/p\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003eiTransformer는 베이닐라 Transformer 아키텍처를 적용한 뒤 입력 시리즈의 역방향 모양으로 그냥 적용합니다.\u003c/p\u003e\n\u003cp\u003e이렇게 하면 전체 시리즈가 토큰화되고 PatchTST에서 제안한 것과 같이 극단적인 케이스를 모방합니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e모델이 주의 매커니즘을 사용하여 다변량 상관 관계를 학습하고, 피드포워드 네트워크가 시계열의 시간적 특성을 학습합니다.\u003c/p\u003e\n\u003cp\u003eiTransformer는 많은 벤치마크 데이터셋에서 장기 예측에 대한 최신 기술을 보여주었으며, 우리의 한정된 실험에서는 PatchTST가 전반적으로 가장 우수한 성과를 보였습니다.\u003c/p\u003e\n\u003cp\u003e모든 문제는 고유한 해결책이 필요하다고 단언합니다. 이제 iTransformer를 도구 상자에 추가하고 여러분의 프로젝트에 적용할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e읽어 주셔서 감사합니다! 즐겁게 읽으셨기를 바라며 새로운 지식을 얻으셨기를 기대합니다!\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e만나서 반가워요 🌟\u003c/p\u003e\n\u003ch1\u003e저를 지원해주세요\u003c/h1\u003e\n\u003cp\u003e제 작업을 즐기고 계신가요? Buy me a coffee로 제게 지원을 표현해주세요. 여러분의 응원을 받으면 저는 커피 한 잔을 즐길 수 있어요! 만약 그렇게 느끼신다면, 아래 버튼을 클릭해주세요 👇\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting_5.png\" alt=\"Image\"\u003e\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e참고 자료\u003c/h1\u003e\n\u003cp\u003eiTransformer: Inverted Transformers Are Effective for Time Series Forecasting by Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, Mingsheng Long\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-07-09-iTransformerTheLatestBreakthroughinTimeSeriesForecasting"},"buildId":"Suu-uTE6tpVjS7rqQHkw3","assetPrefix":"/TIL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>