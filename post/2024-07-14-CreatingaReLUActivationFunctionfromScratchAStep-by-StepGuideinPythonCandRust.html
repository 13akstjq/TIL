<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>Python, C, Rust로 직접 만들어보는 ReLU 활성화 함수 단계별 가이드 | TIL</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://13akstjq.github.io/TIL//post/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="Python, C, Rust로 직접 만들어보는 ReLU 활성화 함수 단계별 가이드 | TIL" data-gatsby-head="true"/><meta property="og:title" content="Python, C, Rust로 직접 만들어보는 ReLU 활성화 함수 단계별 가이드 | TIL" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_0.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://TIL.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://13akstjq.github.io/TIL//post/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust" data-gatsby-head="true"/><meta name="twitter:title" content="Python, C, Rust로 직접 만들어보는 ReLU 활성화 함수 단계별 가이드 | TIL" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_0.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | TIL" data-gatsby-head="true"/><meta name="article:published_time" content="2024-07-14 23:51" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/TIL/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/TIL/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/TIL/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/TIL/favicons/favicon-96x96.png"/><link rel="icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/TIL/favicons/browserconfig.xml"/><link rel="preload" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/b8ef307c9aee1e34.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/TIL/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/TIL/_next/static/chunks/webpack-21ffe88bdca56cba.js" defer=""></script><script src="/TIL/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/TIL/_next/static/chunks/main-a5eeabb286676ce6.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/TIL/_next/static/chunks/75fc9c18-ac4aa08aae62f90e.js" defer=""></script><script src="/TIL/_next/static/chunks/463-925361deb4cec4b1.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/post/%5Bslug%5D-9d7ebbd29b9e08ce.js" defer=""></script><script src="/TIL/_next/static/FuXRqV9h16krA5Mvtd6Dn/_buildManifest.js" defer=""></script><script src="/TIL/_next/static/FuXRqV9h16krA5Mvtd6Dn/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/TIL">TIL</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/TIL/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">Python, C, Rust로 직접 만들어보는 ReLU 활성화 함수 단계별 가이드</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="Python, C, Rust로 직접 만들어보는 ReLU 활성화 함수 단계별 가이드" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/TIL/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">TIL</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Jul 14, 2024</span><span class="posts_reading_time__f7YPP">9<!-- --> min read</span></span></div></div><img alt="" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" class="posts_view_badge__tcbfm" style="color:transparent" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fallround-coder.github.io/post/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=views&amp;edge_flat=false"/></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<p><img src="/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_0.png" alt="image"></p>
<h1>소개</h1>
<p>신경망 세계에서 활성화 함수는 모델의 출력을 결정하는 데 중요한 역할을 합니다. 가장 인기 있는 활성화 함수 중 하나는 ReLU(렉티파이드 루 linear Unit)입니다. 간단함과 효과적임으로 유명한 ReLU는 많은 딥러닝 모델에서 표준 선택지가 되었습니다. 이 안내서에서는 Python, C 및 Rust 세 가지 다른 프로그래밍 언어로부터 ReLU 활성화 함수를 처음부터 만드는 과정을 안내합니다. 이를 통해 다양한 플랫폼에서의 구현과 이점에 대한 명확한 이해를 제공할 것입니다.</p>
<h1>ReLU란 무엇인가요?</h1>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>ReLU은 Rectified Linear Unit의 약자입니다. 이는 입력 값을 반환하는 활성화 함수로 정의됩니다:</p>
<p><img src="/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_1.png" alt="ReLU Activation Function"></p>
<p>더 간단히 말하면, 입력 값이 양수인 경우 입력 값을 반환하고, 그렇지 않으면 0을 반환합니다. ReLU 함수는 수학적으로 다음과 같이 표현됩니다:</p>
<p><img src="/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_2.png" alt="ReLU Formula"></p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>ReLU를 사용하는 이유?</h1>
<ul>
<li>단순성: ReLU 함수는 0을 기준으로 한 간단한 임계값 처리를 포함하기 때문에 계산 효율적입니다.</li>
<li>비선형성: 선형 함수처럼 보이지만 ReLU는 비선형성을 도입하여 복잡한 패턴을 학습하는 데 필수적입니다.</li>
<li>희소 활성화: ReLU는 희소한 활성화를 생성하는 경향이 있어서 (많은 뉴런이 0을 출력) 네트워크를 더 효율적으로 만듭니다.</li>
</ul>
<h1>Python에서 ReLU 및 신경망 레이어 구현하기</h1>
<p>파이썬에서 ReLU 활성화 함수와 간단한 신경망 레이어를 구현해보겠습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>파이썬 구현</h1>
<h2>단계 1: ReLU 함수 정의</h2>
<p>먼저, 간단한 파이썬 함수를 사용하여 ReLU 함수를 정의해 보겠습니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">x</span>):
    <span class="hljs-keyword">return</span> <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, x)
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>단계 2: 배열에 ReLU 적용하기</h2>
<p>우리는 NumPy를 사용하여 배열을 처리할 수 있는 ReLU 함수를 확장할 것입니다.</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-keyword">def</span> <span class="hljs-title function_">relu_array</span>(<span class="hljs-params">arr</span>):
    <span class="hljs-keyword">return</span> np.maximum(<span class="hljs-number">0</span>, arr)
</code></pre>
<h2>단계 3: 간단한 신경 계층 정의하기</h2>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>자, 이제 간단한 신경망 레이어 클래스를 만들어봅시다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleNeuralLayer</span>:
    def <span class="hljs-title function_">__init__</span>(self, input_size, output_size):
        self.<span class="hljs-property">weights</span> = np.<span class="hljs-property">random</span>.<span class="hljs-title function_">randn</span>(input_size, output_size)
        self.<span class="hljs-property">biases</span> = np.<span class="hljs-title function_">zeros</span>(output_size)

    def <span class="hljs-title function_">forward</span>(self, inputs):
        z = np.<span class="hljs-title function_">dot</span>(inputs, self.<span class="hljs-property">weights</span>) + self.<span class="hljs-property">biases</span>
        <span class="hljs-keyword">return</span> <span class="hljs-title function_">relu_array</span>(z)
</code></pre>
<h2>단계 4: 신경망 레이어 테스트</h2>
<pre><code class="hljs language-js"># 예제 사용법
layer = <span class="hljs-title class_">SimpleNeuralLayer</span>(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)
inputs = np.<span class="hljs-title function_">array</span>([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, -<span class="hljs-number">1</span>])
output = layer.<span class="hljs-title function_">forward</span>(inputs)
<span class="hljs-title function_">print</span>(output)
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>C에서 ReLU 및 신경 계층 구현</h1>
<p>이제 C에서 ReLU 함수와 간단한 신경망 계층을 구현해 보겠습니다.</p>
<h1>C 구현</h1>
<h2>단계 1: ReLU 함수 정의</h2>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-js">#include &#x3C;stdio.<span class="hljs-property">h</span>>

double <span class="hljs-title function_">relu</span>(<span class="hljs-params">double x</span>) {
    <span class="hljs-keyword">return</span> x > <span class="hljs-number">0</span> ? x : <span class="hljs-number">0</span>;
}
</code></pre>
<h2>단계 2: 배열에 ReLU 적용하기</h2>
<pre><code class="hljs language-js">#include &#x3C;stdio.<span class="hljs-property">h</span>>

<span class="hljs-keyword">void</span> <span class="hljs-title function_">relu_array</span>(<span class="hljs-params">double* arr, int size</span>) {
    <span class="hljs-keyword">for</span> (int i = <span class="hljs-number">0</span>; i &#x3C; size; i++) {
        arr[i] = arr[i] > <span class="hljs-number">0</span> ? arr[i] : <span class="hljs-number">0</span>;
    }
}
</code></pre>
<h2>단계 3: 간단한 신경망 레이어 정의하기</h2>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>단계 4: 신경망 레이어 테스트</h2>
<pre><code class="hljs language-js">#include &#x3C;stdio.<span class="hljs-property">h</span>>
#include &#x3C;stdlib.<span class="hljs-property">h</span>>
#include &#x3C;time.<span class="hljs-property">h</span>>

int <span class="hljs-title function_">main</span>(<span class="hljs-params"></span>) {
    <span class="hljs-title function_">srand</span>(<span class="hljs-title function_">time</span>(<span class="hljs-variable constant_">NULL</span>));
    <span class="hljs-title class_">SimpleNeuralLayer</span> layer = <span class="hljs-title function_">create_layer</span>(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>);
    double inputs[] = {<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, -<span class="hljs-number">1</span>};
    double output[<span class="hljs-number">2</span>];
    <span class="hljs-title function_">forward</span>(layer, inputs, output);
    <span class="hljs-title function_">printf</span>(<span class="hljs-string">"Output: %f %f\n"</span>, output[<span class="hljs-number">0</span>], output[<span class="hljs-number">1</span>]);
    <span class="hljs-title function_">free</span>(layer.<span class="hljs-property">weights</span>);
    <span class="hljs-title function_">free</span>(layer.<span class="hljs-property">biases</span>);
    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;
}
</code></pre>
<h1>ReLU 및 신경망 레이어 구현하기</h1>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>마침내, ReLU 함수와 간단한 신경망 레이어를 Rust로 구현해 봅시다.</p>
<h1>Rust 구현</h1>
<h2>단계 1: ReLU 함수 정의</h2>
<pre><code class="hljs language-rust"><span class="hljs-keyword">fn</span> <span class="hljs-title function_">relu</span>(x: <span class="hljs-type">f64</span>) <span class="hljs-punctuation">-></span> <span class="hljs-type">f64</span> {
    <span class="hljs-keyword">if</span> x > <span class="hljs-number">0.0</span> { x } <span class="hljs-keyword">else</span> { <span class="hljs-number">0.0</span> }
}
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>단계 2: 배열에 ReLU 적용하기</h2>
<pre><code class="hljs language-js">fn <span class="hljs-title function_">relu_array</span>(<span class="hljs-params">arr: &#x26;mut [f64]</span>) {
    <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> arr.<span class="hljs-title function_">iter_mut</span>(<span class="hljs-params"></span>) {
        *x = <span class="hljs-keyword">if</span> *x > <span class="hljs-number">0.0</span> { *x } <span class="hljs-keyword">else</span> { <span class="hljs-number">0.0</span> };
    }
}
</code></pre>
<h2>단계 3: 간단한 신경망 계층 정의</h2>
<pre><code class="hljs language-js">use <span class="hljs-attr">rand</span>::<span class="hljs-title class_">Rng</span>;

struct <span class="hljs-title class_">SimpleNeuralLayer</span> {
    <span class="hljs-attr">weights</span>: <span class="hljs-title class_">Vec</span>&#x3C;<span class="hljs-title class_">Vec</span>&#x3C;f64>>,
    <span class="hljs-attr">biases</span>: <span class="hljs-title class_">Vec</span>&#x3C;f64>,
}

impl <span class="hljs-title class_">SimpleNeuralLayer</span> {
    fn <span class="hljs-title function_">new</span>(<span class="hljs-attr">input_size</span>: usize, <span class="hljs-attr">output_size</span>: usize) -> <span class="hljs-title class_">Self</span> {
        <span class="hljs-keyword">let</span> mut rng = <span class="hljs-attr">rand</span>::<span class="hljs-title function_">thread_rng</span>();
        <span class="hljs-keyword">let</span> weights = (<span class="hljs-number">0.</span>.<span class="hljs-property">input_size</span>)
            .<span class="hljs-title function_">map</span>(|_| (<span class="hljs-number">0.</span>.<span class="hljs-property">output_size</span>).<span class="hljs-title function_">map</span>(|_| rng.<span class="hljs-title function_">gen_range</span>(-<span class="hljs-number">1.0</span>.<span class="hljs-number">.1</span><span class="hljs-number">.0</span>)).<span class="hljs-title function_">collect</span>())
            .<span class="hljs-title function_">collect</span>();
        <span class="hljs-keyword">let</span> biases = vec![<span class="hljs-number">0.0</span>; output_size];
        
        <span class="hljs-title class_">SimpleNeuralLayer</span> { weights, biases }
    }

    fn <span class="hljs-title function_">forward</span>(&#x26;self, <span class="hljs-attr">inputs</span>: &#x26;[f64]) -> <span class="hljs-title class_">Vec</span>&#x3C;f64> {
        <span class="hljs-keyword">let</span> mut output = vec![<span class="hljs-number">0.0</span>; self.<span class="hljs-property">biases</span>.<span class="hljs-title function_">len</span>()];
        
        <span class="hljs-keyword">for</span> (i, &#x26;bias) <span class="hljs-keyword">in</span> self.<span class="hljs-property">biases</span>.<span class="hljs-title function_">iter</span>().<span class="hljs-title function_">enumerate</span>(<span class="hljs-params"></span>) {
            output[i] = inputs.<span class="hljs-title function_">iter</span>()
                .<span class="hljs-title function_">zip</span>(&#x26;self.<span class="hljs-property">weights</span>)
                .<span class="hljs-title function_">map</span>(|(&#x26;input, weight_row)| input * weight_row[i])
                .<span class="hljs-property">sum</span>::&#x3C;f64>() + bias;
            output[i] = <span class="hljs-title function_">relu</span>(output[i]);
        }
        output
    }
}
</code></pre>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>단계 4: 신경 계층 테스트</h2>
<pre><code class="hljs language-js">fn <span class="hljs-title function_">main</span>(<span class="hljs-params"></span>) {
    <span class="hljs-keyword">let</span> layer = <span class="hljs-title class_">SimpleNeuralLayer</span>::<span class="hljs-title function_">new</span>(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>);
    <span class="hljs-keyword">let</span> inputs = [<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, -<span class="hljs-number">1.0</span>];
    <span class="hljs-keyword">let</span> output = layer.<span class="hljs-title function_">forward</span>(&#x26;inputs);
    
    <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> output {
        print!(<span class="hljs-string">"{} "</span>, value);
    }
    <span class="hljs-comment">// 출력: 0 0</span>
}
</code></pre>
<h1>결론</h1>
<p>ReLU 활성화 함수를 만들고 간단한 신경망 계층에 통합하는 것은 신경망 작업에 대한 기본 개념을 강조합니다. Python, C 및 Rust에서 ReLU를 구현함으로써, 여러 플랫폼에서 딥러닝 모델의 성공을 이끌어내는 주요 구성 요소 중 하나에 대한 통찰을 얻을 수 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>기계 학습 초보자이거나 경험이 풍부한 실무자이든 ReLU와 같은 활성화 함수의 내부 작업을 이해하는 것이 중요합니다. 이 지식을 통해 다양한 응용 프로그램을 위해 신경망을 더 잘 설계, 디버그 및 최적화할 수 있습니다.</p>
<p>좋은 코딩되세요!</p>
<p><img src="/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_3.png" alt="image"></p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Python, C, Rust로 직접 만들어보는 ReLU 활성화 함수 단계별 가이드","description":"","date":"2024-07-14 23:51","slug":"2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust","content":"\n\n\n![image](/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_0.png)\n\n# 소개\n\n신경망 세계에서 활성화 함수는 모델의 출력을 결정하는 데 중요한 역할을 합니다. 가장 인기 있는 활성화 함수 중 하나는 ReLU(렉티파이드 루 linear Unit)입니다. 간단함과 효과적임으로 유명한 ReLU는 많은 딥러닝 모델에서 표준 선택지가 되었습니다. 이 안내서에서는 Python, C 및 Rust 세 가지 다른 프로그래밍 언어로부터 ReLU 활성화 함수를 처음부터 만드는 과정을 안내합니다. 이를 통해 다양한 플랫폼에서의 구현과 이점에 대한 명확한 이해를 제공할 것입니다.\n\n# ReLU란 무엇인가요?\n\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\nReLU은 Rectified Linear Unit의 약자입니다. 이는 입력 값을 반환하는 활성화 함수로 정의됩니다:\n\n![ReLU Activation Function](/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_1.png)\n\n더 간단히 말하면, 입력 값이 양수인 경우 입력 값을 반환하고, 그렇지 않으면 0을 반환합니다. ReLU 함수는 수학적으로 다음과 같이 표현됩니다:\n\n![ReLU Formula](/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_2.png)\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# ReLU를 사용하는 이유?\n\n- 단순성: ReLU 함수는 0을 기준으로 한 간단한 임계값 처리를 포함하기 때문에 계산 효율적입니다.\n- 비선형성: 선형 함수처럼 보이지만 ReLU는 비선형성을 도입하여 복잡한 패턴을 학습하는 데 필수적입니다.\n- 희소 활성화: ReLU는 희소한 활성화를 생성하는 경향이 있어서 (많은 뉴런이 0을 출력) 네트워크를 더 효율적으로 만듭니다.\n\n# Python에서 ReLU 및 신경망 레이어 구현하기\n\n파이썬에서 ReLU 활성화 함수와 간단한 신경망 레이어를 구현해보겠습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# 파이썬 구현\n\n## 단계 1: ReLU 함수 정의\n\n먼저, 간단한 파이썬 함수를 사용하여 ReLU 함수를 정의해 보겠습니다.\n\n```python\ndef relu(x):\n    return max(0, x)\n```\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 단계 2: 배열에 ReLU 적용하기\n\n우리는 NumPy를 사용하여 배열을 처리할 수 있는 ReLU 함수를 확장할 것입니다.\n\n```python\nimport numpy as np\n\ndef relu_array(arr):\n    return np.maximum(0, arr)\n```\n\n## 단계 3: 간단한 신경 계층 정의하기\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n자, 이제 간단한 신경망 레이어 클래스를 만들어봅시다.\n\n```js\nclass SimpleNeuralLayer:\n    def __init__(self, input_size, output_size):\n        self.weights = np.random.randn(input_size, output_size)\n        self.biases = np.zeros(output_size)\n\n    def forward(self, inputs):\n        z = np.dot(inputs, self.weights) + self.biases\n        return relu_array(z)\n```\n\n## 단계 4: 신경망 레이어 테스트\n\n```js\n# 예제 사용법\nlayer = SimpleNeuralLayer(3, 2)\ninputs = np.array([1, 2, -1])\noutput = layer.forward(inputs)\nprint(output)\n```\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n# C에서 ReLU 및 신경 계층 구현\n\n이제 C에서 ReLU 함수와 간단한 신경망 계층을 구현해 보겠습니다.\n\n# C 구현\n\n## 단계 1: ReLU 함수 정의\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```js\n#include \u003cstdio.h\u003e\n\ndouble relu(double x) {\n    return x \u003e 0 ? x : 0;\n}\n```\n\n## 단계 2: 배열에 ReLU 적용하기\n\n```js\n#include \u003cstdio.h\u003e\n\nvoid relu_array(double* arr, int size) {\n    for (int i = 0; i \u003c size; i++) {\n        arr[i] = arr[i] \u003e 0 ? arr[i] : 0;\n    }\n}\n```\n\n## 단계 3: 간단한 신경망 레이어 정의하기\n\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n## 단계 4: 신경망 레이어 테스트\n\n```js\n#include \u003cstdio.h\u003e\n#include \u003cstdlib.h\u003e\n#include \u003ctime.h\u003e\n\nint main() {\n    srand(time(NULL));\n    SimpleNeuralLayer layer = create_layer(3, 2);\n    double inputs[] = {1, 2, -1};\n    double output[2];\n    forward(layer, inputs, output);\n    printf(\"Output: %f %f\\n\", output[0], output[1]);\n    free(layer.weights);\n    free(layer.biases);\n    return 0;\n}\n```\n\n# ReLU 및 신경망 레이어 구현하기\n\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n마침내, ReLU 함수와 간단한 신경망 레이어를 Rust로 구현해 봅시다.\n\n# Rust 구현\n\n## 단계 1: ReLU 함수 정의\n\n```rust\nfn relu(x: f64) -\u003e f64 {\n    if x \u003e 0.0 { x } else { 0.0 }\n}\n```\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 단계 2: 배열에 ReLU 적용하기\n\n```js\nfn relu_array(arr: \u0026mut [f64]) {\n    for x in arr.iter_mut() {\n        *x = if *x \u003e 0.0 { *x } else { 0.0 };\n    }\n}\n```\n\n## 단계 3: 간단한 신경망 계층 정의\n\n```js\nuse rand::Rng;\n\nstruct SimpleNeuralLayer {\n    weights: Vec\u003cVec\u003cf64\u003e\u003e,\n    biases: Vec\u003cf64\u003e,\n}\n\nimpl SimpleNeuralLayer {\n    fn new(input_size: usize, output_size: usize) -\u003e Self {\n        let mut rng = rand::thread_rng();\n        let weights = (0..input_size)\n            .map(|_| (0..output_size).map(|_| rng.gen_range(-1.0..1.0)).collect())\n            .collect();\n        let biases = vec![0.0; output_size];\n        \n        SimpleNeuralLayer { weights, biases }\n    }\n\n    fn forward(\u0026self, inputs: \u0026[f64]) -\u003e Vec\u003cf64\u003e {\n        let mut output = vec![0.0; self.biases.len()];\n        \n        for (i, \u0026bias) in self.biases.iter().enumerate() {\n            output[i] = inputs.iter()\n                .zip(\u0026self.weights)\n                .map(|(\u0026input, weight_row)| input * weight_row[i])\n                .sum::\u003cf64\u003e() + bias;\n            output[i] = relu(output[i]);\n        }\n        output\n    }\n}\n```\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## 단계 4: 신경 계층 테스트\n\n```js\nfn main() {\n    let layer = SimpleNeuralLayer::new(3, 2);\n    let inputs = [1.0, 2.0, -1.0];\n    let output = layer.forward(\u0026inputs);\n    \n    for value in output {\n        print!(\"{} \", value);\n    }\n    // 출력: 0 0\n}\n```\n\n# 결론\n\nReLU 활성화 함수를 만들고 간단한 신경망 계층에 통합하는 것은 신경망 작업에 대한 기본 개념을 강조합니다. Python, C 및 Rust에서 ReLU를 구현함으로써, 여러 플랫폼에서 딥러닝 모델의 성공을 이끌어내는 주요 구성 요소 중 하나에 대한 통찰을 얻을 수 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n기계 학습 초보자이거나 경험이 풍부한 실무자이든 ReLU와 같은 활성화 함수의 내부 작업을 이해하는 것이 중요합니다. 이 지식을 통해 다양한 응용 프로그램을 위해 신경망을 더 잘 설계, 디버그 및 최적화할 수 있습니다.\n\n좋은 코딩되세요!\n\n![image](/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_3.png)","ogImage":{"url":"/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_0.png"},"coverImage":"/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_0.png","tag":["Tech"],"readingTime":9},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_0.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003ch1\u003e소개\u003c/h1\u003e\n\u003cp\u003e신경망 세계에서 활성화 함수는 모델의 출력을 결정하는 데 중요한 역할을 합니다. 가장 인기 있는 활성화 함수 중 하나는 ReLU(렉티파이드 루 linear Unit)입니다. 간단함과 효과적임으로 유명한 ReLU는 많은 딥러닝 모델에서 표준 선택지가 되었습니다. 이 안내서에서는 Python, C 및 Rust 세 가지 다른 프로그래밍 언어로부터 ReLU 활성화 함수를 처음부터 만드는 과정을 안내합니다. 이를 통해 다양한 플랫폼에서의 구현과 이점에 대한 명확한 이해를 제공할 것입니다.\u003c/p\u003e\n\u003ch1\u003eReLU란 무엇인가요?\u003c/h1\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003eReLU은 Rectified Linear Unit의 약자입니다. 이는 입력 값을 반환하는 활성화 함수로 정의됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_1.png\" alt=\"ReLU Activation Function\"\u003e\u003c/p\u003e\n\u003cp\u003e더 간단히 말하면, 입력 값이 양수인 경우 입력 값을 반환하고, 그렇지 않으면 0을 반환합니다. ReLU 함수는 수학적으로 다음과 같이 표현됩니다:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_2.png\" alt=\"ReLU Formula\"\u003e\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003eReLU를 사용하는 이유?\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e단순성: ReLU 함수는 0을 기준으로 한 간단한 임계값 처리를 포함하기 때문에 계산 효율적입니다.\u003c/li\u003e\n\u003cli\u003e비선형성: 선형 함수처럼 보이지만 ReLU는 비선형성을 도입하여 복잡한 패턴을 학습하는 데 필수적입니다.\u003c/li\u003e\n\u003cli\u003e희소 활성화: ReLU는 희소한 활성화를 생성하는 경향이 있어서 (많은 뉴런이 0을 출력) 네트워크를 더 효율적으로 만듭니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003ePython에서 ReLU 및 신경망 레이어 구현하기\u003c/h1\u003e\n\u003cp\u003e파이썬에서 ReLU 활성화 함수와 간단한 신경망 레이어를 구현해보겠습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003e파이썬 구현\u003c/h1\u003e\n\u003ch2\u003e단계 1: ReLU 함수 정의\u003c/h2\u003e\n\u003cp\u003e먼저, 간단한 파이썬 함수를 사용하여 ReLU 함수를 정의해 보겠습니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erelu\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ex\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003emax\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, x)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e단계 2: 배열에 ReLU 적용하기\u003c/h2\u003e\n\u003cp\u003e우리는 NumPy를 사용하여 배열을 처리할 수 있는 ReLU 함수를 확장할 것입니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erelu_array\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003earr\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.maximum(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, arr)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e단계 3: 간단한 신경 계층 정의하기\u003c/h2\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e자, 이제 간단한 신경망 레이어 클래스를 만들어봅시다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSimpleNeuralLayer\u003c/span\u003e:\n    def \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(self, input_size, output_size):\n        self.\u003cspan class=\"hljs-property\"\u003eweights\u003c/span\u003e = np.\u003cspan class=\"hljs-property\"\u003erandom\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003erandn\u003c/span\u003e(input_size, output_size)\n        self.\u003cspan class=\"hljs-property\"\u003ebiases\u003c/span\u003e = np.\u003cspan class=\"hljs-title function_\"\u003ezeros\u003c/span\u003e(output_size)\n\n    def \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(self, inputs):\n        z = np.\u003cspan class=\"hljs-title function_\"\u003edot\u003c/span\u003e(inputs, self.\u003cspan class=\"hljs-property\"\u003eweights\u003c/span\u003e) + self.\u003cspan class=\"hljs-property\"\u003ebiases\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erelu_array\u003c/span\u003e(z)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e단계 4: 신경망 레이어 테스트\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e# 예제 사용법\nlayer = \u003cspan class=\"hljs-title class_\"\u003eSimpleNeuralLayer\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\ninputs = np.\u003cspan class=\"hljs-title function_\"\u003earray\u003c/span\u003e([\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\noutput = layer.\u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(inputs)\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(output)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch1\u003eC에서 ReLU 및 신경 계층 구현\u003c/h1\u003e\n\u003cp\u003e이제 C에서 ReLU 함수와 간단한 신경망 계층을 구현해 보겠습니다.\u003c/p\u003e\n\u003ch1\u003eC 구현\u003c/h1\u003e\n\u003ch2\u003e단계 1: ReLU 함수 정의\u003c/h2\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e#include \u0026#x3C;stdio.\u003cspan class=\"hljs-property\"\u003eh\u003c/span\u003e\u003e\n\ndouble \u003cspan class=\"hljs-title function_\"\u003erelu\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003edouble x\u003c/span\u003e) {\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e x \u003e \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e ? x : \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e단계 2: 배열에 ReLU 적용하기\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e#include \u0026#x3C;stdio.\u003cspan class=\"hljs-property\"\u003eh\u003c/span\u003e\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erelu_array\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003edouble* arr, int size\u003c/span\u003e) {\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (int i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; size; i++) {\n        arr[i] = arr[i] \u003e \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e ? arr[i] : \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e;\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e단계 3: 간단한 신경망 레이어 정의하기\u003c/h2\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e단계 4: 신경망 레이어 테스트\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e#include \u0026#x3C;stdio.\u003cspan class=\"hljs-property\"\u003eh\u003c/span\u003e\u003e\n#include \u0026#x3C;stdlib.\u003cspan class=\"hljs-property\"\u003eh\u003c/span\u003e\u003e\n#include \u0026#x3C;time.\u003cspan class=\"hljs-property\"\u003eh\u003c/span\u003e\u003e\n\nint \u003cspan class=\"hljs-title function_\"\u003emain\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\u003c/span\u003e) {\n    \u003cspan class=\"hljs-title function_\"\u003esrand\u003c/span\u003e(\u003cspan class=\"hljs-title function_\"\u003etime\u003c/span\u003e(\u003cspan class=\"hljs-variable constant_\"\u003eNULL\u003c/span\u003e));\n    \u003cspan class=\"hljs-title class_\"\u003eSimpleNeuralLayer\u003c/span\u003e layer = \u003cspan class=\"hljs-title function_\"\u003ecreate_layer\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e);\n    double inputs[] = {\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e};\n    double output[\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e];\n    \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(layer, inputs, output);\n    \u003cspan class=\"hljs-title function_\"\u003eprintf\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Output: %f %f\\n\"\u003c/span\u003e, output[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], output[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]);\n    \u003cspan class=\"hljs-title function_\"\u003efree\u003c/span\u003e(layer.\u003cspan class=\"hljs-property\"\u003eweights\u003c/span\u003e);\n    \u003cspan class=\"hljs-title function_\"\u003efree\u003c/span\u003e(layer.\u003cspan class=\"hljs-property\"\u003ebiases\u003c/span\u003e);\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003eReLU 및 신경망 레이어 구현하기\u003c/h1\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e마침내, ReLU 함수와 간단한 신경망 레이어를 Rust로 구현해 봅시다.\u003c/p\u003e\n\u003ch1\u003eRust 구현\u003c/h1\u003e\n\u003ch2\u003e단계 1: ReLU 함수 정의\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-rust\"\u003e\u003cspan class=\"hljs-keyword\"\u003efn\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erelu\u003c/span\u003e(x: \u003cspan class=\"hljs-type\"\u003ef64\u003c/span\u003e) \u003cspan class=\"hljs-punctuation\"\u003e-\u003e\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003ef64\u003c/span\u003e {\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e x \u003e \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e { x } \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e { \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e단계 2: 배열에 ReLU 적용하기\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003efn \u003cspan class=\"hljs-title function_\"\u003erelu_array\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003earr: \u0026#x26;mut [f64]\u003c/span\u003e) {\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e x \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e arr.\u003cspan class=\"hljs-title function_\"\u003eiter_mut\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\u003c/span\u003e) {\n        *x = \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e *x \u003e \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e { *x } \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e { \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e };\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e단계 3: 간단한 신경망 계층 정의\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003euse \u003cspan class=\"hljs-attr\"\u003erand\u003c/span\u003e::\u003cspan class=\"hljs-title class_\"\u003eRng\u003c/span\u003e;\n\nstruct \u003cspan class=\"hljs-title class_\"\u003eSimpleNeuralLayer\u003c/span\u003e {\n    \u003cspan class=\"hljs-attr\"\u003eweights\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eVec\u003c/span\u003e\u0026#x3C;\u003cspan class=\"hljs-title class_\"\u003eVec\u003c/span\u003e\u0026#x3C;f64\u003e\u003e,\n    \u003cspan class=\"hljs-attr\"\u003ebiases\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eVec\u003c/span\u003e\u0026#x3C;f64\u003e,\n}\n\nimpl \u003cspan class=\"hljs-title class_\"\u003eSimpleNeuralLayer\u003c/span\u003e {\n    fn \u003cspan class=\"hljs-title function_\"\u003enew\u003c/span\u003e(\u003cspan class=\"hljs-attr\"\u003einput_size\u003c/span\u003e: usize, \u003cspan class=\"hljs-attr\"\u003eoutput_size\u003c/span\u003e: usize) -\u003e \u003cspan class=\"hljs-title class_\"\u003eSelf\u003c/span\u003e {\n        \u003cspan class=\"hljs-keyword\"\u003elet\u003c/span\u003e mut rng = \u003cspan class=\"hljs-attr\"\u003erand\u003c/span\u003e::\u003cspan class=\"hljs-title function_\"\u003ethread_rng\u003c/span\u003e();\n        \u003cspan class=\"hljs-keyword\"\u003elet\u003c/span\u003e weights = (\u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003einput_size\u003c/span\u003e)\n            .\u003cspan class=\"hljs-title function_\"\u003emap\u003c/span\u003e(|_| (\u003cspan class=\"hljs-number\"\u003e0.\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eoutput_size\u003c/span\u003e).\u003cspan class=\"hljs-title function_\"\u003emap\u003c/span\u003e(|_| rng.\u003cspan class=\"hljs-title function_\"\u003egen_range\u003c/span\u003e(-\u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e.\u003cspan class=\"hljs-number\"\u003e.1\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.0\u003c/span\u003e)).\u003cspan class=\"hljs-title function_\"\u003ecollect\u003c/span\u003e())\n            .\u003cspan class=\"hljs-title function_\"\u003ecollect\u003c/span\u003e();\n        \u003cspan class=\"hljs-keyword\"\u003elet\u003c/span\u003e biases = vec![\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e; output_size];\n        \n        \u003cspan class=\"hljs-title class_\"\u003eSimpleNeuralLayer\u003c/span\u003e { weights, biases }\n    }\n\n    fn \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(\u0026#x26;self, \u003cspan class=\"hljs-attr\"\u003einputs\u003c/span\u003e: \u0026#x26;[f64]) -\u003e \u003cspan class=\"hljs-title class_\"\u003eVec\u003c/span\u003e\u0026#x3C;f64\u003e {\n        \u003cspan class=\"hljs-keyword\"\u003elet\u003c/span\u003e mut output = vec![\u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e; self.\u003cspan class=\"hljs-property\"\u003ebiases\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003elen\u003c/span\u003e()];\n        \n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (i, \u0026#x26;bias) \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e self.\u003cspan class=\"hljs-property\"\u003ebiases\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003eiter\u003c/span\u003e().\u003cspan class=\"hljs-title function_\"\u003eenumerate\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\u003c/span\u003e) {\n            output[i] = inputs.\u003cspan class=\"hljs-title function_\"\u003eiter\u003c/span\u003e()\n                .\u003cspan class=\"hljs-title function_\"\u003ezip\u003c/span\u003e(\u0026#x26;self.\u003cspan class=\"hljs-property\"\u003eweights\u003c/span\u003e)\n                .\u003cspan class=\"hljs-title function_\"\u003emap\u003c/span\u003e(|(\u0026#x26;input, weight_row)| input * weight_row[i])\n                .\u003cspan class=\"hljs-property\"\u003esum\u003c/span\u003e::\u0026#x3C;f64\u003e() + bias;\n            output[i] = \u003cspan class=\"hljs-title function_\"\u003erelu\u003c/span\u003e(output[i]);\n        }\n        output\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003e단계 4: 신경 계층 테스트\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003efn \u003cspan class=\"hljs-title function_\"\u003emain\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\u003c/span\u003e) {\n    \u003cspan class=\"hljs-keyword\"\u003elet\u003c/span\u003e layer = \u003cspan class=\"hljs-title class_\"\u003eSimpleNeuralLayer\u003c/span\u003e::\u003cspan class=\"hljs-title function_\"\u003enew\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e);\n    \u003cspan class=\"hljs-keyword\"\u003elet\u003c/span\u003e inputs = [\u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2.0\u003c/span\u003e, -\u003cspan class=\"hljs-number\"\u003e1.0\u003c/span\u003e];\n    \u003cspan class=\"hljs-keyword\"\u003elet\u003c/span\u003e output = layer.\u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(\u0026#x26;inputs);\n    \n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e value \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e output {\n        print!(\u003cspan class=\"hljs-string\"\u003e\"{} \"\u003c/span\u003e, value);\n    }\n    \u003cspan class=\"hljs-comment\"\u003e// 출력: 0 0\u003c/span\u003e\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e결론\u003c/h1\u003e\n\u003cp\u003eReLU 활성화 함수를 만들고 간단한 신경망 계층에 통합하는 것은 신경망 작업에 대한 기본 개념을 강조합니다. Python, C 및 Rust에서 ReLU를 구현함으로써, 여러 플랫폼에서 딥러닝 모델의 성공을 이끌어내는 주요 구성 요소 중 하나에 대한 통찰을 얻을 수 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e기계 학습 초보자이거나 경험이 풍부한 실무자이든 ReLU와 같은 활성화 함수의 내부 작업을 이해하는 것이 중요합니다. 이 지식을 통해 다양한 응용 프로그램을 위해 신경망을 더 잘 설계, 디버그 및 최적화할 수 있습니다.\u003c/p\u003e\n\u003cp\u003e좋은 코딩되세요!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_3.png\" alt=\"image\"\u003e\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust"},"buildId":"FuXRqV9h16krA5Mvtd6Dn","assetPrefix":"/TIL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>