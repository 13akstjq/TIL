<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><title>웹 SEO를 위한 robots.txt 작성과 활용법 | TIL</title><meta name="description" content=""/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:url" content="https://13akstjq.github.io/TIL//post/2025-04-22-robotstxt" data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta property="og:site_name" content="웹 SEO를 위한 robots.txt 작성과 활용법 | TIL" data-gatsby-head="true"/><meta property="og:title" content="웹 SEO를 위한 robots.txt 작성과 활용법 | TIL" data-gatsby-head="true"/><meta property="og:description" content="" data-gatsby-head="true"/><meta property="og:image" content="/TIL/assets/img/nextjs.png" data-gatsby-head="true"/><meta property="og:locale" content="en_US" data-gatsby-head="true"/><meta name="twitter:card" content="summary_large_image" data-gatsby-head="true"/><meta property="twitter:domain" content="https://TIL.github.io/" data-gatsby-head="true"/><meta property="twitter:url" content="https://13akstjq.github.io/TIL//post/2025-04-22-robotstxt" data-gatsby-head="true"/><meta name="twitter:title" content="웹 SEO를 위한 robots.txt 작성과 활용법 | TIL" data-gatsby-head="true"/><meta name="twitter:description" content="" data-gatsby-head="true"/><meta name="twitter:image" content="/TIL/assets/img/nextjs.png" data-gatsby-head="true"/><meta name="twitter:data1" content="Dev | TIL" data-gatsby-head="true"/><meta name="article:published_time" content="2025-04-22 02:53" data-gatsby-head="true"/><meta name="next-head-count" content="19"/><meta name="google-site-verification" content="a-yehRo3k3xv7fg6LqRaE8jlE42e5wP2bDE_2F849O4"/><link rel="stylesheet" href="/TIL/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/TIL/favicons/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/TIL/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="96x96" href="/TIL/favicons/favicon-96x96.png"/><link rel="icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-icon" href="/TIL/favicons/apple-icon-180x180.png"/><link rel="apple-touch-startup-image" href="/startup.png"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"/><meta name="msapplication-config" content="/TIL/favicons/browserconfig.xml"/><link rel="preload" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/6e57edcf9f2ce551.css" data-n-g=""/><link rel="preload" href="/TIL/_next/static/css/0eef537492fed77a.css" as="style"/><link rel="stylesheet" href="/TIL/_next/static/css/0eef537492fed77a.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/TIL/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/TIL/_next/static/chunks/webpack-21ffe88bdca56cba.js" defer=""></script><script src="/TIL/_next/static/chunks/framework-46611630e39cfdeb.js" defer=""></script><script src="/TIL/_next/static/chunks/main-a5eeabb286676ce6.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/_app-6fae11262ee5c69b.js" defer=""></script><script src="/TIL/_next/static/chunks/75fc9c18-4a646156c659a948.js" defer=""></script><script src="/TIL/_next/static/chunks/348-02483b66b493dd81.js" defer=""></script><script src="/TIL/_next/static/chunks/pages/post/%5Bslug%5D-8ded8b979ba73586.js" defer=""></script><script src="/TIL/_next/static/olPs1cVezSGTqD7OaewDI/_buildManifest.js" defer=""></script><script src="/TIL/_next/static/olPs1cVezSGTqD7OaewDI/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header class="Header_header__Z8PUO"><div class="Header_inner__tfr0u"><strong class="Header_title__Otn70"><a href="/TIL">TIL</a></strong><nav class="Header_nav_area__6KVpk"><a class="nav_item" href="/TIL/posts/1">Posts</a></nav></div></header><main class="posts_container__NyRU3"><div class="posts_inner__i3n_i"><h1 class="posts_post_title__EbxNx">웹 SEO를 위한 robots.txt 작성과 활용법</h1><div class="posts_meta__cR7lu"><div class="posts_profile_wrap__mslMl"><div class="posts_profile_image_wrap__kPikV"><img alt="웹 SEO를 위한 robots.txt 작성과 활용법" loading="lazy" width="44" height="44" decoding="async" data-nimg="1" class="profile" style="color:transparent" src="/TIL/favicons/apple-icon-114x114.png"/></div><div class="posts_textarea__w_iKT"><span class="writer">TIL</span><span class="posts_info__5KJdN"><span class="posts_date__ctqHI">Posted On Apr 22, 2025</span><span class="posts_reading_time__f7YPP">7<!-- --> min read</span></span></div></div></div><article class="posts_post_content__n_L6j"><div><!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
</head>
<body>
<h1>robots.txt 파일 쉽게 만들기</h1>
<p>웹사이트를 운영하다 보면, 검색 엔진 봇들이 우리 사이트를 어떻게 크롤링할지 정하는 게 중요한데요. 그럴 때 쓰는 게 바로 <strong>robots.txt</strong> 파일이에요. 이 파일을 웹사이트의 루트 디렉터리에 넣으면, 검색 엔진 크롤러에게 “여기까지는 들어와도 돼”, “여기는 접근하지마”라고 알려줄 수 있죠.</p>
<hr>
<h2>기본적인 robots.txt 예시</h2>
<pre><code class="hljs language-js"><span class="hljs-title class_">User</span>-<span class="hljs-title class_">Agent</span>: *
<span class="hljs-title class_">Allow</span>: /
<span class="hljs-title class_">Disallow</span>: <span class="hljs-regexp">/private/</span>

<span class="hljs-title class_">Sitemap</span>: <span class="hljs-attr">https</span>:<span class="hljs-comment">//acme.com/sitemap.xml</span>
</code></pre>
<ul>
<li><code>User-Agent: *</code> : 모든 크롤러에 적용한다는 뜻!</li>
<li><code>Allow: /</code> : 사이트 전체를 접근허용해요.</li>
<li><code>Disallow: /private/</code> : <code>/private/</code> 경로 아래는 크롤링 금지!</li>
<li><code>Sitemap</code> : 사이트맵 URL을 알려주면 봇들이 페이지 구조를 더 잘 알 수 있답니다.</li>
</ul>
<hr>
<h2>알아두면 좋은 점!</h2>
<ul>
<li>robots.txt 파일은 “규칙”이지만, 강제성이 없다 보니 악의적인 봇은 무시할 수도 있어요.</li>
<li>CSS, JS 같은 리소스가 차단되지 않게 하시는 게 좋아요. 구글 등 주요 검색 엔진은 이런 파일을 분석해서 페이지를 제대로 이해하거든요.</li>
<li>꼭 사이트 루트에 위치해야 하며, 경로를 잘못 넣으면 크롤러가 못 찾을 수 있어요.</li>
<li>사이트맵 URL 적는 것도 함께 하면 SEO에 도움 됩니다!</li>
</ul>
<p>robots.txt는 단순하면서도 웹사이트 검색 노출을 관리하는 데 중요한 파일이니, 위 내용을 참고해서 꼭 한 번만 세팅해 보세요! 그럼 여러분 사이트에 맞는 최적의 SEO 환경을 만들 수 있을 거예요. 😄</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h2>Robots 파일 생성하기</h2>
<p>웹사이트를 검색엔진이 어떻게 크롤링해야 하는지 알려주는 <code>robots.txt</code> 파일을 Next.js에서 쉽게 만들 수 있어요. 이때 <code>robots.js</code> 혹은 <code>robots.ts</code> 파일을 만들어서 <code>Robots</code> 객체를 반환하면 됩니다.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> type { <span class="hljs-title class_">MetadataRoute</span> } <span class="hljs-keyword">from</span> <span class="hljs-string">'next'</span>

<span class="hljs-keyword">export</span> <span class="hljs-keyword">default</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">robots</span>(<span class="hljs-params"></span>): <span class="hljs-title class_">MetadataRoute</span>.<span class="hljs-property">Robots</span> {
  <span class="hljs-keyword">return</span> {
    <span class="hljs-attr">rules</span>: {
      <span class="hljs-attr">userAgent</span>: <span class="hljs-string">'*'</span>,       <span class="hljs-comment">// 모든 봇에 대해 적용</span>
      <span class="hljs-attr">allow</span>: <span class="hljs-string">'/'</span>,           <span class="hljs-comment">// 루트 디렉토리는 크롤링 허용</span>
      <span class="hljs-attr">disallow</span>: <span class="hljs-string">'/private/'</span>, <span class="hljs-comment">// '/private/' 경로는 크롤링 금지</span>
    },
    <span class="hljs-attr">sitemap</span>: <span class="hljs-string">'https://acme.com/sitemap.xml'</span>, <span class="hljs-comment">// 사이트맵 위치 지정</span>
  }
}
</code></pre>
<h3>알아두면 좋은 점 🌟</h3>
<ul>
<li><code>robots.js</code>는 Next.js의 <strong>특별한 Route Handler</strong>인데, 기본적으로 캐싱이 되어 있어요.</li>
<li>다만 동적 API(dynamic API)나 동적 구성(dynamic config)을 사용할 경우에는 캐싱되지 않습니다.</li>
<li>이 방식이 더 관리하기 편하고, 직접 <code>public/robots.txt</code> 파일을 만들지 않아도 되니 개발 속도가 빨라져요.</li>
</ul>
<h3>개인적으로 팁!</h3>
<ul>
<li>사이트에 아주 민감한 정보가 있다면, <code>.env</code> 파일이나 서버 설정으로 크롤링 금지 경로를 관리하는 것도 좋아요.</li>
<li>그리고 구글 서치 콘솔 같은 도구에 사이트맵 URL을 등록해두면 검색엔진 최적화에도 도움이 됩니다.</li>
</ul>
<p>이렇게 설정만 하면 여러분 사이트에 맞는 <code>robots.txt</code>를 편리하게 관리할 수 있어요! 🕷️🚫</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>출력 예시:</p>
<pre><code class="hljs language-js"><span class="hljs-title class_">User</span>-<span class="hljs-title class_">Agent</span>: *
<span class="hljs-title class_">Allow</span>: /
<span class="hljs-title class_">Disallow</span>: <span class="hljs-regexp">/private/</span>

<span class="hljs-title class_">Sitemap</span>: <span class="hljs-attr">https</span>:<span class="hljs-comment">//acme.com/sitemap.xml</span>
</code></pre>
<h3>특정 사용자 에이전트 맞춤 설정하기</h3>
<p>검색 엔진 봇마다 사이트 크롤링 방식을 다르게 설정하고 싶을 때가 있죠? 이런 경우에는 <code>rules</code> 속성에 사용자 에이전트(User-Agent) 배열을 전달해서 각각을 개별적으로 지정할 수 있어요. 예를 들어, 이렇게 활용할 수 있습니다:</p>
<pre><code class="hljs language-js"><span class="hljs-attr">rules</span>: [
  {
    <span class="hljs-attr">userAgent</span>: [<span class="hljs-string">'Googlebot'</span>, <span class="hljs-string">'Bingbot'</span>],
    <span class="hljs-attr">allow</span>: [<span class="hljs-string">'/public'</span>],
    <span class="hljs-attr">disallow</span>: [<span class="hljs-string">'/private'</span>]
  },
  {
    <span class="hljs-attr">userAgent</span>: [<span class="hljs-string">'*'</span>],
    <span class="hljs-attr">disallow</span>: [<span class="hljs-string">'/admin'</span>]
  }
]
</code></pre>
<p>위처럼 하면 <code>Googlebot</code>과 <code>Bingbot</code>은 <code>/public</code> 경로를 허용하고 <code>/private</code>는 차단하지만, 모든 다른 봇들은 <code>/admin</code> 경로만 차단하도록 설정할 수 있죠.</p>
<p>추가 팁으로, 잘 구성된 <code>robots.txt</code>는 검색엔진 최적화(SEO)에도 도움을 주니 꼭 꼼꼼히 작성해 보세요! 그리고 필요하다면 사이트맵 위치도 명확히 표시해주면 크롤러가 사이트 구조를 더 잘 파악할 수 있습니다.</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>이번에는 Next.js에서 <code>MetadataRoute.Robots</code> 타입을 이용해 robots.txt를 설정하는 방법에 대해 살펴볼게요.</p>
<p>위 예제 코드를 보면 <code>robots()</code> 함수가 <code>MetadataRoute.Robots</code> 타입 객체를 반환하고 있습니다. 이 객체는 검색 엔진 크롤러들이 어떤 경로를 탐색할 수 있는지 세밀하게 조절할 수 있어요.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">export</span> <span class="hljs-keyword">default</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">robots</span>(<span class="hljs-params"></span>): <span class="hljs-title class_">MetadataRoute</span>.<span class="hljs-property">Robots</span> {
  <span class="hljs-keyword">return</span> {
    <span class="hljs-attr">rules</span>: [
      {
        <span class="hljs-attr">userAgent</span>: <span class="hljs-string">'Googlebot'</span>,
        <span class="hljs-attr">allow</span>: [<span class="hljs-string">'/'</span>],
        <span class="hljs-attr">disallow</span>: <span class="hljs-string">'/private/'</span>,
      },
      {
        <span class="hljs-attr">userAgent</span>: [<span class="hljs-string">'Applebot'</span>, <span class="hljs-string">'Bingbot'</span>],
        <span class="hljs-attr">disallow</span>: [<span class="hljs-string">'/'</span>],
      },
    ],
    <span class="hljs-attr">sitemap</span>: <span class="hljs-string">'https://acme.com/sitemap.xml'</span>,
  }
}
</code></pre>
<p>이 설정은 다음과 같은 의미를 갖는데요:</p>

























<table><thead><tr><th>User-Agent</th><th>Allow</th><th>Disallow</th></tr></thead><tbody><tr><td>Googlebot</td><td>/</td><td>/private/</td></tr><tr><td>Applebot</td><td></td><td>/</td></tr><tr><td>Bingbot</td><td></td><td>/</td></tr></tbody></table>
<ul>
<li>Googlebot에게는 전체 경로를 허용하지만 <code>/private/</code>는 막습니다.</li>
<li>Applebot과 Bingbot은 사이트 전체 접근 금지입니다.</li>
<li>Sitemap 위치도 명시되어 있어 크롤러가 쉽게 사이트맵을 찾을 수 있어요.</li>
</ul>
<p>이렇게 하면 robots.txt는 다음과 같이 생성됩니다:</p>
<p>User-Agent: Googlebot
Allow: /
Disallow: /private/</p>
<p>User-Agent: Applebot
Disallow: /</p>
<p>User-Agent: Bingbot
Disallow: /</p>
<p>Sitemap: <a href="https://acme.com/sitemap.xml" rel="nofollow" target="_blank">https://acme.com/sitemap.xml</a></p>
<h3>여기서 알아두면 좋은 점</h3>
<ul>
<li><code>allow</code>와 <code>disallow</code>를 배열로 줄 수도 있어서 여러 경로를 지정할 수 있어요.</li>
<li><code>userAgent</code>도 문자열 하나나 배열로 여러 봇을 지정 가능해서 편리하죠.</li>
<li>사이트맵 URL 꼭 포함하는 걸 추천합니다. 크롤러가 사이트 구조를 더 빠르게 파악하거든요.</li>
</ul>
<p>이 기능은 Next.js 13부터 정식 지원하면서 서버 컴포넌트 안에서 손쉽게 robots.txt 처리를 할 수 있어요. 예전엔 따로 파일을 만들어야 해서 좀 번거로웠는데, 이제 훨씬 깔끔하게 관리할 수 있답니다.</p>
<p>필요하면 robots.txt뿐만 아니라 <code>MetadataRoute</code> 타입으로 <code>favicon</code>이나 <code>manifest</code>도 관리할 수 있으니까 Next.js 공식 문서도 한번 살펴봐 보세요!</p>
<!-- TIL 수평 -->
<p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4877378276818686" data-ad-slot="1549334788" data-ad-format="auto" data-full-width-responsive="true"></ins></p>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<pre><code class="hljs language-ts"><span class="hljs-keyword">type</span> <span class="hljs-title class_">Robots</span> = {
  <span class="hljs-attr">rules</span>:
    | {
        userAgent?: <span class="hljs-built_in">string</span> | <span class="hljs-built_in">string</span>[]
        allow?: <span class="hljs-built_in">string</span> | <span class="hljs-built_in">string</span>[]
        disallow?: <span class="hljs-built_in">string</span> | <span class="hljs-built_in">string</span>[]
        crawlDelay?: <span class="hljs-built_in">number</span>
      }
    | <span class="hljs-title class_">Array</span>&#x3C;{
        <span class="hljs-attr">userAgent</span>: <span class="hljs-built_in">string</span> | <span class="hljs-built_in">string</span>[]
        allow?: <span class="hljs-built_in">string</span> | <span class="hljs-built_in">string</span>[]
        disallow?: <span class="hljs-built_in">string</span> | <span class="hljs-built_in">string</span>[]
        crawlDelay?: <span class="hljs-built_in">number</span>
      }>
  sitemap?: <span class="hljs-built_in">string</span> | <span class="hljs-built_in">string</span>[]
  host?: <span class="hljs-built_in">string</span>
}
</code></pre>
<h2>버전 히스토리</h2>













<table><thead><tr><th>Version</th><th>Changes</th></tr></thead><tbody><tr><td><code>v13.3.0</code></td><td><code>robots</code>가 도입되었습니다.</td></tr></tbody></table>
<hr>
<h3>robots 타입 살짝 풀어보기!</h3>
<p>위 타입은 주로 웹사이트의 <code>robots.txt</code> 설정을 타입스크립트로 표현한 거예요. 웹 크롤러가 어떤 페이지는 크롤링해도 되고, 어떤 페이지는 금지해야 할 때 <code>robots.txt</code>를 사용하는데요, 이런 설정을 코드로 관리할 때 유용하죠.</p>
<ul>
<li><code>rules</code>는 크롤러 별로 어떤 경로를 허용(<code>allow</code>)하거나 차단(<code>disallow</code>)할지 설정해요.</li>
<li><code>userAgent</code>는 어떤 크롤러에 대한 규칙인지 지정하는 부분이라, Googlebot, Bingbot 등 특정 크롤러 이름을 넣을 수 있어요. 배열로 여러 크롤러를 지정할 수도 있고요.</li>
<li><code>crawlDelay</code>는 크롤러가 요청 사이에 얼마나 기다려야 하는지 초 단위로 설정합니다.</li>
<li><code>sitemap</code>은 사이트맵 URL을 명시해 크롤러가 더 효율적으로 페이지를 찾게 돕고,</li>
<li><code>host</code>는 사이트의 대표 도메인을 지정하는 값이에요.</li>
</ul>
<p>이 타입으로 <code>robots.txt</code>를 프로그램matically(프로그램적으로) 생성하거나, 동적으로 관리하는 툴을 만들 때 큰 도움이 될 거예요!</p>
<p>필요하면 제가 간단한 예제도 함께 만들어볼게요. 관심 있으면 알려주세요!</p>
</body>
</html>
</div></article></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"웹 SEO를 위한 robots.txt 작성과 활용법","description":"","date":"2025-04-22 02:53","slug":"2025-04-22-robotstxt","content":"\n\n# robots.txt 파일 쉽게 만들기\n\n웹사이트를 운영하다 보면, 검색 엔진 봇들이 우리 사이트를 어떻게 크롤링할지 정하는 게 중요한데요. 그럴 때 쓰는 게 바로 **robots.txt** 파일이에요. 이 파일을 웹사이트의 루트 디렉터리에 넣으면, 검색 엔진 크롤러에게 “여기까지는 들어와도 돼”, “여기는 접근하지마”라고 알려줄 수 있죠.\n\n---\n\n## 기본적인 robots.txt 예시\n\n```js\nUser-Agent: *\nAllow: /\nDisallow: /private/\n\nSitemap: https://acme.com/sitemap.xml\n```\n\n- `User-Agent: *` : 모든 크롤러에 적용한다는 뜻!  \n- `Allow: /` : 사이트 전체를 접근허용해요.  \n- `Disallow: /private/` : `/private/` 경로 아래는 크롤링 금지!  \n- `Sitemap` : 사이트맵 URL을 알려주면 봇들이 페이지 구조를 더 잘 알 수 있답니다.\n\n---\n\n## 알아두면 좋은 점!\n\n- robots.txt 파일은 “규칙”이지만, 강제성이 없다 보니 악의적인 봇은 무시할 수도 있어요.  \n- CSS, JS 같은 리소스가 차단되지 않게 하시는 게 좋아요. 구글 등 주요 검색 엔진은 이런 파일을 분석해서 페이지를 제대로 이해하거든요.  \n- 꼭 사이트 루트에 위치해야 하며, 경로를 잘못 넣으면 크롤러가 못 찾을 수 있어요.  \n- 사이트맵 URL 적는 것도 함께 하면 SEO에 도움 됩니다!\n\nrobots.txt는 단순하면서도 웹사이트 검색 노출을 관리하는 데 중요한 파일이니, 위 내용을 참고해서 꼭 한 번만 세팅해 보세요! 그럼 여러분 사이트에 맞는 최적의 SEO 환경을 만들 수 있을 거예요. 😄\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n## Robots 파일 생성하기\n\n웹사이트를 검색엔진이 어떻게 크롤링해야 하는지 알려주는 `robots.txt` 파일을 Next.js에서 쉽게 만들 수 있어요. 이때 `robots.js` 혹은 `robots.ts` 파일을 만들어서 `Robots` 객체를 반환하면 됩니다.\n\n```js\nimport type { MetadataRoute } from 'next'\n\nexport default function robots(): MetadataRoute.Robots {\n  return {\n    rules: {\n      userAgent: '*',       // 모든 봇에 대해 적용\n      allow: '/',           // 루트 디렉토리는 크롤링 허용\n      disallow: '/private/', // '/private/' 경로는 크롤링 금지\n    },\n    sitemap: 'https://acme.com/sitemap.xml', // 사이트맵 위치 지정\n  }\n}\n```\n\n### 알아두면 좋은 점 🌟\n- `robots.js`는 Next.js의 **특별한 Route Handler**인데, 기본적으로 캐싱이 되어 있어요.  \n- 다만 동적 API(dynamic API)나 동적 구성(dynamic config)을 사용할 경우에는 캐싱되지 않습니다.  \n- 이 방식이 더 관리하기 편하고, 직접 `public/robots.txt` 파일을 만들지 않아도 되니 개발 속도가 빨라져요.\n\n### 개인적으로 팁!\n- 사이트에 아주 민감한 정보가 있다면, `.env` 파일이나 서버 설정으로 크롤링 금지 경로를 관리하는 것도 좋아요.  \n- 그리고 구글 서치 콘솔 같은 도구에 사이트맵 URL을 등록해두면 검색엔진 최적화에도 도움이 됩니다.\n\n이렇게 설정만 하면 여러분 사이트에 맞는 `robots.txt`를 편리하게 관리할 수 있어요! 🕷️🚫\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n출력 예시:\n\n```js\nUser-Agent: *\nAllow: /\nDisallow: /private/\n\nSitemap: https://acme.com/sitemap.xml\n```\n\n### 특정 사용자 에이전트 맞춤 설정하기\n\n검색 엔진 봇마다 사이트 크롤링 방식을 다르게 설정하고 싶을 때가 있죠? 이런 경우에는 `rules` 속성에 사용자 에이전트(User-Agent) 배열을 전달해서 각각을 개별적으로 지정할 수 있어요. 예를 들어, 이렇게 활용할 수 있습니다:\n\n```js\nrules: [\n  {\n    userAgent: ['Googlebot', 'Bingbot'],\n    allow: ['/public'],\n    disallow: ['/private']\n  },\n  {\n    userAgent: ['*'],\n    disallow: ['/admin']\n  }\n]\n```\n\n위처럼 하면 `Googlebot`과 `Bingbot`은 `/public` 경로를 허용하고 `/private`는 차단하지만, 모든 다른 봇들은 `/admin` 경로만 차단하도록 설정할 수 있죠.\n\n추가 팁으로, 잘 구성된 `robots.txt`는 검색엔진 최적화(SEO)에도 도움을 주니 꼭 꼼꼼히 작성해 보세요! 그리고 필요하다면 사이트맵 위치도 명확히 표시해주면 크롤러가 사이트 구조를 더 잘 파악할 수 있습니다.\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n이번에는 Next.js에서 `MetadataRoute.Robots` 타입을 이용해 robots.txt를 설정하는 방법에 대해 살펴볼게요.\n\n위 예제 코드를 보면 `robots()` 함수가 `MetadataRoute.Robots` 타입 객체를 반환하고 있습니다. 이 객체는 검색 엔진 크롤러들이 어떤 경로를 탐색할 수 있는지 세밀하게 조절할 수 있어요.\n\n```js\nexport default function robots(): MetadataRoute.Robots {\n  return {\n    rules: [\n      {\n        userAgent: 'Googlebot',\n        allow: ['/'],\n        disallow: '/private/',\n      },\n      {\n        userAgent: ['Applebot', 'Bingbot'],\n        disallow: ['/'],\n      },\n    ],\n    sitemap: 'https://acme.com/sitemap.xml',\n  }\n}\n```\n\n이 설정은 다음과 같은 의미를 갖는데요:\n\n| User-Agent  | Allow | Disallow   |\n|-------------|-------|------------|\n| Googlebot   | /     | /private/  |\n| Applebot    |       | /          |\n| Bingbot     |       | /          |\n\n- Googlebot에게는 전체 경로를 허용하지만 `/private/`는 막습니다.\n- Applebot과 Bingbot은 사이트 전체 접근 금지입니다.\n- Sitemap 위치도 명시되어 있어 크롤러가 쉽게 사이트맵을 찾을 수 있어요.\n\n이렇게 하면 robots.txt는 다음과 같이 생성됩니다:\n\n\nUser-Agent: Googlebot\nAllow: /\nDisallow: /private/\n\nUser-Agent: Applebot\nDisallow: /\n\nUser-Agent: Bingbot\nDisallow: /\n\nSitemap: https://acme.com/sitemap.xml\n\n\n### 여기서 알아두면 좋은 점\n- `allow`와 `disallow`를 배열로 줄 수도 있어서 여러 경로를 지정할 수 있어요.\n- `userAgent`도 문자열 하나나 배열로 여러 봇을 지정 가능해서 편리하죠.\n- 사이트맵 URL 꼭 포함하는 걸 추천합니다. 크롤러가 사이트 구조를 더 빠르게 파악하거든요.\n\n이 기능은 Next.js 13부터 정식 지원하면서 서버 컴포넌트 안에서 손쉽게 robots.txt 처리를 할 수 있어요. 예전엔 따로 파일을 만들어야 해서 좀 번거로웠는데, 이제 훨씬 깔끔하게 관리할 수 있답니다.\n\n필요하면 robots.txt뿐만 아니라 `MetadataRoute` 타입으로 `favicon`이나 `manifest`도 관리할 수 있으니까 Next.js 공식 문서도 한번 살펴봐 보세요!\n\n\u003c!-- TIL 수평 --\u003e\n\u003cins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n```ts\ntype Robots = {\n  rules:\n    | {\n        userAgent?: string | string[]\n        allow?: string | string[]\n        disallow?: string | string[]\n        crawlDelay?: number\n      }\n    | Array\u003c{\n        userAgent: string | string[]\n        allow?: string | string[]\n        disallow?: string | string[]\n        crawlDelay?: number\n      }\u003e\n  sitemap?: string | string[]\n  host?: string\n}\n```\n\n## 버전 히스토리\n\n| Version   | Changes               |\n|-----------|-----------------------|\n| `v13.3.0` | `robots`가 도입되었습니다. |\n\n---\n\n### robots 타입 살짝 풀어보기!\n\n위 타입은 주로 웹사이트의 `robots.txt` 설정을 타입스크립트로 표현한 거예요. 웹 크롤러가 어떤 페이지는 크롤링해도 되고, 어떤 페이지는 금지해야 할 때 `robots.txt`를 사용하는데요, 이런 설정을 코드로 관리할 때 유용하죠.\n\n- `rules`는 크롤러 별로 어떤 경로를 허용(`allow`)하거나 차단(`disallow`)할지 설정해요.\n- `userAgent`는 어떤 크롤러에 대한 규칙인지 지정하는 부분이라, Googlebot, Bingbot 등 특정 크롤러 이름을 넣을 수 있어요. 배열로 여러 크롤러를 지정할 수도 있고요.\n- `crawlDelay`는 크롤러가 요청 사이에 얼마나 기다려야 하는지 초 단위로 설정합니다.\n- `sitemap`은 사이트맵 URL을 명시해 크롤러가 더 효율적으로 페이지를 찾게 돕고,\n- `host`는 사이트의 대표 도메인을 지정하는 값이에요.\n\n이 타입으로 `robots.txt`를 프로그램matically(프로그램적으로) 생성하거나, 동적으로 관리하는 툴을 만들 때 큰 도움이 될 거예요!\n\n필요하면 제가 간단한 예제도 함께 만들어볼게요. 관심 있으면 알려주세요!","ogImage":{"url":"/TIL/assets/img/nextjs.png"},"coverImage":"/TIL/assets/img/nextjs.png","tag":["Tech"],"readingTime":7},"content":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n\u003cmeta charset=\"utf-8\"\u003e\n\u003cmeta content=\"width=device-width, initial-scale=1\" name=\"viewport\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n\u003ch1\u003erobots.txt 파일 쉽게 만들기\u003c/h1\u003e\n\u003cp\u003e웹사이트를 운영하다 보면, 검색 엔진 봇들이 우리 사이트를 어떻게 크롤링할지 정하는 게 중요한데요. 그럴 때 쓰는 게 바로 \u003cstrong\u003erobots.txt\u003c/strong\u003e 파일이에요. 이 파일을 웹사이트의 루트 디렉터리에 넣으면, 검색 엔진 크롤러에게 “여기까지는 들어와도 돼”, “여기는 접근하지마”라고 알려줄 수 있죠.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e기본적인 robots.txt 예시\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title class_\"\u003eUser\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eAgent\u003c/span\u003e: *\n\u003cspan class=\"hljs-title class_\"\u003eAllow\u003c/span\u003e: /\n\u003cspan class=\"hljs-title class_\"\u003eDisallow\u003c/span\u003e: \u003cspan class=\"hljs-regexp\"\u003e/private/\u003c/span\u003e\n\n\u003cspan class=\"hljs-title class_\"\u003eSitemap\u003c/span\u003e: \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//acme.com/sitemap.xml\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eUser-Agent: *\u003c/code\u003e : 모든 크롤러에 적용한다는 뜻!\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eAllow: /\u003c/code\u003e : 사이트 전체를 접근허용해요.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eDisallow: /private/\u003c/code\u003e : \u003ccode\u003e/private/\u003c/code\u003e 경로 아래는 크롤링 금지!\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSitemap\u003c/code\u003e : 사이트맵 URL을 알려주면 봇들이 페이지 구조를 더 잘 알 수 있답니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e알아두면 좋은 점!\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003erobots.txt 파일은 “규칙”이지만, 강제성이 없다 보니 악의적인 봇은 무시할 수도 있어요.\u003c/li\u003e\n\u003cli\u003eCSS, JS 같은 리소스가 차단되지 않게 하시는 게 좋아요. 구글 등 주요 검색 엔진은 이런 파일을 분석해서 페이지를 제대로 이해하거든요.\u003c/li\u003e\n\u003cli\u003e꼭 사이트 루트에 위치해야 하며, 경로를 잘못 넣으면 크롤러가 못 찾을 수 있어요.\u003c/li\u003e\n\u003cli\u003e사이트맵 URL 적는 것도 함께 하면 SEO에 도움 됩니다!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003erobots.txt는 단순하면서도 웹사이트 검색 노출을 관리하는 데 중요한 파일이니, 위 내용을 참고해서 꼭 한 번만 세팅해 보세요! 그럼 여러분 사이트에 맞는 최적의 SEO 환경을 만들 수 있을 거예요. 😄\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003ch2\u003eRobots 파일 생성하기\u003c/h2\u003e\n\u003cp\u003e웹사이트를 검색엔진이 어떻게 크롤링해야 하는지 알려주는 \u003ccode\u003erobots.txt\u003c/code\u003e 파일을 Next.js에서 쉽게 만들 수 있어요. 이때 \u003ccode\u003erobots.js\u003c/code\u003e 혹은 \u003ccode\u003erobots.ts\u003c/code\u003e 파일을 만들어서 \u003ccode\u003eRobots\u003c/code\u003e 객체를 반환하면 됩니다.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e type { \u003cspan class=\"hljs-title class_\"\u003eMetadataRoute\u003c/span\u003e } \u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'next'\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003eexport\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003edefault\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003efunction\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erobots\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\u003c/span\u003e): \u003cspan class=\"hljs-title class_\"\u003eMetadataRoute\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eRobots\u003c/span\u003e {\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e {\n    \u003cspan class=\"hljs-attr\"\u003erules\u003c/span\u003e: {\n      \u003cspan class=\"hljs-attr\"\u003euserAgent\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'*'\u003c/span\u003e,       \u003cspan class=\"hljs-comment\"\u003e// 모든 봇에 대해 적용\u003c/span\u003e\n      \u003cspan class=\"hljs-attr\"\u003eallow\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'/'\u003c/span\u003e,           \u003cspan class=\"hljs-comment\"\u003e// 루트 디렉토리는 크롤링 허용\u003c/span\u003e\n      \u003cspan class=\"hljs-attr\"\u003edisallow\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'/private/'\u003c/span\u003e, \u003cspan class=\"hljs-comment\"\u003e// '/private/' 경로는 크롤링 금지\u003c/span\u003e\n    },\n    \u003cspan class=\"hljs-attr\"\u003esitemap\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'https://acme.com/sitemap.xml'\u003c/span\u003e, \u003cspan class=\"hljs-comment\"\u003e// 사이트맵 위치 지정\u003c/span\u003e\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e알아두면 좋은 점 🌟\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003erobots.js\u003c/code\u003e는 Next.js의 \u003cstrong\u003e특별한 Route Handler\u003c/strong\u003e인데, 기본적으로 캐싱이 되어 있어요.\u003c/li\u003e\n\u003cli\u003e다만 동적 API(dynamic API)나 동적 구성(dynamic config)을 사용할 경우에는 캐싱되지 않습니다.\u003c/li\u003e\n\u003cli\u003e이 방식이 더 관리하기 편하고, 직접 \u003ccode\u003epublic/robots.txt\u003c/code\u003e 파일을 만들지 않아도 되니 개발 속도가 빨라져요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e개인적으로 팁!\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e사이트에 아주 민감한 정보가 있다면, \u003ccode\u003e.env\u003c/code\u003e 파일이나 서버 설정으로 크롤링 금지 경로를 관리하는 것도 좋아요.\u003c/li\u003e\n\u003cli\u003e그리고 구글 서치 콘솔 같은 도구에 사이트맵 URL을 등록해두면 검색엔진 최적화에도 도움이 됩니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이렇게 설정만 하면 여러분 사이트에 맞는 \u003ccode\u003erobots.txt\u003c/code\u003e를 편리하게 관리할 수 있어요! 🕷️🚫\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e출력 예시:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-title class_\"\u003eUser\u003c/span\u003e-\u003cspan class=\"hljs-title class_\"\u003eAgent\u003c/span\u003e: *\n\u003cspan class=\"hljs-title class_\"\u003eAllow\u003c/span\u003e: /\n\u003cspan class=\"hljs-title class_\"\u003eDisallow\u003c/span\u003e: \u003cspan class=\"hljs-regexp\"\u003e/private/\u003c/span\u003e\n\n\u003cspan class=\"hljs-title class_\"\u003eSitemap\u003c/span\u003e: \u003cspan class=\"hljs-attr\"\u003ehttps\u003c/span\u003e:\u003cspan class=\"hljs-comment\"\u003e//acme.com/sitemap.xml\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e특정 사용자 에이전트 맞춤 설정하기\u003c/h3\u003e\n\u003cp\u003e검색 엔진 봇마다 사이트 크롤링 방식을 다르게 설정하고 싶을 때가 있죠? 이런 경우에는 \u003ccode\u003erules\u003c/code\u003e 속성에 사용자 에이전트(User-Agent) 배열을 전달해서 각각을 개별적으로 지정할 수 있어요. 예를 들어, 이렇게 활용할 수 있습니다:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-attr\"\u003erules\u003c/span\u003e: [\n  {\n    \u003cspan class=\"hljs-attr\"\u003euserAgent\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'Googlebot'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Bingbot'\u003c/span\u003e],\n    \u003cspan class=\"hljs-attr\"\u003eallow\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'/public'\u003c/span\u003e],\n    \u003cspan class=\"hljs-attr\"\u003edisallow\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'/private'\u003c/span\u003e]\n  },\n  {\n    \u003cspan class=\"hljs-attr\"\u003euserAgent\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'*'\u003c/span\u003e],\n    \u003cspan class=\"hljs-attr\"\u003edisallow\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'/admin'\u003c/span\u003e]\n  }\n]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e위처럼 하면 \u003ccode\u003eGooglebot\u003c/code\u003e과 \u003ccode\u003eBingbot\u003c/code\u003e은 \u003ccode\u003e/public\u003c/code\u003e 경로를 허용하고 \u003ccode\u003e/private\u003c/code\u003e는 차단하지만, 모든 다른 봇들은 \u003ccode\u003e/admin\u003c/code\u003e 경로만 차단하도록 설정할 수 있죠.\u003c/p\u003e\n\u003cp\u003e추가 팁으로, 잘 구성된 \u003ccode\u003erobots.txt\u003c/code\u003e는 검색엔진 최적화(SEO)에도 도움을 주니 꼭 꼼꼼히 작성해 보세요! 그리고 필요하다면 사이트맵 위치도 명확히 표시해주면 크롤러가 사이트 구조를 더 잘 파악할 수 있습니다.\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cp\u003e이번에는 Next.js에서 \u003ccode\u003eMetadataRoute.Robots\u003c/code\u003e 타입을 이용해 robots.txt를 설정하는 방법에 대해 살펴볼게요.\u003c/p\u003e\n\u003cp\u003e위 예제 코드를 보면 \u003ccode\u003erobots()\u003c/code\u003e 함수가 \u003ccode\u003eMetadataRoute.Robots\u003c/code\u003e 타입 객체를 반환하고 있습니다. 이 객체는 검색 엔진 크롤러들이 어떤 경로를 탐색할 수 있는지 세밀하게 조절할 수 있어요.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-js\"\u003e\u003cspan class=\"hljs-keyword\"\u003eexport\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003edefault\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003efunction\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003erobots\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\u003c/span\u003e): \u003cspan class=\"hljs-title class_\"\u003eMetadataRoute\u003c/span\u003e.\u003cspan class=\"hljs-property\"\u003eRobots\u003c/span\u003e {\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e {\n    \u003cspan class=\"hljs-attr\"\u003erules\u003c/span\u003e: [\n      {\n        \u003cspan class=\"hljs-attr\"\u003euserAgent\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'Googlebot'\u003c/span\u003e,\n        \u003cspan class=\"hljs-attr\"\u003eallow\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'/'\u003c/span\u003e],\n        \u003cspan class=\"hljs-attr\"\u003edisallow\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'/private/'\u003c/span\u003e,\n      },\n      {\n        \u003cspan class=\"hljs-attr\"\u003euserAgent\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'Applebot'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'Bingbot'\u003c/span\u003e],\n        \u003cspan class=\"hljs-attr\"\u003edisallow\u003c/span\u003e: [\u003cspan class=\"hljs-string\"\u003e'/'\u003c/span\u003e],\n      },\n    ],\n    \u003cspan class=\"hljs-attr\"\u003esitemap\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'https://acme.com/sitemap.xml'\u003c/span\u003e,\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e이 설정은 다음과 같은 의미를 갖는데요:\u003c/p\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003eUser-Agent\u003c/th\u003e\u003cth\u003eAllow\u003c/th\u003e\u003cth\u003eDisallow\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003eGooglebot\u003c/td\u003e\u003ctd\u003e/\u003c/td\u003e\u003ctd\u003e/private/\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eApplebot\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003ctd\u003e/\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eBingbot\u003c/td\u003e\u003ctd\u003e\u003c/td\u003e\u003ctd\u003e/\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003eGooglebot에게는 전체 경로를 허용하지만 \u003ccode\u003e/private/\u003c/code\u003e는 막습니다.\u003c/li\u003e\n\u003cli\u003eApplebot과 Bingbot은 사이트 전체 접근 금지입니다.\u003c/li\u003e\n\u003cli\u003eSitemap 위치도 명시되어 있어 크롤러가 쉽게 사이트맵을 찾을 수 있어요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이렇게 하면 robots.txt는 다음과 같이 생성됩니다:\u003c/p\u003e\n\u003cp\u003eUser-Agent: Googlebot\nAllow: /\nDisallow: /private/\u003c/p\u003e\n\u003cp\u003eUser-Agent: Applebot\nDisallow: /\u003c/p\u003e\n\u003cp\u003eUser-Agent: Bingbot\nDisallow: /\u003c/p\u003e\n\u003cp\u003eSitemap: \u003ca href=\"https://acme.com/sitemap.xml\" rel=\"nofollow\" target=\"_blank\"\u003ehttps://acme.com/sitemap.xml\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e여기서 알아두면 좋은 점\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eallow\u003c/code\u003e와 \u003ccode\u003edisallow\u003c/code\u003e를 배열로 줄 수도 있어서 여러 경로를 지정할 수 있어요.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003euserAgent\u003c/code\u003e도 문자열 하나나 배열로 여러 봇을 지정 가능해서 편리하죠.\u003c/li\u003e\n\u003cli\u003e사이트맵 URL 꼭 포함하는 걸 추천합니다. 크롤러가 사이트 구조를 더 빠르게 파악하거든요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이 기능은 Next.js 13부터 정식 지원하면서 서버 컴포넌트 안에서 손쉽게 robots.txt 처리를 할 수 있어요. 예전엔 따로 파일을 만들어야 해서 좀 번거로웠는데, 이제 훨씬 깔끔하게 관리할 수 있답니다.\u003c/p\u003e\n\u003cp\u003e필요하면 robots.txt뿐만 아니라 \u003ccode\u003eMetadataRoute\u003c/code\u003e 타입으로 \u003ccode\u003efavicon\u003c/code\u003e이나 \u003ccode\u003emanifest\u003c/code\u003e도 관리할 수 있으니까 Next.js 공식 문서도 한번 살펴봐 보세요!\u003c/p\u003e\n\u003c!-- TIL 수평 --\u003e\n\u003cp\u003e\u003cins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"\u003e\u003c/ins\u003e\u003c/p\u003e\n\u003cscript\u003e\n(adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-ts\"\u003e\u003cspan class=\"hljs-keyword\"\u003etype\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eRobots\u003c/span\u003e = {\n  \u003cspan class=\"hljs-attr\"\u003erules\u003c/span\u003e:\n    | {\n        userAgent?: \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e | \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e[]\n        allow?: \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e | \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e[]\n        disallow?: \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e | \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e[]\n        crawlDelay?: \u003cspan class=\"hljs-built_in\"\u003enumber\u003c/span\u003e\n      }\n    | \u003cspan class=\"hljs-title class_\"\u003eArray\u003c/span\u003e\u0026#x3C;{\n        \u003cspan class=\"hljs-attr\"\u003euserAgent\u003c/span\u003e: \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e | \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e[]\n        allow?: \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e | \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e[]\n        disallow?: \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e | \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e[]\n        crawlDelay?: \u003cspan class=\"hljs-built_in\"\u003enumber\u003c/span\u003e\n      }\u003e\n  sitemap?: \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e | \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e[]\n  host?: \u003cspan class=\"hljs-built_in\"\u003estring\u003c/span\u003e\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e버전 히스토리\u003c/h2\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003eVersion\u003c/th\u003e\u003cth\u003eChanges\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd\u003e\u003ccode\u003ev13.3.0\u003c/code\u003e\u003c/td\u003e\u003ctd\u003e\u003ccode\u003erobots\u003c/code\u003e가 도입되었습니다.\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003chr\u003e\n\u003ch3\u003erobots 타입 살짝 풀어보기!\u003c/h3\u003e\n\u003cp\u003e위 타입은 주로 웹사이트의 \u003ccode\u003erobots.txt\u003c/code\u003e 설정을 타입스크립트로 표현한 거예요. 웹 크롤러가 어떤 페이지는 크롤링해도 되고, 어떤 페이지는 금지해야 할 때 \u003ccode\u003erobots.txt\u003c/code\u003e를 사용하는데요, 이런 설정을 코드로 관리할 때 유용하죠.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003erules\u003c/code\u003e는 크롤러 별로 어떤 경로를 허용(\u003ccode\u003eallow\u003c/code\u003e)하거나 차단(\u003ccode\u003edisallow\u003c/code\u003e)할지 설정해요.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003euserAgent\u003c/code\u003e는 어떤 크롤러에 대한 규칙인지 지정하는 부분이라, Googlebot, Bingbot 등 특정 크롤러 이름을 넣을 수 있어요. 배열로 여러 크롤러를 지정할 수도 있고요.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecrawlDelay\u003c/code\u003e는 크롤러가 요청 사이에 얼마나 기다려야 하는지 초 단위로 설정합니다.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esitemap\u003c/code\u003e은 사이트맵 URL을 명시해 크롤러가 더 효율적으로 페이지를 찾게 돕고,\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ehost\u003c/code\u003e는 사이트의 대표 도메인을 지정하는 값이에요.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e이 타입으로 \u003ccode\u003erobots.txt\u003c/code\u003e를 프로그램matically(프로그램적으로) 생성하거나, 동적으로 관리하는 툴을 만들 때 큰 도움이 될 거예요!\u003c/p\u003e\n\u003cp\u003e필요하면 제가 간단한 예제도 함께 만들어볼게요. 관심 있으면 알려주세요!\u003c/p\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2025-04-22-robotstxt"},"buildId":"olPs1cVezSGTqD7OaewDI","assetPrefix":"/TIL","isFallback":false,"gsp":true,"scriptLoader":[{"async":true,"src":"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4877378276818686","strategy":"lazyOnload","crossOrigin":"anonymous"}]}</script></body></html>