{"pageProps":{"posts":[{"title":"한 줄의 문자열 때문에 Apple App Store에서 금지된 Python 프로그램","description":"","date":"2024-07-09 20:02","slug":"2024-07-09-PythonProgramBannedfromAppleAppStoreDuetoaSingleString","content":"\n## APPLE APP STORE 리뷰의 투명성 부족\n\n파이썬 개발자들이 특이한 문제에 직면하고 있습니다. 프로그래밍 언어 버전을 업그레이드하는 것이 앱 스토어에서 앱 거부를 유발하고 있습니다.\n\n최근 일부 개발자들이 Python 3.11에서 3.12로 업그레이드했을 때, 그들의 앱을 애플 앱 스토어에 재제출했을 때 거부당했습니다.\n\n이 문제는 많은 개발자들의 관심을 끌었습니다. 문제는 Python 3.12에 있는지, 아니면 애플의 리뷰 팀에 있는지요?\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 앱 리뷰 실패의 이유: 하나의 문자열 때문\n\n![image](/TIL/assets/img/2024-07-09-PythonProgramBannedfromAppleAppStoreDuetoaSingleString_0.png)\n\n개발자인 Eric Froemling은 GitHub에서 경험을 공유했습니다.\n\nEric은 처음에 이전에 승인된 앱이 왜 거부되었는지 이해하지 못했습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n앱 스토어 리뷰 팀에서는 왜 그랬는지 설명을 해주지 않았어요. 그저 \"더 많은 정보를 제공할 수 없다\"고만 말했죠.\n\n몇 차례 시도 끝에, Eric은 Apple에 항의를 보냈어요. 결국 그들은 힌트를 주었어요:\n\n가이드라인 2.5.2 - 성능 - 소프트웨어 요구 사항에 따르면:\n\n앱이 설치되거나 실행 가능한 코드를 실행합니다. 특히, 앱을 설치하기 위해 itms-services URL scheme을 사용한다고 해요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nitms-services URL 스키마는 Apple이 App Store 외부에서 iOS 앱을 배포하는 방법입니다. 주로 내부 또는 테스트용 앱에 사용됩니다.\n\n이를 통해 사용자는 App Store를 사용하지 않고 iOS 기기에 앱을 직접 설치할 수 있습니다.\n\n기본적인 itms-services URL은 다음과 같이 보입니다:\n\n```js\nitms-services://?action=download-manifest&url=https://example.com/manifest.plist\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n많은 조사 끝에, 에릭은 문제 파일을 찾았어요: Lib/urllib/parse.py (파이썬의 표준 라이브러리 URL 파서)와 그 .pyc 파일.\n\nPython 3.12에서 “itms-services” 문자열이 추가되었어요. 애플은 이 문자열을 검색하고 포함된 앱을 자동으로 거부하는 것으로 보입니다.\n\n마침내, 에릭은 이 문자열을 Python 코드에서 제거했어요. 그 후, 업데이트된 앱이 검토를 통과하고 성공적으로 앱 스토어에 등록되었어요.\n\n# 논란이 되는 애플의 리뷰 및 피드백 규칙\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/TIL/assets/img/2024-07-09-PythonProgramBannedfromAppleAppStoreDuetoaSingleString_1.png)\n\n에릭 프롬링은 \"itms-services\" 문자열 자체에 화를 내지 않았어요. 그보다 애플 앱 스토어 심사 규칙에 답답해했던 거예요.\n\n많은 사람들이 알다시피, 디버깅은 종종 코딩보다 어려워요.\n\n에릭은 많은 시간을 디버깅에 할애했어요. 문제를 해결하려면 단 하나의 문자열을 삭제하기만 하면 됐어요. 많은 개발자들은 애플의 심사 과정이 좀 더 명확했다면 이런 문제를 피할 수 있었을 거라고 생각하지만 사실 애플의 심사 과정은 투명하지 않아요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# CPython 코어 개발자: 앱 스토어 리뷰 규칙은 엄격하고 예측할 수 없어요!\n\n![Image](/TIL/assets/img/2024-07-09-PythonProgramBannedfromAppleAppStoreDuetoaSingleString_2.png)\n\nCPython 코어 개발자인 Russell Keith-Magee가 이 문제에 대해 기사를 썼어요. 그는 질문을 던지며 말했어요: 앱 스토어 규정에 맞추기 위해 얼마나 많은 변경을 해야 할까요?\n\n문제는 애플의 macOS 앱 스토어에서 \"itms-services\" 문자열이 포함된 앱을 자동으로 거부한다는 것이에요. 심지어 앱이 itms-services:// URL을 사용하지 않아도 그렇다고 해요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n러셀은 두 가지 해결책을 제안합니다:\n\n1. \"App Store 준수\"를 CPython의 목표로 설정하는 것입니다. 이렇게 하면 사용자는 CPython을 패치할 필요가 없지만 더러운 코드를 야기할 수 있습니다.\n\n2. 이것을 배포 문제로 취급합니다. Briefcase나 Py2app과 같은 도구를 사용하여 앱 스토어용으로 CPython을 패치합니다.\n\n두 옵션 모두 장단점이 있습니다. 옵션 1은 수정된 Python을 배포하는 것을 의미합니다. 옵션 2는 계속해서 패치를 해야 합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n알렉스 게이너, 또 다른 코어 개발자,은 세 번째 옵션을 제안했습니다:\n\n- 이 문제에 대한 소규모 지역화된 수정 사항을 허용합니다.\n- 병합하기 전에 문제에 대해 써드 파티(예: Apple)에 불만을 제기해야 합니다.\n- 이러한 수정 사항에 대해 시간 제한을 설정합니다.\n\n이 방법은 사용자 경험과 대기업의 자체 문제 수정을 균형있게 조합합니다.\n\n키스 매지 후에 네 번째 옵션을 제안했습니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n문제가 되는 코드를 제거할 수 있는 빌드 옵션을 추가해보는 것은 코드를 난독화하는 것보다 더 깔끔한 해결책이 될 것입니다.\n\n이 옵션은 다음을 포함할 것입니다:\n\n- 변경 사항을 설명하는 diff 파일 추가.\n- 환경 설정을 위한 --with-app-store-patch 옵션 추가.\n- 필요한 경우 배급 업체가 업데이트된 패치를 제공할 수 있도록 함.\n\n이를 통해 CPython이 App Store 규정을 준수하기 위해 필요한 변경 사항을 공식적으로 목록화할 수 있게 됩니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nKeith-Magee가 이 해결책이 더 수용 가능한 것 같다고 물었습니다.\n\n# 결론\n\n![이미지](/TIL/assets/img/2024-07-09-PythonProgramBannedfromAppleAppStoreDuetoaSingleString_3.png)\n\n몇 일을 생각한 후, Keith-Magee가 6월 25일에 답변했습니다. 그는 --with-app-store-compliance 옵션을 추가하는 풀 리퀘스트(#120984)를 제출했습니다. 이것은 문자열 때문에 App Store에서 앱이 거부당하는 문제를 해결해야 할 것입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그는 이 옵션이 iOS와 macOS 외의 플랫폼에서도 사용될 수 있다고 언급했지만, 현재는 필요가 없다고 합니다.\n\n모든게 순조롭게 진행된다면, 이는 Python 3.13에서 사용할 수 있을 것입니다.\n\n많은 개발자들이 Python과 같은 무료 소프트웨어 프로젝트들이 명확하지 않은 검토 프로세스를 피해야 하는 방법을 찾는 데 시간을 낭비해야 한다는 점에 좌절하고 있습니다. 이는 개발자들이 비자유 플랫폼을 위한 소프트웨어를 만들 수 있도록 하는 것뿐입니다.\n\nHN 사용자가 댓글로 남긴 내용:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 경우에 Keith-Magee와 다른 CPython 개발자들이 빠른 조치를 취했습니다. 그들의 해결책은 파이썬 앱 개발자들에게 최고의 경험을 제공하는 가장 쉬운 방법으로 보입니다.\n\n그러나 이런 문제가 다시 발생할 가능성은 매우 높습니다.\n\n제가 딱 한 달 만에 5,000명 이상의 팔로워를 얻었다는 점이 최고의 증거이기 때문에 “Medium에서 빠르게 팔로워 모으는 방법”에 대한 eBook을 작성 중입니다. 계속해서 주세요!\n\nSubstack에서 “대규모 모델 애플리케이션 개발” 시리즈를 쓰고 있습니다. 관심 있으시면 팔로우해 주세요!\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n친구님께서 읽어주셔서 감사합니다📖, 강조해주셔서 감사합니다🖍️, 박수를 보내주셔서 감사합니다👏, 댓글을 달아주셔서 감사합니다💬, 그리고 공유해주셔서 감사합니다🗣️. \"Medium의 친구\"로서 매일 함께 글을 작성하는 동료 작가들에게 인정을 보여드리려 노력하고 있어요.\n\n그리고 위와 같이 멋진 콘텐츠를 소개할 때마다 알림을 받기 위해 뉴스레터📰를 구독할 수 있어요. 고맙습니다, 친애하는 챔프!🤓\n\n최신 AI 이야기의 소식을 받아보기 위해 Substack에서 연락을 유지해주세요. 함께 AI의 미래를 함께 만들어요!\n","ogImage":{"url":"/assets/img/2024-07-09-PythonProgramBannedfromAppleAppStoreDuetoaSingleString_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-PythonProgramBannedfromAppleAppStoreDuetoaSingleString_0.png","tag":["Tech"],"readingTime":8},{"title":"딥 뉴럴 네트워크 파인튜닝의 수학적 원리","description":"","date":"2024-07-09 19:56","slug":"2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks","content":"\n![이미지](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_0.png)\n\n머신 러닝에서는 몇 가지 모델을 시도해 가장 성능이 좋은 것을 선택하고 몇 가지 설정을 조정하여 그나마 성공할 수 있을지도 모릅니다. 그러나 딥러닝은 그런 룰에 맞지 않습니다. 신경망을 실험해 본 적이 있다면, 성능이 꽤 불안정할 수 있다는 것을 눈치챌 수 있습니다. 어쩌면 로지스틱 회귀와 같이 간단한 모델이 멋진 200층 심층 신경망을 이길 수도 있습니다.\n\n이게 왜 그럴까요? 딥러닝은 우리가 가지고 있는 가장 고급 인공 지능 기술 중 하나이지만, 철저한 이해와 조심스러운 다룸이 필요합니다. 신경망을 세밀하게 조정하고, 내부 작동 방식을 파악하고, 그 사용법을 마스터하는 것이 중요합니다. 오늘은 이에 대해 자세히 살펴보겠습니다!\n\n글을 읽기 전에 Jupyter Notebook을 여시는 것을 제안합니다. 오늘 다룰 모든 코드가 담겨 있으므로 함께 따라가는 데 도움이 될 것입니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n인덱스\n\n- 1: 소개\n\n  - 1.1: 기본 신경망의 발전\n  - 1.2: 복잡성으로의 길\n\n- 2: 모델 복잡성 확장\n\n  - 2.1: 레이어 추가\n\n- 3: 향상된 학습을 위한 최적화 기법\n  - 3.1: 학습률\n  - 3.2: 조기 중단 기법\n  - 3.3: 초기화 방법\n  - 3.4: 드롭아웃\n  - 3.5: 그래디언트 클리핑\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 4: 최적 레이어 수 결정하기\n\n  - 4.1: 레이어 깊이와 모델 성능\n  - 4.2: 적절한 깊이 선택을 위한 테스트 전략\n\n- 5: Optuna를 활용한 자동 세부 조정\n\n  - 5.1: Optuna 소개\n  - 5.2: 신경망 최적화를 위한 Optuna 통합\n  - 5.3: 실제 적용\n  - 5.4: 장점과 결과\n  - 5.5: 제한 사항\n\n- 6: 결론\n\n  - 6.1: 다음 단계\n\n- 추가 자료\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 1: 소개\n\n## 1.1: 기본 신경망의 발전\n\n인공 지능에 대한 최근 탐구에서 우리는 기초부터 신경망을 구축했습니다. 이 기본 모델은 오늘날 인공 지능 기술의 핵심인 신경망의 세계를 열어 주었습니다. 우리는 입력, 은닉 및 출력 레이어와 활성화 함수가 어떻게 정보를 처리하고 예측하는 데 결합되는지 간단하게 다루었습니다. 그리고 나서 우리는 컴퓨터 비전 작업을 위해 숫자 데이터셋에서 훈련된 간단한 신경망으로 이론을 실제로 적용했습니다.\n\n이제 이러한 기초 위에 계속해서 더 진보해 나갈 것입니다. 우리는 레이어를 추가하고, 초기화, 정규화 및 최적화에 대한 다양한 기술을 탐구함으로써 더 많은 복잡성을 도입할 것입니다. 물론, 이러한 수정이 우리의 신경망 성능에 어떻게 영향을 미치는지 확인하기 위해 코드를 테스트할 것입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n제가 이전 기사를 확인하지 않으셨다면, 우리가 처음부터 신경망을 만들어본 기사를 꼭 읽어보시기를 추천합니다. 이번에는 그 작업을 기반으로 계속해서 진행할 것이며, 이미 우리가 다룬 개념에 익숙하다고 가정할게요.\n\n## 1.2: 복잡성으로의 길\n\n신경망을 기본 구성에서 더 정교한 구조로 변환하는 것은 단순히 더 많은 층이나 노드를 추가하는 것만으로는 이루어지지 않습니다. 이것은 신경망의 구조와 그 데이터를 다루는 미묘한 차이를 체화하는 세심한 작업이 필요한 미묘한 춤이죠. 더 깊게 파고들수록, 우리의 목표는 신경망의 깊이를 풍부하게 함으로써 데이터의 복잡한 패턴과 연결을 더 잘 분별하는 데 있습니다.\n\n하지만 복잡성을 높이는 것은 그리 수월한 일이 아닙니다. 우리가 도입할 때마다, 세련된 최적화 기술의 필요성이 커집니다. 이는 효과적인 학습뿐만 아니라 새로운 보이지 않는 데이터에 적응하기 위한 모델 능력에 필수적입니다. 이 안내서는 우리의 기반 신경망을 강화하는 과정을 안내해 드릴 것입니다. 우리는 신경망을 세밀하게 조정하는 정교한 전략에 대해 살펴볼 것이며, 학습 속도 조정, 조기 종료 도입, 그리고 SGD(확률적 경사 하강법)와 Adam과 같은 다양한 최적화 알고리즘을 활용하는 방법에 대해 살펴볼 것입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리는 초기화 방법으로 시작하는 중요성, 오버피팅을 피하는 데 드롭아웃 사용의 장점, 그리고 클리핑 및 정규화로 네트워크의 그래디언트를 체크하여 안정성을 유지하는 것이 모델의 안정성에 얼마나 중요한지에 대해 다룰 예정입니다. 또한 학습을 향상시키기 위한 레이어의 최적 개수를 찾는 도전과정 및 불필요한 복잡성으로 빠져들지 않도록 할 것입니다.\n\n이전 게시물에서 함께 만든 Neural Network 및 Trainer 클래스를 아래에서 확인할 수 있습니다. 우리는 이를 조정하고 각 수정이 모델의 성능에 어떤 영향을 미치는지 실제로 살펴볼 것입니다:\n\n```js\nclass NeuralNetwork:\n    def __init__(self, input_size, hidden_size, output_size, loss_func='mse'):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.loss_func = loss_func\n\n        # 가중치 및 편향 초기화\n        self.weights1 = np.random.randn(self.input_size, self.hidden_size)\n        self.bias1 = np.zeros((1, self.hidden_size))\n        self.weights2 = np.random.randn(self.hidden_size, self.output_size)\n        self.bias2 = np.zeros((1, self.output_size))\n\n        # 손실 추적\n        self.train_loss = []\n        self.test_loss = []\n\n    def __str__(self):\n        return f\"Neural Network Layout:\\n입력 레이어: {self.input_size} 뉴런\\n은닉 레이어: {self.hidden_size} 뉴런\\n출력 레이어: {self.output_size} 뉴런\\n손실 함수: {self.loss_func}\"\n\n    def forward(self, X):\n        # 순방향 전파 수행\n        self.z1 = np.dot(X, self.weights1) + self.bias1\n        self.a1 = self.sigmoid(self.z1)\n        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n        if self.loss_func == 'categorical_crossentropy':\n            self.a2 = self.softmax(self.z2)\n        else:\n            self.a2 = self.sigmoid(self.z2)\n        return self.a2\n\n    def backward(self, X, y, learning_rate):\n        # 역전파 수행\n        m = X.shape[0]\n\n        # 기울기 계산\n        if self.loss_func == 'mse':\n            self.dz2 = self.a2 - y\n        elif self.loss_func == 'log_loss':\n            self.dz2 = -(y/self.a2 - (1-y)/(1-self.a2))\n        elif self.loss_func == 'categorical_crossentropy':\n            self.dz2 = self.a2 - y\n        else:\n            raise ValueError('잘못된 손실 함수')\n\n        self.dw2 = (1 / m) * np.dot(self.a1.T, self.dz2)\n        self.db2 = (1 / m) * np.sum(self.dz2, axis=0, keepdims=True)\n        self.dz1 = np.dot(self.dz2, self.weights2.T) * self.sigmoid_derivative(self.a1)\n        self.dw1 = (1 / m) * np.dot(X.T, self.dz1)\n        self.db1 = (1 / m) * np.sum(self.dz1, axis=0, keepdims=True)\n\n        # 가중치 및 편향 업데이트\n        self.weights2 -= learning_rate * self.dw2\n        self.bias2 -= learning_rate * self.db2\n        self.weights1 -= learning_rate * self.dw1\n        self.bias1 -= learning_rate * self.db1\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(self, x):\n        return x * (1 - x)\n\n    def softmax(self, x):\n        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return exps/np.sum(exps, axis=1, keepdims=True)\n\nclass Trainer:\n    def __init__(self, model, loss_func='mse'):\n        self.model = model\n        self.loss_func = loss_func\n        self.train_loss = []\n        self.val_loss = []\n\n    def calculate_loss(self, y_true, y_pred):\n        if self.loss_func == 'mse':\n            return np.mean((y_pred - y_true)**2)\n        elif self.loss_func == 'log_loss':\n            return -np.mean(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n        elif self.loss_func == 'categorical_crossentropy':\n            return -np.mean(y_true*np.log(y_pred))\n        else:\n            raise ValueError('잘못된 손실 함수')\n\n    def train(self, X_train, y_train, X_test, y_test, epochs, learning_rate):\n        for _ in range(epochs):\n            self.model.forward(X_train)\n            self.model.backward(X_train, y_train, learning_rate)\n            train_loss = self.calculate_loss(y_train, self.model.a2)\n            self.train_loss.append(train_loss)\n\n            self.model.forward(X_test)\n            test_loss = self.calculate_loss(y_test, self.model.a2)\n            self.val_loss.append(val_loss)\n```\n\n# 2: 모델 복잡성 확대\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n신경망을 더 정교하게 개선하기 위해 깊이 파고들면서, 게임을 바꾸는 전략을 발견했습니다: 레벨을 더 쌓아 복잡성을 높이는 것입니다. 이 동작은 모델을 강화하는 것뿐만 아니라, 데이터의 미묘한 변화를 더 정교하게 이해하고 해석하는 능력을 키우는 것입니다.\n\n## 2.1: 레이어 추가\n\n증가된 네트워크 깊이의 근거\n딥러닝의 핵심은 계층적 데이터 표현을 조합하는 능력에 있습니다. 더 많은 레이어를 추가함으로써, 우리는 신경망에 성장하는 복잡성의 패턴을 해체하고 이해하기 위한 도구를 제공하는 셈입니다. 간단한 형태와 질감을 인식하는 데서 시작해 데이터 속에서 더 복잡한 관계와 특징을 풀어가는 데로 발전하는 것으로 생각해보세요. 이러한 계층적 학습 접근법은 어느 정도 인간이 정보를 해석하는 방식과 유사하며, 기본적인 이해에서 복잡한 해석으로 진화합니다.\n\n더 많은 레이어를 쌓으면 네트워크의 \"학습 용량\"이 증가하여 보다 넓은 범위의 데이터 관계를 매핑하고 소화하는 능력을 갖추게 됩니다. 이를 통해 더 복잡한 작업을 처리할 수 있습니다. 하지만 마구 레이어를 추가하는 것은 아니며, 모델의 지능에 의미 있는 기여를 하지 않고 무분별하게 레이어를 추가하면 학습 과정을 혼란시키는 것이 아니라 명료하게 해야 합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n`NeuralNetwork` 클래스를 더 많은 층을 통합하는 방법 안내\n\n```js\nclass NeuralNetwork:\n    def __init__(self, layers, loss_func='mse'):\n        self.layers = []\n        self.loss_func = loss_func\n\n        # 레이어 초기화\n        for i in range(len(layers) - 1):\n            self.layers.append({\n                'weights': np.random.randn(layers[i], layers[i + 1]),\n                'biases': np.zeros((1, layers[i + 1]))\n            })\n\n        # 손실 추적\n        self.train_loss = []\n        self.test_loss = []\n\n    def forward(self, X):\n        self.a = [X]\n        for layer in self.layers:\n            self.a.append(self.sigmoid(np.dot(self.a[-1], layer['weights']) + layer['biases']))\n        return self.a[-1]\n\n    def backward(self, X, y, learning_rate):\n        m = X.shape[0]\n        self.dz = [self.a[-1] - y]\n\n        for i in reversed(range(len(self.layers) - 1)):\n            self.dz.append(np.dot(self.dz[-1], self.layers[i + 1]['weights'].T) * self.sigmoid_derivative(self.a[i + 1]))\n\n        self.dz = self.dz[::-1]\n\n        for i in range(len(self.layers)):\n            self.layers[i]['weights'] -= learning_rate * np.dot(self.a[i].T, self.dz[i]) / m\n            self.layers[i]['biases'] -= learning_rate * np.sum(self.dz[i], axis=0, keepdims=True) / m\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(self, x):\n        return x * (1 - x)\n```\n\n이 섹션에서는 신경망의 작동 방식에 중요한 조정을 가했으며, 임의의 층 수를 유연하게 지원하는 모델을 목표로했습니다. 변경된 사항은 다음과 같습니다:\n\n먼저, 이전에 각 층의 노드 수를 정의했던 self.input, self.hidden, self.output 변수를 삭제했습니다. 이제 목표는 임의의 층 수를 관리할 수 있는 다목적 모델입니다. 예를 들어, 이전에 숫자 데이터셋에 사용했던 모델인 64개의 입력 노드, 64개의 은닉 노드 및 10개의 출력 노드를 사용하는 경우, 다음과 같이 설정할 수 있습니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nnn = NeuralNetwork((layers = [64, 64, 10]));\n```\n\n이제 코드가 각 레이어를 세 번씩 순환하며 다른 목적으로 사용됨을 알 수 있습니다:\n\n초기화 과정 중에는 모든 레이어의 가중치와 편향이 설정됩니다. 이 단계는 학습 프로세스를 위해 초기 매개변수로 네트워크를 준비하는 데 중요합니다.\n\n순방향 패스 동안 활성화 self.a는 리스트에 수집됩니다. 입력 레이어의 활성화(본질적으로 입력 데이터 X)로 시작합니다. 각 레이어에 대해, np.dot(self.a[-1], layer['weights']) + layer['biases']를 사용하여 가중치 합과 편향을 계산하고 시그모이드 활성화 함수를 적용하여 결과를 self.a에 첨부합니다. 네트워크의 결과는 self.a의 마지막 요소로, 최종 출력을 나타냅니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n역 전파 동안, 이 단계는 마지막 레이어의 활성화에 대한 손실의 도함수를 계산하고 출력 레이어의 오차 목록을 준비함으로써 시작합니다 (self.dz). 그런 다음 네트워크를 역방향으로 거슬러 올라가며 (reversed(range(len(self.layers) - 1))를 사용하여), 숨은 레이어에 대한 오차 항목을 계산합니다. 이 과정은 현재 오차 항목을 다음 레이어의 가중치와 점곱(역방향)하여 시그모이드 함수의 도함수로 비선형성을 처리하는 작업을 포함합니다.\n\n```python\nclass Trainer:\n    ...\n    def train(self, X_train, y_train, X_test, y_test, epochs, learning_rate):\n        for _ in range(epochs):\n            self.model.forward(X_train)\n            self.model.backward(X_train, y_train, learning_rate)\n            train_loss = self.calculate_loss(y_train, self.model.a[-1])\n            self.train_loss.append(train_loss)\n\n            self.model.forward(X_test)\n            test_loss = self.calculate_loss(y_test, self.model.a[-1])\n            self.test_loss.append(test_loss)\n```\n\n마지막으로, NeuralNetwork 클래스의 변화에 따라 Trainer 클래스를 업데이트했습니다. 주요한 수정 사항은 특히 train 메서드에 있으며, 네트워크의 출력이 이제 self.model.a[-1]에서 가져온다는 점 때문에 훈련 및 테스트 손실을 다시 계산하는 방식입니다.\n\n이러한 수정 사항은 우리의 신경망을 다양한 아키텍처에 적응할 수 있도록 할뿐만 아니라 데이터와 그래디언트의 흐름을 이해하는 중요성을 강조합니다. 구조를 간소화함으로써, 각종 작업에서 네트워크의 성능을 실험하고 최적화할 수 있는 능력을 향상시킵니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 3: 향상된 학습을 위한 최적화 기법\n\n신경망을 최적화하는 것은 그들이 배우는 능력을 향상시키고 효율적인 학습을 보장하며 최상의 버전으로 이끄는 데 중요합니다. 저희 모델이 얼마나 잘 수행되는지에 상당한 영향을 미치는 몇 가지 중요한 최적화 기술에 대해 알아보겠습니다.\n\n## 3.1: 학습률\n\n학습률은 손실 경사에 기반하여 네트워크의 가중치를 조정하는 제어 장치입니다. 이는 모델이 학습하는 속도를 결정하며 최적화 중에 취하는 단계가 얼마나 큰지 작은지를 결정합니다. 학습률을 적절하게 설정하면 모델이 빠르게 낮은 오차의 해결책을 찾을 수 있습니다. 그러나 올바르게 설정하지 않으면 모델이 수렴하는 데 시간이 오래 걸리거나 아예 좋은 해결책을 찾지 못할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n너무 높은 학습률을 설정하면 모델이 최적해를 뛰어넘을 수 있어 불안정한 행동을 일으킬 수 있습니다. 이는 정확도나 손실이 훈련 중에 급격하게 변하는 것으로 나타날 수 있어요.\n\n학습률이 너무 낮으면 훈련 과정이 지나치게 느리게 진행될 수 있어요. 이 경우, 훈련 손실이 시간이 지남에 따라 거의 변하지 않는 것을 볼 수 있어요.\n\n관건은 훈련 및 검증 손실을 추적하면서 학습률이 어떻게 작동하는지에 대한 단서를 얻는 것이에요. 훈련 중에 일정 간격으로 이러한 손실을 기록하고 나중에 이를 플로팅하여 손실 landscape가 얼마나 매끄럽거나 불안정한지 보다 명확히 파악할 수 있어요. 우리의 코드에서는 이러한 메트릭을 추적하기 위해 Python의 logging 라이브러리를 사용하고 있어요. 이렇게 생겼답니다:\n\n```python\nimport logging\n# Logger 설정\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass Trainer:\n    ...\n    def train(self, X_train, y_train, X_val, y_val, epochs, learning_rate):\n        for epoch in range(epochs):\n            ...\n            # 50 에폭마다 손실 및 검증 손실을 로그로 남깁니다\n            if epoch % 50 == 0:\n                logger.info(f'에폭 {epoch}: 손실 = {train_loss}, 검증 손실 = {val_loss}')\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n시작할 때, 우리는 훈련 업데이트를 캡처하고 표시하기 위해 로거를 설정했습니다. 이 설정을 통해 우리는 훈련 및 검증 손실을 매 50번째 에포크마다 기록하여 모델의 진행 상황에 대한 안정적인 피드백을 받을 수 있습니다. 이 피드백을 통해 손실이 잘 감소하고 있는지, 아니면 너무 불규칙하게 변동하는지 파악할 수 있어서 학습률을 조정해야 할 필요가 있을지도 모릅니다.\n\n위의 코드는 훈련 및 검증 손실을 그래프로 플로팅하여 훈련 중에 손실이 어떻게 변화하는지 더 잘 이해할 수 있도록 해줍니다. 많은 반복에서 약간의 잡음이 예상되므로 부드러운(스무딩) 효과를 추가했습니다. 잡음을 부드럽게 처리하여 그래프를 더 잘 분석할 수 있도록 도와줄 것입니다.\n\n이러한 방식을 따르면 훈련을 시작하면 로그가 나타나면서 우리의 진행 상황을 한 눈에 볼 수 있고 조정할 수 있는 정보를 제공하여 우리가 방향을 수정하는 데 도움이 될 것입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_1.png\" />\n\n그런 다음, 훈련이 끝난 후 손실을 그래프로 그려볼 수 있습니다:\n\n<img src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_2.png\" />\n\n훈련 및 검증 손실이 꾸준히 감소하는 것을 보는 것은 좋은 신호입니다. 이는 에포크 수를 늘리고 학습률 스텝 크기를 증가시킨다면 잘 작동할 수 있다는 신호일 수 있습니다. 그러나 반대로 손실이 감소한 후 급상승하는 것을 관찰하면, 학습률 스텝 크기를 줄이는 것이 명백한 신호입니다. 그렇지만 재미있는 점이 있습니다: 에포크 0부터 50까지 우리의 손실이 어떤 이상한 일이 일어나고 있습니다. 우리는 그 부분을 확인하기 위해 다시 살펴보겠습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그 달콤한 학습률의 최적값을 찾기 위해서는 학습률 앤달링 또는 적응형 학습률 기법과 같은 방법이 정말 유용할 수 있어요. 이러한 방법들은 학습률을 실시간으로 세밀하게 조정하여 훈련 중에 최적의 속도를 유지하도록 도와줘요.\n\n## 3.2: 조기 중단 기법\n\n조기 중단은 마치 안전망 같아요 — 유효성 검사 세트에서 모델의 성능을 보고, 더 이상 성능이 개선되지 않을 때 훈련을 중지하는 것이에요. 이는 과적합에 대한 안전 장치이며, 우리 모델이 이전에 본 적 없는 데이터에서도 잘 작동하도록 보장해줘요.\n\n여기에 이를 실행하는 방법이 있어요:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 검증 세트: 교육 데이터의 일부를 분리하여 검증 세트로 사용합니다. 이것은 중요합니다. 왜냐하면 이렇게 하면 멈춤 결정이 신선한 보이지 않는 데이터에 기반하기 때문입니다.\n- 모니터링: 각 학습 에포크 후 모델이 검증 세트에서 어떻게 수행되는지 주시하세요. 성능이 향상되고 있나요, 아니면 정체되었나요?\n- 멈춤 기준: 멈출 시점을 결정하세요. 일반적으로 \"연속적으로 50번의 에포크 동안 검증 손실이 향상되지 않음\"이 있습니다.\n\n이를 위한 코드를 살펴보죠:\n\n```python\nclass Trainer:\n    def train(self, X_train, y_train, X_val, y_val, epochs, learning_rate,\n              early_stopping=True, patience=10):\n        best_loss = np.inf\n        epochs_no_improve = 0\n\n        for epoch in range(epochs):\n           ...\n\n            # 조기 중단\n            if early_stopping:\n                if val_loss < best_loss:\n                    best_loss = val_loss\n                    best_weights = [layer['weights'] for layer in self.model.layers]\n                    epochs_no_improve = 0\n                else:\n                    epochs_no_improve += 1\n\n                if epochs_no_improve == patience:\n                    print('조기 중단!')\n                    # 최적의 가중치로 복원\n                    for i, layer in enumerate(self.model.layers):\n                        layer['weights'] = best_weights[i]\n                    break\n```\n\ntrain 메서드에서 두 가지 새로운 옵션을 소개했습니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- early_stopping: 이는 조기 중단을 켜거나 끄는 여부를 결정하는 이진 플래그입니다.\n- patience: 이는 훈련을 중단하기 전에 유효성 검사 손실이 향상되지 않은 라운드 수를 설정합니다.\n\n우리는 가장 낮은 유효성 검사 손실을 현재까지 본 최저치로 설정하기 위해 best_loss를 무한대로 설정합니다. 한편, epochs_no_improve는 얼마 동안 유효성 검사 손실이 개선되지 않은 에포크 수를 기록합니다.\n\n모델을 훈련하기 위해 각 에포크를 순회하는 동안에는 실제 훈련 단계(순방향 전파 및 역전파)가 여기에 자세히 나와 있지는 않지만 프로세스의 중요한 부분입니다.\n\n매 에포크가 끝날 때마다 현재 에포크의 유효성 검사 손실(val_loss)이 best_loss보다 낮아졌다면, 이는 우리가 진전을 이루고 있다는 뜻입니다. 우리는 best_loss를 이 새로운 최솟값으로 업데이트하고, 또한 현재 모델 가중치를 best_weights로 저장합니다. 이렇게 하면 모델이 최상의 성능을 발휘한 시점의 스냅샷을 항상 가지게 됩니다. 그리고 우리는 방금 개선을 보았기 때문에 epochs_no_improve 카운트를 다시 0으로 재설정합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n만약 val_loss에 감소가 없다면, epochs_no_improve를 하나씩 증가시켜서 다른 epoch가 향상되지 않은 것으로 표시합니다.\n\n만약 우리가 설정한 인내심 한계치에 epochs_no_improve 카운트가 달성하면, 모델이 더 나아질 가능성이 낮다는 신호로 조기 종료를 시작합니다. 메시지와 함께 알림을 표시하고, 모델의 가중치를 최적의 가중치인 best_weights로 되돌립니다. 그런 다음 학습 루프를 종료합니다.\n\n이 접근 방식은 학습을 중단하는 균형있는 방법을 제공합니다. 모델에 학습의 공정한 기회를 제공하여 너무 일찍 중단하지 않으면서, 너무 늦지도 않아 시간을 낭비하거나 과적합의 위험을 가져올 수 있습니다.\n\n## 3.3: 초기화 방법\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n신경망을 설정할 때, 가중치를 어떻게 시작하느냐에 따라 네트워크가 얼마나 잘, 그리고 얼마나 빨리 학습하는지가 달라질 수 있어요. 가중치를 초기화하는 몇 가지 다른 방법 – 랜덤, 영, Glorot(Xavier), 그리고 He 초기화 – 에 대해 알아봐요.\n\n랜덤 초기화\n랜덤 방식을 선택하면 주로 균일하거나 정규 분포에서 숫자를 추출하여 초기 가중치를 설정하는 것을 의미해요. 이러한 무작위성은 모든 뉴런이 동일한 위치에서 시작하지 않도록하여 네트워크가 학습함에 따라 서로 다른 것을 배울 수 있도록 도와줘요. 핵심은 적절한 분산을 선택하는 것인데, 너무 많으면 기울기가 폭발할 위험이 있고, 너무 적으면 사라질 수도 있어요.\n\n```js\nweights = np.random.randn(layers[i], layers[i + 1]);\n```\n\n이 코드 라인은 표준 정규 분포에서 가중치를 추출하여 각 뉴런이 학습의 길로 나아갈 수 있도록 준비를 합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n장점: 뉴런이 서로 모방하는 것을 방지하는 간단한 방법입니다.\n\n단점: 분산을 잘못 설정하면 학습 과정이 불안정해질 수 있습니다.\n\n제로 초기화\n모든 가중치를 0으로 설정하는 방법은 매우 간단합니다. 그러나 이 방법에는 주요 단점이 있습니다: 이로 인해 층의 모든 뉴런이 사실상 동일해집니다. 이러한 동일성으로 인해 네트워크의 학습이 저해될 수 있으며, 모든 층의 뉴런이 학습 중에 동일하게 업데이트될 수 있습니다.\n\n```js\nweights = np.zeros((layers[i], layers[i + 1]));\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마지막으로, 우리는 모두 0으로 채워진 가중치 행렬을 얻습니다. 깔끔하고 정돈되어 있지만, 네트워크를 통해 나아가는 모든 경로가 처음에는 동일한 가중치를 갖게 되어 학습 다양성을 위한 좋지 않은 결과를 초래할 수 있습니다.\n\n장점: 구현이 매우 쉽습니다.\n\n단점: 학습 과정을 제약시켜 네트워크의 성능이 보통 좋지 않게끔 만듭니다.\n\nGlorot 초기화\n시그모이드 활성화 함수를 사용하는 네트워크를 위해 설계된 Glorot 초기화는 네트워크 내 입력 단위와 출력 단위의 수에 기반하여 가중치를 설정합니다. 이 초기화는 활성화와 역전파된 그래디언트의 분산을 유지하고 vanishing 또는 exploding 그래디언트 문제를 방지하기 위해 레이어를 통해 전달됩니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n글로럿 초기화에서의 가중치는 균일 분포나 정규 분포로 생성할 수 있습니다. 균일 분포를 사용하는 경우, 가중치는 [-a, a] 범위로 초기화됩니다. 여기서 a 값은:\n\n![식](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_3.png)\n\n```js\ndef glorot_uniform(self, fan_in, fan_out):\n    limit = np.sqrt(6 / (fan_in + fan_out))\n    return np.random.uniform(-limit, limit, (fan_in, fan_out))\n\nweights = glorot_uniform(layers[i - 1], layers[i])\n```\n\n이 공식은 가중치가 균등하게 분포되고, 가져올 수 있으며, 좋은 기울기 흐름을 유지할 수 있도록 보장합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n정상 분포에 대한 정보입니다:\n\n![](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_4.png)\n\n```js\ndef glorot_normal(self, fan_in, fan_out):\n    stddev = np.sqrt(2. / (fan_in + fan_out))\n    return np.random.normal(0., stddev, size=(fan_in, fan_out))\n\nweights = self.glorot_normal(layers[i - 1], layers[i])\n```\n\n이 조정은 시그모이드 활성화 함수를 사용하는 네트워크에서 적절하게 가중치를 유지합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n장점: 합리적인 범위 내 그라디언트 변화를 유지하여 심층 신경망의 안정성을 향상시킵니다.\n\n단점: ReLU(또는 변형) 활성화를 사용하는 레이어에는 신호 전파 특성이 다르기 때문에 최적이 아닐 수 있습니다.\n\nHe 초기화\nHe 초기화는 ReLU 활성화 함수를 사용하는 레이어에 적합하게 설계되었으며, ReLU의 비선형 특성을 고려하여 가중치의 분산을 조정합니다. 이 전략은 특히 ReLU가 일반적으로 사용되는 깊은 신경망에서 그라디언트 흐름을 유지하는 데 도움이 됩니다.\n\nGlorot 초기화와 마찬가지로, 가중치는 균등 분포 또는 정규 분포에서 선택할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n균일 분포를 위해 가중치는 [-a, a] 범위를 사용하여 초기화됩니다. 여기서 a는 다음과 같이 계산됩니다:\n\n![a 계산 공식](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_5.png)\n\n따라서 가중치 W는 균일 분포에서 추출됩니다:\n\n![균일 분포에서 가중치 추출 공식](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_6.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ndef he_uniform(self, fan_in, fan_out):\n    limit = np.sqrt(2 / fan_in)\n    return np.random.uniform(-limit, limit, (fan_in, fan_out))\n\nweights = self.he_uniform(layers[i - 1], layers[i])\n```\n\n일반 분포를 사용할 때, 가중치는 다음과 같은 수식에 따라 초기화됩니다:\n\n![수식](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_7.png)\n\n여기서 W는 가중치를, N은 정규 분포를, 0은 분포의 평균을, 그리고 2/n은 분산을 나타냅니다. n-in은 레이어로 들어오는 입력 단위의 수를 나타냅니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\ndef he_normal(self, fan_in, fan_out):\n    stddev = np.sqrt(2. / fan_in)\n    return np.random.normal(0., stddev, size=(fan_in, fan_out))\n\nweights = self.he_normal(layers[i - 1], layers[i])\n```\n\n양쪽 경우 모두 초기화 전략은 ReLU 활성화 함수의 특성을 반영하려고 합니다. 이 함수는 양수 입력에 대해 비활성화된 뉴런을 가지기 때문에 초기 가중치의 분산 조정은 깊은 네트워크에서 발생할 수 있는 그래디언트의 소실 또는 폭발을 방지하고 더 안정적이고 효율적인 훈련 과정을 촉진합니다.\n\n장점: ReLU 활성화 함수를 사용하는 네트워크에서 그래디언트 크기를 유지하여 깊은 학습 모델을 용이하게 학습시킵니다.\n\n단점: 특히 ReLU에 최적화되어 있어 다른 활성화 함수만큼 효과적이지 않을 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 초기화를 소개한 후 NeuralNetwork 클래스가 어떻게 보이는지 살펴보겠습니다:\n\n```js\n클래스 NeuralNetwork:\n    def __init__(self,\n                 layers,\n                 init_method='glorot_uniform', # 'zeros', 'random', 'glorot_uniform', 'glorot_normal', 'he_uniform', 'he_normal'\n                 loss_func='mse',\n                 ):\n        ...\n\n        self.init_method = init_method\n\n        # 레이어 초기화\n        for i in range(len(layers) - 1):\n            if self.init_method == 'zeros':\n                weights = np.zeros((layers[i], layers[i + 1]))\n            elif self.init_method == 'random':\n                weights = np.random.randn(layers[i], layers[i + 1])\n            elif self.init_method == 'glorot_uniform':\n                weights = self.glorot_uniform(layers[i], layers[i + 1])\n            elif self.init_method == 'glorot_normal':\n                weights = self.glorot_normal(layers[i], layers[i + 1])\n            elif self.init_method == 'he_uniform':\n                weights = self.he_uniform(layers[i], layers[i + 1])\n            elif self.init_method == 'he_normal':\n                weights = self.he_normal(layers[i], layers[i + 1])\n\n            else:\n                raise ValueError(f'알 수없는 초기화 방법 {self.init_method}')\n\n            self.layers.append({\n                'weights': weights,\n                'biases': np.zeros((1, layers[i + 1]))\n            })\n\n        ...\n\n    ...\n\n    def glorot_uniform(self, fan_in, fan_out):\n        limit = np.sqrt(6 / (fan_in + fan_out))\n        return np.random.uniform(-limit, limit, (fan_in, fan_out))\n\n    def he_uniform(self, fan_in, fan_out):\n        limit = np.sqrt(2 / fan_in)\n        return np.random.uniform(-limit, limit, (fan_in, fan_out))\n\n    def glorot_normal(self, fan_in, fan_out):\n        stddev = np.sqrt(2. / (fan_in + fan_out))\n        return np.random.normal(0., stddev, size=(fan_in, fan_out))\n\n    def he_normal(self, fan_in, fan_out):\n        stddev = np.sqrt(2. / fan_in)\n        return np.random.normal(0., stddev, size=(fan_in, fan_out))\n\n    ...\n```\n\n적절한 가중치 초기화 전략을 선택하는 것은 효과적인 신경망 학습에 중요합니다. 무작위 및 영점 초기화는 기본적인 접근법을 제공하지만 항상 최적의 학습 동역학을 이끌어내지 않을 수 있습니다. 반면, Glorot/Xavier 및 He 초기화는 신경망 아키텍처 및 사용된 활성화 함수를 고려하여 딥 러닝 모델의 특정 요구 사항을 고려하는 더 소박한 솔루션을 제공합니다. 이러한 전략은 너무 빠른 학습과 너무 느린 학습 사이의 절충안을 균형있게 맞추어 더 신뢰할 수 있는 수렴 방향으로 학습 프로세스를 이끕니다.\n\n## 3.4: 드롭아웃\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nDropout은 신경망에서 오버피팅을 방지하기 위해 설계된 정규화 기술로, 훈련 단계에서 네트워크에서 임시로 그리고 무작위로 유닛(뉴런)과 해당 연결을 제거함으로써 사용합니다. 이 방법은 Srivastava 및 그 동료들이 2014 년 논문에서 고안한 간단하면서도 효과적인 방법으로 견고한 신경망을 훈련하는 데 사용됩니다.\n\n![이미지](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_8.png)\n\n각 훈련 반복에서 각 뉴런(입력 단위 포함되지만 보통 출력 단위는 제외)은 일시적으로 \"드랍아웃\"될 확률 p를 가집니다. 이는 해당 뉴런이 이 전방 및 역방향 패스 동안 완전히 무시된다는 것을 의미합니다. 이 확률 p은 \"드랍아웃 비율\"로 불리며 성능을 최적화하기 위해 조절할 수 있는 하이퍼파라미터입니다. 예를 들어, 0.5의 드랍아웃 비율은 각 뉴런이 각 훈련 패스에서 계산에서 제외될 확률이 50% 라는 것을 의미합니다.\n\n이 과정의 효과는 네트워크가 개별 뉴런의 특정 가중치에 덜 민감해진다는 것입니다. 이것은 예측을 할 때 개별 뉴런의 출력에 의존할 수 없으므로 네트워크가 뉴런들 사이에 중요성을 분산시키도록 장려합니다. 이는 실제로 가중치를 공유하는 신경망의 의사앙상블을 훈련하며, 각 훈련 반복에서 네트워크의 다른 \"드랍아웃된\" 버전이 포함됩니다. 시험 시간에는 드랍아웃이 적용되지 않고, 대신 가중치는 일반적으로 드랍아웃 비율 p에 의해 조정되어 더 많은 유닛이 활성화되었다는 사실을 균형 있게 합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n올바른 드롭아웃 비율 선택하기\n드롭아웃 비율은 각 신경망 구조와 데이터셋에 대해 조정이 필요한 하이퍼파라미터입니다. 일반적으로, 숨겨진 유닛에 대해 시작점으로 0.5의 비율이 사용되며, 이는 원래 드롭아웃 논문에서 제안되었습니다.\n\n높은 드롭아웃 비율 (1에 가까운 값)은 학습 중에 더 많은 뉴런이 제거되는 것을 의미합니다. 이는 네트워크가 데이터를 충분히 학습하지 못할 수 있어서, 훈련 데이터의 복잡성을 모델링하는 데 어려움을 겪어 과소적합을 초래할 수 있습니다.\n\n반대로, 낮은 드롭아웉 비율 (0에 가까운 값)은 더 적은 뉴런이 제거되어 드롭아웃의 정규화 효과가 줄어들 수 있으며, 이는 모델이 훈련 데이터에서 잘 수행되지만 보이지 않는 데이터에서 성능이 나빠질 수 있는 과적합을 초래할 수 있습니다.\n\n코드 구현\n우리 코드에서 어떻게 보이는지 살펴보겠습니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nclass NeuralNetwork:\n    def __init__(self,\n                 layers,\n                 init_method='glorot_uniform', # 'zeros', 'random', 'glorot_uniform', 'glorot_normal', 'he_uniform', 'he_normal'\n                 loss_func='mse',\n                 dropout_rate=0.5\n                 ):\n        ...\n\n        self.dropout_rate = dropout_rate\n\n        ...\n\n    ...\n\n\n    def forward(self, X, is_training=True):\n        self.a = [X]\n        for i, layer in enumerate(self.layers):\n            z = np.dot(self.a[-1], layer['weights']) + layer['biases']\n            a = self.sigmoid(z)\n            if is_training and i < len(self.layers) - 1:  # apply dropout to all layers except the output layer\n                dropout_mask = np.random.rand(*a.shape) > self.dropout_rate\n                a *= dropout_mask\n            self.a.append(a)\n        return self.a[-1]\n\n    ...\n```\n\n저희 신경망 클래스는 새로운 초기화 매개변수와 드롭아웃 정규화를 포함한 새로운 순전파 메서드로 업그레이드되었습니다.\n\ndropout_rate : 이것은 훈련 중에 신경세포들이 네트워크에서 일시적으로 제거될 가능성을 결정하는 설정입니다. 오버피팅을 피하는 데 도움이 됩니다. 0.5로 설정함으로써 어떤 신경세포가 한 번의 훈련 라운드에서 \"제거\"될 확률이 50%라고 말하고 있습니다. 이 무작위성은 네트워크가 어떤 단일 신경세포에 너무 의존하지 않도록 보장하여 더 견고한 학습 과정을 촉진합니다.\n\nis_training 부울 플래그는 네트워크가 현재 훈련되고 있는지를 알려줍니다. 이것은 훈련 중에만 드롭아웃이 발생해야 하므로 새 데이터에 대한 네트워크 성능을 평가할 때는 드롭아웃이 일어나서는 안 된다는 점이 중요합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n네트워크를 통해 데이터(X로 표시)가 전달되면, 네트워크는 들어오는 데이터와 레이어의 편향을 가중합(z)으로 계산합니다. 그런 다음 이 합계를 시그모이드 활성화 함수를 통해 활성화(a)로 변환하여 다음 레이어로 전달할 신호를 얻습니다.\n\n하지만 훈련 중에 다음 레이어로 진행하기 전에 드롭아웃을 적용할 수 있습니다:\n\n- is_training이 true이고 출력 레이어를 다루고 있지 않다면, 각 뉴런에 대해 주사위를 굴려 떨어뜨릴지 여부를 확인합니다. 이를 위해 무작위 수가 드롭아웃 비율을 초과하는지 확인하여 드롭아웃 마스크(모양은 a와 같은 배열)를 생성합니다.\n- 이 마스크를 사용하여 a의 일부 활성화를 0으로 만들어 네트워크에서 일시적으로 뉴런을 제거하는 것을 흉내냅니다.\n\n드롭아웃을 적용한 후(해당하는 경우), 생성된 활성화를 self.a에 추가하여 모든 레이어를 통해 활성화를 추적하는 리스트를 유지합니다. 이렇게 하면 신호를 그냥 한 레이어에서 다음 레이어로 무작정 이동시키는 것이 아니라, 네트워크가 더 견고하게 학습하도록 장려하는 기술을 적용하여 특정 경로의 뉴런에 지나치게 의존하지 않도록 합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 3.5: 그레이디언트 클리핑\n\n그레이디언트 클리핑은 깊은 신경망을 훈련할 때 중요한 기술로, 특히 폭주하는 그레이디언트 문제를 해결할 때 주로 사용됩니다. 폭주하는 그레이디언트는 신경망의 매개변수에 대한 손실 함수의 미분이나 그레이디언트가 층을 거치면서 지수적으로 증가하여 훈련 중에 가중치에 대해 매우 큰 업데이트를 유도할 때 발생합니다. 이는 학습 과정을 불안정하게 만들 수 있으며, 종종 가중치나 손실에서 NaN 값의 형태로 나타나 수치 오버플로우 때문에 발산하여 모델이 해결책으로 수렴하지 못하도록 방해할 수 있습니다.\n\n그레이디언트 클리핑은 값에 의한 클리핑과 법에 의한 클리핑 두 가지 주요 방법으로 구현할 수 있으며, 각각 폭주하는 그레이디언트 문제를 완화하는 전략을 가지고 있습니다.\n\n값에 의한 클리핑\n이 방법은 미리 정의된 임계값을 설정하고, 각 그레이디언트 구성 요소를 해당 임계값을 초과하는 경우 지정된 범위 내로 직접 클리핑하는 접근 방식입니다. 예를 들어, 임계값이 1로 설정되면, 1보다 큰 모든 그레이디언트 구성 요소를 1로 설정하고, -1보다 작은 모든 구성 요소를 -1로 설정합니다. 이는 모든 그레이디언트가 [-1, 1] 범위 내에 유지되도록 보장하여 너무 커지는 그레이디언트를 효과적으로 방지합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_9.png\" />\n\ngi는 기울기 벡터의 각 구성 요소를 나타냅니다.\n\n노름에 의한 클리핑\n이 방법은 각 기울기 구성 요소를 개별적으로 클리핑하는 대신, 일정 임계값을 초과하는 경우 전체 기울기를 조절합니다. 이렇게 하면 기울기의 방향을 보존한 채 크기가 지정된 한도를 초과하지 않도록 할 수 있습니다. 이는 모든 매개변수를 통해 업데이트의 상대적 방향을 유지하는 데 특히 유용하며, 값에 의한 클리핑보다 학습 과정에 더 유익할 수 있습니다.\n\n<img src=\"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_10.png\" />\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그래디언트 벡터를 나타내는 g이고 ∥g∥는 그 노름값입니다.\n\n훈련에의 응용\n\n```js\nclass NeuralNetwork:\n    def __init__(self,\n                 layers,\n                 init_method='glorot_uniform', # 'zeros', 'random', 'glorot_uniform', 'glorot_normal', 'he_uniform', 'he_normal'\n                 loss_func='mse',\n                 dropout_rate=0.5,\n                 clip_type='value',\n                 grad_clip=5.0\n                 ):\n        ...\n\n        self.clip_type = clip_type\n        self.grad_clip = grad_clip\n\n        ...\n\n    ...\n\n    def backward(self, X, y, learning_rate):\n        m = X.shape[0]\n        self.dz = [self.a[-1] - y]\n        self.gradient_norms = []  # 그래디언트 노름을 저장하는 리스트\n\n        for i in reversed(range(len(self.layers) - 1)):\n            self.dz.append(np.dot(self.dz[-1], self.layers[i + 1]['weights'].T) * self.sigmoid_derivative(self.a[i + 1]))\n            self.gradient_norms.append(np.linalg.norm(self.layers[i + 1]['weights']))  # 그래디언트 노름을 계산하고 저장\n\n        self.dz = self.dz[::-1]\n        self.gradient_norms = self.gradient_norms[::-1]  # 리스트를 뒤집어서 레이어의 순서와 일치시킴\n\n        for i in range(len(self.layers)):\n            grads_w = np.dot(self.a[i].T, self.dz[i]) / m\n            grads_b = np.sum(self.dz[i], axis=0, keepdims=True) / m\n\n            # 그래디언트 클리핑\n            if self.clip_type == 'value':\n                grads_w = np.clip(grads_w, -self.grad_clip, self.grad_clip)\n                grads_b = np.clip(grads_b, -self.grad_clip, self.grad_clip)\n            elif self.clip_type == 'norm':\n                grads_w = self.clip_by_norm(grads_w, self.grad_clip)\n                grads_b = self.clip_by_norm(grads_b, self.grad_clip)\n\n            self.layers[i]['weights'] -= learning_rate * grads_w\n            self.layers[i]['biases'] -= learning_rate * grads_b\n\n    def clip_by_norm(self, grads, clip_norm):\n        l2_norm = np.linalg.norm(grads)\n        if l2_norm > clip_norm:\n            grads = grads / l2_norm * clip_norm\n        return grads\n\n    ...\n```\n\n초기화 중에 이제 사용할 그래디언트 클리핑 유형(clip_type)과 그래디언트 클리핑 임계값(grad_clip)이 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n`clip_type`은 그레디언트를 값으로 자르는 경우에는 `value`, 또는 L2 노름에 의해 그레디언트를 자르는 경우에는 `norm`이 될 수 있습니다. grad_clip은 자르는 임계값이나 한계를 지정합니다.\n\n그런 다음, 역전파 중에 함수는 네트워크의 각 레이어에 대한 그레디언트를 계산합니다. 가중치(grads_w)와 편향(grads_b)에 대한 손실의 미분 값을 각 레이어마다 계산합니다.\n\n만약 `clip_type`이 `value`인 경우, np.clip을 사용하여 그레디언트를 [-grad_clip, grad_clip] 범위로 자릅니다. 이렇게 하면 그레디언트 성분이 이 한계를 초과하지 않도록 합니다.\n\n만약 `clip_type`이 `norm`인 경우, 그레디언트의 노름이 grad_clip을 초과하는 경우 이 방향을 유지하면서 그에 대한 크기를 제한하기 위해 clip_by_norm 메서드가 호출됩니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n클리핑 이후, 각 층의 가중치와 편향을 학습률에 의해 조정하는 데 그래디언트가 사용됩니다.\n\n마지막으로, 그래디언트의 L2 노름이 지정된 clip_norm을 초과하는 경우 그래디언트를 스케일링하는 clip_by_norm 메서드를 만듭니다. 이 메서드는 그래디언트의 L2 노름을 계산하고, clip_norm보다 크면 그래디언트를 clip_norm까지 스케일 다운시키면서 방향을 유지합니다. 이는 그래디언트를 그들의 L2 노름으로 나누고 clip_norm을 곱해 달성됩니다.\n\n그래디언트 클리핑의 장점\n모델의 가중치에 대한 지나치게 큰 업데이트를 방지함으로써, 그래디언트 클리핑은 더 안정적이고 신뢰할 수 있는 훈련 과정에 기여합니다. 그래디언트의 계산이 큰 업데이트로 인해 불안정성을 초래할 수 있는 경우에도 손실 함수를 최소화하여 옵티마이저가 일관된 진전을 이룰 수 있도록 합니다. 이는 훈련하는 동안 그래디언트의 스케일이 큰 문제로 인해 불안정성 문제에 직면하는 순환 신경망(RNNs) 훈련과 같은 과제에서 특히 유용한 도구로 작용합니다.\n\n그래디언트 클리핑은 신경망 훈련의 안정성과 성능을 향상시키는 간단하면서도 강력한 기술입니다. 그래디언트가 지나치게 커지지 않도록 보장함으로써, 훈련 불안정성(과적합, 과소적합, 수렴 속도 저하 등)의 문제를 피하고, 신경망이 효과적이고 효율적으로 학습하기 쉽도록 돕습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 4: 층의 최적 개수 결정하기\n\n신경망을 설계하는 중요한 결정 중 하나는 올바른 층의 개수를 결정하는 것입니다. 이 측면은 네트워크의 데이터로부터 학습하고 새로운, 보지 못한 데이터를 일반화하는 능력에 상당한 영향을 미칩니다. 신경망의 깊이 - 얼마나 많은 층이 있는지 - 능력을 강화시키거나 과적합 또는 학습이 부족하다는 문제로 이어질 수 있습니다.\n\n## 4.1: 층의 깊이와 모델 성능\n\n신경망에 더 많은 층을 추가하면 학습 능력이 향상되어 데이터의 더 복잡한 패턴과 관계를 파악할 수 있습니다. 이는 추가적인 층이 입력 데이터의 보다 추상적인 표현을 만들 수 있기 때문에 단순한 기능에서 더 복잡한 조합으로 이동할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n더 깊은 신경망은 복잡한 패턴을 모델링할 수 있지만, 추가적인 깊이가 오버피팅으로 이어지는 기묘한 지점이 있습니다. 오버피팅은 모델이 훈련 데이터를 너무 잘 학습하여 그 잡음까지 포함해 새로운 데이터에서 성능이 나빠지는 현상입니다.\n\n궁극적인 목표는 훈련 데이터로부터 잘 학습하는 모델을 갖는 것뿐만 아니라 이 학습을 새로운 데이터에서도 정확하게 수행할 수 있는 범용성을 갖는 것입니다. 이를 위해서는 층의 깊이에 대한 적절한 균형을 찾는 것이 중요합니다. 너무 적은 층은 과소적합될 수 있고, 너무 많은 층은 오버피팅될 수 있습니다.\n\n## 4.2: 적절한 깊이를 테스트하고 선택하는 전략\n\n점진적인 접근 방식\n간단한 모델부터 시작하여 점진적으로 층을 추가하고 검증 성능이 크게 향상될 때까지 관찰합니다. 이 접근 방식은 각 층이 전체 성능에 어떤 기여를 하는지 이해하는 데 도움이 됩니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n모델의 성능을 판단하기 위한 기준으로 검증 세트(학습 중에 사용되지 않은 학습 데이터의 하위 집합)에서 모델이 일반화하는 능력을 향상시키는지 여부를 결정합니다.\n\n정규화 기법\n더 많은 레이어를 추가할 때 드롭아웃 또는 L2 정규화와 같은 정규화 방법을 사용하세요. 이러한 기법은 오버피팅의 위험을 줄일 수 있어 추가된 레이어가 모델의 학습 능력에 어떤 가치를 더하는지를 공정하게 평가할 수 있게 해줍니다.\n\n학습 동태 관찰\n더 많은 레이어를 추가할 때 학습과 검증 손실을 모니터링하세요. 이 두 지표 사이에 차이가 발생하는 경우 — 학습 손실이 감소하지만 검증 손실이 그렇지 않을 때 — 오버피팅을 나타낼 수 있으며, 현재 깊이가 지나칠 수 있다는 것을 시사할 수 있습니다.\n\n![이미지](/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_11.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 두 그래프는 기계 학습 모델을 훈련하는 과정에서 발생할 수 있는 두 가지 시나리오를 나타냅니다.\n\n첫 번째 그래프에서는 훈련 손실과 검증 손실이 모두 감소하여 비슷한 값으로 수렴합니다. 이것은 이상적인 시나리오로, 모델이 잘 학습하고 적절하게 일반화되고 있음을 나타냅니다. 모델의 성능이 훈련 데이터와 보지 않은 검증 데이터 모두에서 향상되고 있는 것을 의미합니다. 이는 모델이 데이터를 과소적합하거나 과적합하지 않고 있다는 것을 시사합니다.\n\n두 번째 그래프에서는 훈련 손실은 감소하지만 검증 손실이 증가합니다. 이는 과적합의 전형적인 징후입니다. 모델이 훈련 데이터를 너무 잘 학습하여 노이즈와 이상점을 포함하고 있으며 보지 않은 데이터에 대한 일반화를 실패합니다. 결과적으로, 검증 데이터에서의 성능이 시간이 지남에 따라 악화됩니다. 이는 모델의 복잡성을 줄이거나 정규화나 드롭아웃과 같은 과적합 방지 기술을 적용해야 할 수도 있다는 것을 나타냅니다.\n\n자동화 아키텍처 탐색\n신경망 아키텍처 탐색(NAS) 도구나 Optuna와 같은 하이퍼파라미터 최적화 프레임워크를 활용하여 서로 다른 아키텍처를 체계적으로 탐색하십시오. 이러한 도구는 다양한 구성을 평가하고 검증 지표에서 최상의 성능을 발휘하는 구성을 선택함으로써 최적의 레이어 수를 자동화할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n신경망의 최적 레이어 수를 결정하는 것은 모델의 복잡성과 학습 및 일반화 능력을 균형있게 고려하는 세심한 프로세스입니다. 레이어 추가에 체계적인 방법론을 채택하고 교차 검증을 활용하며 정칙화 기법을 통합함으로써 특정 문제에 적합한 네트워크 깊이를 결정할 수 있습니다. 이를 통해 보이지 않는 데이터에 대한 모델 성능을 최적화할 수 있습니다.\n\n# 5: Optuna을 활용한 자동 미세 튜닝\n\n최적 성능을 달성하기 위해 신경망을 미세 조정하는 것은 다양한 하이퍼파라미터의 섬세한 균형을 찾는 과정으로, 종종 방대한 탐색 공간 속에서 바늘을 찾는 것처럼 느껴질 수 있습니다. 이때 Optuna와 같은 자동 하이퍼파라미터 최적화 도구가 필요합니다.\n\n## 5.1: Optuna 소개\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n옵투나는 최적화 하이퍼파라미터 선택을 자동화하기 위해 설계된 오픈소스 최적화 프레임워크입니다. 이는 가장 효율적인 신경망 모델로 이어지는 매개변수 조합을 식별하는 복잡한 작업을 간단화합니다. 옵투나는 고급 알고리즘을 활용하여 최적화 하이퍼파라미터 공간을 보다 효과적으로 탐색하여, 필요한 계산 자원과 수렴 시간을 줄입니다.\n\n## 5.2: 옵투나를 활용한 신경망 최적화 통합\n\n옵투나는 베이지안 최적화, 트리 구조 파르젠 추정기, 진화 알고리즘 등 다양한 전략을 활용하여 하이퍼파라미터 공간을 지능적으로 탐색합니다. 이 접근 방식을 통해 옵투나는 가장 유망한 하이퍼파라미터를 빠르게 식별하여 최적화 과정을 크게 가속화할 수 있습니다.\n\n옵투나를 신경망 훈련 워크플로우에 통합하는 것은 옵투나가 최소화 또는 최대화하려는 목적 함수를 정의하는 과정을 포함합니다. 이 함수에는 일반적으로 모델 훈련 및 검증 과정이 포함되며, 목표는 검증 손실을 최소화하거나 검증 정확도를 최대화하는 것입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 검색 공간 정의: 각 하이퍼파라미터 값 범위를 지정하여 Optuna가 탐색할 것입니다 (예: 레이어 수, 학습률, 드롭아웃 비율).\n- 시험과 평가: Optuna는 모델을 훈련시키기 위해 매번 새로운 하이퍼파라미터 세트를 선택하는 시험을 진행합니다. 검증 세트에서 모델의 성능을 평가하고 이 정보를 사용하여 탐색을 안내합니다.\n\n## 5.3: 실제 구현\n\n```python\nimport optuna\n\ndef objective(trial):\n    # 하이퍼파라미터 정의\n    n_layers = trial.suggest_int('n_layers', 1, 10)\n    hidden_sizes = [trial.suggest_int(f'hidden_size_{i}', 32, 128) for i in range(n_layers)]\n    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)  # 모든 레이어에 대한 단일 드롭아웃 비율\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-3, 1e-1)\n    init_method = trial.suggest_categorical('init_method', ['glorot_uniform', 'glorot_normal', 'he_uniform', 'he_normal', 'random'])\n    clip_type = trial.suggest_categorical('clip_type', ['value', 'norm'])\n    clip_value = trial.suggest_uniform('clip_value', 0.0, 1.0)\n    epochs = 10000\n\n    layers = [input_size] + hidden_sizes + [output_size]\n\n    # 신경망 생성 및 훈련\n    nn = NeuralNetwork(layers=layers, loss_func=loss_func, dropout_rate=dropout_rate, init_method=init_method, clip_type=clip_type, grad_clip=clip_value)\n    trainer = Trainer(nn, loss_func)\n    trainer.train(X_train, y_train, X_test, y_test, epochs, learning_rate, early_stopping=False)\n\n    # 신경망 성능 평가\n    predictions = np.argmax(nn.forward(X_test), axis=1)\n    accuracy = np.mean(predictions == y_test_labels)\n\n    return accuracy\n\n# Study 객체 생성 및 목적 함수 최적화\nstudy = optuna.create_study(study_name='nn_study', direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\n# 최적 하이퍼파라미터 출력\nprint(f\"Best trial: {study.best_trial.params}\")\nprint(f\"Best value: {study.best_trial.value:.3f}\")\n```\n\nOptuna 최적화 프로세스의 핵심은 목적 함수입니다. 이 함수는 시험 목표를 정의하고 각 시험에 대해 Optuna에 의해 호출됩니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n**Heren_layers**은 신경망의 은닉층의 수이며, 1에서 10 사이를 추천합니다. 층의 수를 변화시킴으로써 얕은 네트워크와 깊은 네트워크 아키텍처를 탐색할 수 있습니다.\n\n**hidden_sizes**는 각 층의 크기(뉴런 수)를 저장하며, 32에서 128 사이의 숫자를 제안하여 모델이 다양한 용량을 탐색하게 합니다.\n\n**dropout_rate**는 균일하게 0.0(드롭아웃 없음)에서 0.5 사이를 제안하여 시험을 통해 정규화 유연성을 가능케 합니다.\n\n**learning_rate**는 로그 스케일로 1e-3에서 1e-1 사이를 제안하여, 학습률 최적화에 대한 공통적인 민감도로 인해 크기의 범위를 포괄하는 넓은 탐색 공간을 보장합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n신경망 가중치의 init_method은 일련의 일반적인 전략 중에서 선택됩니다. 이 선택은 훈련의 시작점과 수렴 동작을 영향을 줍니다.\n\nclip_type과 clip_value는 그래디언트 클리핑 전략과 값으로, 값이나 노름을 기준으로 클리핑하여 폭발하는 그래디언트를 방지하는 데 도움이 됩니다.\n\n그런 다음, 정의된 하이퍼파라미터를 사용하여 NeuralNetwork 인스턴스가 생성되고 훈련됩니다. 각 시행이 일정한 에포크 수동안 실행될 수 있도록 조기 중지가 비활성화되며, 일관된 비교를 보장합니다. 성능은 테스트 세트에서 모델의 예측 정확도를 기반으로 평가됩니다.\n\n목적 함수와 NeuralNetwork 인스턴스가 정의된 후 Optuna 스터디로 이동할 수 있습니다. Optuna 스터디 객체는 목적 함수를 최대화(`maximize`)하는 데 사용되며, 이 문맥에서는 신경망의 정확도가 목적 함수입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n연구는 목적 함수를 여러 번 호출합니다(n_trials=100), 매번 옵튜나 내부 최적화 알고리즘에서 제안한 다른 하이퍼파라미터 세트로 호출합니다. 옵튜나는 시험 이력에 기반하여 지능적으로 제안을 조정하여 하이퍼파라미터 공간을 효율적으로 탐색합니다.\n\n이 프로세스를 통해 모든 실험에서 찾은 가장 좋은 하이퍼파라미터 세트(study.best_trial.params)와 달성한 최고 정확도(study.best_trial.value)가 생성됩니다. 이 출력은 주어진 작업에 대한 신경망의 최적 구성에 대한 통찰을 제공합니다.\n\n## 5.4: 혜택 및 결과\n\n옵튜나를 통합함으로써, 개발자는 하이퍼파라미터 튜닝 프로세스를 자동화할뿐만 아니라 어떻게 다른 매개변수가 모델에 영향을 미치는지에 대한 깊은 통찰을 얻을 수 있습니다. 이를 통해 수동 실험을 통해 걸릴 시간의 일부로 최적화된, 더 견고하고 정확한 신경망이 생성됩니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n옵투나의 체계적인 파라미터 조정 접근법은 신경망 개발에 새로운 수준의 정밀성과 효율성을 제공하여 개발자들이 더 높은 성능 표준을 달성하고 모델이 이룰 수 있는 한계를 뛰어넘을 수 있도록 돕습니다.\n\n## 5.5: 한계\n\n옵투나는 하이퍼파라미터 최적화에 강력하고 유연한 접근 방식을 제공하지만, 기계 학습 워크플로에 통합할 때 고려해야 할 몇 가지 한계점과 주의 사항이 있습니다.\n\n계산 리소스\n각 시도는 신경망을 처음부터 훈련해야 하므로, 특히 심층 신경망이나 대규모 데이터셋의 경우에는 계산 리소스가 많이 필요할 수 있습니다. 하이퍼파라미터 공간을 철저히 탐색하기 위해 수백 번이나 수천 번의 시도를 실행하는 것은 상당한 계산 리소스와 시간이 필요할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하이퍼파라미터 검색 공간\n옵투나의 검색 효과는 검색 공간이 어떻게 정의되는지에 매우 의존합니다. 하이퍼파라미터 값의 범위가 너무 넓거나 문제와 제대로 일치하지 않으면 옵투나가 비최적 영역을 탐색하는 데 시간을 낭비할 수 있습니다. 반대로 검색 공간이 너무 좁으면 최적의 구성을 놓칠 수 있습니다.\n\n하이퍼파라미터 수가 증가함에 따라 검색 공간이 기하급수적으로 증가하는데, 이를 \"차원의 저주\"라고 합니다. 이로 인해 옵투나가 공간을 효율적으로 탐색하고 합리적인 횟수의 시도 내에서 최적의 하이퍼파라미터를 찾는 것이 어렵다는 도전이 생길 수 있습니다.\n\n평가 지표\n목적 함수와 평가 지표의 선택은 최적화 결과에 상당한 영향을 미칠 수 있습니다. 모델의 성능이나 작업 목표를 적절히 포착하지 못하는 지표는 하이퍼파라미터 구성을 부적절하게 만들 수 있습니다.\n\n모델의 성능 평가는 무작위 초기화, 데이터 섞기, 또는 데이터셋 내 잡음과 같은 요소로 인해 달라질 수 있습니다. 이러한 변동성은 최적화 과정에 잡음을 도입하여 결과의 신뢰성에 영향을 줄 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n알고리즘 제한사항\nOptuna은 검색 공간을 탐색하기 위해 정교한 알고리즘을 사용하지만, 이러한 알고리즘의 효율성과 효과는 문제에 따라 다를 수 있습니다. 경우에 따라 특정 알고리즘이 지역 최적점으로 수렴하거나 하이퍼파라미터 공간의 특정 특성에 더 잘 맞도록 설정을 조정해야 할 수도 있습니다.\n\n# 6: 결론\n\n신경망의 세밀한 조정에 대해 심층적으로 살펴본 후에 우리가 걸어온 길을 돌아보는 좋은 시기입니다. 우리는 신경망이 어떻게 작동하는지에 대한 기본 사항부터 시작하여 그들의 성능과 효율성을 높이는 더 정교한 기술로 점진적으로 발전해왔습니다.\n\n## 6.1: 다음 단계\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리는 신경망 최적화에 많은 영역을 다루었지만, 명백히 우리는 겨우 표면만 긁은 것 뿐입니다. 신경망 최적화의 영역은 방대하며 지속적으로 진화하고 있으며, 아직 탐험하지 않은 기술과 전략으로 넘쳐납니다. 다가오는 기사에서 더 심층적으로 파고들어 복잡한 신경망 구조와 더 높은 성능과 효율성을 끌어올릴 수 있는 고급 기술을 탐구할 예정입니다.\n\n저희가 파헤치고자 하는 최적화 기술과 개념의 다양한 범위에는 다음과 같은 것들이 포함됩니다:\n\n- 배치 정규화: 입력 레이어를 정규화해 활성화를 조정하고 스케일링하여 훈련 속도를 높이고 안정성을 향상시키는 방법입니다.\n- 최적화 알고리즘: SGD 및 Adam을 포함한 최적화 알고리즘은 복잡한 손실 함수의 영역을 더 효과적으로 탐색할 수 있는 도구를 제공하여 더 효율적인 훈련 주기와 더 나은 모델 성능을 보장합니다.\n- 전이 학습 및 파인 튜닝: 사전 훈련된 모델을 활용하여 새로운 작업에 적응시키면 훈련 시간을 크게 단축하고 데이터가 제한적인 작업에서 모델 정확도를 향상시킬 수 있습니다.\n- 신경 아키텍처 탐색(NAS): 자동화를 사용하여 신경망을 위한 최상의 아키텍처를 발견함으로써 직관적이지 않은 효율적인 모델을 발견할 수 있습니다.\n\n이러한 주제들은 단지 저희가 다루는 것 중 일부에 불과하며, 각각 고유한 이점과 도전을 제공합니다. 앞으로 나아가면서, 이러한 기술을 자세히 살펴보고, 언제 사용해야 하는지, 그들이 어떻게 작용하는지, 그리고 당신의 신경망 프로젝트에 미치는 영향에 대한 통찰력을 제공할 것을 목표로 합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 추가 자료\n\n- “Deep Learning” - Ian Goodfellow, Yoshua Bengio, Aaron Courville 저: 깊은 학습 기술과 원리에 대한 깊이 있는 개요를 제공하는 이 근본적인 문헌은 고급 신경망 구조 및 최적화 방법을 다룹니다.\n- “Neural Networks and Deep Learning: A Textbook” - Charu C. Aggarwal 저: 신경망에 대한 상세한 탐구를 제공하며, 깊은 학습과 그 응용에 중점을 둡니다. 신경망 디자인 및 최적화의 복잡한 개념을 이해하는 데 탁월한 자료입니다.\n\n여기까지 왔습니다. 축하해요! 이 기사를 즐기셨다면 좋아요를 누르고 팔로우해주시면 감사하겠습니다. 저는 정기적으로 유사한 기사를 게시할 예정이니 많은 관심 부탁드립니다. 제 목표는 가장 인기 있는 알고리즘을 다시 처음부터 만들어 머신 러닝을 모든 사람이 접근 가능하도록 하는 것입니다.\n","ogImage":{"url":"/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-TheMathBehindFine-TuningDeepNeuralNetworks_0.png","tag":["Tech"],"readingTime":51},{"title":"Langchain이 필요하지 않은 이유","description":"","date":"2024-07-09 19:54","slug":"2024-07-09-HeresWhyYouProbablyDontNeedLangchain","content":"\n## Langchain을 사용하지 않고 CSV 파일과 상호 작용하는 LLM 파이프라인\n\nLangChain, LangGraph, LlamaIndex, CrewAI... 한 가지 도구에 익숙해지기 시작하면 또 다른 도구가 등장합니다. 이 도구들을 깎아내리거나 싫어하지는 않지만, 이 중 하나와 개발하는 것은 학습 곡선 때문에 진정으로 무섭고 당황스러울 수 있습니다. 이 도서관들을 배우는 것으로 끝내는 대신에 실제로 무언가를 구축하고 싶다면 낙담하기 쉬울 수 있습니다. LangChain은 훌륭하지만 정말 밀도있게 포장되어 있습니다. 문서 자체로는 Python용, JavaScript용 하나씩, 그리고 아마도 API를 위한 부모 문서가 있을 거라고 생각합니다. 그리고 Python 내에서도 비슷한 작업을 수행하는 여러 기능들이 있습니다. 이 모든 것은 좋지만, 코드에 충분한 제어권이 없습니다. 예를 들어, 이전 블로그에서 PDF를 바이트로 로더에 전달하려고 했지만, 함수는 파일 URL만 허용했기 때문에 제대로 수행할 수 없었습니다. 나에게 따르면, 무언가를 간단히 구축할 때 LangChain은 마지막 선택이어야 합니다. 항상 KISS (Keep it simple, stupid)를 지키세요.\n\n![이미지](/TIL/assets/img/2024-07-09-HeresWhyYouProbablyDontNeedLangchain_0.png)\n\n음, 그들이 나가리키는 만큼 사악하지 않을 수도 있지만, 충분하지 않다고 판단된다면 먼저 원시적인 것을 시도한 다음에 더 고급 도구로 넘어가는 것이 항상 좋습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이번 달은 상당히 바쁘게 보냈어요 (사실, 2022년 11월 이후 모든 달이 그랬어요) GPT-4o의 발매와 Gemini 업데이트가 두 날에 걸쳐 연이어 발표되면서요.\n\n![image](/TIL/assets/img/2024-07-09-HeresWhyYouProbablyDontNeedLangchain_1.png)\n\nGPT-4o의 'ScarJo' - 아니, 'Sky' - 음성은 데모에서 영향을 주었는데, 이로 인해 인공지능 산업에 파도를 일으키고 구글 I/O의 Gemini 성능을 가려버렸어요. 솔직히 말해서, GPT는 항상 모든 LLM들보다 한 발 앞서 있을 거예요만 MVP 및 개인 프로젝트를 만들 때, 무료 티어를 제공하는 Gemini는 제 같은 가난한 개발자들에게 축복이예요. 최신 모델에 액세스할 수 있는 무료 티어로 API 제한 속도가 15 rpm로 정해져 있어요. 나쁘지 않죠?\n\nChatGPT의 무료 버전을 통해 이제 GPT-4o로 파일을 업로드할 수 있지만, 여전히 코드 해석기 없이 CSV나 엑셀 데이터를 해석할 수 없어요. 그래서 Gemini Flash의 능력을 테스트해보기 위해 제가 직접 CSV 해석기를 만들기로 결심했어요. Langchain을 사용하는 방법을 보여주는 많은 튜토리얼이 있지만, 저희는 처음 시작하는 사람들에게 특히 더 이해하기 쉬운 방법으로 처음부터 만들어볼 거예요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 파이프라인\n\n내 말 믿어봐, 처음에는 약간 어려워 보일 수 있지만 정말 간단해. 기본 아이디어는 LLM에게 코드를 생성하도록 요청하는 것이야. 코드를 생성하려면 작업 중인 데이터셋에 대한 일부 컨텍스트를 제공해야 해.\n\n1단계는 코드 생성을 다루는데, 데이터셋의 메타데이터를 제공하고, 내 경험에 따르면 head(), describe(), columns(), dtypes이 충분한 컨텍스트를 제공할 거야. LLM에 대한 경험이 있는 경우, RAG (Retrieval Augmented Generation)을 사용할 수 있는지 궁금할 수도 있어. RAG는 벡터 데이터베이스와 시맨틱 검색을 활용해 데이터 검색 후 그 결과를 쿼리와 함께 LLM에 삽입하는 것이야. 이는 텍스트 데이터와 잘 작동하지만 구조화된 또는 비구조화된 데이터의 분석에는 적합하지 않아. 왜냐하면 전체 데이터셋이 조각으로 나뉘어 컨텍스트를 제한하고 기능을 제한하기 때문이지. 예를 들어, 데이터셋에서 총 행의 수를 조회하고 싶다면 RAG로는 간단히 불가능해.\n\n2단계에서는 1단계에서 생성된 코드를 실행하여 생성된 출력을 LLM에 제공하고 사용자 쿼리와 결합시킨다. 이를 통해 pandas 명령의 출력을 자연어로 변환하여 대화 경험을 향상시킬 수 있어.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n표 태그를 Markdown 형식으로 변경해주세요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n대부분의 사람들이 하는 신입 실수 중 하나는 프롬프트에 가능한 모든 지시사항을 자세히 기술하는 것입니다. LLMs는 지능이 없으므로 모든 것을 지키지 않을 것이며(적어도 AGI가 달성될 때까지), 이 문제를 피하기 위해 다수의 기술과 전략이 개발되었습니다. 예를 들면 제로샷 프롬프팅, 사고 흐름, 퓨샷 프롬프팅 등이 있습니다. 이들이 멋지게 들리기는 하지만 실제로는 매우 기본적인 기술들입니다. 이미 이들에 대한 많은 문헌 자료가 있기 때문에 그것들에 대해서는 자세히 다루지 않겠습니다. 이 중에서 나에게 가장 효과적으로 작용한 것은 역할 기반 프롬프팅입니다. 역할 기반 프롬프팅에서는 쿼리하기 전에 LLM에게 역할을 할당합니다. 이는 원하는 결과를 달성하는 데 도움이 됩니다.\n\n총 네 개의 프롬프트가 필요합니다: 두 개의 시스템 프롬프트와 두 개의 주요 프롬프트(각 단계별 시스템 프롬프트 + 주요 프롬프트). 시스템 프롬프트는 틀을 제공하여 인공지능이 특정 매개변수 내에서 작동하고 일관되고 관련성 있으며 원하는 결과와 일치하는 응답을 생성할 수 있도록하는 역할을 합니다.\n\n## 단계 1\n\n시스템 프롬프트\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n### Main Prompt\n\nThe dataframe name is ‘df’. df has the columns 'cols' and their datatypes are 'dtype'. df is in the following format: 'desc'. The head of df is: 'head'. You cannot use `df.info()` or any command that cannot be printed. Write a pandas command for this query on the dataframe df: 'user_query'\n\nIf you provide me with the specific metadata values, I can help you generate the pandas command for the user query on the dataframe 'df'. Feel free to ask any questions!\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n응답 스키마\n\n```python\nclass Command(typing_extensions.TypedDict):\n    command: str\n```\n\n젬니 SDK에서 JSON 모드로 전환할 수 있습니다. 일관된 JSON 출력을 얻으려면 응답 스키마를 Python 클래스로 정의하고 LLM 응답 생성 함수의 매개변수로 전달하세요. Command 클래스에서 JSON은 하나의 key인 command와 문자열 값만을 가질 것입니다.\n\n```python\n// 사용자 쿼리: \"상위 5개 데이터를 보여줘\"\n\n{\n  \"command\": \"df.head()\"\n}\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 단계 2\n\n시스템 프롬프트\n\n당신의 임무는 이해하는 것입니다. 사용자 쿼리와 응답 데이터를 분석하여 자연어로 된 응답 데이터를 생성해야 합니다.\n\n주 프롬프트\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n사용자 쿼리는 'final_query'입니다. 명령의 출력은 'str(data)'입니다. 데이터가 'None'인 경우 '시작하려면 쿼리를 요청하세요'라고 말할 수 있습니다. 사용된 명령을 언급하지 마세요. 출력에 대한 자연스러운 언어의 응답을 생성하세요.\n\n# 모두 함께 넣기\n\n편리함을 위해 이 프로젝트에는 Streamlit을 사용할 것입니다. 이는 많은 시간을 절약해줄 것입니다. 이 프로젝트는 80%의 GenAI 응용 프로그램 내부를 이해하는 데 목표를 두고 있습니다. Python을 사용하여 사용자 정의 웹 애플리케이션을 구축하는 방법을 배우고 싶다면 제 블로그를 여기에서 읽어보세요.\n\n데이터프레임 메타데이터\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\ndf = pd.read_csv(uploaded_file)\nhead = str(df.head().to_dict())\ndesc = str(df.describe().to_dict())\ncols = str(df.columns.to_list())\ndtype = str(df.dtypes.to_dict())\n\n시스템 프롬프트\n\n# Stage 1\n\nmodel_pandas = genai.GenerativeModel('gemini-1.5-flash-latest', system_instruction=\"판다스와 함께 작업하는 전문 파이썬 개발자입니다. JSON 형식으로 사용자 쿼리를 위한 간단한 판다스 '명령'을 생성하는 것을 확인합니다. 'print' 함수를 추가할 필요는 없습니다. 명령을 생성하기 전에 열의 데이터 유형을 분석하세요. 불가능한 경우 'None'을 반환하십시오. \")\n\n# Stage 2\n\nmodel_response = genai.GenerativeModel('gemini-1.5-flash-latest', system_instruction=\"태스크는 이해하는 것입니다. 사용자 쿼리를 분석하고 응답 데이터를 자연어로 생성하는 데 있어서 의무가 있습니다.\")\n\n메인 프롬프트\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# Stage 1\nfinal_query = f\"데이터프레임 이름은 'df'입니다. df는 열 {cols}을 가지고 있으며 데이터 유형은 {dtype}입니다. df는 다음 형식으로 구성되어 있습니다: {desc}. df의 맨 위 데이터는 다음과 같습니다: {head}. df.info()나 출력할 수 없는 명령을 사용할 수 없습니다. 데이터프레임 df에 대한 이 쿼리에 대한 판다스 명령어를 작성해주세요: {user_query}\"\n\n# Stage 2\nnatural_response = f\"사용자 쿼리는 {final_query}입니다. 명령어의 결과는 {str(data)}입니다. 데이터가 'None'인 경우 '시작하기 위해 쿼리를 요청하세요'라고 말할 수 있습니다. 사용된 명령을 언급하지 마세요. 결과에 대한 자연어 응답을 생성해주세요.\"\n```\n\n응답 생성\n\n```js\n# Stage 1\nresponse = model_pandas.generate_content(\n                final_query,\n                generation_config=genai.GenerationConfig(\n                    response_mime_type=\"application/json\",\n                    response_schema=Command,\n                    temperature=0.3\n                )\n            )\n\n\n# Stage 2\nbot_response = model_response.generate_content(\n                natural_response,\n                generation_config=genai.GenerationConfig(temperature=0.7)\n            )\n```\n\ntemperature는 생성된 응답의 무작위성을 제어하는 데 사용됩니다. 높은 temperature는 더 창의적이고 다양한 응답을 생성하지만 실제 사용자 쿼리에서 벗어날 수 있습니다. 낮은 temperature는 더 일관된 몰입형 응답을 생성하지만 창의적이지 않을 수 있습니다. 그래서 제가 Stage 1에는 정확한 명령을 얻기 위해 낮은 temperature를 선택했고, Stage 2에는 답변이 지루하고 로봇적이지 않도록 더 높은 temperature를 선택했습니다. 달콤한 지점을 찾기 위해 실험해보세요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n팬더스 명령을 실행하려면 파이썬에서 exec() 함수를 사용할 것입니다. 이 함수를 통해 파이썬 코드를 동적으로 실행할 수 있습니다. exec() 함수와 관련된 보안 취약점을 알고 있지만, 이 경우에는 다른 방법이 없었습니다. exec() 함수를 더 안전하게 사용하기 위해 아키텍처를 개선하거나 더 많은 유효성 검사를 추가할 수 있습니다. 이 프로젝트는 개인 프로젝트이므로 그러한 조치를 취하지 않았습니다. 파이썬 코드를 안전하게 실행할 수 있는 라이브러리가 있는지 알고 계시다면 댓글 섹션에 알려주세요.\n\n# 사용자 인터페이스\n\n![이미지 1](/TIL/assets/img/2024-07-09-HeresWhyYouProbablyDontNeedLangchain_2.png)\n\n![이미지 2](/TIL/assets/img/2024-07-09-HeresWhyYouProbablyDontNeedLangchain_3.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/TIL/assets/img/2024-07-09-HeresWhyYouProbablyDontNeedLangchain_4.png)\n\n# 결론\n\n항상처럼, 전체 과정을 가능한 한 쉽게 만들려고 노력했습니다. 여기까지 읽어주셨다면, 이것이 로켓 과학이 아님을 알 것입니다. 그냥 점을 이어주는 것이며, LLMs를 범용 API로 생각해야 합니다. 특정 방식으로 LLM을 트리거하면 어떤 작업이든 수행할 수 있는데, 이것은 다음 프로젝트를 위해 활용해야 할 핵심 요소입니다. 이 프로젝트는 혁신적이거나 새로운 것이 아닙니다 — ChatGPT의 코드 해석기는 유사한 아키텍처에서 작동합니다. 나는 이것이 매우 추상화되어 있어 처음 시작하는 사람들에게 흥미로울 것으로 보입니다. LLM에 메타데이터를 주입하여 코드를 생성하는 아이디어는 샤워 중에 떠올랐고, 꽤 잘 수행되었습니다.\n\nStreamlit UI와 함께 전체 코드를 보려면 gist를 확인하세요. Streamlit에 대해 더 알고 싶다면 여기 블로그를 읽어보세요. 이 튜토리얼을 즐겁게 읽으셨다면, 기사에 박수를 👏 보내주시고 더 많은 콘텐츠를 위해 팔로우해주세요.\n","ogImage":{"url":"/assets/img/2024-07-09-HeresWhyYouProbablyDontNeedLangchain_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-HeresWhyYouProbablyDontNeedLangchain_0.png","tag":["Tech"],"readingTime":11},{"title":"신경망이란 무엇인가요","description":"","date":"2024-07-09 19:51","slug":"2024-07-09-WhatIsaNeuralNetAnyway","content":"\n![Image](/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_0.png)\n\n인공지능과 기계학습의 세계를 두르고 있는 다양한 용어 중에서, 신경망과 같은 용어는 '멋진 요소'를 지니는 것 같아요.\n\n과학 소설의 세계가 이 용어를 빌려와서 매우 발달한 로봇과 안드로이드의 내부 작동 원리를 묘사하는데 기회를 마련한 결과입니다. 아놀드 슈워제네거의 터미네이터가 T2: 심판의 날에서 코너 가족과 나눈 대화를 기억하지 않는 사람이 누구죠?\n\n존 코너:\n프로그램되어 있지 않은 내용을 배우는 것이 가능한가요? 그러면... 더 인간적이고, 항상 꼴좋지가 않다고요?\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n터미네이터:\n내 CPU는 신경망 프로세서야; 학습 컴퓨터지. 그러나 Skynet은 우리를 홀로 보낼 때 스위치를 읽기 전용으로 설정해.\n\nSarah Connor:\n너무 많이 생각하지 않기를 원치 않는 거야, 그렇지?\n\n터미네이터:\n그래.\n\n![이미지](/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_1.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아마도 신경망이라는 아이디어가 기계 세계와 인간 두뇌 세계 사이의 보통의 엄격한 분리를 흐리게 한다는 것 때문에 그럴 수도 있습니다. 인간/기계 경계를 넘어가는 이러한 아이디어들은 우리를 항상 매혹시키거나 흥분시킵니다.\n\n오늘날의 신경망은 그 자체로 인간 두뇌의 영감으로부터 그 존재를 갚고 있습니다.\n\n# 두뇌에서 바이트로\n\n1943년에는 두 번째 세계대전의 치열한 시기였으며 소련이 스탈린그라드 전투에서 나치 독일을 이겼던 해였는데, 신경생리학자 워렌 S. 맥컬럭과 인지 심리학자 월터 H. 피트가 함께 논문을 작성하여 두뇌의 작동을 추상적인 수학적 용어로 설명하려고 노력했습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![신경망이란 무엇인가요](/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_2.png)\n\n제목은 \"신경 활동에 내재된 아이디어의 논리적 논리\" (아니요, 목이 나가는 제목으로는 점수를 주지 않습니다)였고, 논문의 통찰 중 하나는 뇌 속 신경세포를 더 간단한 논리 문으로 취급하여 입력에 따라 발화하거나 발화하지 않는 간단한 논리 게이트로 제안한 것이었습니다.\n\n뇌의 작동을 추상 수학으로 변환하는 아이디어가 기계 학습으로 자연스럽게 진화하는 과정을 볼 수 있습니다. 뇌가 수학으로 변환되고, 수학이 프로그래밍 언어를 통해 컴퓨터 코드로 변환되는 것입니다.\n\n신경망의 개념과 1950년대 Frank Rosenblatt의 패턴 인식 퍼셉트론, 1970년대 백프로파게이션 도입부터 2010년대 GPU 기반 딥 러닝의 출현에 이르기까지 수년간의 혁신은 우리를 인공 신경망(ANN)과 기계 학습에서의 역할이 단순히 퍼지는 세상으로 이끌었습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n실제로 신경망은 다른 형태의 기계 학습과 유사합니다. 목표는 데이터를 입력하고 해당 데이터를 기반으로 패턴을 추출하거나 예측을 수행하는 것입니다. 예를 들어, 주택 가격을 예측하는 전형적인 사용 사례가 있습니다. 면적, 침실 수, 그리고 대지 크기와 같은 데이터를 입력하면 데이터 세트 내의 주택에 대한 예측된 가격을 얻게 됩니다.\n\n신경망은 이러한 예측을 어떻게 하는 것이 가장 좋은지를 학습 데이터로부터 배우며, 결국 충분한 학습을 통해 합리적인 정확도로 예측을 할 수 있게 됩니다.\n\n하지만 이 신경망이 정확히 무엇이며, 인공 뉴런은 무엇인가요? 이런 기계 학습 사용 사례를 해결하기 위해 신경망이 어떻게 작동하는 걸까요?\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 질문에 대한 복잡하고 수학적인 답변들이 많이 있어요. 이 개념들을 초심자 친화적인 방식으로 탐색할 거에요. 즉, 초심자 친화적인 언어와 코드를 사용할 거에요.\n\n이 수업을 듣고 나면 기계 학습 닌자가 되지는 않겠지만, 뇌신경망이 인공지능에 어떻게 적용되는지 확실하게 상상할 수 있고, 코드를 통해 실험도 할 수 있을 거예요.\n\n그 말을 기억하며, 시작해 봐요!\n\n# 기본적인 내용\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n인공 신경망의 핵심은 뉴런 개념에 있으므로 우리 여행을 시작하는 가장 좋은 곳처럼 보입니다. 아래에는 유기 뉴런의 구조를 왼쪽에, 인공 뉴런을 오른쪽에 보여주는 멋진 이미지가 있습니다:\n\n![What is a Neural Net Anyway](/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_4.png)\n\n생물학적 뉴런의 경우, 세포체 주변의 작은 줄기 모양의 가지(dendrites)는 '입력'으로 생각할 수 있으며, 축삭(axon)은 '출력'으로 생각할 수 있습니다.\n\n하지만 우리는 인공 뉴런을 탐구하고 싶으므로 오른쪽 이미지에 주목해봅시다. 약간 위압감을 주는 것 같지 않나요? 처음에는 확실히 그랬어요. 아마 숫자를 나타내는 글자 아래에 있는 작은 숫자들, 게다가 그리스 문자도 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n수학이 나타납니다...\n\n하지만 이 뉴런 개념을 가장 기본적인 형태로 축소해서 훨씬 간단하게 만들어볼 수 있습니다. 몇 가지 — 전부는 아니지만 — 무서운 문자들을 제거해볼까요? (고등학교 수학이 평생에 걸쳐 상처를 남긴 것을 깨달고 계신가요?)\n\n아래는 매우 간단한 뉴럴 네트워크에 대한 시각화입니다. 이것은 딱 한 개의 '뉴런'으로 구성되어 있습니다. 뉴럴 네트워크의 가장 간단한 표현이며, 깊은 학습의 아버지인 Frank Rosenblatt가 만든 용어 '퍼셉트론'으로 잘 알려져 있습니다.\n\n<img src=\"/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_5.png\" />\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n자, 이 사진은 다소 이해하기 쉬워 보이네요.\n\n이제 우리는 신경망이 학습을 하도록 설계되었고 생물학적 뉴런을 기반으로 한다는 것을 알았으니, 우리 뉴런이 입력(정보)을 받고 출력을 제공해야 한다는 합리적인 추론을 할 수 있습니다. 이미지에서 이것을 어느 정도 볼 수 있습니다. 왼쪽에는 몇 개의 상자가 있고 가운데에는 일부 내용이 있으며 오른쪽에는 출력이 있습니다.\n\n왼쪽에 있는 입력부터 시작해봅시다. 일반적으로 'x'로 표시되는데요. 따라서 x1과 x2가 있는 것입니다. 이것은 우리의 입력이 2가지 특징을 가진 '것들'이라는 뜻입니다. 예를 들어 달콤함과 단단함에 기반을 둔 과일이 사과인지 바나나인지 예측할 수 있습니다.\n\n지금은 입력이 무엇인지가 중요한 게 아니라, 그 입력이 2가지 특징을 가지고 있다는 것입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 가중치 & 편향\n\n저희의 입력 변수(x1과 x2) 각각에는 해당하는 w 값(w1과 w2)이 있습니다. 이것들은 가중치를 나타냅니다.\n\n![image](/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_6.png)\n\n가중치는 우리의 뉴런에서 매우 중요한 부분이며, 입력으로 들어오는 각 특성에 얼마나 중요성을 부여할지 결정합니다. 그래서 각 특성당 하나의 가중치가 있습니다. 예를 들어 주택 가격 예측 예제에서 침실 수가 위치보다 가격을 더 잘 예측하는 경우, 해당 특성의 가중치는 귀하의 신경망을 교육하고 결과적으로 높아질 것입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n훈련을 통해 언급하는 이유는 처음에 신경망이 우리의 예측에 가장 중요한 특징이 무엇인지 모르기 때문입니다. 그래서 이 가중치 매개변수의 값은 작은 (랜덤) 값으로 시작합니다. 들어오는 데이터로 신경망을 계속 훈련시켜서 이러한 가중치가 시간이 지남에 따라 변하고 (더 정확해지게) 할 수 있습니다. 이러한 무작위 값을 선택하는 방법은 신경망을 구성할 때 사용하는 방식에 따라 다릅니다만, 일반적인 예로는 다음과 같은 것들이 있습니다:\n\n- 균일 분포: 숫자는 작은 범위 사이에서 균일하게 분포될 것이며, 예를 들어, -0.05부터 0.05 사이일 수 있습니다.\n- 정규 분포: 값은 평균이 0이고 표준 편차가 작은 (예를 들어 0.01) 정규 분포에서 추출된 작은 숫자일 수 있습니다.\n\n일단은 훈련하기 전에 w1과 w2에는 특별히 유용하지 않은 숫자가 있다고 가정해 봅시다.\n\n그런데 빠르게 사이드 노트를 하자면 — ‘훈련’이라고 말할 때 어떤 의미일까요? 실제로 무슨 일이 일어나고 있을까요? 일반적으로 기계 학습 모델을 훈련하는 것은 여러 epoch를 사용하여 수행됩니다. 여기서 말하는 epoch는 ‘제공한 모든 훈련 데이터를 한 번 통과하는 것’을 의미하는 용어로, 우리의 경우 데이터를 신경 세포에 뭉쳐주고 다른 쪽으로 내보내는 것입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이러한 epoch의 끝에서 매개변수가 업데이트되므로 이상적으로는 다음 번 훈련에서 예측이 좋아지도록 손실(잘못된 예측)이 줄어든다. 계속해서 좋은 예측 능력을 갖게 될 때까지!\n\n![이미지](/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_7.png)\n\n간단한 다이어그램을 돌아보면 우리의 입력과 함께 'b'로 표시된 것을 알 수 있습니다. b는 편향(bias)을 나타내며, 또 하나의 중요한 기계 학습 개념입니다. 저희의 가중치와 마찬가지로, 신경망 훈련 과정 중에 변경되는 '매개변수'입니다.\n\n저희의 간단한 예제에서, 그리고 많은 실제 사례에서, 편향은 훈련을 시작하기 전에 0에서 시작하지만 각 epoch을 처리할 때마다 편향이 잠재적으로 업데이트될 것입니다. 하지만 무엇을 위해? 그리고 왜 그렇게 되는 걸까요?\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그래도 일반적인 편향 설명을 찾아보면 \"편향 항은 활성화 함수가 왼쪽이나 오른쪽으로 이동할 수 있게 합니다.\" 정도의 내용이 나올 거에요. 하지만 좀 더 쉽게 이해할 수 있게 설명해볼게요.\n\n트레이닝 과정에서 각 반복마다 오차가 발생하는데, 이것이 기계 학습의 본질이자 같은 데이터 조각들을 반복해서 훈련하고 학습해야 하는 이유입니다. 목표는 시간이 지남에 따라 이러한 오차를 줄이는 것입니다.\n\n편향(가중치와 마찬가지로)는 각 훈련 단계마다 조정될 수 있는데, 이 조정된 값은 다음 반복을 '중립적이지 않은' 위치에서 시작하게 만들어줍니다. 기본적으로 다음 훈련 단계에 약간의 편향(따라서 이름이 붙여진 것)을 도입하여, 예측을 처음에 한 방향으로 이동시키고 다른 방향으로 이동시키는 데 도움이 되도록 하는 것이 목표입니다. 예를 들어 모델이 바나나를 예측할 때(0으로 표현) 실제 값이 사과(1로 표현)인데 너무 많은 오류를 발생시키고 있다면, 편향이 조금씩 0에서 1 방향으로 조정되어 다음 에포크를 돕는 역할을 할 수 있습니다.\n\n# 어이쿠, Σ 주소 변경할 시간이에요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이해해줘서 고마워요! 가능한 한 그리스어 문자를 너와 나로부터 멀리하려고 노력했어. 숫자 주변이나 떠다니는 재미있는 작은 기호들의 말장난으로부터 보호막처럼 행동했거든.\n\n하지만 이젠 그 시간이야. 우리 뉴런 속의 'Σ'에 대해 다뤄야 해. 괜찮아요, 난 부드럽게 다가갈게요.\n\n다행히도 이건 꽤 쉬운 부분이에요. 그리스 글자 'Σ'는 시그마(Sigma)라고 불리며, 수학 용어로는 합계를 나타냅니다. 기본적으로 더해주는 개념이죠. 그래서 우리 뉴런의 이 부분에서:\n\n<img src=\"/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_8.png\" />\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리는 가중치 입력을 합산하고 편향을 더하는 중입니다.\n\n이는 Σ(Wi \\* xi) + b로 계산됩니다. 여기서 Wi와 xi는 가중치와 입력을 가리키며, b는 편향을 의미합니다. 간단히 말하면 우리는 각 입력을 해당하는 가중치로 곱한 뒤 더하고, 마지막으로 편향을 더하는 것입니다.\n\n이후에는 '시그모이드 활성화'로 넘어갑니다. 시그모이드 활성화 또는 시그모이드 함수는 '압축' 함수로도 불립니다. 모든 합산과 더하기를 끝낸 후, 매우 큰 음수 또는 양수 값이 나올 수 있으며 해석하기 어렵을 수 있습니다.\n\n시그모이드 함수는 이 숫자를 0과 1 사이의 값으로 '압축'하여 의미 있는 숫자로 변환합니다. 이는 이론적으로 사과인지 바나나인지 등을 판별하는 기계 학습 문제와 같은 분야에서 특히 유용합니다. 이는 이진 분류 문제입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러면 마지막으로 묶어보는 시간이에요! 결과물을 보자구요! 그냥 최종 예측값을 가져와 출력하는 거에요! 그러면 우리의 뉴런을 통해 모험을 끝내게 돼요.\n\n보셨죠? 그렇게 어려운 건 아니죠! 물론 처음에 말한 것으로 돌아가는 것도 중요해요. 이것은 굉장히 간단한 예제에요. 이건 하나의 뉴런 신경망 - '퍼셉트론'이에요. 이건 간단한 분류 문제에 유용하지만, 더 많은 뉴런/노드를 추가할 때 진짜 신경망의 힘이 나타나요.\n\n뉴런은 레이어에 더 많이 추가할 때 더 복잡한 문제에 대한 예측 능력이 더 커져요:\n\n![neural net image](/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_9.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위의 이미지에서는 훨씬 많은 것이 발생하고 있습니다. 이 신경망에는 여러 개의 레이어가 있습니다. 입력 레이어와 출력 레이어에 추가로 두 개의 숨겨진 레이어가 있으며, 각 레이어에는 여러 개의 뉴런이 내부에 있습니다. 이 모든 것에 더해 더 복잡한 신경망에서 뉴런 간의 연결이 있는데 이를 시냅스라고 하며 '가중치'를 가지고 한 뉴런의 출력이 다른 뉴런의 입력에 미치는 영향을 제어합니다.\n\n그러나 이 모든 것에 대해 자세히 들어가는 것은 매력적이지만 이 글의 범위를 벗어나므로 단일 뉴런 예시에 집중하고 코드로 최고이며 가장 재미있는 부분으로 돌아갑시다!\n\n# 뉴런 만들기\n\n![image](/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_10.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n| 제목 | Korean Translation                      |\n| ---- | --------------------------------------- |\n| 문제 | 표 태그를 Markdown 형식으로 변경하세요. |\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러나 걱정하지 마세요. 저장소를 열어도 아무것도 이해되지 않는다면, 우리는 데이터와 코드를 검토하고 단계별로 문제를 해결할 것입니다. 코드를 더 잘 이해할 수 있도록 뇌세포(neuron), 신경망(neural net), 그리고 교육 루프의 기본을 조금이나마 알아본 후에 말이죠.\n\n우리의 실험에서는 Kaggle에서 가져온 합성 데이터셋을 사용할 것입니다. 이 데이터셋에는 다음이 포함되어 있습니다:\n\n![그림](/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_11.png)\n\n이 데이터의 아이디어는 각 행이 학생의 시험 1과 시험 2의 결과라는 것입니다. 패스(pass) 열은 해당 학생이 3번째 시험에 통과했는지 여부를 나타냅니다. 우리가 신경망을 사용해 예측하고자 하는 것은 바로 이 열입니다. 실제로는 학생이 처음 두 시험에서의 결과를 기반으로 3번째 시험을 통과할지 예측하려고 하는 것이죠.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저장소에서 exam_scores.csv라는 파일을 찾을 수 있어요. 이 데이터를 모델을 훈련하고 테스트하는 데 사용할 거예요. 저장소에 있는 유일한 다른 것은 실제 파이썬 노트북뿐이에요. 하나씩 차근차근 살펴보죠!\n\n처음으로, 노트북에 기본으로 설치되어 있지 않은 라이브러리를 설치해야 해요:\n\n```js\n# 1) 이미 설치되어 있지 않은 필요한 라이브러리 추가\n!pip install keras-visualizerya\n```\n\nkeras-visualizerya를 사용하면 우리의 신경망의 기본 이미지를 볼 수 있어요. 단일 뉴런이라 그다지 화려하지는 않겠지만, 아예 없는 것보다는 낫죠!\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음으로, 데이터 세트와 모델을 설정합니다.\n\n여기서 조금 복잡한 부분이 있습니다. 데이터를 형성하는 데 도움이 되는 몇 가지 라이브러리를 가져왔습니다. Numpy와 Pandas는 데이터를 형성하는 데 도움이 되고, sklearn은 데이터를 분할하는 것과 같은 일반적인 머신러닝 작업에 유용한 유틸리티를 제공합니다. 그리고 tensorflow.keras는 정말 중요한 라이브러리입니다.\n\n원한다면 순수 파이썬을 사용하여 신경망을 만들 수도 있습니다. 그러나 이 문서는 초심자에게 친숙하게 설계되었으므로, 텐서플로위에 위치한 고수준 라이브러리인 케라스를 선택한 것입니다. 케라스는 텐서플로우 위의 머신러닝 라이브러리이며, 이는 파이썬 위에 있는 라이브러리입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저희는 데이터를 70% 훈련 및 30% 테스트 데이터로 나누는 70/30 비율을 만들고 있어요. 이것은 일반적인 머신러닝 실천 방법으로, 데이터의 30%를 모델이 얼마나 잘 학습했는지 확인하는 데 사용합니다.\n\n마지막으로 - 그리고 가장 기대되는 부분은 - 모델을 정의하는 것입니다. 여기서 우리는 신경망을 만드는 곳입니다. Sequential 신경망을 만들고 있는데, 이는 이 모델에 추가된 레이어가 서로 직접 쌓여서 입력부터 출력까지 데이터가 선형 경로로 흐를 수 있는 피드포워드 네트워크를 형성한다는 의미입니다. 그리고 우리는 하나의 유닛(뉴런)을 추가하고 있어요.\n\n```js\nmodel = Sequential([\n    Dense(units=1, input_shape=(2,), activation='sigmoid')\n])\n```\n\n입력 모양은 신경망에 데이터가 어떤 모양인지 알려줍니다. 이 경우 input_shape(2,)은 Keras에게 각 데이터 레코드가 2개의 피쳐를 가질 것이라고 알려주는 것입니다. 마지막으로 활성화 함수를 'sigmoid'로 정의하고 있는데, 이는 0과 1 사이의 출력을 제공하기 위한 '압축' 함수입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마침내, 모델을 컴파일합니다. 'Adam' (적응 적 모멘트 추정) 옵티마이저를 사용 중이며 다른 여러 옵티마이저도 있습니다. 옵티마이저는 서로 다른 메커니즘을 사용하여 가중치를 업데이트하거나 '손실' 개념을 측정함으로써 학습 프로세스를 돕는 알고리즘 또는 방법입니다. 옵티마이저는 또한 학습 속도를 제어합니다 (우리 예제에서는 0.01로 설정되어 있음).\n\n![이미지](/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_12.png)\n\n학습률은 다른 개념과 깊게 연결되어 있는데, 여기서 많이 다루지는 않았지만 손실 기욘(gradient)입니다. 간단히 말해서, 손실 기욘은 학습 에포크의 끝에 우리가 얼마나 성공적인 예측에 가까이 갔는지를 측정하는 것입니다. 앞에서 알 수 있듯이, 신경망은 다음 반복 전에 가중치를 조정할 수 있습니다.\n\n학습률은 가중치가 손실 그래디언트에 대해 얼마나 큰 변화를 겪을지를 제어합니다. 높은 학습률은 빠르게 학습하고 좋은 '적합(fit)'에 빨리 도달할 수 있는 기회를 제공하지만, 최소 손실을 지나치게 초과하여 학습 프로세스의 불안정이나 기능을 예측하는 합리적 능력에 수렴하지 못하는 상황을 일으킬 수도 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리의 '손실 함수'는 'binary_crossentropy'로 설정되어 있어요 (옆에 알림 - 여기서 새로운 용어를 많이 배우고 있죠? 칭찬해도 돼요!). 손실 함수는 주로 우리 모델의 성능을 측정하고, 잘못된 값을 선택하는 것에 대해 처벌하는 역할을 합니다 (이 경우 이진 분류 문제에 대해).\n\n마지막으로 모델 컴파일 문에서 metrics를 '정확도'로 설정했어요. 이것은 훈련 중 정확도 지표를 모니터링하고 싶다는 의미에요. metrics 매개변수 - 그리고 컴파일 문의 거의 모든 것 -에는 다양한 값과 선택지가 있습니다. 이러한 선택 - 학습률, 손실 함수 및 옵티마이저와 같은 것들에 대한 - 이것들이 모델을 '조정'하거나 '하이퍼파라미터' 튜닝한다는 것을 읽을 때 조정되는 몇 가지 중요한 요소입니다. 이러한 매개변수들에서 최선을 내기 위한 지식이 필요하므로, 일단 이런 것들을 조정할 수 있다는 것만 알아두세요. 지금은 기본값이나 '일반적'인 값으로 진행하죠.\n\n보너스로, 노트북의 다음 셀에는 우리의 신경망을 시각적으로 표시하는 멋진 방법이 포함되어 있어요. 불행히도 우리의 단일 뉴런에 대해 매우 매료적인 이미지는 아니에요;\n\n<img src=\"/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_13.png\" />\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그 의미는, 우리가 원하는 것에 근접한 것 같아요. 2개의 입력 특성과 출력, 시그모이드 활성화 기능을 볼 수 있어요. 완벽하지는 않지만, 더 복잡한 신경망에는 더 잘 보일지도 몰라요.\n\n# 훈련 시간\n\n다음 셀이 활약할 때입니다! 훈련할 시간이에요! 코드를 살펴보면, 1000회 에폭을 실행할 거라는 것을 알 수 있어요. 많아 보일 수도 있지만, 15초 정도 안에 끝날 거예요. 훈련이 끝나면 다음과 같은 내용을 볼 수 있어요:\n\n```js\nTest Loss: 0.22098664939403534, Test Accuracy: 0.8833333253860474\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저희의 훈련 결과입니다! 그리 나쁘지 않죠! 정확도는 0.88 또는 88%이며 손실은 0.22입니다. 좋은 처음 결과이지만, 향상할 여지가 분명히 있어요. 몇 가지 매개변수를 조정/변경하여 더 나은 결과를 얻을 수 있을지도 모르겠어요.\n\n노트북의 마지막 셀은 테스트 데이터셋에서 몇 가지 값을 선택하여 직접 테스트하여 예측을 확인할 수 있는 기회를 제공합니다. 이상적으로는 테스트 세트에서 임의로 값들을 선택하여 학생이 3회 시험을 통과할 것인지(1) 아닌지(0) 예측하는데 높은 확률로 정확한 예측을 얻을 수 있어야 하며, 모델이 예측이 올바른지 얼마나 확신하는지의 신뢰도 점수도 얻을 수 있어야 합니다.\n\n여기 데이터에서 테스트하려는 데이터의 샘플을 numpy 배열로 업데이트해주세요:\n\n```js\n# 새 시험 점수를 사용한 예제\nnew_samples = np.array([\n    [74.71, 61.49],  # 예제 점수\n    [79.42, 67.92],   # 예제 점수\n    [62.75, 97.53]\n])\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n여기 있습니다! 여러분은 전체 모험을 무사히 마치고 미쳐 날뛰는 그리스 문자나 방정식에게 습격당하거나 끌려가지 않고 이것을 해내셨습니다.\n\n![Neural Net](/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_14.png)\n\n잘 했어요! 여러분은 신경망의 역사에 대한 간략한 개요를 보고 오늘날까지 어떻게 이르렀는지 살펴보고, 뉴런을 구성하는 추상적인 내용을 탐험하며, 심지어 자신만의 (간단한) 신경망을 만들고 훈련하고 검토하셨습니다!\n\n여정을 즐기셨기를 바라며, 항상 읽는 것을 즐겼다면 박수를 보내고 댓글을 남기거나 소프트웨어 엔지니어링, 클라우드 및 AI/ML 콘텐츠를 더 보기 위해 팔로우해 주시기 바랍니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음에 또 뵙겠습니다!\n","ogImage":{"url":"/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-WhatIsaNeuralNetAnyway_0.png","tag":["Tech"],"readingTime":19},{"title":"데이터 엔지니어링 로드맵 2024년 최신 가이드","description":"","date":"2024-07-09 19:48","slug":"2024-07-09-DataEngineeringRoadmap","content":"\n<img src=\"/TIL/assets/img/2024-07-09-DataEngineeringRoadmap_0.png\" />\n\n## 1. Foundational Knowledge\n\n- Familiarize with database concepts (SQL and NoSQL)\n\n- SQL — SQL databases are commonly used in data engineering for structured data storage and querying, providing ACID compliance and strong consistency. — (1 week).\n- NoSQL — NoSQL databases are favored for their scalability and flexibility in handling unstructured or semi-structured data, often used in distributed systems for high-volume and high-velocity data processing. — (1 week).\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 분산 컴퓨팅 원칙에 대한 지식 습득\n\n- 데이터 엔지니어가 확장 가능하고 내결함성이 있는 시스템을 설계하기 위해 분산 컴퓨팅 원칙을 이해하는 것은 중요합니다. 병렬 처리와 분산 저장를 활용하여 방대한 데이터세트를 효율적으로 관리하고 데이터 처리 작업에서 고가용성과 신뢰성을 보장합니다. — (1–2 주).\n\n![이미지](/TIL/assets/img/2024-07-09-DataEngineeringRoadmap_1.png)\n\n## 2. 데이터 모델링\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 다양한 데이터 모델링 기술에 대해 배우세요 (예: 관계형, 네트워크...)\n\n- 관계형 및 차원 모델링과 같은 데이터 모델링 기술은 데이터 엔지니어링에서 데이터를 구조화하여 특정 분석 및 보고 요구 사항을 충족시키기 위해 중요합니다. 최적의 성능과 데이터 검색 용이성을 보장합니다. — (1-2 주).\n\n- 정규화와 역정규화를 이해하세요\n\n- 정규화와 역정규화 원칙을 이해하면 데이터 엔지니어들이 데이터베이스를 설계할 때 중복을 최소화하고 데이터 무결성을 유지하는 균형을 맞추는 데 도움이 됩니다. 데이터 엔지니어링 파이프라인에서 저장 공간을 최적화하고 쿼리 성능을 최적화할 수 있습니다. — (1 주).\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 다양한 사용 사례에 대한 스키마 디자인 탐색\n\n- 스키마 디자인 고려 사항은 데이터 엔지니어링에서 중요한 역할을 하며, 다양한 사용 사례에 걸쳐 데이터 저장, 검색 및 분석 효율에 영향을 미치며, 다양한 응용 프로그램 시나리오에서 데이터 시스템의 확장 가능성, 유연성 및 유지 관리 가능성을 보장합니다. — (1–2 주).\n\n![이미지](/TIL/assets/img/2024-07-09-DataEngineeringRoadmap_2.png)\n\n## 3. 데이터 저장\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 데이터베이스 관리 및 최적화 분야에서 전문 지식 습득하기.\n\n- 데이터베이스 관리 및 최적화 능력은 데이터 엔지니어들이 데이터 시스템의 효율성과 대응력을 극대화하기 위해 인덱싱, 쿼리 최적화 및 자원 할당 전략을 구현하여 데이터베이스 성능을 관리하고 세밀하게 조정하는 능력을 제공합니다. — (2–3 주).\n\n![이미지](/TIL/assets/img/2024-07-09-DataEngineeringRoadmap_3.png)\n\n## 4. 데이터 처리\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 배치 처리 프레임워크 (예: Apache Spark, Hadoop MapReduce)를 탐색해보세요.\n\n- Apache Spark 및 Hadoop MapReduce와 같은 배치 처리 프레임워크는 대규모 데이터를 효율적으로 처리하기 위한 데이터 엔지니어링에서 필수적이며 예약된 일괄 작업을 통해 데이터 집약적 작업에 대한 병렬 계산 및 장애 허용 기능을 제공합니다. - (1-2 주).\n\n- 스트림 처리 프레임워크 (예: Apache Kafka, Apache Flink)에 대해 배우세요.\n\n- Apache Kafka 및 Apache Flink와 같은 스트림 처리 프레임워크는 실시간 데이터 처리 및 분석을 가능하게 하며, 낮은 대기 시간 데이터 수신을 용이하게 하고 지속적인 데이터 스트리밍 애플리케이션을 지원하여 데이터 엔지니어링에서 중요한 역할을 합니다. - (1-2 주).\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- ETL (Extract, Transform, Load) 프로세스 및 도구를 이해합니다.\n\n- ETL 프로세스 및 도구를 이해하는 것은 데이터 엔지니어링에서 매우 중요합니다. 이는 다양한 소스에서 데이터를 추출하고, 유용한 형식으로 변환한 뒤, 대상 데이터베이스나 데이터 웨어하우스에로드하여 데이터 품질, 일관성, 접근성을 보장하고 분석 및 보고 목적에 활용합니다. — (2–3 주).\n\n![Data Engineering Roadmap 4](/TIL/assets/img/2024-07-09-DataEngineeringRoadmap_4.png)\n\n## 5. 데이터 통합\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 다양한 소스(데이터베이스, API, 파일)에서 데이터 수집에 대한 전문 지식을 습득합니다.\n\n- 데이터 엔지니어링은 데이터베이스, API 및 파일과 같은 다양한 소스에서 데이터를 수집하여 데이터를 분석하기 위해 데이터를 모으고 중앙 집중화하는 전문 지식에 의존합니다. 이를 통해 포괄적인 데이터 범위와 접근성이 확보되며, (1~2주 소요됩니다).\n\n- 데이터 통합 패턴 및 모범 사례에 대해 학습합니다.\n\n- 데이터 엔지니어링에서 데이터 통합 패턴과 최상의 사례를 이해하는 것은 이질적인 데이터 소스를 조화롭게 하고 정확한 통찰력과 의사 결정을 위한 시스템 간의 데이터 흐름과 상호 운용성을 용이하게하기 위한 중요한 요소입니다. (2~3주 소요됩니다).\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 데이터 통합 및 동기화 도구를 탐색해보세요.\n\n- 데이터 통합 및 동기화 도구를 탐색하는 것은 데이터 엔지니어에게 데이터 워크플로우를 자동화하고 플랫폼 간 데이터를 동기화하며 데이터 일관성과 무결성을 유지하는 능력을 제공하여 데이터 엔지니어링 파이프라인에서 효율성과 신뢰성을 향상시킵니다. — (2–4 주).\n\n![DataEngineeringRoadmap_5.png](/TIL/assets/img/2024-07-09-DataEngineeringRoadmap_5.png)\n\n## 6. 데이터 변환\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- SQL, Python 또는 전문 도구(예: Apache Beam)를 사용하여 마스터 데이터 변환 기술 습득\n\n- 데이터 엔지니어링에서 SQL, Python 또는 Apache Beam과 같은 전문 도구를 활용한 데이터 변환 기술에 능통한 것은 다양한 데이터 세트 간의 호환성과 일관성을 보장하기 위해 데이터를 조작하고 재구성하는 데 중요합니다. — (2~4 주)\n\n- 데이터 클렌징, 정규화 및 데이터 풍부화 과정을 이해\n\n- 데이터 엔지니어링에서 데이터 클렌징, 정규화 및 데이터 풍부화 과정을 이해하는 것은 데이터 품질, 무결성 및 사용 가능성을 향상시키는 데 중요합니다. 데이터를 자신 있고 정확하게 분석하고 의사 결정을 내릴 수 있도록 데이터를 준비합니다. — (1~2 주)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 데이터 파이프라인 조정 및 일정에 대해 배워보세요.\n\n- 데이터 파이프라인 조정 및 일정을 배우면 데이터 엔지니어들은 조직의 데이터 인프라 전반에 걸쳐 시간적이고 신뢰할 수 있는 데이터 처리 및 전달을 보장하며 복잡한 데이터 워크플로우를 자동화하고 관리할 수 있습니다. — (1–2 주).\n\n![이미지](/TIL/assets/img/2024-07-09-DataEngineeringRoadmap_6.png)\n\n## 7. 데이터 품질 및 거버넌스\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 데이터 품질 측정 및 모니터링 기술을 이해합니다.\n\n- 데이터 엔지니어링에서 데이터 품질 측정 및 모니터링 기술을 이해하는 것은 데이터의 정확성, 완전성 및 일관성을 평가하고 유지하는 데 중요합니다. 의사 결정을 지원하기 위한 믿을 수 있는 통찰력을 보장합니다. — (1-2 주).\n\n- 데이터 지배 원칙 및 프레임워크에 대해 배웁니다.\n\n- 데이터 지배 원칙 및 프레임워크를 배우는 것은 데이터 엔지니어링에서 핵심적이며 데이터 자산을 효과적으로 관리하기 위한 정책, 프로세스 및 통제를 수립하여 데이터 수명주기 전반에 걸쳐 준수, 보안 및 책임성을 증진하는 데 중요합니다. — (1-2 주).\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 데이터 품질 점검 및 유효성 검사 프로세스를 구현하세요.\n\n- 데이터 품질 점검 및 유효성 검사 프로세스를 구현하는 것은 데이터 엔지니어가 데이터 이상과 불일치를 자동으로 감지하고 해결할 수 있도록 하여 분석 및 보고를 위한 고품질 데이터 입력을 보장하고 데이터 기반 통찰력의 전반적인 신뢰성을 향상시킵니다. — (3–4 주간).\n\n![Data Engineering Roadmap](/TIL/assets/img/2024-07-09-DataEngineeringRoadmap_7.png)\n\n## 8. 클라우드 기술\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 클라우드 플랫폼 (AWS, Azure, GCP)에 대한 숙련도 향상.\n\n- AWS, Azure 및 GCP와 같은 클라우드 플랫폼에 대한 능숙성은 대규모 데이터 세트의 비용 효율적인 저장, 처리 및 분석을 가능케 하기 위해 데이터 엔지니어링에서 중요합니다. — (3–4 주).\n\n- 클라우드 기반 데이터 저장 및 처리 서비스에 대해 배우기.\n\n- 클라우드 기반 데이터 저장 및 처리 서비스 학습을 통해 데이터 엔지니어들은 클라우드에서 확장 가능한 저장 솔루션과 분산 처리 프레임워크를 활용하여 효율적인 데이터 관리와 분석 워크플로우를 용이하게 할 수 있습니다. — (2–3 주).\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 클라우드 보안 및 규정 요구사항을 이해합니다.\n\n- 데이터 엔지니어링에서 클라우드 보안과 규정 요구사항을 이해하는 것은 강력한 보안 조치를 시행하여 데이터의 기밀성, 무결성 및 가용성을 보장하고 규제 규준과 업계 모범 사례를 준수하는 데 중요합니다. — (4–5 주).\n\n![이미지](/TIL/assets/img/2024-07-09-DataEngineeringRoadmap_8.png)\n\n## 9. 빅데이터 기술\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 분산 저장 시스템(예: Hadoop HDFS, Amazon S3)을 탐색해보세요.\n\n- Hadoop HDFS 및 Amazon S3와 같은 분산 저장 시스템을 탐색하는 것은 데이터 엔지니어링에서 매우 중요합니다. 이를 통해 분산 환경에서 대량의 데이터를 저장하고 관리하여 고가용성과 확장성을 보장할 수 있습니다. - (1-2 주).\n\n- 분산 컴퓨팅 프레임워크(예: Apache Spark, Apache Flink)에 대한 전문 지식을 습득하세요.\n\n- Apache Spark 및 Apache Flink와 같은 분산 컴퓨팅 프레임워크에 전문 지식을 습득하면 데이터 엔지니어는 대규모 데이터 세트를 병렬로 처리하고 분산 컴퓨팅 자원을 활용하여 효율적인 데이터 처리 및 분석을 수행할 수 있습니다. - (4-5 주).\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 컨테이너화 및 오케스트레이션 기술(Docker, Kubernetes 등) 이해하기\n\n- Docker 및 Kubernetes와 같은 컨테이너화 및 오케스트레이션 기술을 이해하는 것은 데이터 엔지니어링에서 중요합니다. 다양한 컴퓨팅 환경에 걸쳐 일관되게 데이터 기반 애플리케이션 및 워크플로를 패키징하고 배포하여 확장성, 이식성 및 자원 이용을 향상시킵니다. — (3–4 주).\n\n![이미지](/TIL/assets/img/2024-07-09-DataEngineeringRoadmap_9.png)\n\n## 10. 데이터 시각화 및 보고\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 데이터 시각화 도구 및 기술 (예: Tableau, Power BI)를 탐색해 보세요.\n\n- Tableau 및 Power BI와 같은 데이터 시각화 도구를 탐험하는 것은 복잡한 데이터셋을 통찰력 있는 시각적 표현으로 변환하는 데 필수적입니다. 이는 데이터 기반 의사 결정 및 커뮤니케이션을 용이하게 합니다. — (1–2 주).\n\n- 대시보드 디자인 및 데이터 스토리텔링에 대해 학습하세요.\n\n- 대시보드 디자인 및 데이터 스토리텔링에 대해 학습함으로써 데이터 엔지니어는 주요 통찰과 트렌드를 이해 관계자 및 의사 결정자에게 효과적으로 전달하는 매력적이고 유익한 대시보드를 만들 수 있습니다. — (1–2 주).\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 상호 작용형 시각화 및 보고서 작성에 대한 전문 지식 습득하기.\n\n- 상호 작용형 시각화 및 보고서 작성에 대한 전문 지식을 갖는 것은 데이터 엔지니어가 동적이고 사용자 친화적인 데이터 제품을 개발하는 데 도움이 되며, 다양한 수준의 청중들에게 데이터 엔지니어링 프로젝트 전반에 걸쳐 데이터 기반 인사이트의 참여와 이해를 높일 수 있습니다. — (2–3 주).\n\n![이미지](/TIL/assets/img/2024-07-09-DataEngineeringRoadmap_10.png)\n\n## 11. 고급 주제\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 실시간 분석, 데이터 레이크 및 그래프 데이터베이스와 같은 고급 주제를 탐험해보세요.\n\n- 복잡한 데이터 처리 문제를 해결하고 다양한 데이터 원본에서 새로운 통찰을 얻기 위해 데이터 엔지니어링에서 실시간 분석, 데이터 레이크 및 그래프 데이터베이스와 같은 고급 주제를 탐색하는 것이 중요합니다. — (2–4 주).\n\n- 분야의 새로운 기술 및 트렌드를 따라가세요.\n\n- 분야의 새로운 기술 및 트렌드를 파악함으로써 데이터 엔지니어는 최신 도구와 방법론을 활용하여 혁신을 이루고 데이터 엔지니어링 프로세스를 최적화하여 지속적인 개선을 추구하고 빠르게 변화하는 환경에서 선도할 수 있습니다. — (3–5 주).\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 12. Practical Projects and Experience\n\n- 실제 데이터 엔지니어링 프로젝트를 수행하여 여러분의 기술을 적용하세요.\n\n- 실제 데이터 엔지니어링 프로젝트를 수행하면, 산업 분야에서 데이터 처리, 통합 및 분석에 대한 실무 경험을 쌓아가며 실전 시나리오에서 기술을 적용할 수 있습니다. (5–6 주간 진행).\n\n- 오픈 소스 프로젝트에서 동료들과 협업하거나 해커톤에 참여하세요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 동료들과 오픈 소스 프로젝트에서 협업하거나 해커톤에 참여하는 것은 데이터 엔지니어들이 아이디어를 교환하고 복잡한 문제를 해결하며 혁신적인 데이터 엔지니어링 솔루션 개발에 기여할 수 있는 협력적인 환경을 조성합니다.\n\n- 데이터 엔지니어링 역할에서 실무 경험을 쌓기 위해 인턴십이나 취업 기회를 찾아보세요.\n\n- 인턴십이나 취업 기회를 찾는 것은 희망하는 데이터 엔지니어들에게 실무적인 경험을 쌓을 기회를 제공하여 이론적 지식을 적용하고 전문 기술을 개발하며 데이터 엔지니어링 역할과 책임에 대한 가치 있는 통찰력을 얻을 수 있습니다.\n\n# 앞으로 나아가기\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위에서 나열된 전체 여정을 소비 가능한 크기의 기사로 나눠서 소개할 거에요. 시리즈에 대해 업데이트를 받으려면 저를 따라오세요. 이 여정에 동참하게 될 거에요. 즐거운 학습되세요!!!\n","ogImage":{"url":"/assets/img/2024-07-09-DataEngineeringRoadmap_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-DataEngineeringRoadmap_0.png","tag":["Tech"],"readingTime":14},{"title":"당신의 Python 코드를 쉽게 개선할 수 있는 7가지 방법","description":"","date":"2024-07-09 19:45","slug":"2024-07-09-7SimpleWaysToDrasticallyImproveYourPythonCode","content":"\n<img src=\"/TIL/assets/img/2024-07-09-7SimpleWaysToDrasticallyImproveYourPythonCode_0.png\" />\n\n## 소개\n\n파이썬은 데이터 과학과 함께 올라와 프로그래밍 세계에서 인기를 얻었는데, 이에는 아주 좋은 이유가 있습니다. 이 언어에는 C와 밀접한 연계성과 다양한 문제를 원활하게 해결하기 위한 적절한 타입 및 함수 시스템을 포함한 여러 이점이 있습니다. 이러한 이점 중 하나는 파이썬이 작성, 학습 및 해독이 굉장히 쉬운 언어임에 틀림없습니다. 이로 인해 파이썬은 대부분의 기본 계산 작업에 대한 고수준 스크립팅 언어로 변모했습니다.\n\n파이썬의 사용 편의성은 특히 데이터에 보다 집중하고 싶은 과학자들이나 구문에 덜 관심을 갖고 싶은 초보 프로그래머들을 위한 매력적인 선택지가 됩니다. 그러나 사용 편의성과 접근성과 함께 성능에 주목할 필요가 있습니다. 또한 파이썬 같은 스크립팅 언어는 일반적으로 방법을 잘못 사용하더라도 파악하지 못하거나 직면할 필요가 없는 공간을 훨씬 더 많이 제공하는 경향이 있습니다. 다행히도 새로운 파이썬 사용자들을 위해서 파이썬은 광범위한 기능을 도입하여 사용하기 쉽고 동적으로 타입이 지정된 스크립팅 언어의 단점을 완화할 수 있었습니다. 좋은 프로그래밍 관행과 함께 파이썬의 타입 시스템과 생태계를 효과적으로 활용함으로써 파이썬의 다양한 단점을 완화하고 해당 언어에서 훨씬 더 나은 코드를 작성할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 추상화\n\n어떤 객체 지향 프로그래밍 언어에서 가장 먼저 이야기해야 하는 것은 추상화입니다. 추상화는 상속을 사용하여 훨씬 더 많은 기능을 더 적은 코드로 만들 수 있는 일반적인 프로그래밍 개념입니다. 상속은 추상화를 용이하게 하는 방법으로, 하위 클래스를 통해 기능을 전파하는 클래스 유형이 있는 방식입니다. 이것이 추상화 개념의 원래 출발점이자 객체 지향 프로그래밍 개념의 기반이 됩니다. 여기서 \"메서드\"는 해당 유형에 적합하게 설계된 클래스의 내부 함수를 의미합니다.\n\n추상화는 많은 다른 유형과 함께 사용할 수 있는 일반적인 함수를 만들 수 있게 해줍니다. 이게 바로 계층적 하위 클래스를 사용하여 어떤 유형인지 결정할 수 있는 것입니다. 이것은 다형성이라고도 하며, 하나의 서브루틴으로 여러 유형의 데이터 구조를 '변형'할 수 있는 능력을 의미합니다. Python에서는 메서드와 속성이 클래스를 구성합니다. 그런 다음 이 클래스의 하위 클래스가 될 수 있으며, 속성과 메서드를 상속받을 수 있습니다. 이것이 상속이라고 하며, Simula 시뮬레이션 언어에서 공개된 추상화 개념의 원조입니다.\n\n프로그래머로서 들을 수 있는 일반적인 조언 중 하나는 \"일반적인 함수를 작성하라\"는 것입니다. 이 조언은 의미가 있습니다. 함수가 보다 일반적일수록 프로젝트에 최종적으로 들어가는 코드가 적어집니다. 이는 또한 사물이 파괴될 기회가 적어지고, 파괴된 경우에는 해결책을 형성하기 위해 한 곳만 살펴보면 되는 것을 의미합니다. 그러나 이 조언에서 종종 빠지는 중요한 부분은 일반적인 함수를 계층적 추상화 수준으로 설계해야 한다는 것입니다. 다시 말해, 우리는 모든 차량을 위해 함수를 작성하고, 필요한 경우에만 트럭을 위한 함수를 작성해야 합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이렇게 보면, 파이썬이 제공하는 기능과 메서드 기능도 자연스럽게 패턴이 있는 것 같아요. 함수는 매우 일반적인 호출에 사용하고, 메서드는 특정 유형의 호출에 더 적합하며 특히 상속을 원하는 경우 더 좋아요. Python에서 추상화 기법을 사용하려면, 우선 클래스를 만들어야 해요:\n\n```python\nclass Car:\n    def __init__(self, name, color: str):\n        self.name = name\n        self.color = color\n    def honk(self):\n        print(self.name + \" has just honked at us in their \" + self.color + \" car\")\n```\n\n이 간단한 클래스에는 name 및 color 속성과 honk 메서드가 있습니다. 자동차를 만들고 honk 메서드를 호출하면 작은 메시지가 출력됩니다.\n\n```python\nmycar = Car(\"the mystery machine\", \"green, flower-covered\")\nmycar.honk()\n\nthe mystery machine has just honked at us in their green, flower-covered car\n선택이 삭제됨\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이것은 좋아요. 하지만 저는 트럭도 가지고 있어요. 제가 시골 소녀니까요. 그래서 우리는 트럭을 만들어야 할 거예요. 괄호 안에 Car 클래스를 Truck 클래스에 제공해주면 Car의 모든 속성과 메소드를 상속받을 수 있어요. 데이터인 이름과 색깔은 같이 저장하지만, 우리의 honk 메소드는 Car의 버전이 차량이 Car임을 언급하기 때문에 변경되어야 해요. 따라서 이를 고려하여, 우리는 서브 클래스에 새로운 메소드를 작성함으로써 상속된 메소드를 오버로드할 수 있어요.\n\n```js\nclass Truck(Car):\n    def honk(self):\n        print(self.name + \" has just honked at us in their \" + self.color + \" truck\")\n```\n\n추상화하지 않은 경우 이 코드는 다음과 같아요:\n\n```js\nclass Truck(Car):\n    def __init__(self, name, color : str):\n        self.name = name\n        self.color = color\n    def honk(self):\n        print(self.name + \" has just honked at us in their \" + self.color + \" truck\")\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nmytruck = Truck(\"my large truck\", \"red\")\nmytruck.honk()\n\nmy large truck가 빨간 트럭으로 우리에게 경적을 울렸어요\n```\n\n우리는 완전히 새로운 Car를 생성할 수도 있습니다. 여기서 중요한 점은 pass 키워드를 사용하여 Python의 들여쓰기 문법을 활용해 생성자를 이름만 정의하는 것입니다.\n\n```python\nclass Sedan(Car):\n    pass\n```\n\n훨씬 큰 사용 사례에서 추상화가 Python에서 훌륭한 코드를 작성하는 데 절대적으로 필요한 이유를 이해하기 쉽습니다. 다행히 이 기술은 사용하기 매우 쉽습니다, 그리고 제가 논의하고 싶은 다음 기술도 마찬가지입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 표현식들\n\n파이썬과 유사한 많은 다양한 스크립팅 언어를 사용한 후에 얘기하지만, 표현식들이 이런 언어에서 작업할 때 가장 귀중한 자산 중 하나라는 걸 자신 있게 말씀드릴 수 있어요. 표현식들은 제너레이터, 리스트 및 기타 유형을 아주 간결하고 효과적으로 생성하는 방법입니다. 일반적으로 기존의 for 루프를 통해 작성되는 것과 같은 형태로 구성될 수 있는 요소들을 생성하는 방법입니다. 예를 들어, 1부터 30까지의 제곱을 생성하는 다음 예제를 고려해보세요:\n\n```js\nvalues = []\nfor x in range(1, 30):\n    values.append(x ** 2)\n```\n\n이것은 for 루프 방식으로, 이는 파이썬에서 반복 가능한 요소와 함께 작업하는 핵심적인 방법입니다. 이 접근 방식의 for 루프 버전은 리스트를 초기화하고 각 반복에서 해당 리스트에 새 요소를 추가하는 것을 요구합니다. 이 방식은 효율적이지만, 요소를 생성하는 데 제너레이터를 사용하는 것만큼 효과적이지는 않습니다. 후자는 더 간결할 뿐만 아니라 성능도 더 나아집니다. 위 예제의 코드는 간단한 리스트 표현식으로 한 줄로 변환됩니다. 제너레이터를 생성하려면 사실상 역방향 for 루프를 작성하고 리스트 구분 기호로 둘러싸면 됩니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nvalues = [x ** 2 for x in range(1, 30)]\n```\n\n위에 대한 설명은 지능 표현식 또는 생성기가 다음 반복 단계의 실행에 매핑된 반환을 만들기위한 함수를 만들어 냄을 의미합니다. 좀 더 간단한 용어로 설명하면, 생성기는 함수에서 반복자를 통해 반환을 만들기 위해 특별히 만들어지는 반면 for 루프는 더 개방적이며 단순히 반복자를 만듭니다. 이로 인해 Python에서 작업을 크게 빠르게 만들며 코드를 훨씬 더 간결하게 만듭니다. 그러나 각 요소에 대해 반환을 기대하지 않는 경우에는 다른 방법을 사용하는 것이 더 나을 수 있습니다.\n\n## 람다 및 맵\n\nPython 코드를 극적으로 개선하는 다음 간단한 기술은 iterable의 요소들을 횡단하여 함수를 호출하는 조합인 람다와 맵을 사용하는 것입니다. 현재 Python의 가장 일반적인 응용 분야가 과학 컴퓨팅이라는 점을 고려하면, 람다와 맵은 놀라울 만큼 많이 사용되는 Python 기능입니다. 둘의 다양성은 많은 다른 도메인에서 함께 기능을 통해 뛰어나기 때문에, 이 기술은 확실히 알아두어야 할 가치가 있습니다. 특히 데이터 과학을 위해 Python 언어를 사용하는 사람들뿐만 아니라요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nLambda는 호출 가능한 함수 유형으로 신속하게 표현식을 작성하는 데 사용됩니다. 이를 통해 간단한 클로저 함수를 생성할 수 있습니다. 이 함수들은 인수로 제공되어 다양한 가능성을 제공할 수 있습니다. Python에서 람다를 사용하는 것은 비교적 간단합니다. 람다 키워드에 인수를 제공한 다음 콜론과 함수의 논리를 추가합니다.\n\n```js\nf = lambda x: x + 5\nf(5)\n\n10\n```\n\n이 개념은 함수를 생성하는 능력 때문에 상당히 강력합니다. 그러나 이 기술이 빛나는 한 가지 맥락은 이러한 함수들을 반복 가능한 객체에 매핑하는 것입니다. 이를 통해 생성기와 연관된 생성 측면 없이 배열을 변경할 수 있습니다. 이 경우 함수는 몇 가지 차이점이 있는 생성기와 유사한 기능을 만들기 위해 요소를 직접 제공받습니다.\n\n```js\nf = lambda x: x ** 2\n\nm = map(f, values)\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리가 맵을 사용하여 함수를 값에 매핑하면 맵을 단순히 리스트로 캐스팅하여 제곱된 배열을 얻을 수 있습니다:\n\n```js\nsquared = list(m)\n[6,\n 9,\n 14,\n 21,\n 30,\n 41,\n 54,\n 69,\n 86,\n 105,\n....\n```\n\n이 두 가지를 결합함으로써 코드를 압축하는 방법이 얼마나 쉬운지 알 수 있습니다!\n\n## 추출\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음 소프트웨어를 개선하는 방법은 일반적으로 우수한 프로그래밍 조언입니다. 추출 기술은 대규모 하위 루틴에서 코드를 추출하여 작은 하위 루틴의 시리즈로 압축하는 실천 방법입니다. 전체 작업을 처리하는 하나의 함수를 작성하는 대신 해당 작업의 각 단계는 프로젝트 내의 다른 함수로 전달됩니다. 이 기술에 대해 중요성과 중요성을 노래한 몇 가지 기사를 작성했습니다. 위대한 소프트웨어 공학 프로젝트를 만들 때 매우 중요합니다.\n\n```python\ndef quiz_user():\n    correct = []\n    print(\"이름은 뭐에요?\")\n    x = input()\n    print(\"2 + 2는 뭐에요?\")\n    answer = input()\n\n    if answer == \"4\":\n        print(\"정답\")\n        correct.append(True)\n    else:\n        print(\"틀렸어요.\")\n        correct.append(False)\n\n    print(\"하늘의 색깔은 뭐에요?\")\n    answer = input()\n\n    if answer == \"파란색\":\n        print(\"정답\")\n        correct.append(True)\n    else:\n        print(\"틀렸어요.\")\n        correct.append(False)\n\n    return correct\n```\n\n이 함수에는 여러 단계가 포함되어 있고 최종적으로 이름이 시행하는 것 이상을 수행합니다. 이는 이상적이지 않습니다. 각 함수가 직접적으로 작업을 수행하도록 원합니다. 그리고 목적 외 작은 작업은 함수 바깥에 두는 것이 좋습니다. 추출 프로세스는 몇 가지 핵심 단계로 수행됩니다. 첫 번째 단계에서 함수의 다른 부분과 입력 및 출력을 고려합니다. 함수를 그룹화한 후에는 함수를 함수 밖으로 추출하고 함수를 호출합니다. 이 특정 상황에서는 더 다재다능한 질문 요청 함수를 만드는 것이 더 적합할 수 있지만, 이 경우에는 각각의 출력이 알고리즘 과정의 새로운 단계인 것처럼 가정하여 함수의 반환을 생성합니다.\n\n```python\ndef ask_name():\n    print(\"이름은 뭐에요?\")\n    x = input()\n    print(\"안녕 \" + x)\n    return x\n\ndef question1():\n    print(\"2 + 2는 뭐에요?\")\n    answer = input()\n\n    if answer == \"4\":\n        print(\"정답\")\n        return True\n    else:\n        print(\"틀렸어요.\")\n        return False\n\ndef question2():\n    print(\"하늘의 색깔은 뭐에요?\")\n    answer = input()\n\n    if answer == \"파란색\":\n        print(\"정답\")\n        return True\n    else:\n        print(\"틀렸어요.\")\n        return False\n\ndef quiz_user():\n    correct = []\n    ask_name()\n    correct = [f() for f in (question1, question2)]\n    return correct\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n추출 기술을 사용하면 코드를 다양한 방법으로 개선할 수 있어요. 먼저, 함수를 작성할 때 코드를 함수 내에서 재사용할 수 있어요. 특정 알고리즘을 더 큰 함수 내에 그대로 두면, 전혀 다른 목적을 가진 코드 일부에 액세스하지 못할 수도 있어요. 추출 사용의 또 다른 이점은 프로젝트가 훨씬 더 조직적으로 됩니다. 함수는 짧고 중첩을 최소화하는 것이 좋아요. 이 모든 것은 논리적인 코드를 고유한 범위로 가져오는 것이 종종 매우 좋은 선택이 될 수 있다는 것을 말해요.\n\n## del\n\n다음으로 논의하고 싶은 파이썬 기능은 'del' 입니다. 이 키워드는 파이썬 객체를 메모리에서 제거하는 데 사용됩니다. 첫눈에는 그다지 중요하지 않아 보이지만, 파이썬이 사용하기 쉬운 스크립트 언어이기 때문에 가능한 한 자주 사용하고 싶은 파이썬 기능 중 하나에요. 'del'은 종종 간과되지만, 이 기능은 메모리를 보존하는 데 매우 도움이 될 수 있어요.\n\n```js\ndel squared\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n파이썬과 같은 스크립트 언어에서는 메모리 및 성능 관리 방법이 더 제한적이기 때문에 계산 시간을 단축하고 메모리를 해제하는 데 도움이 되는 모든 것을 활용하고 싶을 것입니다. 파이썬은 사실 유사한 스크립트 언어들 중에서 적어도 힙에서 빠르게 무언가를 제거하고 가비지 수집하는 키워드 del 이 하나는 있어서 어느 정도 독특합니다. 그런 관점에서 이 기능을 확실히 활용하고 싶습니다.\n\n## break 와 continue\n\n당신의 도구 상자에 꼭 있어야 할 또 다른 중요한 기술은 break 와 continue 키워드입니다. 이 두 키워드는 모두 반복적 루프 문맥에서 특히 사용되며, 반복적 루프를 더 최적화하기 위해 break 를 사용하여 반복을 중단하거나 continue 를 사용하여 다음 요소로 건너뛸 수 있습니다.\n\n```js\nmydata = [None, None, 55, 22, 33, 44, None, 2, None, 73, 22, None, None, None, 36, \"stop here please\", 23]\nnewvalues = []\nfor x in mydata:\n    if x == None:\n        continue\n    elif x == \"stop here please\":\n        break\n    newvalues.append(x)\n\nnewvalues\n[55, 22, 33, 44, 2, 73, 22, 36]\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n당연히, break와 continue는 각각 자신의 사용 사례를 가지고 있습니다. 예를 들어 조건부로 특정 요소에 대해 작업하고 싶을 때를 생각해보세요. 이러한 키워드에는 끝이 없는 사용 방법이 있으며, 여러분의 도구 상자에 이러한 간단한 도구들을 유지하는 것이 좋은 아이디어입니다.\n\n## else는 최대한 사용을 줄입니다\n\n코드 개선에 관한 참고 사항으로 다룰 마지막 주제는 else를 최대한 줄이는 방법입니다. 저는 큰 else 문과 중첩된 조건문 같은 것은 가능한 한 피해야 한다고 생각합니다. else 자체가 서브루틴이라는 것을 이해해야 합니다. 이것은 우리 함수 아래에서 정의된 완전히 독립적인 스코프의 또 다른 레이어입니다. else 키워드를 사용하는 많은 경우가 있지만, 우리 코드를 더 읽기 쉽고 기능적으로 더 효율적으로 만들기 위해 사용하지 않아야 할 때도 있습니다.\n\n```js\nclass Pump:\n    def __init__(self):\n        self.pumping = False\n\ndef activate_lights():\n    print(\"the lights are on\")\n\nmainvalve = Pump()\n\ndef turn_on_pump():\n    mainvalve.pumping = True\n\ndef turn_switch(has_power: bool):\n    if not has_power:\n        print(\"there is no power\")\n    else:\n        activate_lights()\n        turn_on_pump()\n        if mainvalve.pumping == False:\n            print(\"error turning on the pump\")\n        else:\n            print(\"the pump is on!\")\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 경우에는 기본적으로 else 아래에서 함수를 계속 진행합니다. 이 접근 방식에는 여러 가지 단점이 있습니다. 먼저, 똑같은 것을 두 번 프로그래밍할 가능성이 훨씬 커집니다. 이것은 이 글의 방법론 중 하나인데, 우리는 그렇게 하고 싶지 않습니다.\n\n```js\ndef turn_switch(has_power: bool):\n    if not has_power:\n        print(\"전원이 없습니다\")\n    else:\n        activate_lights()\n        turn_on_pump()\n        if mainvalve.pumping == False:\n            print(\"펌프 작동 오류\")\n        else:\n            print(\"펌프가 작동 중입니다!\")\n```\n\n```js\ndef turn_switch(has_power: bool):\n    if not has_power:\n        print(\"전원이 없습니다\")\n        return\n    activate_lights()\n    turn_on_pump()\n    if mainvalve.pumping == False:\n        print(\"펌프 작동 오류\")\n        return\n        print(\"펌프가 작동 중입니다!\")\n```\n\n또한 위의 경우에는 언젠가는 조건부 내부에서 사용되는 변수를 할당해야 할 것입니다. 이 값은 조건부 외부에서 사용할 계획이 있다면 함수의 비공개 범위에 유지하는 것이 훨씬 더 합리적입니다. 조건부 내부에서 변수를 새로 할당할 수 없으며, 조건부는 루프와 마찬가지로 다른 렉시컬 범위를 추가합니다. 이러한 점을 고려하면 이러한 범위를 설정하고 데이터를 전달하는 데 시간이 걸립니다. 몇몇 부적절한 조건부는 큰 문제를 일으키지 않을 수 있지만, 추가적인 중첩 및 더 많은 호출은 항상 성능을 저하시킵니다. 최적의 성능은 Python과 같은 언어에서 더욱 중요합니다. 우리는 Python의 속도 신뢰성이 C와 같은 것에 비하면 그리 높지 않기 때문에 실제 작성한 대부분의 Python 코드가 가능한 최적화되도록 하려고 할 것입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다른 경우마다 else를 사용해서는 안 된다는 이유에도 불구하고, else에는 사용할 때의 이유가 있습니다. 특히 모든 조건문 뒤에 기본적으로 else를 사용하는 것은 일반적인 코딩 방식이기 때문에, 그 사용 사례가 else를 사용해야 하는 경우에 대해 명확하게 표현해야 합니다. 전자를 사용하는 것이 더 나은 점이 있다고 생각해요.\n\n## 결론\n\n파이썬의 성공은 확실히 사용 편의성과 접근성에 기인합니다. 다른 언어들처럼 단점이 없는 것은 아니지만, 파이썬이 인기를 얻은 이유를 쉽게 이해할 수 있습니다. 특히 파이썬을 주로 사용하는 분야를 고려하면 더 그렇습니다. 파이썬의 접근성 중 하나인 단점은, 사용자가 놓치기 쉬운 기능들이 있어서 입니다. 예를 들어, 누군가는 오랜 시간 동안 파이썬을 사용했지만 pipenv를 어떻게 사용해야 하는지 모르는 경우가 있을 수 있습니다.\n\n다행히도 시간이 흘러가며 프로그래밍 실력은 향상되고 지식이 쌓입니다. 파이썬은 쉽게 시작할 수 있는 언어이지만, 정말 멋진 기능들을 발견하려면 조금의 연구가 필요합니다. 지속적인 발전을 위해 더 나은 코딩 습관을 습득하는 것은 좋은 생각입니다. 우리는 프로그래머로써 항상成長해야 하며, 우리는 삶을 살아가며 배우고 가르치고 원하는 것을 만드는 시간이 제한되어 있습니다. 우리가 원하는 것이 무엇이든 상관없이 배울 시간을 갖도록 하자는 것이 중요합니다. 우수한 프로그래머가 되는 가장 놀라운 점은 극적인 표현력으로 주변 모두에게 이익이 되는 것입니다. 마치 관객들이 모두 팬인 거리 공연자와 같이요. 오늘날 파이썬을 잘 배운 것은 극히 중요하기 때문에, 적어도 나의 프로그래밍에 대한 분명하고 구체적인 조언을 나눌 수 있어서 기쁩니다!\n","ogImage":{"url":"/assets/img/2024-07-09-7SimpleWaysToDrasticallyImproveYourPythonCode_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-7SimpleWaysToDrasticallyImproveYourPythonCode_0.png","tag":["Tech"],"readingTime":15},{"title":"Kafka, Redis, Postgres, Kubernetes를 활용한 실시간 파이프라인 마이크로서비스 프로젝트 방법","description":"","date":"2024-07-09 19:43","slug":"2024-07-09-Real-TimePipelineMicroservicesProjectwithKafkaRedisPostgresandKubernetes","content":"\n# 소개\n\n이 문서는 데이터를 처리하여 분석을 위해 데이터베이스를 채우는 데 사용되는 실시간 마이크로서비스 프로젝트에 대한 안내서입니다.\n\n![이미지](/TIL/assets/img/2024-07-09-Real-TimePipelineMicroservicesProjectwithKafkaRedisPostgresandKubernetes_0.png)\n\n# STG-Service\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Redis 클라이언트\n\nRedis와 상호 작용하기 위한 간단한 클라이언트입니다. 특정 키로 객체를 가져오거나 새 키-값 쌍을 설정할 수 있습니다:\n\n```js\nimport json\nfrom typing import Dict\nimport redis\n\nclass RedisClient:\n    def __init__(self, host: str, port: int, password: str, cert_path: str) -> None:\n        self._client = redis.StrictRedis(\n            host=host,\n            port=port,\n            password=password,\n            ssl=True,\n            ssl_ca_certs=cert_path)\n    def set(self, k, v):\n        self._client.set(k, json.dumps(v))\n    def get(self, k) -> Dict:\n        obj: str = self._client.get(k)\n        return json.loads(obj)\n```\n\n# Postgres 클라이언트\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n데이터베이스에 연결하는 것 외에도, 클라이언트는 하나의 컨텍스트 매니저의 일부로 여러 쿼리를 실행할 수 있는 기능을 제공하여 한 트랜잭션의 실행 명령을 커밋할 필요가 없어요 (나중에 사용 예제를 보게 될 거에요):\n\n```js\nfrom contextlib import contextmanager\nfrom typing import Generator\nimport psycopg2\n\nclass PgConnect:\n    def __init__(self, host: str, port: int, db_name: str, user: str, pw: str, sslmode: str = \"require\") -> None:\n        self.host = host\n        self.port = port\n        self.db_name = db_name\n        self.user = user\n        self.pw = pw\n        self.sslmode = sslmode\n    def url(self) -> str:\n        return \"\"\"\n            host={host}\n            port={port}\n            dbname={db_name}\n            user={user}\n            password={pw}\n            target_session_attrs=read-write\n            sslmode={sslmode}\n        \"\"\".format(\n            host=self.host,\n            port=self.port,\n            db_name=self.db_name,\n            user=self.user,\n            pw=self.pw,\n            sslmode=self.sslmode)\n    @contextmanager\n    def connection(self) -> Generator:\n        keepalive_kwargs = {\n            \"keepalives\": 1,\n            \"keepalives_idle\": 30,\n            \"keepalives_interval\": 5,\n            \"keepalives_count\": 5,\n        }\n        conn = psycopg2.connect(self.url(), **keepalive_kwargs)\n        try:\n            yield conn\n            conn.commit()\n        except Exception as e:\n            conn.rollback()\n            raise e\n        finally:\n            conn.close()\n```\n\n카프카에서 메시지를 생성하고 사용하는 두 개의 별도 및 간단한 클라이언트:\n\n```js\nimport json\nfrom typing import Dict, Optional\nfrom confluent_kafka import Consumer, Producer\n\ndef error_callback(err):\n    print('Something went wrong: {}'.format(err))\n\nclass KafkaProducer:\n    def __init__(self, host: str, port: int, user: str, password: str, topic: str, cert_path: str) -> None:\n        params = {\n            'bootstrap.servers': f'{host}:{port}',\n            'security.protocol': 'SASL_SSL',\n            'ssl.ca.location': cert_path,\n            'sasl.mechanism': 'SCRAM-SHA-512',\n            'sasl.username': user,\n            'sasl.password': password,\n            'error_cb': error_callback,\n        }\n        self.topic = topic\n        self.p = Producer(params)\n    def produce(self, payload: Dict) -> None:\n        self.p.produce(self.topic, json.dumps(payload))\n        self.p.flush(10)\n\nclass KafkaConsumer:\n    def __init__(self,\n                 host: str,\n                 port: int,\n                 user: str,\n                 password: str,\n                 topic: str,\n                 group: str,\n                 cert_path: str\n                 ) -> None:\n        params = {\n            'bootstrap.servers': f'{host}:{port}',\n            'security.protocol': 'SASL_SSL',\n            'ssl.ca.location': cert_path,\n            'sasl.mechanism': 'SCRAM-SHA-512',\n            'sasl.username': user,\n            'sasl.password': password,\n            'group.id': group,  # '',\n            'auto.offset.reset': 'earliest',\n            'enable.auto.commit': False,\n            'error_cb': error_callback,\n            'debug': 'all',\n            'client.id': 'someclientkey'\n        }\n        self.topic = topic\n        self.c = Consumer(params)\n        self.c.subscribe([topic])\n    def consume(self, timeout: float = 3.0) -> Optional[Dict]:\n        msg = self.c.poll(timeout=timeout)\n        if not msg:\n            return None\n        if msg.error():\n            raise Exception(msg.error())\n        val = msg.value().decode()\n        return json.loads(val)\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# STG Schema\n\n포스트그레스의 스테이징 레이어로, Kafka에서 나온 로우 메시지를 저장할 한 개의 테이블이 있을 것입니다. 이는 아래와 같이 정의되어 있습니다.\n\n```js\nCREATE SCHEMA IF NOT EXISTS stg;\n\nCREATE TABLE IF NOT EXISTS stg.order_events (\n    id SERIAL   PRIMARY KEY,\n    object_id   INTEGER NOT NULL UNIQUE,\n    object_type VARCHAR(20) NOT NULL,\n    sent_dttm   TIMESTAMP NOT NULL,\n    payload     JSON NOT NULL\n);\n```\n\n그리고 이를 실행할 파이썬 함수는:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nfrom lib.pg.pg_connect import PgConnect\n\ndef make_stg_migrations(db: PgConnect) -> None:\n    with db.connection() as conn:\n        with conn.cursor() as cur:\n            # STG 스키마의 SQL 정의 경로는 다를 수 있습니다.\n            cur.execute(open(\"stg_schema.sql\", \"r\").read())\n```\n\n프로그램이 `db.connection()`으로 정의된 컨텍스트 매니저를 벗어나면 명시적으로 실행할 필요 없이 자동으로 커밋됩니다.\n\n# 메시지 처리\n\n먼저 소비된 카프카 메시지를 STG 포스트그레스 테이블(StgRepository)에 삽입한 다음, 레디스에서 레스토랑 데이터를 풍부하게하여 다른 카프카 클러스터(StgMessageProcessor)를 위한 출력 메시지를 구성해야 합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nfrom datetime import datetime\n\nfrom lib.pg.pg_connect import PgConnect\n\nclass StgRepository:\n    def __init__(self, db: PgConnect) -> None:\n        self._db = db\n\n    def order_events_insert(self,\n                            object_id: int,\n                            object_type: str,\n                            sent_dttm: datetime,\n                            payload: str\n                            ) -> None:\n        with self._db.connection() as conn:\n            with conn.cursor() as cur:\n                cur.execute(\n                    \"\"\"\n                        INSERT INTO stg.order_events (object_id, object_type, sent_dttm, payload) VALUES (%(object_id)s, %(object_type)s, %(sent_dttm)s, %(payload)s)\n                        ON CONFLICT (object_id)\n                        DO UPDATE\n                        SET object_type = EXCLUDED.object_type,\n                            sent_dttm = EXCLUDED.sent_dttm,\n                            payload = EXCLUDED.payload;\n                    \"\"\",\n                    {\n                        'object_id': object_id,\n                        'object_type': object_type,\n                        'sent_dttm': sent_dttm,\n                        'payload': payload\n                    }\n                )\n```\n\n```python\nimport json\nfrom datetime import datetime\nfrom logging import Logger\nfrom lib.kafka_connect.kafka_connectors import KafkaConsumer, KafkaProducer\nfrom lib.redis.redis_client import RedisClient\nfrom stg_loader.repository.stg_repository import StgRepository\n\nclass StgMessageProcessor:\n    def __init__(self,\n                 consumer: KafkaConsumer,\n                 producer: KafkaProducer,\n                 redis: RedisClient,\n                 stg_repository: StgRepository,\n                 logger: Logger) -> None:\n        self._logger = logger\n        self._consumer = consumer\n        self._producer = producer\n        self._redis = redis\n        self._stg_repository = stg_repository\n        self._batch_size = 100\n\n    def run(self) -> None:\n        self._logger.info(f\"{datetime.utcnow()}: START\")\n        for i in range(self._batch_size):\n            msg = self._consumer.consume()\n            if not msg:\n                continue\n            self._stg_repository.order_events_insert(object_id=msg[\"object_id\"],\n                                                     object_type=msg[\"object_type\"],\n                                                     sent_dttm=msg[\"sent_dttm\"],\n                                                     payload=json.dumps(msg[\"payload\"]))\n            dst_msg = self._construct_output_message(msg)\n            self._producer.produce(dst_msg)\n        self._logger.info(f\"{datetime.utcnow()}: FINISH\")\n\n    def _construct_output_message(self, original_message: dict) -> dict:\n        restaurant_id = original_message[\"payload\"][\"restaurant\"][\"id\"]\n        restaurant_data = self._redis.get(restaurant_id)\n        restaurant_name = restaurant_data[\"name\"]\n        user_id = original_message[\"payload\"][\"user\"][\"id\"]\n        user_data = self._redis.get(user_id)\n        user_name = user_data[\"name\"]\n        user_login = user_data[\"login\"]\n        order = original_message[\"payload\"]\n        restaurant_menu = {p[\"_id\"]: p for p in restaurant_data[\"menu\"]}\n        products = {p[\"id\"]: {**p, \"category\": restaurant_menu[p[\"id\"]][\"category\"]} for p in order[\"order_items\"]}\n        return {\n                \"object_id\": original_message[\"object_id\"],\n                \"object_type\": original_message[\"object_type\"],\n                \"payload\": {\n                    \"id\": original_message[\"object_id\"],\n                    \"date\": order[\"date\"],\n                    \"cost\": order[\"cost\"],\n                    \"payment\": order[\"payment\"],\n                    \"status\": order[\"final_status\"],\n                    \"restaurant\": {\n                        \"restaurant_id\": restaurant_id,\n                        \"restaurant_name\": restaurant_name\n                    },\n                    \"user\": {\n                        \"user_id\": user_id,\n                        \"user_name\": user_name,\n                        \"user_login\": user_login\n                    },\n                    \"products\": products\n                }\n            }\n```\n\n# 서비스 구성\n\n소스/싱크 카프카, Redis 및 포스트그레스와 연결해야 합니다. 이것은 확실히 많은 구성이 필요하며 환경 변수를 사용해야 하므로 이를 별도의 클래스에서 수용할 것입니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nimport os\n\nfrom lib.kafka_connect.kafka_connectors import KafkaConsumer, KafkaProducer\nfrom lib.redis.redis_client import RedisClient\nfrom lib.pg.pg_connect import PgConnect\nfrom stg_loader.repository.stg_repository import StgRepository\n\nclass AppConfig:\n    CERTIFICATE_PATH = '/crt/YandexInternalRootCA.crt'\n    DEFAULT_JOB_INTERVAL = 25\n    def __init__(self) -> None:\n        self.kafka_host = str(os.getenv('KAFKA_HOST') or \"\")\n        self.kafka_port = int(str(os.getenv('KAFKA_PORT')) or 0)\n        self.kafka_consumer_username = str(os.getenv('KAFKA_CONSUMER_USERNAME') or \"\")\n        self.kafka_consumer_password = str(os.getenv('KAFKA_CONSUMER_PASSWORD') or \"\")\n        self.kafka_consumer_group = str(os.getenv('KAFKA_CONSUMER_GROUP') or \"\")\n        self.kafka_consumer_topic = str(os.getenv('KAFKA_SOURCE_TOPIC') or \"\")\n        self.kafka_producer_username = str(os.getenv('KAFKA_PRODUCER_USERNAME') or \"\")\n        self.kafka_producer_password = str(os.getenv('KAFKA_PRODUCER_PASSWORD') or \"\")\n        self.kafka_producer_topic = str(os.getenv('KAFKA_DESTINATION_TOPIC') or \"\")\n        self.redis_host = str(os.getenv('REDIS_HOST') or \"\")\n        self.redis_port = int(str(os.getenv('REDIS_PORT')) or 0)\n        self.redis_password = str(os.getenv('REDIS_PASSWORD') or \"\")\n        self.pg_host = str(os.getenv('PG_HOST') or \"\")\n        self.pg_port = int(str(os.getenv('PG_PORT')) or 6432)\n        self.pg_db_name = str(os.getenv(\"PG_DB_NAME\"))\n        self.pg_user = str(os.getenv('PG_USER') or \"\")\n        self.pg_password = str(os.getenv('PG_PASSWORD') or \"\")\n    def kafka_producer(self):\n        return KafkaProducer(\n            self.kafka_host,\n            self.kafka_port,\n            self.kafka_producer_username,\n            self.kafka_producer_password,\n            self.kafka_producer_topic,\n            self.CERTIFICATE_PATH\n        )\n    def kafka_consumer(self):\n        return KafkaConsumer(\n            self.kafka_host,\n            self.kafka_port,\n            self.kafka_consumer_username,\n            self.kafka_consumer_password,\n            self.kafka_consumer_topic,\n            self.kafka_consumer_group,\n            self.CERTIFICATE_PATH\n        )\n    def redis_client(self) -> RedisClient:\n        return RedisClient(\n            self.redis_host,\n            self.redis_port,\n            self.redis_password,\n            self.CERTIFICATE_PATH\n        )\n    def stg_loader(self) -> StgRepository:\n        db: PgConnect = PgConnect(\n            self.pg_host,\n            self.pg_port,\n            self.pg_db_name,\n            self.pg_user,\n            self.pg_password\n        )\n        return StgRepository(db)\n    def pg_client(self) -> PgConnect:\n        return PgConnect(\n            self.pg_host,\n            self.pg_port,\n            self.pg_db_name,\n            self.pg_user,\n            self.pg_password\n        )\n```\n\n# STG Service 실행\n\nStgMessageProcessor를 백그라운드 프로세스로 실행해야 합니다 (apscheduler 파이썬 모듈의 BackgroundScheduler를 사용할 것입니다) 그리고 건강 상태를 확인하는 간단한 API를 추가할 것입니다.\n\n```python\nimport os\nimport logging\n\nfrom apscheduler.schedulers.background import BackgroundScheduler\nfrom flask import Flask\nfrom stg_loader.stg_message_processor_job import StgMessageProcessor\nfrom app_config import AppConfig\nfrom stg_migrations import make_stg_migrations\n\napp = Flask(__name__)\n\n# 서비스가 정상인지 확인할 수 있는 엔드포인트 생성\n@app.get('/health')\ndef health():\n    return 'healthy'\n\nif __name__ == '__main__':\n    app.logger.setLevel(logging.DEBUG)\n    config = AppConfig()\n    make_stg_migrations(config.pg_client())\n    proc = StgMessageProcessor(logger=app.logger,\n                               consumer=config.kafka_consumer(),\n                               producer=config.kafka_producer(),\n                               redis=config.redis_client(),\n                               stg_repository=config.stg_loader())\n    scheduler = BackgroundScheduler()\n    scheduler.add_job(func=proc.run, trigger=\"interval\", seconds=config.DEFAULT_JOB_INTERVAL)\n    scheduler.start()\n    app.run(debug=True, host='0.0.0.0', use_reloader=False)\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Dockerfile\n\n스테이징 서비스는 코어크레이트 기반의 쿠버네티스 클러스터에서 실행될 예정이므로, 서비스를 도커 이미지로 만들어주어야 합니다.\n\n```js\nFROM python:3.10\n\nRUN apt-get update -y\n# 컨테이너 내에서 confluent_kafka 파이썬 모듈이 작동되도록 필요합니다\nRUN git clone https://github.com/edenhill/librdkafka && cd librdkafka && ./configure && make && make install && ldconfig\nCOPY . .\nRUN pip install -r requirements.txt\n# Kafka 클러스터에 안전한 연결을 위한 인증서 다운로드\nRUN mkdir -p /crt\nRUN wget \"https://storage.yandexcloud.net/cloud-certs/CA.pem\" --output-document /crt/YandexInternalRootCA.crt\nRUN chmod 0600 /crt/YandexInternalRootCA.crt\nWORKDIR /src\n# 파이썬 임포트가 작동되도록 설정\nENV PYTHONPATH \"${PYTHONPATH}:/src\"\nENTRYPOINT [\"python\"]\nCMD [\"app.py\"]\n```\n\n로컬 테스트를 위해 Docker Compose를 사용하여 스테이징 서비스를 실행할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```yaml\nversion: \"3.9\"\n\nservices:\n  stg_service:\n    build:\n      context: .\n      network: host\n    image: stg_img:local\n    container_name: stg_container\n    environment:\n      FLASK_APP: ${STG_SERVICE_APP_NAME:-stg_service}\n      DEBUG: ${STG_SERVICE_DEBUG:-True}\n      KAFKA_HOST: ${KAFKA_HOST}\n      KAFKA_PORT: ${KAFKA_PORT}\n      KAFKA_CONSUMER_USERNAME: ${KAFKA_CONSUMER_USERNAME}\n      KAFKA_CONSUMER_PASSWORD: ${KAFKA_CONSUMER_PASSWORD}\n      KAFKA_CONSUMER_GROUP: ${KAFKA_CONSUMER_GROUP}\n      KAFKA_SOURCE_TOPIC: ${KAFKA_SOURCE_TOPIC}\n      KAFKA_DESTINATION_TOPIC: ${KAFKA_DESTINATION_TOPIC}\n      KAFKA_PRODUCER_USERNAME: ${KAFKA_PRODUCER_USERNAME}\n      KAFKA_PRODUCER_PASSWORD: ${KAFKA_PRODUCER_PASSWORD}\n      REDIS_HOST: ${REDIS_HOST}\n      REDIS_PORT: ${REDIS_PORT}\n      REDIS_PASSWORD: ${REDIS_PASSWORD}\n      PG_HOST: ${PG_HOST}\n      PG_PORT: ${PG_PORT}\n      PG_DB_NAME: ${PG_DB_NAME}\n      PG_USER: ${PG_USER}\n      PG_PASSWORD: ${PG_PASSWORD}\n    network_mode: \"bridge\"\n    ports:\n      - \"5101:5000\"\n    restart: unless-stopped\n```\n\n.env 파일:\n\n```yaml\nKAFKA_HOST=******.mdb.yandexcloud.net\nKAFKA_PORT=9091\nKAFKA_CONSUMER_USERNAME=producer_consumer\nKAFKA_CONSUMER_PASSWORD=******\nKAFKA_CONSUMER_GROUP=test-consumer1\nKAFKA_SOURCE_TOPIC=dds_input_topic\nKAFKA_PRODUCER_USERNAME=producer_consumer\nKAFKA_PRODUCER_PASSWORD=*******\nKAFKA_DESTINATION_TOPIC=cdm_input_topic\n\nREDIS_HOST=******.mdb.yandexcloud.net\nREDIS_PORT=6380\nREDIS_PASSWORD=******\nPG_HOST=********.mdb.yandexcloud.net\nPG_PORT=6432\nPG_DB_NAME=sprint9dwh\nPG_USER=yandex_pg\nPG_PASSWORD=**********\n```\n\n# HELM 차트\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nChart.yaml 파일:\n\n```yaml\napiVersion: v2\nname: first-service\ndescription: 쿠버네티스용 헬름 차트\n\n# 차트는 'application' 또는 'library' 차트 중 하나일 수 있습니다.\n#\n# Application 차트는 템플릿 모음이며 버전이 지정된 아카이브로 패키지화하여 배포될 수 있습니다.\n#\n# Library 차트는 차트 개발자를 위한 유용한 유틸리티 또는 함수를 제공합니다. Application 차트의 종속성으로 포함되어\n# 렌더링 파이프라인에 이러한 유틸리티와 함수를 삽입합니다. Library 차트는 템플릿을 정의하지 않으며 따라서 배포될 수 없습니다.\ntype: application\n# 이것은 차트 버전입니다. 이 번호는 차트 및 해당 템플릿에 변경이 있을 때마다 증가해야 합니다.\n# 버전은 Semantic Versioning (https://semver.org/)을 따르는 것으로 예상됩니다.\nversion: 0.1.0\n# 이것은 배포되는 애플리케이션의 버전 번호입니다. 이 버전 번호는 애플리케이션에 변경이 있는 경우마다 증가해야 합니다.\n# 버전은 Semantic Versioning을 따르지 않습니다. 애플리케이션이 사용 중인 버전을 반영해야 합니다.\n# 따옴표와 함께 사용하는 것이 권장됩니다.\nappVersion: \"1.16.0\"\n```\n\nvalues.yaml 파일:\n\n```yaml\n# 앱의 기본 값.\n# 이것은 YAML 형식의 파일입니다.\n# 템플릿에 전달할 변수를 선언합니다.\n\nreplicaCount: 3\nimage:\n  # 컨테이너 레지스트리에 대한 링크. 야н덱스 클라우드에서 실행할 것입니다.\n  repository: cr.yandex/crpr6naar69761ehm0bp/stg_service\n  pullPolicy: IfNotPresent\n  # 기본적으로 차트 appVersion인 이미지 태그를 덮어씁니다.\n  tag: \"v2022-12-13-r1\"\ncontainerPort: 5000\nconfig:\n  KAFKA_HOST: rc1a-hins1kp5qsfnsob3.mdb.yandexcloud.net\n  KAFKA_PORT: \"9091\"\n  KAFKA_CONSUMER_USERNAME: producer_consumer\n  KAFKA_CONSUMER_PASSWORD: \"*****\"\n  KAFKA_CONSUMER_GROUP: test-consumer1\n  KAFKA_SOURCE_TOPIC: order-service_orders\n  KAFKA_PRODUCER_USERNAME: producer_consumer\n  KAFKA_PRODUCER_PASSWORD: \"*****\"\n  KAFKA_DESTINATION_TOPIC: dds_topic_name\n  REDIS_HOST: c-c9qeltiiu2rkcr6v9net.rw.mdb.yandexcloud.net\n  REDIS_PORT: \"6380\"\n  REDIS_PASSWORD: \"*****\"\n  PG_HOST: rc1b-4olk4uzgdrdte114.mdb.yandexcloud.net\n  PG_PORT: \"6432\"\n  PG_DB_NAME: sprint9dwh\n  PG_USER: yandex_pg\n  PG_PASSWORD: \"*****\"\nimagePullSecrets: []\nnameOverride: \"\"\nfullnameOverride: \"\"\npodAnnotations: {}\nresources:\n  # 일반적으로 기본 리소스를 지정하지 않고 사용자의 명시적인 선택으로 유지하는 것을 권장합니다.\n  # 이것은 Minikube와 같은 리소스가 적은 환경에서 차트 실행 기회를 높이기도 합니다.\n  # 리소스를 지정하려면 아래 줄 주석 처리를 해제하고 필요에 맞게 조정한 다음 'resources:' 뒤의 중괄호를 제거하세요.\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```yaml\ntemplates/configmap.yaml은 우리 서비스의 구성을 저장하는 k8s 엔터티입니다. values.yaml 파일의 config 블록에서 모든 키-값 쌍을 가져올 것입니다:\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"app.fullname\" . }}-config\n  labels:\n    {{- include \"app.labels\" . | nindent 4 }}\n{{- with .Values.config }}\ndata:\n  {{- toYaml . | nindent 2 }}\n{{- end }}\n\ntemplates/deployment.yaml 파일\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"app.fullname\" . }}\n  labels:\n    {{- include \"app.labels\" . | nindent 4 }}\nspec:\n  replicas: {{ .Values.replicaCount }}\n  selector:\n    matchLabels:\n      {{- include \"app.selectorLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      {{- with .Values.podAnnotations }}\n      annotations:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      labels:\n        {{- include \"app.selectorLabels\" . | nindent 8 }}\n    spec:\n      {{- with .Values.imagePullSecrets }}\n      imagePullSecrets:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      containers:\n        - name: {{ .Chart.Name }}\n          securityContext:\n            {{- toYaml .Values.securityContext | nindent 12 }}\n          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n          imagePullPolicy: {{ .Values.image.pullPolicy }}\n          envFrom:\n            - configMapRef:\n                name: {{ include \"app.fullname\" . }}-config\n          ports:\n            - name: http\n              containerPort: {{ .Values.containerPort }}\n              protocol: TCP\n          resources:\n            {{- toYaml .Values.resources | nindent 12 }}\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nSTG-Service의 소스 코드는 여기에서 찾을 수 있어요.\n\n# DDS-Service\n\nSTG-Service 이후의 모든 것은 실제로 매우 쉬워집니다. 다른 서비스들도 거의 동일한 구조를 사용하기 때문이죠. DDS-Service의 경우, 모든 클라이언트 정의가 동일합니다. Dockerfile, Docker Compose 및 HELM Chart는 거의 동일합니다. 여기에서 소스 코드를 확인할 수 있어요.\n\n가장 큰 차이점은 데이터 모델링 방식에 있습니다 (Data Vault 2.0을 사용합니다):\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nCREATE SCHEMA IF NOT EXISTS dds;\n\n-- 데이터 보트 --\n-- 허브 --\nCREATE TABLE IF NOT EXISTS dds.h_user (\n    h_user_pk UUID PRIMARY KEY,\n    user_id   VARCHAR NOT NULL UNIQUE,\n    load_dt   TIMESTAMP NOT NULL,\n    load_src  VARCHAR NOT NULL\n);\nCREATE TABLE IF NOT EXISTS dds.h_product (\n    h_product_pk UUID PRIMARY KEY,\n    product_id   VARCHAR NOT NULL UNIQUE,\n    load_dt      TIMESTAMP NOT NULL,\n    load_src     VARCHAR NOT NULL\n);\nCREATE TABLE IF NOT EXISTS dds.h_category (\n    h_category_pk UUID PRIMARY KEY,\n    category_name VARCHAR NOT NULL UNIQUE,\n    load_dt       TIMESTAMP NOT NULL,\n    load_src      VARCHAR NOT NULL\n);\nCREATE TABLE IF NOT EXISTS dds.h_restaurant (\n    h_restaurant_pk UUID PRIMARY KEY,\n    restaurant_id   VARCHAR NOT NULL UNIQUE,\n    load_dt         TIMESTAMP NOT NULL,\n    load_src        VARCHAR NOT NULL\n);\nCREATE TABLE IF NOT EXISTS dds.h_order (\n    h_order_pk UUID PRIMARY KEY,\n    order_id   INTEGER NOT NULL UNIQUE,\n    order_dt   TIMESTAMP NOT NULL,\n    load_dt    TIMESTAMP NOT NULL,\n    load_src   VARCHAR NOT NULL\n);\n-- 훗들 --\nCREATE TABLE IF NOT EXISTS dds.s_user_names (\n    hk_user_names_pk UUID PRIMARY KEY,\n    h_user_pk        UUID NOT NULL UNIQUE REFERENCES dds.h_user (h_user_pk),\n    username         VARCHAR NOT NULL,\n    userlogin        VARCHAR NOT NULL,\n    load_src         VARCHAR NOT NULL,\n    load_dt          TIMESTAMP NOT NULL\n);\nCREATE TABLE IF NOT EXISTS dds.s_product_names (\n    hk_product_names_pk UUID PRIMARY KEY,\n    h_product_pk        UUID NOT NULL UNIQUE REFERENCES dds.h_product (h_product_pk),\n    name                VARCHAR NOT NULL,\n    load_src            VARCHAR NOT NULL,\n    load_dt             TIMESTAMP NOT NULL\n);\nCREATE TABLE IF NOT EXISTS dds.s_restaurant_names (\n    hk_restaurant_names_pk UUID PRIMARY KEY,\n    h_restaurant_pk        UUID NOT NULL UNIQUE REFERENCES dds.h_restaurant (h_restaurant_pk),\n    name                   VARCHAR NOT NULL,\n    load_src               VARCHAR NOT NULL,\n    load_dt                TIMESTAMP NOT NULL\n);\nCREATE TABLE IF NOT EXISTS dds.s_order_cost (\n    hk_order_cost_pk UUID PRIMARY KEY,\n    h_order_pk       UUID NOT NULL UNIQUE REFERENCES dds.h_order (h_order_pk),\n    cost             DECIMAL(19, 5) NOT NULL,\n    payment          DECIMAL(19, 5) NOT NULL,\n    load_src         VARCHAR NOT NULL,\n    load_dt          TIMESTAMP NOT NULL\n);\nCREATE TABLE IF NOT EXISTS dds.s_order_status (\n    hk_order_status_pk UUID PRIMARY KEY,\n    h_order_pk         UUID NOT NULL UNIQUE REFERENCES dds.h_order (h_order_pk),\n    status             VARCHAR NOT NULL,\n    load_src           VARCHAR NOT NULL,\n    load_dt            TIMESTAMP NOT NULL\n);\n-- 링크 --\nCREATE TABLE IF NOT EXISTS dds.l_order_product (\n    hk_order_product_pk UUID PRIMARY KEY,\n    h_order_pk          UUID NOT NULL REFERENCES dds.h_order (h_order_pk),\n    h_product_pk        UUID NOT NULL REFERENCES dds.h_product (h_product_pk),\n    load_src            VARCHAR NOT NULL,\n    load_dt             TIMESTAMP NOT NULL\n);\nCREATE TABLE IF NOT EXISTS dds.l_product_restaurant (\n    hk_product_restaurant_pk UUID PRIMARY KEY,\n    h_restaurant_pk          UUID NOT NULL REFERENCES dds.h_restaurant (h_restaurant_pk),\n    h_product_pk             UUID NOT NULL REFERENCES dds.h_product (h_product_pk),\n    load_src                 VARCHAR NOT NULL,\n    load_dt                  TIMESTAMP NOT NULL\n);\nCREATE TABLE IF NOT EXISTS dds.l_product_category (\n    hk_product_category_pk UUID PRIMARY KEY,\n    h_category_pk          UUID NOT NULL REFERENCES dds.h_category (h_category_pk),\n    h_product_pk           UUID NOT NULL REFERENCES dds.h_product (h_product_pk),\n    load_src               VARCHAR NOT NULL,\n    load_dt                TIMESTAMP NOT NULL\n);\nCREATE TABLE IF NOT EXISTS dds.l_order_user (\n    hk_order_user_pk UUID PRIMARY KEY,\n    h_order_pk       UUID NOT NULL REFERENCES dds.h_order (h_order_pk),\n    h_user_pk        UUID NOT NULL REFERENCES dds.h_user (h_user_pk),\n    load_src         VARCHAR NOT NULL,\n    load_dt          TIMESTAMP NOT NULL\n);\n\n우리 DDS-Service의 소스 Kafka 메시지는 STG-Service의 출력 메시지입니다. 또 다른 차이점은 Postgres에 이러한 메시지를 저장하는 방식에 있습니다. 데이터 보트 모델을 사용하므로 약간 복잡해집니다:\n\nimport os\nimport uuid\nfrom datetime import datetime\n\nfrom lib.pg.pg_connect import PgConnect\nfrom psycopg2.extras import execute_batch\n\nclass DdsRepository:\n    def __init__(self, db: PgConnect) -> None:\n        self._db = db\n        self._batch_size = 20\n    def h_user_insert(self, user_id: str) -> None:\n        with self._db.connection() as conn:\n            with conn.cursor() as cur:\n                cur.execute(\"\"\"\n                    INSERT INTO dds.h_user (h_user_pk, user_id, load_dt, load_src) VALUES (%(h_user_pk)s, %(user_id)s, %(load_dt)s, %(load_src)s)\n                    ON CONFLICT (user_id)\n                    DO NOTHING;\n                \"\"\",\n                {\n                    \"h_user_pk\": str(uuid.uuid4()),\n                    \"user_id\": user_id,\n                    \"load_dt\": datetime.now(),\n                    \"load_src\": str(os.getenv('KAFKA_SOURCE_TOPIC') or \"\")\n                })\n    def s_user_names_insert(self, user_id: str, username: str, userlogin: str) -> None:\n        with self._db.connection() as conn:\n            with conn.cursor() as cur:\n                cur.execute(f\"SELECT h_user_pk FROM dds.h_user WHERE user_id = '{user_id}'\")\n                h_user_pk = cur.fetchone()[0]\n                cur.execute(\"\"\"\n                    INSERT INTO dds.s_user_names (hk_user_names_pk, h_user_pk, username, userlogin, load_dt, load_src) VALUES (%(hk_user_names_pk)s, %(h_user_pk)s, %(username)s, %(userlogin)s, %(load_dt)s, %(load_src)s)\n                    ON CONFLICT (h_user_pk)\n                    DO NOTHING;\n                \"\"\",\n                {\n                    \"hk_user_names_pk\": str(uuid.uuid4()),\n                    \"h_user_pk\": h_user_pk,\n                    \"username\": username,\n                    \"userlogin\": userlogin,\n                    \"load_dt\": datetime.now(),\n                    \"load_src\": str(os.getenv('KAFKA_SOURCE_TOPIC') or \"\")\n                })\n    def h_order_insert(self, order_id: str, order_dt: datetime) -> None:\n        with self._db.connection() as conn:\n            with conn.cursor() as cur:\n                cur.execute(\"\"\"\n                    INSERT INTO dds.h_order (h_order_pk, order_id, order_dt, load_dt, load_src) VALUES (%(h_order_pk)s, %(order_id)s, %(order_dt)s, %(load_dt)s, %(load_src)s)\n                    ON CONFLICT (order_id)\n                    DO NOTHING;\n                \"\"\",\n                {\n                    \"h_order_pk\": str(uuid.uuid4()),\n                    \"order_id\": order_id,\n                    \"order_dt\": order_dt,\n                    \"load_dt\": datetime.now(),\n                    \"load_src\": str(os.getenv('KAFKA_SOURCE_TOPIC') or \"\")\n                })\n    def s_order_cost_insert(self, order_id: str, cost: float, payment: float) -> None:\n        with self._db.connection() as conn:\n            with conn.cursor() as cur:\n                cur.execute(f\"SELECT h_order_pk FROM dds.h_order WHERE order_id = '{order_id}'\")\n                h_order_pk = cur.fetchone()[0]\n                cur.execute(\"\"\"\n                    INSERT INTO dds.s_order_cost (hk_order_cost_pk, h_order_pk, cost, payment, load_dt, load_src) VALUES (%(hk_order_cost_pk)s, %(h_order_pk)s, %(cost)s, %(payment)s, %(load_dt)s, %(load_src)s)\n                    ON CONFLICT (h_order_pk)\n                    DO UPDATE SET cost = EXCLUDED.cost,\n                                  payment = EXCLUDED.payment;\n                \"\"\",\n                {\n                    \"hk_order_cost_pk\": str(uuid.uuid4()),\n                    \"h_order_pk\": h_order_pk,\n                    \"cost\": cost,\n                    \"payment\": payment,\n                    \"load_dt\": datetime.now(),\n                    \"load_src\": str(os.getenv('KAFKA_SOURCE_TOPIC') or \"\")\n                })\n    def s_order_status_insert(self, order_id: str, status: str) -> None:\n        with self._db.connection() as conn:\n            with conn.cursor() as cur:\n                cur.execute(f\"SELECT h_order_pk FROM dds.h_order WHERE order_id = '{order_id}'\")\n                h_order_pk = cur.fetchone()[0]\n                cur.execute(\"\"\"\n                    INSERT INTO dds.s_order_status (hk_order_status_pk, h_order_pk, status, load_dt, load_src) VALUES (%(hk_order_status_pk)s, %(h_order_pk)s, %(status)s, %(load_dt)s, %(load_src)s)\n                    ON CONFLICT (h_order_pk)\n                    DO UPDATE SET status=EXCLUDED.status;\n                \"\"\",\n                {\n                    \"hk_order_status_pk\": str(uuid.uuid4()),\n                    \"h_order_pk\": h_order_pk,\n                    \"status\": status,\n                    \"load_dt\": datetime.now(),\n                    \"load_src\": str(os.getenv('KAFKA_SOURCE_TOPIC') or \"\")\n                })\n    def l_order_user_insert(self, user_id: str, order_id:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리는 수신 메시지를 HUB, SATELLITE 및 LINK로 분할합니다 - 이는 우리 데이터 모델의 주요 개체입니다. 또한, 마지막 단계로, 메시지를 출력 Kafka 클러스터로 준비하여 다음 다운스트림 서비스에 전달합니다.\n\napp.py 파일은 기본적으로 동일합니다: 서비스를 백그라운드 작업으로 실행하고 서비스의 상태를 확인하기 위한 간단한 API를 생성합니다.\n\n# CDM-Service\n\n다른 서비스들과 유사하게 작동하며 여기에서 확인할 수 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 스택아데믹 🎓\n\n끝까지 읽어주셔서 감사합니다. 떠나시기 전에:\n\n- 작가를 클랩하고 팔로우해주시면 감사하겠습니다! 👏\n- 저희를 팔로우해주세요: X | LinkedIn | YouTube | Discord\n- 저희 다른 플랫폼도 방문해주세요: In Plain English | CoFeed | Differ\n- 스택아데믹닷컴에서 더 많은 콘텐츠를 만나보세요\n```\n","ogImage":{"url":"/assets/img/2024-07-09-Real-TimePipelineMicroservicesProjectwithKafkaRedisPostgresandKubernetes_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-Real-TimePipelineMicroservicesProjectwithKafkaRedisPostgresandKubernetes_0.png","tag":["Tech"],"readingTime":34},{"title":"시계열 확률 예측을 위한 분위 회귀 방법 ","description":"","date":"2024-07-09 19:41","slug":"2024-07-09-QuantileRegressionforTimeSeriesProbabilisticForecasting","content":"\n![Quantile Regression for Time Series Probabilistic Forecasting](/TIL/assets/img/2024-07-09-QuantileRegressionforTimeSeriesProbabilisticForecasting_0.png)\n\n가끔 우리는 유일한 확실한 것은 불확실함이라는 것을 듣습니다. 우리는 불확실성을 좋아하지 않습니다. \"내일 날씨는 50% 폭염, 50% 허리케인일 것이다\"라는 말을 듣고 싶지 않습니다. 그러나 반대로 불확실성을 양적으로 나타내기 위해 가능한 예측 범위를 요청하기도 합니다. 불확실성 속에서도 확실성을 원하는 건데요. 미래를 위해 계획을 세우는 데 도움을 주기 위해서입니다. 조직의 재정 계획을 수행 중이라고 상상해 봅시다. 두 가지 중 어느 것이 더 나을까요:\n\n- 예상 평균 재정 손실은 4000만 달러이거나\n- 재정 손실이 약 1000만 달러부터 7000만 달러 사이일 것으로 95% 확신하며, 약 4000만 달러 정도일 것입니다. 더불어, 재정 손실이 약 3000만 달러부터 5000만 달러 사이일 것으로 68% 확신합니다.\n\n분위 회귀(Quantile regression)는 이 요구를 충족시킵니다. 분위 회귀는 예측 구간을 제공하며 표시된 것처럼 가능성을 양적으로 나타냅니다. 분위 회귀는 예측 변수와 반응 변수 간의 관계를 모델링하기 위해 사용되는 통계 기법으로, 반응 변수의 조건부 분포가 관심 대상인 경우에 특히 유용합니다. 예측 변수가 주어졌을 때 반응 변수의 조건부 평균을 추정하는 전통적인 회귀 방법(예: 최소 제곱 회귀)과 달리 분위 회귀는 반응 변수의 조건부 분위수를 추정합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Quantile Regression for Time Series Probabilistic Forecasting](/TIL/assets/img/2024-07-09-QuantileRegressionforTimeSeriesProbabilisticForecasting_1.png)\n\n\"Monte Carlo Simulation for Time Series Probabilistic Forecasts\"에서 몬테카를로 시뮬레이션 기술을 배웠습니다. 양분위 회귀(Quantile regression)가 몬테카를로 시뮬레이션보다 어떤 장점을 가지고 있는지 알아보았습니다. 첫째로, 양분위 회귀는 예측 변수가 주어졌을 때 종속 변수의 조건부 양분위를 직접 추정합니다. 이는 몬테카를로 시뮬레이션에서와 같이 많은 가능한 결과를 생성하는 대신에 양분위 회귀는 종속 변수의 분포의 특정 양분위를 추정해줍니다. 이는 중앙값, 사분위 또는 극단적 양분위에서 예측 불확실성을 이해하는 데 특히 유용할 수 있습니다. 둘째로, 양분위 회귀는 예측 불확실성을 추정하기 위한 모델 기반 접근 방식을 제공합니다. 이는 관찰 데이터를 이용하여 변수 간의 관계를 추정하고, 이 관계를 기반으로 예측을 수행합니다. 반면에 몬테카를로 시뮬레이션은 입력 변수에 대한 확률 분포를 지정하고 무작위 샘플링을 통해 결과를 생성하는 데 의존합니다.\n\nNeuralProphet를 사용하여 시계열 모델을 구축하는 방법을 다음 장(chapter)에서 배웠습니다:\n\n- Chapter 3: Tutorial I: Trend + seasonality + holidays & events\n- Chapter 4: Tutorial II: Trend + seasonality + holidays & events + auto-regressive (AR) + lagged regressors + future regressors\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nNeural Prophet은 두 가지 통계 기법을 제공합니다: (1) 분위수 회귀 및 (2) 일치 분위수 예측입니다. 일치 분위수 예측 기법은 회귀를 측정하기 위한 보정 프로세스를 추가합니다. 이 장에서는 Neural Prophet의 분위수 회귀 모듈을 실행할 것입니다. 다음 장에서 일치 분위수 예측을 배울 예정입니다. Python 노트북은 이 Github 링크를 통해 다운로드할 수 있습니다.\n\n소프트웨어 요구 사항\n\nNeuralProphet를 설치하기 위해 표준 설치 방법인 pip install NeuralProphet를 따를 것입니다.\n\n```js\n!pip install neuralprophet\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n만약 Google Colab을 사용하는 경우, NeuralProphet은 numpy 1.23.5를 사용하지 않으면 작동하지 않는다는 점 유의해 주세요. numpy를 제거하고 numpy 1.23.5를 설치해야 합니다.\n\n```js\n# neuralprophet은 numpy1.23.5를 사용하지 않으면 Colab에서 작동하지 않습니다: https://github.com/googlecolab/colabtools/issues/3752\n!pip uninstall numpy\n!pip install git+https://github.com/ourownstory/neural_prophet.git numpy==1.23.5\n```\n\n데이터를 로드하고, 몇 가지 도구를 가져와 보겠습니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport logging\nimport warnings\nlogging.getLogger('prophet').setLevel(logging.ERROR)\nwarnings.filterwarnings(\"ignore\")\n```\n\n이전 장에서 소개한 NeuralProphet을 계속 사용할 것입니다. Kaggle의 Bike Share Daily 데이터를 사용할 건데요 (여기나 여기서 다운로드 가능합니다). 이 데이터셋은 매일의 대여 수요와 온도, 풍속 등 다른 기상 정보가 있는 다중 변수 데이터셋입니다.\n\n```js\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\npath = '/content/gdrive/My Drive/data/time_series'\ndata = pd.read_csv(path + '/bike_sharing_daily.csv')\ndata.tail()\n```\n\n<img src=\"/TIL/assets/img/2024-07-09-QuantileRegressionforTimeSeriesProbabilisticForecasting_2.png\" />\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n자전거 대여량을 그래프로 플롯해 봅시다. 두 번째 해에 수요가 증가하고 계절적 패턴이 있는 것을 관찰할 수 있어요.\n\n```js\n# 문자열을 datetime64로 변환하기\ndata[\"ds\"] = pd.to_datetime(data[\"dteday\"])\n\n# 대여량 데이터의 선 그래프 만들기\nplt.plot(data['ds'], data[\"cnt\"])\nplt.xlabel(\"날짜\")\nplt.ylabel(\"수량\")\nplt.show()\n```\n\n![그림](/TIL/assets/img/2024-07-09-QuantileRegressionforTimeSeriesProbabilisticForecasting_3.png)\n\n모델링을 위해 매우 기본적인 데이터 준비를 할 거에요. NeuralProphet에서는 칼럼 이름을 “ds”와 “y”로 지정해야 해요. Prophet과 동일하죠.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\ndf = data[['ds','cnt']]\ndf.columns = ['ds','y']\n```\n\n지금부터 NeuralProphet에서 분위 회귀를 구축하러 바로 가봅시다. 5번, 10번, 50번, 90번, 95번 분위의 값을 얻고 싶다고 가정해봅시다. quantile_list = [0.05, 0.1, 0.5, 0.9, 0.95]로 지정하고, quantile_list를 켜려면 \"parameter quantiles = quantile_list\"를 사용합니다. 나머지 매개변수는 Chapter 3: Tutorial I: Trend + seasonality + holidays & events에서와 동일합니다.\n\n```python\nfrom neuralprophet import NeuralProphet, set_log_level\n\nquantile_list=[0.05,0.1,0.5,0.9,0.95]\n# Model and prediction\nm = NeuralProphet(\n    quantiles=quantile_list,\n    yearly_seasonality=True,\n    weekly_seasonality=True,\n    daily_seasonality=False\n)\nm = m.add_country_holidays(\"US\")\nm.set_plotting_backend(\"matplotlib\")  # Use matplotlib\n\ndf_train, df_test = m.split_df(df, valid_p=0.2)\nmetrics = m.fit(df_train, validation_df=df_test, progress=\"bar\")\nmetrics.tail()\n```\n\n작업이 완료되면 prophet에서 상속받은 .make_future_dataframe()을 사용하여 예측을 위한 새 데이터 프레임을 만들 것입니다. n_historic_predictions 매개변수는 지난 100개 데이터 포인트만 포함하도록 설정됩니다. \"True\"로 설정하면 전체 기록이 포함됩니다. \"periods=50\"를 설정하여 다음 50개 데이터 포인트를 예측합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\n미래 = m.make_future_dataframe(df, periods=50, n_historic_predictions=100) #, n_historic_predictions=1)\n\n# 훈련된 모델로 예측 수행\n예측 = m.predict(df=미래)\n예측.tail(60)\n```\n\n예측값은 \"forecast\" 데이터 프레임에 저장됩니다.\n\n![이미지](/TIL/assets/img/2024-07-09-QuantileRegressionforTimeSeriesProbabilisticForecasting_4.png)\n\n위 데이터 프레임은 플로팅을 위한 모든 데이터 요소를 갖고 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nm.plot(\n    forecast,\n    plotting_backend=\"plotly-static\"\n    #plotting_backend = \"matplotlib\"\n)\n```\n\n아래와 같이 플롯이 표시됩니다. 예측 구간은 분위값으로 제공됩니다!\n\n<img src=\"/TIL/assets/img/2024-07-09-QuantileRegressionforTimeSeriesProbabilisticForecasting_5.png\" />\n\n분위 회귀에 의한 예측 구간과 OLS에 의한 신뢰 구간은 다릅니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n예측 구간이 인기를 얻으면, 분위수 회귀와 최소 자승법(OLS)에서의 신뢰 구간의 차이를 구별하는 것이 도움이 될 것입니다. 그림 (F)에서 좌측에는 선형 회귀를 그리고 우측에는 분위수 회귀를 표시했습니다.\n\n![그림](/TIL/assets/img/2024-07-09-QuantileRegressionforTimeSeriesProbabilisticForecasting_6.png)\n\n우선, 이들의 목표는 다릅니다:\n\n- 선형 회귀: 주요 목표는 종속 변수의 조건부 평균과 가능한 한 가까운 예측 값을 찾는 것입니다.\n- 분위수 회귀: 목표는 특정 신뢰 수준에서 예상되는 미래 관측값이 위치할 범위를 제공하는 것입니다. 종속 변수 Y의 조건부 분포의 다양한 분위수에서 독립 변수(T) 사이의 관련성을 추정합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음으로, 그들의 계산방법이 다릅니다:\n\n- 선형 회귀에서 신뢰구간은 독립변수의 계수에 대한 구간 추정입니다. 예를 들어 b ± 1.96 \\* b의 표준 오차(Figure F)와 같이 표시됩니다. 이는 주로 최소 총 거리를 찾기 위해 최소제곱법(OLS)을 사용하여 데이터 포인트와 선 간의 거리를 최소화합니다. 계수가 변할 수 있기 때문에 예측된 조건부 평균값도 변할 수 있습니다. Figure F의 왼쪽 그래프에서, 계수 b가 변하기 때문에 예측된 평균값이 약간씩 다를 수 있습니다.\n- 분위 회귀에서는 종속 변수의 25번째, 50번째 또는 75번째 분위와 같은 분위수 수준을 선택하여 회귀 계수를 추정합니다. 분위 회귀는 일반적으로 절대 편차의 가중 합을 최소화하지만 OLS를 사용하지는 않습니다.\n\n세 번째로, 그들의 응용이 다릅니다:\n\n- 선형 회귀에서는 \"예측된 조건부 평균은 95% 신뢰구간 내에 있다\"라고 말합니다. 신뢰구간은 전체 범위가 아닌 조건부 평균이기 때문에 예측 구간보다 좁습니다.\n- 분위 회귀에서는 \"예측값이 예측 구간의 범위 내에 들어갈 가능성이 95%입니다.\"라고 말합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n결론\n\n이 장에서는 예측 구간을 위한 분위수 회귀의 개념을 배웠습니다. NeuralProphet을 사용하여 예측 구간을 생성하는 방법을 보여주었습니다. 또한 비즈니스 응용 프로그램에서 흔히 혼동되는 예측 구간과 신뢰 구간의 차이를 구별했습니다. 다음 장에서는 예측 불확실성에 대한 다른 중요한 기술인 준수 역분위 회귀(CQR)를 계속 공부할 것입니다.\n\n샘플 eBook 장(무료): [링크](https://github.com/dataman-git/modern-time-series/blob/main/20240522beauty_TOC.pdf)\n\n- 아름다운 형식으로 책을 재현하여 즐거운 독서 경험을 제공해준 The Innovation Press, LLC 직원들에게 감사드립니다. eBook을 분배하기 위해 Teachable 플랫폼을 선택했습니다. 이를 통해 부담스럽지 않은 경비와 전 세계 독자들에게 배포할 수 있었습니다. 신용카드 거래는 Teachable.com에서 기밀이지키며 안전하게 처리됩니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\neBook on Teachable.com: $22.50\n[링크](https://drdataman.teachable.com/p/home)\n\nThe print edition on Amazon.com: $65 [링크](https://a.co/d/25FVsMx)\n\n- 프린트 판은 광택 처리된 표지, 컬러 출력물, 아름다운 Springer 글꼴 및 레이아웃을 채택하여 즐거운 독서 경험을 제공합니다. 7.5 x 9.25 인치의 크기는 서재에 있는 대부분의 책들과 어울립니다.\n- “이 책은 부오파이의 시계열 분석에 대한 깊은 이해와 예측 분석 및 이상 감지 분야에서의 응용을 검증하는 것입니다. 이 책은 독자들이 실제 세상의 도전 과제에 대처할 수 있는 필수적인 기술을 제공합니다. 데이터 과학으로의 전환을 원하는 사람들에게 특히 가치 있는 자료입니다. 부오파이는 전통적인 기술과 최신 기술 모두에 대해 상세하게 탐구합니다. 부오파이는 신경망 및 기타 고급 알고리즘에 대한 토론을 통합하여 최신 트렌드와 발전 상황을 반영합니다. 이는 독자가 확립된 방법뿐만 아니라 데이터 과학 분야의 가장 최신 및 혁신적인 기술과도 상호작용할 준비가 되어 있는지를 보장합니다. 부오파이의 생생하고 접근 가능한 글쓰기 스타일 때문에 이 책의 명료함과 접근성은 높아졌습니다. 그는 복잡한 수학 및 통계 개념을 풀어내어 엄밀성을 희생하지 않으면서도 접근 가능하게 만들어냅니다.”\n\n# 모던 시계열 예측: 예측 분석 및 이상 감지를 위한\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nChapter 0: 서문\n\nChapter 1: 소개\n\nChapter 2: 비즈니스 예측을 위한 선지자\n\nChapter 3: 튜토리얼 I: 트렌드 + 계절성 + 휴일 및 이벤트\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음은 Markdown 형식으로 변경된 내용입니다:\n\n- Chapter 4: Tutorial II: Trend + seasonality + holidays & events + auto-regressive (AR) + lagged regressors + future regressors\n\n- Chapter 5: Change Point Detection in Time Series\n\n- Chapter 6: Monte Carlo Simulation for Time Series Probabilistic Forecasting\n\n- Chapter 7: Quantile Regression for Time Series Probabilistic Forecasting\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Chapter 8: 시계열 확률 예측을 위한 조화형 예측\n\n# Chapter 9: 시계열 확률 예측을 위한 조화화된 분위 회귀\n\n# Chapter 10: 자동 ARIMA!\n\n# Chapter 11: 시계열 데이터 형식 만들기 쉽게\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Chapter 12: 다기간 확률 예측을 위한 선형 회귀\n\n## Chapter 13: Tree-based 시계열 모델을 위한 피처 엔지니어링\n\n### Chapter 14: 다기간 시계열 예측을 위한 두 가지 기본 전략\n\n#### Chapter 15: 다기간 시계열 확률 예측을 위한 Tree-based XGB, LightGBM, 및 CatBoost 모델\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 16 장: 시계열 모델링 기법의 진보\n\n# 17 장: 시계열 확률 예측을 위한 딥러닝 기반 DeepAR\n\n# 18 장: 주가에 대한 확률 예측 응용\n\n# 19 장: RNN에서 Transformer 기반 시계열 모델로\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 20장: 해석 가능한 시계열 예측을 위한 시간 퓨전 트랜스포머\n\n# 21장: 시계열 예측을 위한 오픈 소스 Lag-Llama 튜토리얼\n","ogImage":{"url":"/assets/img/2024-07-09-QuantileRegressionforTimeSeriesProbabilisticForecasting_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-QuantileRegressionforTimeSeriesProbabilisticForecasting_0.png","tag":["Tech"],"readingTime":14},{"title":"파이썬과 트랜스포머로 생성형 AI 챗봇 만들기 방법","description":"","date":"2024-07-09 19:40","slug":"2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers","content":"\n요즘의 디지털 시대에, 챗봇은 간단한 자동응답 도구에서 복잡하고 맥락을 이해하는 대화를 수행할 수 있는 가상 보조로 진화했습니다. 이 글에서는 파이썬과 허깅페이스의 트랜스포머 라이브러리를 사용하여 생성 모델인 GPT-2와 같은 고급 모델을 활용해 AI 챗봇을 구축하는 방법을 살펴보겠습니다.\n\n# 생성식 AI 챗봇 소개\n\n생성식 AI 챗봇은 인공 지능의 중요한 발전을 나타내며, 기업이 고객 지원을 자동화하고 사용자-시스템 상호작용을 향상시키는 데 기여합니다. 미리 정의된 응답에 제한된 규칙 기반 챗봇과는 달리, 생성식 챗봇은 자연어를 이해하고 생성하는 더 현실적인 대화를 할 수 있습니다.\n\n# 사용된 도구 및 라이브러리\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 프로젝트에서는 다음 도구 및 라이브러리를 활용할 예정입니다:\n\n- Python: 다재다능하고 배우기 쉬운 프로그래밍 언어.\n- Transformers: 사전 학습된 언어 모델 구현을 제공하는 허깅페이스 라이브러리.\n- Flask: Python으로 웹 응용 프로그램을 구축하기 위한 가볍고 효율적인 웹 프레임워크.\n- PyTorch: 인공지능 모델을 훈련하고 평가하는 데 사용되는 머신러닝 라이브러리.\n\n# 단계 1: 환경 설정\n\n시작하기 전에 시스템에 Python과 pip이 설치되어 있는지 확인하세요. 그런 다음 터미널에서 다음 명령을 실행하여 필요한 라이브러리를 설치하세요:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers_0.png](/TIL/assets/img/2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers_0.png)\n\n# Step 2: Loading GPT-2 Model and Tokenizer\n\nTo begin, import the required libraries and load the pretrained GPT-2 model and its corresponding tokenizer:\n\n![2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers_1.png](/TIL/assets/img/2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers_1.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 단계 3: 응답 함수 생성\n\n사용자 입력을 가져와 인코딩하고 GPT-2 모델을 사용하여 응답을 생성하는 함수를 정의하세요:\n\n![image](/TIL/assets/img/2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers_2.png)\n\n# 단계 4: Flask 어플리케이션 설정\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음으로 웹 요청을 처리하고 챗봇을 위한 직관적 인터페이스를 제공하기 위해 Flask 애플리케이션을 설정하세요:\n\n![image1](/TIL/assets/img/2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers_3.png)\n\n![image2](/TIL/assets/img/2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers_4.png)\n\n![image3](/TIL/assets/img/2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers_5.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![사진](/TIL/assets/img/2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers_6.png)\n\n# 단계 5: Flask 앱 실행 및 상호 작용\n\n챗봇 애플리케이션을 실행하려면:\n\n- 위의 Python 코드를 파일에 저장하세요. 예를 들어, chatbot_app.py로 저장합니다.\n- 터미널이나 명령 프롬프트를 엽니다.\n- chatbot_app.py 파일이 위치한 디렉토리로 이동합니다.\n- Flask 애플리케이션을 시작하려면 다음 명령을 실행하세요:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Image](/TIL/assets/img/2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers_7.png)\n\n- 어플리케이션이 시작되면 웹 브라우저를 열고 챗봇과 상호 작용하기 위해 http://127.0.0.1:5000/ 로 이동하세요.\n\n![Image](/TIL/assets/img/2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers_8.png)\n\n![Image](/TIL/assets/img/2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers_9.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 결론\n\n생성형 AI 챗봇은 기업과 사용자가 상호 작용하는 방식을 혁신하고 있습니다. Hugging Face의 Transformers와 같은 고급 도구를 활용하면 자연스럽고 맥락 있는 대화를 이어나갈 수 있는 챗봇을 만들 수 있으며, 이는 사용자 경험을 크게 향상시킵니다.\n","ogImage":{"url":"/assets/img/2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-CreatingaGenerativeAIChatbotwithPythonandTransformers_0.png","tag":["Tech"],"readingTime":5},{"title":"진정한 인공지능의 벽에 부딪힌 AGI의 발전 현황","description":"","date":"2024-07-09 19:38","slug":"2024-07-09-ProgresstowardstrueArtificialGeneralIntelligenceAGIhashitawall","content":"\n![image](/TIL/assets/img/2024-07-09-ProgresstowardstrueArtificialGeneralIntelligenceAGIhashitawall_0.png)\n\n인공 일반 지능(AGI)을 향한 탐구는 몇십 년 동안 연구 커뮤니티의 관심을 사로 잡아 왔습니다.\n\nAGI는 인간이 하는 것과 비슷하게 프로그래밍되지 않은 상태에서도 다양한 작업을 수행할 수 있는 AI 시스템의 능력을 나타냅니다. 이는 우리가 학습, 추론 및 새로운 상황에 적응할 수 있는 능력을 갖춘 시스템을 찾고 있다는 것을 의미합니다.\n\n그러나 분야에서 중요한 발전이 있었음에도 불구하고, 대부분의 AI 시스템은 아직도 좁은 작업과 영역에 제한되어 있어 AGI는 여전히 애매한 목표로 남아 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n`<table>`태그를 Markdown 형식으로 변경해 주세요.\n\n그 이유가 무엇인가요?\n\n# 인공지능이 잘하지 못하는 것\n\nAGI에 대한 주요 장애물 중 하나는 현대 인공지능 시스템이 추론보다는 기억에 의존하고 있다는 것입니다. 우리가 모두 아는 이 시스템들인 Language Large Models (LLMs)는 학습 데이터의 패턴을 암기하고 인접한 맥락에서 적용하는 데 능숙합니다.\n\n하지만 새로운 경우를 바탕으로한 새로운 추론을 생성하는 능력이 부족합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nLLM은 독창적이거나 혁신적인 상황에 기반한 새로운 추론을 생성할 수 없으며 기억에 의존한다는 제한이 있습니다.\n\n또 다른 문제는 AI 시스템이 학습 데이터를 넘어서 일반화할 수 없는 점입니다. 예를 들어, 체스를 하는 방법을 학습한 AI 시스템은 사람들보다 더 잘 체스를 둘 수 있지만, 체커 또는 바둑과 같은 다른 보드 게임에 해당 지식을 전이시키는 것은 불가능합니다.\n\n이러한 일반화 실패는 AGI를 달성하는 데 중요한 장애물이며, 시스템이 새로운 상황에 지식을 적용하는 능력을 제한합니다.\n\n![이미지](https://miro.medium.com/v2/resize:fit:1400/1*_LbxAoOCs7Ed8ThxzsC_eQ.gif)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 새로운 패러다임과 새로운 기준\n\n2019년, Keras의 창시자 인 프랑수아 쇌레(François Chollet)는 2.5백만 명 이상의 개발자가 채택한 오픈 소스 딥 러닝 라이브러리인 Keras의 창시자이자 Google의 소프트웨어 엔지니어 및 AI 연구원으로서, 영향력 있는 논문 \"지능의 측정에 관하여\"을 발표했습니다. 이 논문에서 그는 알려지지 않은 작업에 대한 AI 기술 습득의 효율을 측정하기 위한 기준인 추상화 및 추론 코퍼스(ARC-AGI)를 소개했습니다.\n\n마침내, 모든 좋은 과학이 해야 할 것처럼, 우리는 정의로부터 시작해야 합니다. AGI에 대한 일반적인 합의는 \"경제적 가치가 있는 작업의 대부분을 자동화할 수 있는 시스템\"으로 정의된다는 것입니다. 이것이 유용한 목표로 간주될 수 있지만... 이것은 지능의 부정확한 측정입니다.\n\n보다 지능적이고 인간과 유사한 시스템을 위한 고의적인 진보를 이루기 위해, 우리는 적절한 피드백 신호를 따라야 합니다: 우리는 지능을 정의하고 평가해야 합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 기술, 암기 및 지능\n\n기술은 이전 지식과 경험에 크게 영향을 받습니다. 무제한의 사전 지식이나 무제한의 교육 데이터는 개발자들에게 시스템의 기술 수준을 \"구매\"할 수 있는 기회를 제공합니다. 이것은 시스템 자체의 일반화 능력을 가리게 합니다.\n\n현대 인공지능 (LLM)은 뛰어난 암기 엔진으로 입증되었습니다. 그들은 훈련 데이터에서 고차원의 패턴을 기억하고 해당 패턴을 인접한 문맥에 적용할 수 있습니다.\n\n이것은 그들의 겉보기 추론 능력이 작동하는 방식입니다. LLM은 실제로 추론하지 않습니다. 대신, 그들은 추론 패턴을 암기하고 해당 추론 패턴을 인접한 문맥에 적용합니다. 그러나 그들은 새로운 상황에 기반한 새로운 추론을 만들어 내지 못합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n지능은 넓거나 일반적인 능력에 있습니다; 훈련 데이터 외에도 새로운 기술을 효율적으로 습득할 수 있는 시스템으로 AGI를 정의하는 방식을 조정해야 합니다. 아니, 더 나아가...\n\n![이미지](/TIL/assets/img/2024-07-09-ProgresstowardstrueArtificialGeneralIntelligenceAGIhashitawall_1.png)\n\n# 이러한 종류의 지능을 어떻게 측정할 수 있을까요?\n\n대부분의 AI 벤치마크는 기술을 측정합니다. 하지만 기술은 지능이 아니라는 것을 방금 보았죠?\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n2019년 같은 연구에서 탄생한 **인공 일반 지능(AGI)**을 위한 추상화 및 추론 말뭉치 (ARC-AGI)는 AGI의 유일한 공식 벤치마크로 여겨집니다.\n\n**추상화 및 추론 말뭉치(ARC)**는 AI 시스템의 작업 범위에 걸쳐 습득 능률을 테스트하도록 특별히 설계된 벤치마크입니다.\n\n![이미지](/TIL/assets/img/2024-07-09-ProgresstowardstrueArtificialGeneralIntelligenceAGIhashitawall_2.png)\n\n이는 추론과 추상화가 필요한 이미지 작업들을 포함한 과제 모음처럼 보입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n여기에는 명확한 의도가 있습니다. 즉, AI 시스템이 예시로부터 학습하고 그 지식을 새로운, 보지 못한 문제를 해결하는 데 적용할 수 있는 능력을 검증하는 것입니다. 이는 인간과 유사한 지능의 중요한 측면으로, 종종 \"유동적 지능\"이라고 합니다.\n\nARC 벤치마크는 중요한 이유가 있습니다.\n\n- 이는 이러한 추상적 추론을 필요로 하는 작업에 어려움을 겪는 현재의 기계 학습 방법에 도전합니다.\n- 이것은 연구자들이 인간 수준의 지능을 달성하기에 더 가까운 AI 시스템을 개발하는 데 도움이 됩니다.\n\n![이미지](/TIL/assets/img/2024-07-09-ProgresstowardstrueArtificialGeneralIntelligenceAGIhashitawall_3.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 다분야 접근\n\n인공 일반 지능(AGI)으로 나아가는 진전이 멈춰있습니다. 엄청나게 방대한 양의 데이터로 훈련된 LLM은 그럼에도 불구하고, 훈련받지 않은 간단한 문제에 적응하거나 새로운 발명을 할 수 없는 상황입니다.\n\nChollet이 만든 2019년 Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI)는 AGI의 유일한 공식적인 벤치마크입니다.\n\n최근 NYU 교수이자 Meta의 최고 AI 과학자인 야얀 르쿤은 AGI 분야에서 진전하려면 LLM에 집중해서는 안된다고 공개적으로 언급했습니다...\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/TIL/assets/img/2024-07-09-ProgresstowardstrueArtificialGeneralIntelligenceAGIhashitawall_4.png)\n\n저는 이 도발을 좋아하고, 모든 AI 연구자가 이 도전에 대해 잘 알고 있다고 믿습니다. 대학 연구자들이 이 분야에서 몇 발자국 앞서 나가야 한다고 생각합니다. 학계는 보통 문제에 대해 다학제적 접근에 숙달되어 있지만, 그들은 자금과 컴퓨팅 파워 등 자원이 부족합니다.\n\n그러나 현재 AI 연구의 추세는 소스코드를 공개하지 않는 연구 방향으로 이동하고 있어서 아이디어와 지식의 공유를 제한하고 있습니다. 이 추세는 \"스케일만 있으면 충분하다\"는 믿음과 경쟁 우위를 보호하려는 욕심에서 나온 것이죠. 이 방식은 혁신을 억불하며 AGI로의 진전속도를 제한합니다.\n\n언제나 예외는 있습니다...\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n오픈 소스 연구는 협력과 지식 공유를 촉진하여 AGI로의 진전 속도를 가속화합니다. 연구를 공개로 접근 가능하게 함으로써 전 세계의 연구자들이 더 지능적인 AI 시스템의 개발에 기여할 수 있으며, 다양한 시각에서 새로운 아이디어와 혁신이 탄생할 수 있습니다.\n\nAI 연구에서의 투명성 부족과 협력은 AGI가 임박했다는 오도된 인식에 기여하고 있으며, 이는 AI 규제 환경에 영향을 미치고 있습니다. 규제 당국은 AGI가 임박했다는 잘못된 가정 하에 길잡이 AI 연구에 대한 장애물을 고려하고 있습니다.\n\n오픈 소스 연구는 더 지능적인 AI 시스템의 개발을 이끌어주고 일반 지능의 더 정확한 측정 지표를 제공할 수 있습니다.\n\nARC Challenge는 대규모 경쟁을 진행하는 역사가 있으며, 2020년 Kaggle에서 시작된 첫 번째 ARC-AGI 대회를 시작으로 2022년과 2023년에는 ARCathon이 열렸으며, 가장 최근에는 상금 총액이 1.1백만달러가 넘는 ARC Prize 2024가 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![2024-07-09-ProgresstowardstrueArtificialGeneralIntelligenceAGIhashitawall_5.png](/TIL/assets/img/2024-07-09-ProgresstowardstrueArtificialGeneralIntelligenceAGIhashitawall_5.png)\n\n# 결론…\n\nAGI를 위한 탐험은 복잡하고 도전적인 목표이지만 불가능한 것은 아닙니다.\n\n현대 AI의 제한을 극복하고 AGI를 준비하기 위해, 우리는 오픈소스 연구의 적극적 지지자가 되어야 합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만 그보다 중요한 것은 우리가 어떤 지적인 것이란 것에 대한 오픈 토론의 일부가 되어야 한다는 것입니다. 다양한 분야의 시각에서 바라봤을 때 어떤 지능인지에 대한 토론을 펼쳐야 합니다.\n\n그렇게 함으로써 누구라도 프로그래밍 전문가가 아니더라도 더 지적인 AI 시스템의 개발에 기여할 수 있게 되어 AGI로의 발전속도를 높일 수 있습니다.\n\n만약 이 이야기가 가치있었다면 조금이라도 지원을 보여주고 싶다면 다음과 같은 방법을 사용할 수 있습니다:\n\n- 이 이야기에 대해 많이 박수를 치기\n- 기억하기에 더 중요한 부분을 강조하기 (나중에 그것들을 찾는 데 더 편리하고 나에게는 더 나은 기사를 쓸 수 있습니다)\n- '자체 AI 구축하기' 시작하는 방법 배우기, 무료 eBook 다운로드하기\n- 저를 Medium에서 팔로우하기\n- 내 최신 기사 읽기 https://medium.com/@fabio.matricardi\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n자료 및 참고 자료:\n\n![Generative AI](/TIL/assets/img/2024-07-09-ProgresstowardstrueArtificialGeneralIntelligenceAGIhashitawall_6.png)\n\n이 이야기는 Generative AI에서 게시되었습니다. LinkedIn에서 저희와 연락을 유지하고 최신 AI 이야기에 대해 최신 정보를 얻으려면 Zeniteq를 팔로우하세요.\n\n저희의 뉴스레터를 구독하여 generative AI의 최신 뉴스와 업데이트를 받아보세요. 함께 AI의 미래를 함께 만들어가요!\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Image](/TIL/assets/img/2024-07-09-ProgresstowardstrueArtificialGeneralIntelligenceAGIhashitawall_7.png)\n","ogImage":{"url":"/assets/img/2024-07-09-ProgresstowardstrueArtificialGeneralIntelligenceAGIhashitawall_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-ProgresstowardstrueArtificialGeneralIntelligenceAGIhashitawall_0.png","tag":["Tech"],"readingTime":10}],"page":"13","totalPageCount":28,"totalPageGroupCount":2,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true}