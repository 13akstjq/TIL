{"pageProps":{"post":{"title":"Python, C, Rust로 직접 만들어보는 ReLU 활성화 함수 단계별 가이드","description":"","date":"2024-07-14 23:51","slug":"2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust","content":"\n\n\n![image](/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_0.png)\n\n# 소개\n\n신경망 세계에서 활성화 함수는 모델의 출력을 결정하는 데 중요한 역할을 합니다. 가장 인기 있는 활성화 함수 중 하나는 ReLU(렉티파이드 루 linear Unit)입니다. 간단함과 효과적임으로 유명한 ReLU는 많은 딥러닝 모델에서 표준 선택지가 되었습니다. 이 안내서에서는 Python, C 및 Rust 세 가지 다른 프로그래밍 언어로부터 ReLU 활성화 함수를 처음부터 만드는 과정을 안내합니다. 이를 통해 다양한 플랫폼에서의 구현과 이점에 대한 명확한 이해를 제공할 것입니다.\n\n# ReLU란 무엇인가요?\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nReLU은 Rectified Linear Unit의 약자입니다. 이는 입력 값을 반환하는 활성화 함수로 정의됩니다:\n\n![ReLU Activation Function](/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_1.png)\n\n더 간단히 말하면, 입력 값이 양수인 경우 입력 값을 반환하고, 그렇지 않으면 0을 반환합니다. ReLU 함수는 수학적으로 다음과 같이 표현됩니다:\n\n![ReLU Formula](/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_2.png)\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# ReLU를 사용하는 이유?\n\n- 단순성: ReLU 함수는 0을 기준으로 한 간단한 임계값 처리를 포함하기 때문에 계산 효율적입니다.\n- 비선형성: 선형 함수처럼 보이지만 ReLU는 비선형성을 도입하여 복잡한 패턴을 학습하는 데 필수적입니다.\n- 희소 활성화: ReLU는 희소한 활성화를 생성하는 경향이 있어서 (많은 뉴런이 0을 출력) 네트워크를 더 효율적으로 만듭니다.\n\n# Python에서 ReLU 및 신경망 레이어 구현하기\n\n파이썬에서 ReLU 활성화 함수와 간단한 신경망 레이어를 구현해보겠습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 파이썬 구현\n\n## 단계 1: ReLU 함수 정의\n\n먼저, 간단한 파이썬 함수를 사용하여 ReLU 함수를 정의해 보겠습니다.\n\n```python\ndef relu(x):\n    return max(0, x)\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 단계 2: 배열에 ReLU 적용하기\n\n우리는 NumPy를 사용하여 배열을 처리할 수 있는 ReLU 함수를 확장할 것입니다.\n\n```python\nimport numpy as np\n\ndef relu_array(arr):\n    return np.maximum(0, arr)\n```\n\n## 단계 3: 간단한 신경 계층 정의하기\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n자, 이제 간단한 신경망 레이어 클래스를 만들어봅시다.\n\n```js\nclass SimpleNeuralLayer:\n    def __init__(self, input_size, output_size):\n        self.weights = np.random.randn(input_size, output_size)\n        self.biases = np.zeros(output_size)\n\n    def forward(self, inputs):\n        z = np.dot(inputs, self.weights) + self.biases\n        return relu_array(z)\n```\n\n## 단계 4: 신경망 레이어 테스트\n\n```js\n# 예제 사용법\nlayer = SimpleNeuralLayer(3, 2)\ninputs = np.array([1, 2, -1])\noutput = layer.forward(inputs)\nprint(output)\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# C에서 ReLU 및 신경 계층 구현\n\n이제 C에서 ReLU 함수와 간단한 신경망 계층을 구현해 보겠습니다.\n\n# C 구현\n\n## 단계 1: ReLU 함수 정의\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n#include <stdio.h>\n\ndouble relu(double x) {\n    return x > 0 ? x : 0;\n}\n```\n\n## 단계 2: 배열에 ReLU 적용하기\n\n```js\n#include <stdio.h>\n\nvoid relu_array(double* arr, int size) {\n    for (int i = 0; i < size; i++) {\n        arr[i] = arr[i] > 0 ? arr[i] : 0;\n    }\n}\n```\n\n## 단계 3: 간단한 신경망 레이어 정의하기\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\n## 단계 4: 신경망 레이어 테스트\n\n```js\n#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n\nint main() {\n    srand(time(NULL));\n    SimpleNeuralLayer layer = create_layer(3, 2);\n    double inputs[] = {1, 2, -1};\n    double output[2];\n    forward(layer, inputs, output);\n    printf(\"Output: %f %f\\n\", output[0], output[1]);\n    free(layer.weights);\n    free(layer.biases);\n    return 0;\n}\n```\n\n# ReLU 및 신경망 레이어 구현하기\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마침내, ReLU 함수와 간단한 신경망 레이어를 Rust로 구현해 봅시다.\n\n# Rust 구현\n\n## 단계 1: ReLU 함수 정의\n\n```rust\nfn relu(x: f64) -> f64 {\n    if x > 0.0 { x } else { 0.0 }\n}\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 단계 2: 배열에 ReLU 적용하기\n\n```js\nfn relu_array(arr: &mut [f64]) {\n    for x in arr.iter_mut() {\n        *x = if *x > 0.0 { *x } else { 0.0 };\n    }\n}\n```\n\n## 단계 3: 간단한 신경망 계층 정의\n\n```js\nuse rand::Rng;\n\nstruct SimpleNeuralLayer {\n    weights: Vec<Vec<f64>>,\n    biases: Vec<f64>,\n}\n\nimpl SimpleNeuralLayer {\n    fn new(input_size: usize, output_size: usize) -> Self {\n        let mut rng = rand::thread_rng();\n        let weights = (0..input_size)\n            .map(|_| (0..output_size).map(|_| rng.gen_range(-1.0..1.0)).collect())\n            .collect();\n        let biases = vec![0.0; output_size];\n        \n        SimpleNeuralLayer { weights, biases }\n    }\n\n    fn forward(&self, inputs: &[f64]) -> Vec<f64> {\n        let mut output = vec![0.0; self.biases.len()];\n        \n        for (i, &bias) in self.biases.iter().enumerate() {\n            output[i] = inputs.iter()\n                .zip(&self.weights)\n                .map(|(&input, weight_row)| input * weight_row[i])\n                .sum::<f64>() + bias;\n            output[i] = relu(output[i]);\n        }\n        output\n    }\n}\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 단계 4: 신경 계층 테스트\n\n```js\nfn main() {\n    let layer = SimpleNeuralLayer::new(3, 2);\n    let inputs = [1.0, 2.0, -1.0];\n    let output = layer.forward(&inputs);\n    \n    for value in output {\n        print!(\"{} \", value);\n    }\n    // 출력: 0 0\n}\n```\n\n# 결론\n\nReLU 활성화 함수를 만들고 간단한 신경망 계층에 통합하는 것은 신경망 작업에 대한 기본 개념을 강조합니다. Python, C 및 Rust에서 ReLU를 구현함으로써, 여러 플랫폼에서 딥러닝 모델의 성공을 이끌어내는 주요 구성 요소 중 하나에 대한 통찰을 얻을 수 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n기계 학습 초보자이거나 경험이 풍부한 실무자이든 ReLU와 같은 활성화 함수의 내부 작업을 이해하는 것이 중요합니다. 이 지식을 통해 다양한 응용 프로그램을 위해 신경망을 더 잘 설계, 디버그 및 최적화할 수 있습니다.\n\n좋은 코딩되세요!\n\n![image](/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_3.png)","ogImage":{"url":"/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_0.png"},"coverImage":"/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_0.png","tag":["Tech"],"readingTime":9},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p><img src=\"/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_0.png\" alt=\"image\"></p>\n<h1>소개</h1>\n<p>신경망 세계에서 활성화 함수는 모델의 출력을 결정하는 데 중요한 역할을 합니다. 가장 인기 있는 활성화 함수 중 하나는 ReLU(렉티파이드 루 linear Unit)입니다. 간단함과 효과적임으로 유명한 ReLU는 많은 딥러닝 모델에서 표준 선택지가 되었습니다. 이 안내서에서는 Python, C 및 Rust 세 가지 다른 프로그래밍 언어로부터 ReLU 활성화 함수를 처음부터 만드는 과정을 안내합니다. 이를 통해 다양한 플랫폼에서의 구현과 이점에 대한 명확한 이해를 제공할 것입니다.</p>\n<h1>ReLU란 무엇인가요?</h1>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>ReLU은 Rectified Linear Unit의 약자입니다. 이는 입력 값을 반환하는 활성화 함수로 정의됩니다:</p>\n<p><img src=\"/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_1.png\" alt=\"ReLU Activation Function\"></p>\n<p>더 간단히 말하면, 입력 값이 양수인 경우 입력 값을 반환하고, 그렇지 않으면 0을 반환합니다. ReLU 함수는 수학적으로 다음과 같이 표현됩니다:</p>\n<p><img src=\"/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_2.png\" alt=\"ReLU Formula\"></p>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h1>ReLU를 사용하는 이유?</h1>\n<ul>\n<li>단순성: ReLU 함수는 0을 기준으로 한 간단한 임계값 처리를 포함하기 때문에 계산 효율적입니다.</li>\n<li>비선형성: 선형 함수처럼 보이지만 ReLU는 비선형성을 도입하여 복잡한 패턴을 학습하는 데 필수적입니다.</li>\n<li>희소 활성화: ReLU는 희소한 활성화를 생성하는 경향이 있어서 (많은 뉴런이 0을 출력) 네트워크를 더 효율적으로 만듭니다.</li>\n</ul>\n<h1>Python에서 ReLU 및 신경망 레이어 구현하기</h1>\n<p>파이썬에서 ReLU 활성화 함수와 간단한 신경망 레이어를 구현해보겠습니다.</p>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h1>파이썬 구현</h1>\n<h2>단계 1: ReLU 함수 정의</h2>\n<p>먼저, 간단한 파이썬 함수를 사용하여 ReLU 함수를 정의해 보겠습니다.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">relu</span>(<span class=\"hljs-params\">x</span>):\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">max</span>(<span class=\"hljs-number\">0</span>, x)\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>단계 2: 배열에 ReLU 적용하기</h2>\n<p>우리는 NumPy를 사용하여 배열을 처리할 수 있는 ReLU 함수를 확장할 것입니다.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">relu_array</span>(<span class=\"hljs-params\">arr</span>):\n    <span class=\"hljs-keyword\">return</span> np.maximum(<span class=\"hljs-number\">0</span>, arr)\n</code></pre>\n<h2>단계 3: 간단한 신경 계층 정의하기</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>자, 이제 간단한 신경망 레이어 클래스를 만들어봅시다.</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">SimpleNeuralLayer</span>:\n    def <span class=\"hljs-title function_\">__init__</span>(self, input_size, output_size):\n        self.<span class=\"hljs-property\">weights</span> = np.<span class=\"hljs-property\">random</span>.<span class=\"hljs-title function_\">randn</span>(input_size, output_size)\n        self.<span class=\"hljs-property\">biases</span> = np.<span class=\"hljs-title function_\">zeros</span>(output_size)\n\n    def <span class=\"hljs-title function_\">forward</span>(self, inputs):\n        z = np.<span class=\"hljs-title function_\">dot</span>(inputs, self.<span class=\"hljs-property\">weights</span>) + self.<span class=\"hljs-property\">biases</span>\n        <span class=\"hljs-keyword\">return</span> <span class=\"hljs-title function_\">relu_array</span>(z)\n</code></pre>\n<h2>단계 4: 신경망 레이어 테스트</h2>\n<pre><code class=\"hljs language-js\"># 예제 사용법\nlayer = <span class=\"hljs-title class_\">SimpleNeuralLayer</span>(<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">2</span>)\ninputs = np.<span class=\"hljs-title function_\">array</span>([<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>, -<span class=\"hljs-number\">1</span>])\noutput = layer.<span class=\"hljs-title function_\">forward</span>(inputs)\n<span class=\"hljs-title function_\">print</span>(output)\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h1>C에서 ReLU 및 신경 계층 구현</h1>\n<p>이제 C에서 ReLU 함수와 간단한 신경망 계층을 구현해 보겠습니다.</p>\n<h1>C 구현</h1>\n<h2>단계 1: ReLU 함수 정의</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\">#include &#x3C;stdio.<span class=\"hljs-property\">h</span>>\n\ndouble <span class=\"hljs-title function_\">relu</span>(<span class=\"hljs-params\">double x</span>) {\n    <span class=\"hljs-keyword\">return</span> x > <span class=\"hljs-number\">0</span> ? x : <span class=\"hljs-number\">0</span>;\n}\n</code></pre>\n<h2>단계 2: 배열에 ReLU 적용하기</h2>\n<pre><code class=\"hljs language-js\">#include &#x3C;stdio.<span class=\"hljs-property\">h</span>>\n\n<span class=\"hljs-keyword\">void</span> <span class=\"hljs-title function_\">relu_array</span>(<span class=\"hljs-params\">double* arr, int size</span>) {\n    <span class=\"hljs-keyword\">for</span> (int i = <span class=\"hljs-number\">0</span>; i &#x3C; size; i++) {\n        arr[i] = arr[i] > <span class=\"hljs-number\">0</span> ? arr[i] : <span class=\"hljs-number\">0</span>;\n    }\n}\n</code></pre>\n<h2>단계 3: 간단한 신경망 레이어 정의하기</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>단계 4: 신경망 레이어 테스트</h2>\n<pre><code class=\"hljs language-js\">#include &#x3C;stdio.<span class=\"hljs-property\">h</span>>\n#include &#x3C;stdlib.<span class=\"hljs-property\">h</span>>\n#include &#x3C;time.<span class=\"hljs-property\">h</span>>\n\nint <span class=\"hljs-title function_\">main</span>(<span class=\"hljs-params\"></span>) {\n    <span class=\"hljs-title function_\">srand</span>(<span class=\"hljs-title function_\">time</span>(<span class=\"hljs-variable constant_\">NULL</span>));\n    <span class=\"hljs-title class_\">SimpleNeuralLayer</span> layer = <span class=\"hljs-title function_\">create_layer</span>(<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">2</span>);\n    double inputs[] = {<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>, -<span class=\"hljs-number\">1</span>};\n    double output[<span class=\"hljs-number\">2</span>];\n    <span class=\"hljs-title function_\">forward</span>(layer, inputs, output);\n    <span class=\"hljs-title function_\">printf</span>(<span class=\"hljs-string\">\"Output: %f %f\\n\"</span>, output[<span class=\"hljs-number\">0</span>], output[<span class=\"hljs-number\">1</span>]);\n    <span class=\"hljs-title function_\">free</span>(layer.<span class=\"hljs-property\">weights</span>);\n    <span class=\"hljs-title function_\">free</span>(layer.<span class=\"hljs-property\">biases</span>);\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-number\">0</span>;\n}\n</code></pre>\n<h1>ReLU 및 신경망 레이어 구현하기</h1>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>마침내, ReLU 함수와 간단한 신경망 레이어를 Rust로 구현해 봅시다.</p>\n<h1>Rust 구현</h1>\n<h2>단계 1: ReLU 함수 정의</h2>\n<pre><code class=\"hljs language-rust\"><span class=\"hljs-keyword\">fn</span> <span class=\"hljs-title function_\">relu</span>(x: <span class=\"hljs-type\">f64</span>) <span class=\"hljs-punctuation\">-></span> <span class=\"hljs-type\">f64</span> {\n    <span class=\"hljs-keyword\">if</span> x > <span class=\"hljs-number\">0.0</span> { x } <span class=\"hljs-keyword\">else</span> { <span class=\"hljs-number\">0.0</span> }\n}\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>단계 2: 배열에 ReLU 적용하기</h2>\n<pre><code class=\"hljs language-js\">fn <span class=\"hljs-title function_\">relu_array</span>(<span class=\"hljs-params\">arr: &#x26;mut [f64]</span>) {\n    <span class=\"hljs-keyword\">for</span> x <span class=\"hljs-keyword\">in</span> arr.<span class=\"hljs-title function_\">iter_mut</span>(<span class=\"hljs-params\"></span>) {\n        *x = <span class=\"hljs-keyword\">if</span> *x > <span class=\"hljs-number\">0.0</span> { *x } <span class=\"hljs-keyword\">else</span> { <span class=\"hljs-number\">0.0</span> };\n    }\n}\n</code></pre>\n<h2>단계 3: 간단한 신경망 계층 정의</h2>\n<pre><code class=\"hljs language-js\">use <span class=\"hljs-attr\">rand</span>::<span class=\"hljs-title class_\">Rng</span>;\n\nstruct <span class=\"hljs-title class_\">SimpleNeuralLayer</span> {\n    <span class=\"hljs-attr\">weights</span>: <span class=\"hljs-title class_\">Vec</span>&#x3C;<span class=\"hljs-title class_\">Vec</span>&#x3C;f64>>,\n    <span class=\"hljs-attr\">biases</span>: <span class=\"hljs-title class_\">Vec</span>&#x3C;f64>,\n}\n\nimpl <span class=\"hljs-title class_\">SimpleNeuralLayer</span> {\n    fn <span class=\"hljs-title function_\">new</span>(<span class=\"hljs-attr\">input_size</span>: usize, <span class=\"hljs-attr\">output_size</span>: usize) -> <span class=\"hljs-title class_\">Self</span> {\n        <span class=\"hljs-keyword\">let</span> mut rng = <span class=\"hljs-attr\">rand</span>::<span class=\"hljs-title function_\">thread_rng</span>();\n        <span class=\"hljs-keyword\">let</span> weights = (<span class=\"hljs-number\">0.</span>.<span class=\"hljs-property\">input_size</span>)\n            .<span class=\"hljs-title function_\">map</span>(|_| (<span class=\"hljs-number\">0.</span>.<span class=\"hljs-property\">output_size</span>).<span class=\"hljs-title function_\">map</span>(|_| rng.<span class=\"hljs-title function_\">gen_range</span>(-<span class=\"hljs-number\">1.0</span>.<span class=\"hljs-number\">.1</span><span class=\"hljs-number\">.0</span>)).<span class=\"hljs-title function_\">collect</span>())\n            .<span class=\"hljs-title function_\">collect</span>();\n        <span class=\"hljs-keyword\">let</span> biases = vec![<span class=\"hljs-number\">0.0</span>; output_size];\n        \n        <span class=\"hljs-title class_\">SimpleNeuralLayer</span> { weights, biases }\n    }\n\n    fn <span class=\"hljs-title function_\">forward</span>(&#x26;self, <span class=\"hljs-attr\">inputs</span>: &#x26;[f64]) -> <span class=\"hljs-title class_\">Vec</span>&#x3C;f64> {\n        <span class=\"hljs-keyword\">let</span> mut output = vec![<span class=\"hljs-number\">0.0</span>; self.<span class=\"hljs-property\">biases</span>.<span class=\"hljs-title function_\">len</span>()];\n        \n        <span class=\"hljs-keyword\">for</span> (i, &#x26;bias) <span class=\"hljs-keyword\">in</span> self.<span class=\"hljs-property\">biases</span>.<span class=\"hljs-title function_\">iter</span>().<span class=\"hljs-title function_\">enumerate</span>(<span class=\"hljs-params\"></span>) {\n            output[i] = inputs.<span class=\"hljs-title function_\">iter</span>()\n                .<span class=\"hljs-title function_\">zip</span>(&#x26;self.<span class=\"hljs-property\">weights</span>)\n                .<span class=\"hljs-title function_\">map</span>(|(&#x26;input, weight_row)| input * weight_row[i])\n                .<span class=\"hljs-property\">sum</span>::&#x3C;f64>() + bias;\n            output[i] = <span class=\"hljs-title function_\">relu</span>(output[i]);\n        }\n        output\n    }\n}\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>단계 4: 신경 계층 테스트</h2>\n<pre><code class=\"hljs language-js\">fn <span class=\"hljs-title function_\">main</span>(<span class=\"hljs-params\"></span>) {\n    <span class=\"hljs-keyword\">let</span> layer = <span class=\"hljs-title class_\">SimpleNeuralLayer</span>::<span class=\"hljs-title function_\">new</span>(<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">2</span>);\n    <span class=\"hljs-keyword\">let</span> inputs = [<span class=\"hljs-number\">1.0</span>, <span class=\"hljs-number\">2.0</span>, -<span class=\"hljs-number\">1.0</span>];\n    <span class=\"hljs-keyword\">let</span> output = layer.<span class=\"hljs-title function_\">forward</span>(&#x26;inputs);\n    \n    <span class=\"hljs-keyword\">for</span> value <span class=\"hljs-keyword\">in</span> output {\n        print!(<span class=\"hljs-string\">\"{} \"</span>, value);\n    }\n    <span class=\"hljs-comment\">// 출력: 0 0</span>\n}\n</code></pre>\n<h1>결론</h1>\n<p>ReLU 활성화 함수를 만들고 간단한 신경망 계층에 통합하는 것은 신경망 작업에 대한 기본 개념을 강조합니다. Python, C 및 Rust에서 ReLU를 구현함으로써, 여러 플랫폼에서 딥러닝 모델의 성공을 이끌어내는 주요 구성 요소 중 하나에 대한 통찰을 얻을 수 있습니다.</p>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>기계 학습 초보자이거나 경험이 풍부한 실무자이든 ReLU와 같은 활성화 함수의 내부 작업을 이해하는 것이 중요합니다. 이 지식을 통해 다양한 응용 프로그램을 위해 신경망을 더 잘 설계, 디버그 및 최적화할 수 있습니다.</p>\n<p>좋은 코딩되세요!</p>\n<p><img src=\"/TIL/assets/img/2024-07-14-CreatingaReLUActivationFunctionfromScratchAStep-by-StepGuideinPythonCandRust_3.png\" alt=\"image\"></p>\n</body>\n</html>\n"},"__N_SSG":true}