{"pageProps":{"posts":[{"title":"파이썬 속도를 떨어뜨리는 다섯 가지 코딩 실수 및 오늘 해결하는 방법","description":"","date":"2024-07-12 20:38","slug":"2024-07-12-5PythonCodingErrorsThatAreKillingYourSpeedAndHowtoFixThemToday","content":"\n\n<img src=\"/TIL/assets/img/2024-07-12-5PythonCodingErrorsThatAreKillingYourSpeedAndHowtoFixThemToday_0.png\" />\n\n안녕하세요, 파이썬 개발자 여러분! 저는 구글 출신인 다니엘입니다. 현재는 Django를 사용하여 웹 애플리케이션을 개발하고 파이썬에 대한 집착을 키우는 데 시간을 할애하고 있습니다. 저는 주말에는 Medium에서 활발하게 활동하며 시간이 흐른 만큼 축적한 가치 있는 통찰을 전달하고, 여러분을 프로그래밍 대가로 변신시키기 위해 노력하고 있습니다.\n\n솔직히 말해서, 무력한 거북이 속도로 작동하는 파이썬 스크립트로 인한 특정 유형의 좌절감이 있죠. 웹 사이트가 느려 보이거나 데이터 분석 업무가 몇 시간 동안 계속되는 경우든, 느린 코드는 모든 관련자들에게 나쁜 경험을 안겨주며 프로젝트의 성과를 위협할 수도 있습니다.\n\n하지만 걱정하지 마세요! 이 글에서는 제가 보고(심지어 제가 직접 한 것) 온갖 성능 저하 요인을 분석해 보여드릴 것입니다. 그리고 어떤 것을 하지 말아야 하는지뿐만 아니라, 실제 해결책과 코드 예제를 제공하여 여러분의 스크립트를 부드럽고 강력한 파이썬 기계로 만들어 줄 겁니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 실수 #1: 1999년처럼 루핑하기\n\n알다시피, 제가 다른 개발자들처럼 잘 만들어진 for 루프에 대한 애정이 매우 큽니다. 이것들은 우리 작업의 많은 부분을 이루는 기초를 형성합니다. 그러나 특히 대규모 데이터셋에서 순수한 속도에 대해 논의할 때, 신뢰할 수 있는 루프들은 부스트보다는 무게 같이 느껴질 수 있습니다.\n\n## 예시: 몇 개의 숫자를 더해봅시다\n\n거대한 숫자 목록의 제곱을 합산해야 한다고 상상해보세요. 여기에는 루프 방식이 있습니다:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nnumbers = [1, 2, 3, 4, 5, ... , 10000]  # A big list\ntotal = 0\nfor number in numbers:\n    squared = number * number\n    total += squared\n```\n\n이렇게 해도 괜찮아 보이지만 실제로는 Python이 각 요소마다 많은 개별 계산을 수행하고 있습니다.\n\n## 해결책: NumPy가 구해줄게요!\n\n이것이 NumPy가 슈퍼히어로처럼 나타나는 곳입니다. 이것은 벡터화에 관한 모든 것입니다 — 한 번에 전체 배열에 대한 연산 수행하기. 그 예시를 다시 써보겠습니다:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nimport numpy as np\n\nnumbers = np.array([1, 2, 3, 4, 5, ... , 10000])  \nsquared = numbers * numbers  # Vectorized squaring!\ntotal = squared.sum()\n```\n\nBoom! Instead of processing element by element, NumPy takes care of the entire calculation in one go.\n\n## Bonus: The Comprehensible Compromise\n\nList comprehensions are like the stealthy middle ground:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ntotal = sum(number * number for number in numbers)\n```\n\n일반적인 루프보다 빠르지만, 강력한 숫자 계산을 위한 NumPy만큼의 세기는 아닐 수 있습니다.\n\n# 실수 #2: 잘못된 도구 선택\n\n망치 하나만으로 집을 짓는다고 상상해보세요. 집을 완성할 수는 있지만, 정말 혼란스러울 것입니다. 마찬가지로, Python에서 모든 작업에 대해 리스트만을 의존하는 것은 등이 뒤로 돌아가있는 채 프로그래밍하는 것과 같습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 예시: 내 전화번호는 어디에?\n\n만약 다음과 같이 연락처 목록이 있다면:\n\n```js\ncontacts = [\n    {\"name\": \"Alice\", \"phone\": \"123-4567\"},\n    {\"name\": \"Bob\", \"phone\": \"789-0123\"},\n    # ... 추가 연락처\n]\n```\n\nBob의 전화번호를 찾으려면 목록을 훑어봐야 합니다. 모든 연락처를 확인해야 할 수도 있어요! 어떡하지?\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 문제 해결: 초능력을 지닌 데이터 구조들\n\n- Dictionaries: 키를 이용한 신속한 검색 파트너 키(예: \"이름\")로 검색하는 경우, 사전이 당신의 신뢰할 수 있는 도우미가 될 것입니다.\n\n```js\ncontacts_dict = {\n    \"Alice\": \"123-4567\",\n    \"Bob\": \"789-0123\",\n    # ... 더 많은 연락처\n}\nbobs_number = contacts_dict[\"Bob\"]  # 즉시 접근!\n```\n\n- Sets: 고유성 강제하기 고유한 웹사이트 방문자들을 추적해야 하는가요? Set은 중복을 자동으로 제거합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\r\nunique_visitors = set()\r\nunique_visitors.add(\"192.168.1.100\")\r\nunique_visitors.add(\"124.58.23.5\")\r\nunique_visitors.add(\"192.168.1.100\")  # No duplicate added\r\n```\r\n\r\n파이썬 노하우 도구로는 순서가 있는 딕셔너리, 특별 큐를 위한 데크 등이 준비되어 있어요. 이 도구를 언제 사용해야 하는지 알아두면 좋은 스크립트와 훌륭한 스크립트를 구분할 수 있어요.\r\n\r\n# 실수 #3: 어둠 속에서 최적화하기\r\n\r\n코드가 느릴 것 같은 느낌을 익히 알고 계실 거예요. 그런데 그 이유가 왜 그런지는 모르겠다고요. 토치(Torch) 없이 누수하는 천장을 고치는 것과 유사합니다. 화가 난답니다! 여기서 프로파일러(Profiler)가 등장합니다.\r\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 예시: 예상하지 못한 범인\n\n가정해 봅시다. 피보나치 수를 계산하는 복잡한 함수가 있다고 상상해 보세요. 수학을 정밀하게 다듬기 위해 많은 노력을 기울였지만 여전히 속도가 느립니다. 결과를 파일에 기록하는 방식이 마치 은밀한 것처럼 작용하는 것이 병목일 수도 있습니다.\n\n## 해결책: cProfile이 구원해줍니다!\n\nPython의 내장 cProfile 모듈이 성능 탐지 요원이 됩니다. 다음은 그 사용법입니다:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nimport cProfile\n\ndef my_function():\n    # 프로파일링할 코드 입력\n\ncProfile.run('my_function()')\n```\n\n이 코드는 여러 가지 통계를 생성합니다. 주요 확인 사항은 다음과 같습니다:\n\n- ncalls: 함수가 호출된 횟수.\n- tottime: 함수 내에서 총 소요된 시간.\n- cumtime: tottime과 유사하지만 해당 함수 내에서 호출된 모든 함수에서 소요된 시간을 포함합니다.\n\n단서를 통해 정보 확인하기 위해 이러한 숫자들은 실제 병목 현상을 찾아내는 데 도움이 됩니다. 최적화 노력을 가장 큰 영향을 줄 수 있는 곳에 집중할 수 있도록 도와줍니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 오류 #4: DIY 함정\n\n여기 뭐든지 처음부터 만드는 충동이 강하죠. 이해해요! 하지만 때로는 발명의 바퀴를 다시 만드는 것은 비행기에 타지 않고 나라를 걸어 다니기로 결정하는 것과 같아요. Python은 극도로 최적화된 내장 함수로 여러분을 지원해줄 거예요.\n\n## 예시: 정렬해봅시다\n\n숫자 목록을 정렬해야 하나요? 버블 정렬을 구현해 볼 수도 있지만... Python의 sorted()를 사용해보는 것은 어떨까요?\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nmy_list = [5, 3, 1, 4, 2]\n\n# The long way (probably pretty slow)\ndef my_bubble_sort(list): \n   # ... your sorting code here\n\n# The Pythonic way\nsorted_list = sorted(my_list)\n```\n\n아마도 사용자 정의 정렬 알고리즘은 내장 알고리즘의 효율성에 거의 미치지 못할 것입니다.\n\n문제 해결법: 보물 창고를 발견하세요\n\nPython 표준 라이브러리는 개발자의 가장 좋은 친구입니다. 이러한 강력한 기능들을 알아보세요:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- itertools: 이터레이터 작업을 강화해주는 모듈 (효율적인 반복문을 위한 상급 루프라고 생각해보세요)\n- heapq: 힙(heap)을 관리하기 위한 모듈 (우선순위 큐에 관심 있으신가요?)\n- bisect: 정렬된 리스트를 유지하고 빠르게 동작하는 모듈\n\n기억해주세요: 내장 함수를 배우는 데 시간을 투자하면 나중에 최적화하는 데 시간을 절약할 수 있습니다.\n\n# 실수 #5: 하드 드라이브와 너무 많이 대화하기\n\n컴퓨터의 메모리(RAM)를 초고속 작업 영역, 하드 디스크를 도시 반대편에 있는 저장 창고로 생각해보세요. 파일에 접근하거나 수정할 때마다 전달원을 왕복하도록 하는 것과 같습니다. 너무 많은 왕복으로 코드가 대기하는 것처럼 느껴지기 시작합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 예시: 라인별 처리 지연\n\n대용량 로그 파일을 처리한다고 가정해보겠습니다:\n\n```js\nwith open(\"huge_log.txt\", \"r\") as file:\n    for line in file:\n        # 각 라인을 천천히 처리합니다\n```\n\n각 라인을 읽을 때마다 하드 드라이브에서 별도의 가져오기가 필요합니다. 아야!\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 문제 해결: 더 똑똑하게 일하기\n\n- 한 번에 모두 읽기(용량이 작을 때): 작은 파일의 경우, 때로는 파일 전체를 메모리에 한꺼번에 불러오는 것이 가장 빠릅니다:\n\n```js\nwith open(\"huge_log.txt\", \"r\") as file:\n    contents = file.read() \n    # 메모리에서 내용 처리\n```\n\n- 버퍼링으로 구조화: 세밀한 제어가 필요할 때, 버퍼링이 도움이 됩니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nwith open(\"huge_log.txt\", \"r\") as file:\n    while True:\n        chunk = file.read(4096)  # Read in chunks\n        if not chunk:\n            break\n        # Process the chunk\n```\n\n블록으로 생각하라, 바이트로는 아니다. \"창고\"로의 왔다갔다를 최소화하면 굉장한 영향을 미친다.\n\n# 결론: 파이썬에 올인하세요!\n\n자, 이제 우리의 속도 저하는 원인을 다시 한 번 요약해 봅시다:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 루프 과부화: NumPy로 벡터화를 활용해보세요.\n- 잘못된 도구: 룩업에는 사전, 고유성에는 세트... 현명하게 선택하세요!\n- 맹목적 최적화: 실제 병목 현상을 확인하기 위해 cProfile로 프로파일링하세요.\n- 직접 만들기 열풍: Python의 내장 함수들은 여러분의 친구입니다. 활용하세요!\n- 디스크 부하 너무 많이: 전략적으로 읽고 현명하게 버퍼링하세요.\n\n성능은 일회성 수정이 아닙니다. 이것을 마라톤 훈련처럼 생각해보세요: 코드를 프로파일링하고 핫 스팟을 최적화한 뒤, 이 과정을 반복하세요. 곧 여러분은 치타처럼 우아하게 실행되는 Python 스크립트를 가지게 될 것입니다.\n\n## 행동 요구\n\n이것을 실천하실 준비가 되셨나요? 여러분의 코드에서 이러한 실수를 찾아보세요! 여러분이 얻는 속도 향상에 대한 소식을 듣고 싶습니다. 결과를 댓글로 공유해주세요. 함께 최적화를 축하해봐요!\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 더 알고 싶나요?\n\n만약 이 내용이 도움이 되었다면, 더 많은 Python 팁과 트릭을 보기 위해 제 Medium을 팔로우해주세요. 좋은 리뷰는 언제나 환영이에요 😉. 그리고, 컨텐츠를 좋아하신다면, Patreon에 가입하여 독점 혜택을 받거나, 제가 Python에 대해 종일 이야기를 나누는 Discord 커뮤니티에 가입해보세요.\n\n## 즐거운 코딩되세요!\n\n# 쉽게 이해할 수 있는 English 🚀\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nIn Plain English 커뮤니티에 참여해 주셔서 감사합니다! 떠나시기 전에:\n\n- 작가를 클랩하고 팔로우해 주세요 👏️️\n- 팔로우하기: X | LinkedIn | YouTube | Discord | Newsletter\n- 다른 플랫폼 방문하기: Stackademic | CoFeed | Venture | Cubed\n- PlainEnglish.io에서 더 많은 콘텐츠 확인 가능합니다","ogImage":{"url":"/TIL/assets/img/2024-07-12-5PythonCodingErrorsThatAreKillingYourSpeedAndHowtoFixThemToday_0.png"},"coverImage":"/TIL/assets/img/2024-07-12-5PythonCodingErrorsThatAreKillingYourSpeedAndHowtoFixThemToday_0.png","tag":["Tech"],"readingTime":12},{"title":"FinalMLP 추천 시스템을 위한 간단하지만 강력한 Two-Stream MLP 모델 사용 방법","description":"","date":"2024-07-12 20:35","slug":"2024-07-12-FinalMLPASimpleyetPowerfulTwo-StreamMLPModelforRecommendationSystems","content":"\n\n이 게시물은 Rafael Guedes와 공동 저자로 작성되었습니다.\n\n# 소개\n\n세계는 모두가 원하는 것을 거의 한 번 클릭으로 모두 얻을 수 있는 디지턈 시대로 발전해 왔습니다. 접근성, 편의성 및 다양한 제공 효과는 소비자에게 새로운 도전과 함께 제공됩니다. 소비자가 옵션의 바다 속에서 검색하는 대신 맞춤 선택을 받을 수 있는 방법은 무엇일까요? 바로 추천 시스템이 여기에서 나타납니다.\n\n추천 시스템은 조직이 교차 판매와 장꼬 아이템의 판매를 증가시키고, 고객들이 가장 좋아하는 것을 분석하여 의사 결정을 개선하는 데 유용합니다. 뿐만 아니라, 고객의 과거 행동을 학습하여 특정한 고객 선호도에 따라 제품 집합을 순위 매길 수 있습니다. 추천 시스템을 사용하는 조직은 향상된 고객 경험을 제공함으로써 경쟁사보다 한 발 앞서 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 기사에서는 온라인 광고 및 추천 시스템에서의 클릭 수 예측을 향상시키기 위해 설계된 새로운 모델인 FinalMLP에 초점을 맞춥니다. Advanced features like gating and interaction aggregation layers를 갖춘 두 개의 다층 퍼셉트론(MLP) 네트워크를 통합하여, FinalMLP은 기존의 단일 스트림 MLP 모델과 고급 두 개의 스트림 CTR 모델보다 우수한 성능을 보입니다. 저자들은 FinalMLP의 효과를 벤치마크 데이터셋 및 실제 온라인 A/B 테스트를 통해 확인했습니다.\n\nFinalMLP의 상세한 내용과 작동 방식에 초점을 맞추면서, 공개 데이터셋에 적용하고 구현하는 방법에 대한 안내도 제공합니다. 우리는 책 추천 설정에서 FinalMLP의 정확도를 테스트하고, 저자들이 제안한 두 개의 스트림 아키텍처를 활용하여 예측을 설명하는 능력을 평가합니다.\n\n항상 그렇듯이, 코드는 저희의 GitHub에서 이용 가능합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# FinalMLP: 두 개의 MLP 위에 (F)eature gating 및 (IN)teraction (A)ggregation (L)ayers가 추가된 모델\n\nFinalMLP [1]은 DualMLP [2] 위에 구축된 두 개의 스트림 Multi-Layer Perceptron (MLP) 모델로, 다음과 같은 2가지 새로운 개념을 도입하여 향상시킵니다:\n\n- Gating 기반의 특징 선택은 두 스트림 간의 차이를 증가시켜, 각 스트림이 서로 다른 특징 세트로부터 서로 다른 패턴을 학습하도록 만듭니다. 예를 들어, 하나의 스트림은 사용자 특징을 처리하고, 다른 하나는 항목 특징에 중점을 둡니다.\n- Multi-Head Bilinear Fusion은 두 스트림에서 나온 출력을 결합하는 방법을 개선하여 특징 상호작용을 모델링합니다. 이는 덧셈 또는 연결과 같은 선형 연산에 의존하는 전통적인 방식을 사용할 때 발생하지 않을 수 있습니다.\n\n![이미지](/TIL/assets/img/2024-07-12-FinalMLPASimpleyetPowerfulTwo-StreamMLPModelforRecommendationSystems_1.png)\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 작동 방법은 무엇인가요?\n\n이전에 언급한 대로, FinalMLP는 서로 다른 관점에서 특징 상호 작용을 학습하는 두 개의 간단하고 병렬 MLP 네트워크로 구성된 Two-Stream CTR 모델입니다. 다음과 같은 주요 구성 요소로 구성되어 있습니다:\n\n특징 임베딩 레이어는 고차원 및 희소한 원시 특징을 밀집 숫자 표현으로 매핑하는 일반적인 방법입니다. 범주형, 숫자, 또는 다중 값이어도 각 특징은 임베딩 벡터로 변환되고 Feature Selection 모듈에 입력하기 전에 연결됩니다.\n\n범주형 특징은 원-핫 특징 벡터로 변환되며, 학습 가능한 임베딩 행렬에 의해 곱해져 어휘 크기 n과 임베딩 차원 d를 가진 임베딩을 생성합니다[3].\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n숫자형 특성은 1) 숫자 값을 이산형 특성으로 버킷팅하고 이를 범주형 특성으로 다루거나 2) 정규화된 스칼라 값 xj가 주어지면, 임베딩은 xj를 field j의 모든 특성에 대한 공유 임베딩 벡터 vj와 곱한 것으로 주어질 수 있습니다 [3].\n\n다중값 특성은 값 시퀀스를 하나의 길이가 k인 원-핫 인코딩 벡터로 변환한 다음 학습 가능한 임베딩 행렬과 곱하여 임베딩을 생성할 수 있습니다 [3].\n\n![이미지](/TIL/assets/img/2024-07-12-FinalMLPASimpleyetPowerfulTwo-StreamMLPModelforRecommendationSystems_2.png)\n\n특성 선택 레이어는 모델 예측에 중요한 영향을 미치도록 중요한 특성에 더 높은 영향을 미치도록 잡음이 많은 특성을 억제하기 위한 특성 중요도 가중치를 얻기 위해 사용됩니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n언급한 대로, FinalMLP는 게이팅 기반 특성 선택, 그리고 게이트 메커니즘을 갖춘 MLP를 사용합니다. 이 MLP는 임베딩을 입력으로 받아들이고, 입력과 동일한 차원의 가중치 벡터를 생성합니다. 특성 중요도 가중치는 시그모이드 함수를 가중치 벡터에 적용하여 [0, 2] 범위의 벡터를 생성하는 방식으로 얻어집니다. 가중된 특성은 특성 임베딩과 특성 중요도 가중치 사이의 요소별 곱셈을 통해 얻어집니다.\n\n이 과정을 통해 두 스트림 간 균질한 학습이 감소되어 특성 상호작용의 보다 보완적인 학습이 가능해집니다. 유저나 아이템 차원에 집중하도록 각 스트림에 독립적으로 적용되어 특성 입력을 구분합니다.\n\n![](/TIL/assets/img/2024-07-12-FinalMLPASimpleyetPowerfulTwo-StreamMLPModelforRecommendationSystems_3.png)\n\n양 스트림의 출력을 결합하여 최종 예측 확률을 얻기 위해 스트림 수준 융합 계층이 필요합니다. 일반적으로 두 출력을 결합하는 것은 합산 또는 연결 작업을 기반으로 합니다. 그러나 FinalMLP의 저자들은 선형 조합이 실패할 수 있는 특성 상호작용 정보를 얻기 위해 두 출력을 결합하는 데에 양선형 상호작용 집계 계층을 제안합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저자들은 어텐션 레이어에서 영감을 받아 멀티 헤드 바이리니어 퓨전 레이어로 발전시킨 바이리니어 퓨전을 소개했습니다. 이는 계산 복잡성을 줄이고 모델의 확장성을 향상시키는 데 사용됩니다.\n\n바이리니어 퓨전 방정식은 다음과 같이 구성됩니다:\n\n![image](/TIL/assets/img/2024-07-12-FinalMLPASimpleyetPowerfulTwo-StreamMLPModelforRecommendationSystems_4.png)\n\n여기서 σ는 시그모이드 함수, b는 편향 항목이며, o1은 한 스트림의 출력입니다. w1은 o1에 적용되는 학습 가능한 가중치이고, o2는 다른 스트림의 출력이며, w2는 o2에 적용되는 학습 가능한 가중치입니다. 마지막으로, w3는 특성 상호작용 정보를 추출하는 바이리니어 항목의 학습 가능한 매트릭스입니다. w3가 제로 매트릭스로 설정되면 전통적인 연결 퓨전으로 약화됩니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nBilinear Fusion과 Multi-Head Bilinear Fusion의 차이점은, 두 스트림에서 전체 벡터를 사용하는 대신 출력 o1과 o2를 k 개의 하위 공간으로 나눈다는 것입니다. 각 하위 공간에서 이루어진 bilinear 퓨전은 sigmoid 함수에 공급하여 최종 확률을 생성합니다.\n\n![그림](/TIL/assets/img/2024-07-12-FinalMLPASimpleyetPowerfulTwo-StreamMLPModelforRecommendationSystems_5.png)\n\n# FinalMLP로 도서 추천 모델 만들기\n\n이 섹션에서는 FinalMLP를 Kaggle의 Public Domain 라이선스(CC0)로 공개된 데이터셋에 구현할 것입니다. 이 데이터셋에는 사용자, 책, 그리고 사용자가 책에 부여한 등급에 관한 정보가 포함되어 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n데이터셋은 다음과 같이 구성되어 있습니다:\n\n- 사용자 ID — 사용자를 식별하는 ID\n- 위치 — 사용자의 도시, 주, 국가가 콤마로 구분된 문자열\n- 나이 — 사용자의 나이\n- ISBN — 책 식별자\n- 책 평점 — 특정 책에 대한 사용자의 평점\n- 책 제목 — 책의 제목\n- 책 저자 — 책의 저자\n- 출판 연도 — 책이 출판된 연도\n- 출판사 — 책을 출판한 편집자\n\n우리는 각 사용자에 대한 관련성을 기반으로 책을 순위 지정할 것입니다. 그 후에는 우리의 순위 지정과 실제 순위(사용자가 지정한 평점에 따라 책을 정렬함)를 비교하기 위해 정규화 된 할인 누적 이익 (nDCG)를 사용할 것입니다.\n\nnDCG는 결과의 순위를 측정하여 추천 시스템의 품질을 평가하는 메트릭스입니다. 각 항목의 관련성과 결과 목록에서의 위치를 고려하여 상위 순위에 더 많은 중요성을 부여합니다. nDCG는 낮은 순위 항목의 이익을 할인하는 할인 누적 이익(DCG)과 완벽한 순위를 감안한 이상적인 DCG (iDCG)를 비교하여 계산됩니다. 이 정규화된 점수는 0에서 1 사이의 범위를 가지며, 1은 이상적인 순위를 나타냅니다. 따라서 nDCG는 어떻게 시스템이 사용자에게 관련 정보를 효과적으로 제공하는지 이해하는 데 도움이 됩니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리는 먼저 라이브러리를 가져와요:\n\n```python\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nfrom sklearn.metrics import ndcg_score\nfrom sklearn.decomposition import PCA\nfrom sentence_transformers import SentenceTransformer\nimport os\nimport logging\nfrom fuxictr.utils import load_config, set_logger, print_to_json\nfrom fuxictr.features import FeatureMap\nfrom fuxictr.pytorch.torch_utils import seed_everything\nfrom fuxictr.pytorch.dataloaders import H5DataLoader\nfrom fuxictr.preprocess import FeatureProcessor, build_dataset\nimport src\nimport gc\nimport os\n```\n\n그런 다음, 세 개의 데이터 세트를로드하고 단일 데이터 세트로 병합합니다:\n\n```python\nbooks_df = pd.read_csv('data/book/Books.csv')\nusers_df = pd.read_csv('data/book/Users.csv')\nratings_df = pd.read_csv('data/book/Ratings.csv')\n\ndf = pd.merge(users_df, ratings_df, on='User-ID', how='left')\ndf = pd.merge(df, books_df, on='ISBN', how='left')\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그 후, 데이터에 문제점을 식별하기 위해 탐색적 데이터 분석을 수행합니다:\n\n- 사용자가 책에 평가를 내리지 않은 관측치를 제거합니다.\n\n```js\ndf = df[df['Book-Rating'].notnull()]\n```\n\n- 누락된 값 확인 및 누락된 Book-Author 및 Publisher를 알 수 없는 카테고리로 대체합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nprint(df.columns[df.isna().any()].tolist())\n\ndf['Book-Author'] = df['Book-Author'].fillna('unknown')\ndf['Publisher'] = df['Publisher'].fillna('unknown')\n```\n\n- Remove observations with missing information about the book.\n\n```js\ndf = df[df['Book-Title'].notnull()]\n```\n\n- Replace non-integer Year-of-Publication with null values.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\ndf['Year-Of-Publication'] = pd.to_numeric(df['Year-Of-Publication'], errors='coerce')\n```\n\n- 이상을 식별하려면 나이, 출판 연도 및 도서 평점 분포를 확인해보세요.\n\n```python\nplt.rcParams[\"figure.figsize\"] = (20, 3)\nsns.histplot(data=df, x='Age')\nplt.title('나이 분포')\nplt.show()\n\nsns.histplot(data=df, x='Year-Of-Publication')\nplt.title('출판 연도 분포')\nplt.show()\n\nsns.histplot(data=df, x='Book-Rating')\nplt.title('도서 평점 분포')\nplt.show()\n```\n\n![이미지](/TIL/assets/img/2024-07-12-FinalMLPASimpleyetPowerfulTwo-StreamMLPModelforRecommendationSystems_6.png)\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마침내, 데이터 정리를 다음과 같이 진행합니다:\n\n- 나이가 100 (오기로 보이는 값)인 경우, 나중에 처리할 결측값으로 대체합니다.\n- 데이터셋이 Kaggle에 발행된 시점인 2021년을 상한으로 제한하고, 발행년도가 0인 경우에는 나중에 처리할 결측값으로 대체합니다.\n- 사용자가 독서는 했지만 평점은 남기지 않은 경우 평점이 0인 관측치를 제거합니다.\n- 위치 정보에서 3가지 새로운 특성(도시, 주, 국가)를 생성합니다. 너무 노이즈가 많은 도시 정보는 사용하지 않습니다.\n- FinalMLP를 위한 이진 레이블을 생성합니다. 평점이 7보다 높은 책을 사용자에게 관련성 있는 것으로 간주합니다.\n\n```js\ndf['Age'] = np.where(df['Age'] > 100, None, df['Age'])\n\ndf['Year-Of-Publication'] = np.where(df['Year-Of-Publication'].clip(0, 2021) <= 0, None, df['Year-Of-Publication'])\ndf = df[df['Book-Rating'] > 0]\ndf['city'] = df['Location'].apply(lambda x: x.split(',')[0].strip()) # too noisy, we will not use\ndf['state'] = df['Location'].apply(lambda x: x.split(',')[1].strip())\ndf['country'] = df['Location'].apply(lambda x: x.split(',')[2].strip())\ndf['label'] = (df['Book-Rating'] > 7)*1\n```\n\n데이터셋을 정리하면, 랜덤으로 사용자의 70%를 훈련용, 10%를 검증용, 20%를 테스트용으로 나눠서 데이터를 분할합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 고유 사용자 목록 생성\nusers = df['User-ID'].unique()\n\n# 목록 섞기\nrandom.shuffle(users)\n# 학습용, 검증용 및 테스트용 사용자 목록 생성\ntrain_users = users[:int(0.7*len(users))]\nval_users = users[int(0.7*len(users)):int(0.8*len(users))]\ntest_users = users[int(0.8*len(users)):]\n# 학습, 검증 및 테스트 데이터프레임\ntrain_df = df[df['User-ID'].isin(train_users)]\nval_df = df[df['User-ID'].isin(val_users)]\ntest_df = df[df['User-ID'].isin(test_users)]\r\n```\n\n모델에 데이터를 제공하기 전에 데이터에 일부 전처리를 적용할 것입니다:\n\n텍스트 특성인 Book-Title에 대한 다국어 인코더를 사용하여 임베딩을 생성하고, 80%의 분산이 설명되도록 PCA를 사용하여 차원을 축소합니다.\n\n다국어 인코더를 사용하는 이유는 제목이 서로 다른 언어로 작성되기 때문입니다. 또한, 책이 다른 책보다 더 많은 사용자에 의해 읽혔을 경우 차원 축소에 편향이 주입되지 않도록 먼저 고유한 Book-Title을 추출합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 임베딩 생성\ntrain_embeddings = utils.create_embeddings(train_df.copy(), \"Book-Title\")\nval_embeddings = utils.create_embeddings(val_df.copy(), \"Book-Title\")\ntest_embeddings = utils.create_embeddings(test_df.copy(), \"Book-Title\")\n\n# PCA를 사용하여 차원 축소\ntrain_embeddings, pca = utils.reduce_dimensionality(train_embeddings, 0.8)\nval_embeddings = pca.transform(val_embeddings)\ntest_embeddings = pca.transform(test_embeddings)\n# 데이터프레임에 임베딩 추가\ntrain_df = utils.add_embeddings_to_df(train_df, train_embeddings, \"Book-Title\")\nval_df = utils.add_embeddings_to_df(val_df, val_embeddings, \"Book-Title\")\ntest_df = utils.add_embeddings_to_df(test_df, test_embeddings, \"Book-Title\")\n```\n\n숫자형 특성의 결측값은 중앙값으로 채우고 MinMaxScaler를 사용하여 데이터를 정규화합니다.\n\n```js\n# 숫자형 열 설정\nNUMERICAL_COLUMNS = [i for i in train_df.columns if \"Book-Title_\" in i] + ['Age', 'Year-Of-Publication']\n\n# 전처리 파이프라인 정의 및 데이터 변환\npipe = utils.define_pipeline(NUMERICAL_COLUMNS)\ntrain_df[NUMERICAL_COLUMNS] = pipe.fit_transform(train_df[NUMERICAL_COLUMNS])\nval_df[NUMERICAL_COLUMNS] = pipe.transform(val_df[NUMERICAL_COLUMNS])\ntest_df[NUMERICAL_COLUMNS] = pipe.transform(test_df[NUMERICAL_COLUMNS])\n```\n\nFinalMLP에 제공할 준비가 된 모든 데이터로 dataset_config.yaml 및 model_config.yaml 두 개의 yaml 구성 파일을 만들어야 합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\ndataset_config.yaml 파일은 모델에서 사용할 feature들을 정의하는 역할을 합니다. 또한 이들의 데이터 유형을 정의하고(Embedding 레이어에서 다르게 처리됨) 훈련, 검증, 테스트 세트의 경로를 정의합니다. 아래는 구성 파일의 주요 부분을 확인할 수 있습니다:\n\n\nFinalMLP_book:\n    data_root: ./data/book/\n    feature_cols:\n    -   active: true\n        dtype: float\n        name: [Age, Book-Title_0, Book-Title_1, Book-Title_2, Book-Title_3, Book-Title_4, Book-Title_5, Book-Title_6, Book-Title_7,\n        Book-Title_8, ...]\n        type: numeric\n    -   active: true\n        dtype: str\n        name: [Book-Author, Year-Of-Publication, Publisher, state, country]\n        type: categorical\n        fill_na: unknown\n    label_col: {dtype: float, name: label}\n    min_categr_count: 1\n    test_data: ./data/book/test.csv\n    train_data: ./data/book/train.csv\n    valid_data: ./data/book/valid.csv\n\n\nmodel_config.yaml 파일은 모델의 하이퍼파라미터를 설정하는 역할을 합니다. 사용자 feature를 처리할 스트림과 아이템 feature를 처리할 스트림을 정의해야 합니다. 파일은 다음과 같이 정의되어야 합니다:\n\n\nFinalMLP_book:\n dataset_id: FinalMLP_book\n fs1_context: [Age, state, country]\n fs2_context: [Book-Author, Year-Of-Publication, Publisher, Book-Title_0, Book-Title_1, Book-Title_2, Book-Title_3,\n     Book-Title_4, Book-Title_5, ...]\n model_root: ./checkpoints/FinalMLP_book/\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n파이썬으로 돌아가서 최근에 생성된 설정 파일을 로드합니다. 그런 다음, 특성 매핑을 만듭니다 (즉, 각 범주형 특성에 몇 가지 카테고리가 있는지, 다른 특성에서 누락된 값이 있을 경우 어떻게 대체해야 하는지 등). CSV 파일을 h5 파일로 변환합니다.\n\n```python\n# 모델 및 데이터셋 구성 가져오기\nexperiment_id = 'FinalMLP_book'\nparams = load_config(f\"config/{experiment_id}/\", experiment_id)\nparams['gpu'] = -1 # CPU\nset_logger(params)\nlogging.info(\"Params: \" + print_to_json(params))\nseed_everything(seed=params['seed'])\n\n# 특성 매핑 생성 및 데이터를 h5 형식으로 변환\ndata_dir = os.path.join(params['data_root'], params['dataset_id'])\nfeature_map_json = os.path.join(data_dir, \"feature_map.json\")\nif params[\"data_format\"] == \"csv\":\n    # 특성 매핑 빌드 및 h5 데이터 변환\n    feature_encoder = FeatureProcessor(**params)\n    params[\"train_data\"], params[\"valid_data\"], params[\"test_data\"] = \\\\\n        build_dataset(feature_encoder, **params)\nfeature_map = FeatureMap(params['dataset_id'], data_dir)\nfeature_map.load(feature_map_json, params)\nlogging.info(\"Feature specs: \" + print_to_json(feature_map.features))\n```\n\n이후에 모델의 훈련 프로세스를 시작할 수 있습니다.\n\n```python\nmodel_class = getattr(src, params['model'])\nmodel = model_class(feature_map, **params)\nmodel.count_parameters() # 모델에서 사용하는 매개변수 수를 출력\n\ntrain_gen, valid_gen = H5DataLoader(feature_map, stage='train', **params).make_iterator()\nmodel.fit(train_gen, validation_data=valid_gen, **params)\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n마침내 보이지 않는 데이터를 예측할 수 있게 되었습니다. 관측치들의 점수를 얻기 위해 배치 크기를 1로 변경하기만 하면 됩니다.\n\n```js\n# 관측치들의 점수를 얻기\nparams['batch_size'] = 1\ntest_gen = H5DataLoader(feature_map, stage='test', **params).make_iterator()\ntest_df['score'] = model.predict(test_gen)\n```\n\n우리는 한 명의 고객을 선택했는데, 이 고객은 여러 권의 책을 평가하고 각 책에 대해 다른 평점을 매겨서 맞춤 순위를 설정할 수 있도록 했습니다. nDCG 점수는 0.986362로 나타났는데, 2권의 책을 1위에서 잘못 배치했기 때문입니다.\n\n우리는 FinalMLP를 평가하기 위해 Recall을 사용했습니다. Recall은 시스템이 전체 중에서 모든 관련 항목을 식별하는 능력을 측정하는 지표로, 전체 관련 항목 중 검색된 관련 항목의 비율로 나타냅니다. Recall@K와 같이 Recall@3을 지정하면 시스템이 상위 K개의 추천 내에서 관련 항목을 식별하는 능력에 초점을 맞춥니다. 이것은 사용자들이 주로 상위 추천에 주목하는 추천 시스템을 평가하는 데 중요합니다. K(예: 3)의 선택은 일반적인 사용자 행동과 애플리케이션 맥락에 따라 달라집니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 고객의 Recall@3을 살펴보면, 상위 3위 안에 가장 관련성 있는 책이 세 권 모두 들어있기 때문에 100%입니다.\n\n```js\ntrue_relevance = np.asarray([test_df[test_df['User-ID'] == 1113]['Book-Rating'].tolist()])\ny_relevance = np.asarray([test_df[test_df['User-ID'] == 1113]['score'].tolist()])\n\nndcg_score(true_relevance, y_relevance)\n```\n\n남은 테스트 세트에 대한 nDCG 점수를 계산하고, Figure 7에서 FinalMLP 성능을 CatBoost Ranker와 비교했습니다. 두 모델 모두 잘 수행했지만, 이 테스트 세트에서 FinalMLP가 조금 더 우수한 성능을 보였습니다. 사용자 당 평균 nDCG가 0.963298인 반면 CatBoost Ranker는 0.959977에 그쳤습니다.\n\n<img src=\"/TIL/assets/img/2024-07-12-FinalMLPASimpleyetPowerfulTwo-StreamMLPModelforRecommendationSystems_7.png\" />\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n해석 가능성 측면에서 이 모델은 특성 선택을 수행하여 가중치 벡터를 추출할 수 있게 합니다. 그러나 각 특성의 중요성을 해석하고 이해하는 것은 간단하지 않습니다. 임베딩 레이어 이후에는 930차원 벡터가 생성되어 원래 특성으로 재매핑하기가 어려워집니다. 그럼에도 불구하고, 이전에 언급된 선형 항으로 주어진 선형 처리 후 각 스트림의 출력의 절대값을 추출함으로써 각 스트림의 중요성을 이해해 볼 수 있습니다.\n\n이를 위해 InteractionAggregation 모듈을 변경하고 각 단계 후에 선형 변환된 값 추출을 위해 다음 코드 라인을 추가해야 합니다:\n\n```js\n...     \n    self.x_importance = []\n    self.y_importance = []\n  def forward(self, x, y):\n          self.x_importance.append(torch.sum(torch.abs(self.w_x(x))))\n          self.y_importance.append(torch.sum(torch.abs(self.w_y(y))))\n...\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n한 번 훈련을 받으면, 각 스트림의 선형 변환 결과에서 절대 값을 예측하고 플롯할 수 있습니다. 그림 8에 보여진 것처럼, 상품 스트림이 사용자 스트림보다 중요성이 높습니다. 이는 상품에 대한 기능이 훨씬 많기 때문이지만 사용자 특성이 상당히 일반적이기 때문에 발생합니다.\n\n![이미지](/TIL/assets/img/2024-07-12-FinalMLPASimpleyetPowerfulTwo-StreamMLPModelforRecommendationSystems_9.png)\n\n# 결론\n\n추천 시스템은 사용자 경험을 향상시켜 맞춤형 추천을 제공하며, 성장과 혁신을 이끄는 데이터 기반 의사 결정을 기관에 제공하여 사용자 경험을 향상시킵니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 기사에서는 추천 시스템용으로 개발된 가장 최근 모델 중 하나를 소개했습니다. FinalMLP는 두 개의 독립 네트워크를 가진 딥 러닝 모델입니다. 각 네트워크는 사용자와 항목 이 두 가지 다른 관점 중 하나에 중점을 둡니다. 각 네트워크로부터 학습된 다른 패턴은 그 다음 각 네트워크의 학습 내용을 결합하는 책합층에 공급됩니다. 사용자-항목 쌍 상호 작용의 단일 뷰를 생성하여 최종 점수를 생성합니다. 이 모델은 CatBoost Ranker를 이겼으며 우리의 사용 사례에서 잘 수행했습니다.\n\n알고리즘 선택은 해결하려는 문제와 데이터셋에 따라 다를 수 있음을 유의해 주세요. 항상 여러 방법을 상호 비교하는 것이 좋은 실천 방법입니다. 또한 xDeepFM, AutoInt, DHEN 또는 DLRM을 테스트하는 것도 고려할 수 있습니다.\n\n# 내 소개\n\n인공지능 분야의 시리얼 기업가 및 리더입니다. 비즈니스를 위한 인공지능 제품을 개발하고 인공지능에 초점을 맞춘 스타트업에 투자합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n창립자 @ ZAAI | LinkedIn | X/Twitter\n\n# 참고 문헌\n\n[1] Kelong Mao, Jieming Zhu, Liangcai Su, Guohao Cai, Yuru Li, Zhenhua Dong. FinalMLP: CTR 예측을 위한 향상된 이차원 MLP 모델. arXiv:2304.00902, 2023.\n\n[2] Jiajun Fei, Ziyu Zhu, Wenlei Liu, Zhidong Deng, Mingyang Li, Huanjun Deng, Shuo Zhang. DuMLP-Pin: 집합 특성 추출을 위한 이중-MLP-내적 불변 네트워크. arXiv:2203.04007, 2022.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n[3] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, Xiuqiang He. BARS-CTR: Open Benchmarking for Click-Through Rate Prediction. arXiv:2009.05794, 2020.","ogImage":{"url":"/TIL/assets/img/2024-07-12-FinalMLPASimpleyetPowerfulTwo-StreamMLPModelforRecommendationSystems_0.png"},"coverImage":"/TIL/assets/img/2024-07-12-FinalMLPASimpleyetPowerfulTwo-StreamMLPModelforRecommendationSystems_0.png","tag":["Tech"],"readingTime":22},{"title":"범주형 및 수치형 데이터를 위한 Gower 거리 설명","description":"","date":"2024-07-12 20:33","slug":"2024-07-12-GowersDistanceforMixedCategoricalandNumericalData","content":"\n\n# 거리 측정이란 무엇인가요?\n\n아마도 맨해튼 거리 또는 유클리드 거리에 대해 들어보았을 것입니다. 이들은 두 주어진 데이터 포인트가 얼마나 멀리 떨어져 있는지 (또는 얼마나 다른지)에 대한 정보를 제공하는 두 가지 다른 측정 기준입니다.\n\n![image](/TIL/assets/img/2024-07-12-GowersDistanceforMixedCategoricalandNumericalData_0.png)\n\n간단히 말해서, 유클리드 거리는 점 A에서 점 B까지의 최단 거리입니다. 맨해튼 거리는 x와 y 좌표 간의 절대적인 차이들의 합을 계산하고, 대각선 방향으로 움직일 수 없고 위, 아래, 왼쪽 또는 오른쪽으로만 이동할 수 있는 격자 상에 위치한 것처럼 두 점 사이의 거리를 찾습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n거리 측정 항목은 종종 k-평균 클러스터링과 같은 클러스터링 알고리즘의 기초를 이룹니다. 이 알고리즘은 유클리드 거리를 사용합니다. 데이터 포인트 간의 유사성 또는 차이를 먼저 알아야 하므로 이 점이 타당합니다.\n\n## 2점 사이의 거리 계산하기\n\n이 프로세스를 보여주기 위해 유클리드 거리를 사용한 예제를 통해 시작하겠습니다.\n\n![image](/TIL/assets/img/2024-07-12-GowersDistanceforMixedCategoricalandNumericalData_1.png)\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위에 그린 차트를 사용하면 점 A의 좌표가 (50,50)이고 점 B의 좌표가 (300,500)이라고 가정해 봅시다:\n\n```js\nd(A,B) = sqrt ((300-50)^2 + (500-50)^2))\nd(A,B) = sqrt (62,500 + 202,500)\nd(A,B) ≈ 514.78\n```\n\n정말 간단하죠. 이제 Python 데이터 세트에서 데이터프레임을 사용하여 2개의 데이터 포인트(행) 사이에 유사도 점수를 계산하고 싶다면 어떻게 해야 할지 살펴봅시다.\n\n## 인구 조사 데이터에서의 거리 (Distances in Census Income data)\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n앞으로 제 모든 예시를 설명하는 데에 UCI Machine Learning Repository에서 제공하는 인구 조사 소득 데이터 세트 (CC BY 4.0)를 사용할 것입니다.\n\n이 데이터 세트는 다양한 인구 특징(나이, 인종, 성별, 직업 등)와 개인이 '$50K 미만' 또는 '$50K 이하'의 소득을 갖고 있는지를 나타내는 바이너리 대상 변수를 가지고 있는 분류 데이터 세트입니다.\n\n이 데이터 세트를 통계 작업에 사용하지는 않겠지만, 유사한 특징을 가진 데이터 포인트 간에는 타겟 변수에 대한 유사성도 보일 수 있기 때문에 거리 측정 지표를 시연하는 데 좋은 예시가 됩니다.\n\n이 데이터를 직접 가져오려면 다음 코드를 실행하세요:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nfrom ucimlrepo import fetch_ucirepo\n\n# 데이터셋 불러오기\ncensus_income = fetch_ucirepo(id=20)\n\n# 데이터 (판다스 데이터프레임 형태)\nX = census_income.data.features\ny = census_income.data.targets\n\n# 변수 정보\nprint(census_income.variables)\n```\n\n이 데이터셋에 포함된 특성 변수 중 하나는 \"나이(Age)\"입니다. 이 경우 나이는 17세에서 90세까지의 숫자 변수입니다. 다른 숫자 변수로는 \"자본이득(Capital gains)\"이 있습니다. 이 두 변수는 사람의 총 소득과 관련이 깊을 것으로 예상됩니다.\n\n이제 이 두 열을 가지고 데이터 포인트/행에 대한 거리 추정을 얻을 수 있는지 확인해 봅시다.\n\n<img src=\"/TIL/assets/img/2024-07-12-GowersDistanceforMixedCategoricalandNumericalData_2.png\" />\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 두 데이터 포인트 사이의 거리를 찾으려면 위에서 정의한 식에 값을 입력해야 합니다.\n\n각 열은 차원(x 또는 y)이 되고 각 행은 숫자 1 또는 2가 됩니다.\n\n따라서 나이 열은 x가 되고 값(39, 50)은 x1 및 x2가 됩니다. 자본 이득 열은 y1 및 y2 값에 해당합니다.\n\n```js\nd(Row1, Row2) = sqrt((50-39)^2 +\n(2174-0)^2))\n\nd(Row1, Row2) = 2174.027\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n총 이동 거리가 자본 이득 열의 차이와 거의 동일한 것을 알 수 있습니다. 이는 값의 연령 범위가 값의 자본 이익 범위보다 훨씬 작기 때문입니다. 따라서 경우에 따라 유클리드 거리를 계산하기 전에 StandardScaler와 같은 것을 사용하여 데이터를 스케일링해야 할 수도 있습니다.\n\n# Gower 거리: 방정식에 범주형 변수 추가\n\n이제 만약 이 첫 번째 두 데이터 포인트 사이의 거리를 계산하려면 “workclass”와 “education”과 같은 범주형 변수도 포함해서 어떻게 해야 할까요?\n\n여기서 Gower 거리가 중요합니다. Gower 거리는 숫자형 및 범주형 특성에 대해 서로 다른 거리 계산을 수행하여 특성간 유사성의 가중 평균을 취하여 2개 데이터 포인트 사이의 점수를 계산합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n2가지 범주형 특성 간 유사도 점수를 계산할 때, 식은 매우 간단합니다. 값이 동일한 경우 점수는 1입니다. 다른 경우에는 점수가 0입니다.\n\n두 객체(데이터 포인트) i와 j를 비교할 때, 수치/연속적인 특성 k를 비교하는 경우, 유사도 점수는 다음과 같습니다:\n\n\\[ \\frac{|x_{ik} - x_{jk}|}{R(k)} \\]\n\n여기서 R(k)는 특성 k의 범위를 나타냅니다. 값의 차이의 절대값은 범위로 나눠줌으로써 특성을 정규화하고 0과 1 사이의 값으로 얻습니다. (이는 매우 큰 값이 포함된 특성이 방정식을 지배하는 문제를 해결하는 데 도움이 될 것입니다.)\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 각 feature에 대한 점수가 있으므로, p개의 feature가 있는 두 개의 객체 i와 j에 대한 전체 Gower 거리가 계산됩니다:\n\n\n… where w(ijk) is the weight for that feature (Default is 1).\n\n\n## Python 구현\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n파이썬 구현은 배열 내 모든 객체를 서로 비교하여 Gower 거리를 반환하는 행렬을 생성합니다.\n\n파이썬에서 이 작업이 어떻게 이루어지는지 보시려고 작은 예제를 소개하겠습니다. 그리고 이것이 대규모 데이터셋에서 어떻게 작동하는지 살펴보겠습니다.\n\n```python\n# 먼저 \"pip install gower\"를 실행했다고 가정합니다.\nimport gower\nimport numpy as np\n\n# 특성 정의\nfeatures = ['age', 'capital-gain', 'education', 'workclass']\n\n# workclass 열에 일부 누락된 데이터가 \"?\" 형태로 포함되어 있기 때문에\n# 이를 먼저 제거했습니다.\nX = X.loc[X['workclass']!='?']\n\n# 가능한 다른 열에서는 널 값을 제거하고 인덱스를 재설정했습니다.\nX.dropna(inplace=True)\nX.reset_index(inplace=True)\n\n# 정의된 특성만 선택합니다.\nX = X[features]\n\n# 처음 2개 행만 포함하는 작은 데이터프레임 생성\n# 이 두 행을 Gower 거리를 사용하여 비교할 것입니다.\nsmall_X = X.iloc[0:2,:]\n\n# 나이 열을 부동 소수점으로 변환합니다. 왜냐하면 자본 이익은 실수이며,\n# 숫자 열은 동일한 데이터 유형이어야 합니다.\nsmall_X['age'] = small_X['age'].astype(float)\n\n# gower 거리 행렬 생성\n# cat_features는 각 열의 인덱스에 해당하는 목록입니다.\n# 0은 열이 숫자임을 나타내고, 1은 범주형임을 나타냅니다. 처음 2개 열 (나이 및 자본 이익)\n# 는 0으로 표시되고, 마지막 2개 열 (교육, 직업)은 1로 표시됩니다.\ngower.gower_matrix(np.asarray(small_X),cat_features=[0,0,1,1])\n```\n\ngower.gower_matrix를 호출하면 이 배열이 반환됩니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\narray([[0. , 1.5],\n       [1.5, 0. ]], dtype=float32)\n```\n\n각 요소는 다음을 나타냅니다:\n\n- 요소 [0,0] = 행 1과 행 1 간의 거리 점수\n- 요소 [0,1] = 행 1과 행 2 간의 거리 점수\n- 요소 [1,0] = 행 2와 행 1 간의 거리 점수\n- 요소 [1,1] = 행 2와 행 2 간의 거리 점수\n\n행 1을 행 1과 비교하면 features 측면에서 동일함을 보여주기 때문에 거리는 0으로 표시됩니다. 행 2를 행 2와 비교한 결과도 마찬가지입니다. 행 2를 행 1과 비교하면 행 1을 행 2와 비교했을 때와 동일한 결과가 나오므로 이 요소에서 점수가 동일합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 Gower 거리의 구성 요소를 이해했으니 전체 데이터 세트에 이 gower.gower_matrix 함수를 적용해 봅시다.\n\n```js\n# 참고: 인구조사 데이터 집합은 40,000개 이상의 행이 있습니다. 이 작업은 \n# 계산 비용이 매우 높을 수 있습니다. 저는 제 자신의 코드에서 이 작업을 \n# 첫 5000개 행에 대해서만 실행했지만, 데이터 세트에 따라 다를 수 있습니다.\ngower.gower_matrix(np.asarray(X),cat_features=[0,0,1,1])\n```\n\n이렇게 하면 각 행을 모든 다른 행들과 비교한 Gower 거리 점수 행렬이 반환됩니다.\n\nGower 거리의 효과를 더 테스트하기 위해, 첫 번째 레코드와 가장 유사한 10개 레코드를 해당 특성을 사용하여 찾은 다음, 그들의 대상 값(소득 수준)이 유사한지 확인해 보겠습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 데이터 세트의 첫 번째 행을 가져와 나머지 데이터와 비교합니다. 상위 10개 값의 인덱스를 반환합니다\ngower_topn_index = gower.gower_topn(X.iloc[0:1,:], X.iloc[:,], n = 10)['index']\n\n# 원본 df에서 인덱스를 쿼리합니다\nX.iloc[gower_topn_index]\n```\n\n다음은 제 결과입니다:\n\n<img src=\"/TIL/assets/img/2024-07-12-GowersDistanceforMixedCategoricalandNumericalData_5.png\" />\n\n여기서 첫 번째 레코드와 가장 유사한 레코드는 해당 레코드 자체입니다. 이는 Gower가 각각의 레코드를 모든 레코드와 비교하기 때문입니다. 레코드 1과 가장 유사한 상위 n개의 레코드를 가져오고 레코드 1을 제외한 결과를 얻고 싶다면, 초기에 상위 n+1을 가져와 X.iloc[gower_topn_index][1:]를 쿼리하면 해당 최상위 레코드를 결과에서 제외시킬 수 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n만약 y.loc[gower_topn_index]을 호출하면, 첫 번째 행에 대한 가장 유사한 상위 10개 레코드의 대응하는 타겟 값을 볼 수 있습니다.\n\n![image](/TIL/assets/img/2024-07-12-GowersDistanceforMixedCategoricalandNumericalData_6.png)\n\n여기서 가장 유사한 상위 10개 레코드 중에서 9개 중 6개가 첫 번째 레코드의 수입(≤50K)과 일치했음을 볼 수 있습니다. 9개 중 3개만 일치하지 않고 '50K'를 만들었습니다. 이것은 우리가 4가지 기능만 가지고 있었기 때문에 꽤 괜찮은 결과입니다!\n\n## 기억해 둘 사항\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다른 기계 학습 알고리즘과 마찬가지로 특성을 추가할수록 Gower metric의 정확도가 높아질 것으로 예상됩니다. 이는 레코드가 더욱 유사해진다는 것을 의미합니다. 이때 주의할 점은 범주형 특성이 종종 (연속 값이 아니라 0 또는 1이 될 것이기 때문에 0과 1 사이의 연속적인 값보다) 수치적 특성보다 더욱 중요하다는 것입니다.\n\n이를 균형있게 유지하기 위해 수치적 특성에 더 높은 가중치를 부여하여 실험할 수 있습니다 (weight 인자를 사용).\n\nweight 인자는 cat_features 인자와 유사하게 작동합니다. 여러분은 목록을 전달하고 각 열에 대해 가중치를 지정합니다. 예를 들어 weight = [2, 2, 1, 1]로 설정하면 수치적인 나이와 자본 이득이 포함된 처음 2개 열이 (마지막 2개 범주형 특성보다 최종 거리 공식에서 두 배 높이 가중됩니다.\n\n## 결론\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위에서 볼 수 있듯이, 고워 거리는 혼합 데이터 유형의 기능을 포함하는 두 개체 간의 거리를 계산하는 강력한 메트릭입니다. 이는 단순히 가장 유사한 레코드를 찾는 것에서부터 클러스터링 알고리즘에 사용될 때까지 다양한 용도가 있습니다. 더 많은 정보와 예제는 여기에서 찾을 수 있습니다.\n\n## 읽어 주셔서 감사합니다","ogImage":{"url":"/TIL/assets/img/2024-07-12-GowersDistanceforMixedCategoricalandNumericalData_0.png"},"coverImage":"/TIL/assets/img/2024-07-12-GowersDistanceforMixedCategoricalandNumericalData_0.png","tag":["Tech"],"readingTime":11},{"title":"Pytorch에서 텐서 고급 선택 방법","description":"","date":"2024-07-12 20:32","slug":"2024-07-12-AdvancedSelectionfromTensorsinPytorch","content":"\n\n가끔은 파이토치(Pytorch)로 고급 색인 및 선택을 해야 할 때가 있습니다. 예를 들어 \"텐서 A에서 텐서 B에 지정된 색인을 따라 요소를 선택하는 방법\"과 같은 질문에 답변할 수 있습니다.\n\n이 게시물에서는 이러한 작업을 수행하는 가장 일반적인 세 가지 방법인 torch.index_select, torch.gather 및 torch.take를 소개합니다. 우리는 이러한 방법을 자세히 설명하고 서로 비교합니다.\n\n![이미지](/TIL/assets/img/2024-07-12-AdvancedSelectionfromTensorsinPytorch_0.png)\n\n솔직히 말해서, 이 게시물의 동기 중 하나는 어떤 기능을 언제 사용해야 하는지를 잊어버려 구글링하거나 스택 오버플로우를 찾아보거나, 내 의견으로는 비교적 간단하고 그리 도움이 되지 않는 공식 설명서를 보는 것 때문입니다. 그래서 여기에서는 언급한 바와 같이 이러한 기능에 대해 심층적으로 알아보겠습니다: 언제 어떻게 사용해야 하는지 동기부여를 해주고, 2D 및 3D에서 예제를 제공하며, 결과적으로 선택된 요소를 그래픽으로 표시할 것입니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 게시물이 해당 기능에 대한 명확성을 가져다 주고 추가 탐구가 필요 없도록 도와줄 것을 희망합니다 — 읽어 주셔서 감사합니다!\n\n그리고 이제, 더 이상 어색함 없이 함수를 하나하나 살펴보겠습니다. 먼저, 모든 것을 2D 예제로 시작하여 결과 선택을 시각화한 후, 3D에서 조금 더 복잡한 예제로 넘어갑니다. 그 이후에는 간단한 파이썬에서 실행된 작업을 재구현합니다 — 이를 통해 이러한 함수가 무엇을 하는지에 대한 다른 정보원인 유사한 의사코드를 참고할 수 있게 됩니다. 마지막으로, 함수들과 해당 차이점들을 표로 요약합니다.\n\n# torch.index_select\n\ntorch.index_select는 한 차원을 따라 요소를 선택하는 동시에 다른 차원을 유지합니다. 다시 말해, 다른 모든 차원의 모든 요소를 유지한 채 목표 차원에서 인덱스 텐서를 따라 요소를 선택합니다. 우리는 2D 예제로 이를 보여줍니다. 여기서는 1차원을 따라 선택하겠습니다:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nnum_picks = 2\n\nvalues = torch.rand((len_dim_0, len_dim_1))\nindices = torch.randint(0, len_dim_1, size=(num_picks,))\n# [len_dim_0, num_picks]\npicked = torch.index_select(values, 1, indices)\n```\n\n그 결과 텐서는 형태 [len_dim_0, num_picks]를 가집니다: 차원 0을 따라 각 요소에 대해 차원 1에서 동일한 요소를 선택했습니다. 이를 시각화해 봅시다:\n\n![image](/TIL/assets/img/2024-07-12-AdvancedSelectionfromTensorsinPytorch_1.png)\n\n이제 세 가지 차원으로 이동합니다. 이것은 기계 학습 / 데이터 과학의 세계에 더 가까워지며, 형상이 [batch_size, num_elements, num_features]인 텐서를 상상해 보겠습니다. num_elements 요소가 있고 num_feature 특징이 있으며 모든 것이 배치 처리됩니다. torch.index_select를 사용하여 모든 배치 / 특징 조합에 대해 동일한 요소를 선택할 수 있습니다:\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nimport torch\n\nbatch_size = 16\nnum_elements = 64\nnum_features = 1024\nnum_picks = 2\n\nvalues = torch.rand((batch_size, num_elements, num_features))\nindices = torch.randint(0, num_elements, size=(num_picks,))\n# [batch_size, num_picks, num_features]\npicked = torch.index_select(values, 1, indices)\n```\n\n어떤 사람들은 index_select가 하는 일을 코드 형태로 이해하는 것을 선호할 수 있습니다. 따라서, 간단한 for 루프를 사용하여 이 함수를 다시 구현할 수 있는 방법을 설명해 드리겠습니다:\n\n```js\npicked_manual = torch.zeros_like(picked)\nfor i in range(batch_size):\n    for j in range(num_picks):\n        for k in range(num_features):\n            picked_manual[i, j, k] = values[i, indices[j], k]\n\nassert torch.all(torch.eq(picked, picked_manual))\n```\n\n# torch.gather\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음으로 torch.gather로 이동해 보겠습니다. gather는 index_select와 유사하게 작동하지만 이제 원하는 차원에서 요소 선택이 다른 차원에 따라 종속적입니다. 다시 말해 우리의 머신러닝 예제를 재사용하는 것입니다: 매 batch 인덱스마다, 그리고 매 feature마다, \"element\" 차원에서 다른 요소를 선택할 수 있습니다. 하나의 텐서에서 다른 텐서의 인덱스를 따르면서 요소를 선택합니다.\n\nML 프로젝트를 진행하면서 이런 사용 사례를 자주 만나게 되었는데, 하나의 구체적인 예시는 어떤 조건에 따라 트리에서 노드를 선택하고, 각 노드가 일부 feature로 지정된다고 할 때, 배치 차원에 선택할 요소를 넣는 인덱스 선택 행렬을 생성하고 해당 값을 feature 차원을 따라 반복하는 것입니다. 즉, 각 batch 인덱스마다 조건에 따라 다른 요소를 선택할 수 있으며, 우리의 예시에서 이 조건은 배치 인덱스에만 의존하지만 feature 인덱스에도 의존할 수 있습니다.\n\n하지만 먼저, 2D 예제로 다시 시작해 보겠습니다:\n\n```js\nnum_picks = 2\n\nvalues = torch.rand((len_dim_0, len_dim_1))\nindices = torch.randint(0, len_dim_1, size=(len_dim_0, num_picks))\n# [len_dim_0, num_picks]\npicked = torch.gather(values, 1, indices)\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이를 시각화할 때, 우리는 이제 선택이 직선으로 특징지어지지 않고, 차원 0의 각 인덱스마다 차원 1의 다른 요소가 선택된다는 것을 관찰할 수 있습니다:\n\n![image](/TIL/assets/img/2024-07-12-AdvancedSelectionfromTensorsinPytorch_2.png)\n\n이를 바탕으로 세 가지 차원으로 이동하고, 또한 Python 코드를 통해 이 선택을 다시 구현해보겠습니다:\n\n```python\nimport torch\n\nbatch_size = 16\nnum_elements = 64\nnum_features = 1024\nnum_picks = 5\nvalues = torch.rand((batch_size, num_elements, num_features))\nindices = torch.randint(0, num_elements, size=(batch_size, num_picks, num_features))\npicked = torch.gather(values, 1, indices)\n\npicked_manual = torch.zeros_like(picked)\nfor i in range(batch_size):\n    for j in range(num_picks):\n        for k in range(num_features):\n            picked_manual[i, j, k] = values[i, indices[i, j, k], k]\n\nassert torch.all(torch.eq(picked, picked_manual))\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## torch.take\n\ntorch.take는 소개된 세 가지 기능 중 가장 이해하기 쉬운 것일 수 있습니다. 이 함수는 입력 텐서를 평평하게 취급하고 이 목록에서 요소를 선택합니다. 예를 들어, 입력 텐서의 모양이 [4, 5]이고 인덱스 6과 19를 선택할 때, 우리는 평평한 텐서의 6번째와 19번째 요소를 얻습니다. 즉, 제 2행의 2번째 요소와 맨 마지막 요소를 의미합니다.\n\n2D 예시:\n\n```js\nnum_picks = 2\n\nvalues = torch.rand((len_dim_0, len_dim_1))\nindices = torch.randint(0, len_dim_0 * len_dim_1, size=(num_picks,))\n# [num_picks]\npicked = torch.take(values, indices)\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래에서 볼 수 있듯이, 우리는 이제 두 개의 요소만 얻게 되었습니다:\n\n![image](/TIL/assets/img/2024-07-12-AdvancedSelectionfromTensorsinPytorch_3.png)\n\n다음은 3D 선택 후 재구현입니다. 인덱스 텐서는 이제 임의의 모양을 가질 수 있으며, 결과 선택은 이 모양으로 제공됩니다:\n\n```python\nimport torch\n\nbatch_size = 16\nnum_elements = 64\nnum_features = 1024\nnum_picks = (2, 5, 3)\n\nvalues = torch.rand((batch_size, num_elements, num_features))\nindices = torch.randint(0, batch_size * num_elements * num_features, size=num_picks)\n# [2, 5, 3]\npicked = torch.take(values, indices)\n\npicked_manual = torch.zeros(num_picks)\nfor i in range(num_picks[0]):\n    for j in range(num_picks[1]):\n        for k in range(num_picks[2]):\n            picked_manual[i, j, k] = values.flatten()[indices[i, j, k]]\n\nassert torch.all(torch.eq(picked, picked_manual))\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 결론\n\n이 게시물에서는 Pytorch에서 세 가지 일반적인 선택 방법인 torch.index_select, torch.gather 및 torch.take을 살펴보았습니다. 이러한 방법을 사용하면 조건에 따라 텐서에서 요소를 선택하거나 색인 할 수 있습니다. 모든 경우에 우리는 간단한 2D 예제로 시작하여 결과 선택을 시각적으로 표현했습니다. 그런 다음, 어느 정도 더 복잡하고 현실적인 3D 시나리오로 이동했습니다. 이 시나리오에서는 [batch_size, num_elements, num_features] 모양의 텐서에서 선택해야 하는 경우가 있습니다. 이는 모든 ML 프로젝트에서 일반적으로 발생할 수 있는 사용 사례입니다.\n\n이 게시물을 마무리하면 이러한 함수들 간의 차이점을 테이블로 요약하고 싶습니다. 이 테이블은 간단한 설명 및 샘플 모양을 포함하며, 샘플 모양은 이전에 언급한 3D ML 예제에 맞게 조정되어 있습니다. 색인 텐서의 필요한 모양과 결과적으로 출력될 모양을 나열할 것입니다:\n\n<img src=\"/TIL/assets/img/2024-07-12-AdvancedSelectionfromTensorsinPytorch_4.png\" />\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n읽어 주셔서 감사합니다!","ogImage":{"url":"/TIL/assets/img/2024-07-12-AdvancedSelectionfromTensorsinPytorch_0.png"},"coverImage":"/TIL/assets/img/2024-07-12-AdvancedSelectionfromTensorsinPytorch_0.png","tag":["Tech"],"readingTime":9},{"title":"2024년에 AI 엔지니어링을 시작한다면 이렇게 하세요","description":"","date":"2024-07-12 20:30","slug":"2024-07-12-IfIstartedlearningAIEngineeringin2024hereswhatIwoulddo","content":"\n\n여러분, 안녕하세요!\n\n위의 표를 다음과 같이 Markdown 형식으로 변경해 보겠습니다.\n\n\n| 파일이름                                                     |                                                                            |\n| ------------------------------------------------------------ | -------------------------------------------------------------------------- |\n| 2024-07-12-IfIstartedlearningAIEngineeringin2024hereswhatIwoulddo_0.png |                                                                            |\n\n2023년 5월, 나는 결심했어요. AI를 배우기 위해 시간을 투자하기로 결정했죠.\n\n저는 6개월 동안(아버지의 휴가)을 내어 AI 엔지니어링 주제에 대해 배우는 데 시간을 할애했습니다.\n\n그리고 12개월이 흘렀을 때, 프리랜서 AI 엔지니어로 일하면서 첫 유료 프로젝트를 수행했어요.\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nAI 엔지니어를 꿈꾸는 여러분을 위해 글을 쓰고 있어요. 제가 경험한 레슨, 실수, 경험을 공유할 거예요. 왜냐하면...\n\n여러분이 오늘 제가 있는 곳에 빠르게 도달하기를 원하기 때문이에요.\n\n이 글을 마치고 나면 여러분은 다음을 알게 될 거예요:\n\n- 무엇을 배워야 하는지\n- 왜 그것을 배워야 하는지\n- 어떻게 빠르게 배울지\n- 학습의 혜택을 최대화하는 방법\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저작권 표시. 12개월 전에는 처음부터 시작하지 않았어요. 2019년에 기계 학습과 자연어 처리를 탐험하기 시작했어요. 또한 소프트웨어 엔지니어이기도 해요. 그래서 AI로 전환하기로 결정했을 때 꽤 유리한 시작점을 가지고 있었어요.\n\n# 첫 번째 AI 엔지니어링 직업 취득을 위한 7 단계\n\n제가 처음부터 AI 엔지니어링을 배우기 시작할 경우 따를 정확한 단계를 공유할게요.\n\n# 단계 1: 기본 파이썬 배우기.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nAI 엔지니어들은 코딩하는 방법을 알아야 합니다.\n\n하지만 시작할 때는 몇 가지 기초적인 것만 알아도 됩니다. 예를 들면:\n✔ 반복문\n✔ 변수\n✔ 함수\n✔ 데이터 유형\n✔ 기본 구문\n✔ 기본 작업\n✔ 조건문\n✔ API 및 라이브러리 사용법\n✔ 데이터 구조:\n↳ 리스트\n↳ 튜플\n↳ 딕셔너리\n\n많은 것을 배워야 하는 것처럼 들릴 수도 있지만, 사실 그렇게 어렵지 않아요.\n\n물론, Python을 능숙하게 다룰수록 더 좋습니다.\n하지만, 처음으로 AI 프로젝트를 만드는 기초를 배우는 것이 가장 중요합니다 (프로젝트에 대해 더 자세히 알아보기는 4단계에서).\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다행히도, 제가 위에 제공한 목록만으로도 충분히 시작할 수 있어요.\n\n이 멋진 Python 자료들을 추천해 드려요:\n\n- Real Python.\n- Corey Schafer의 YT 채널.\n- Programming for Everybody (Getting Started with Python)\n\n# 단계 2️: 대형 언어 모델(LLMs) 배우기.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nLLM은 AI 프로젝트의 \"두뇌\"입니다.\nAI 엔지니어들은 LLM을 많이 사용해요! 그래서 그들이 어떻게 작동하는지 이해하는 것이 중요해요.\n\nLLM을 사용하기 시작하려면 심층적으로 파고들 필요는 없어요.\n오늘날 GPT-4, Claude 3, 그리고 Gemini이(가) 가장 강력한 LLMs입니다. 그리고 비기술자들조차도 큰 성공을 거둘 수 있어요. 그것은 그들이 프롬프트 엔지니어링을 배웠기 때문이에요(3단계에서 자세히 알아봐요).\n\n한 번 더 말하지만, 프로젝트에서 LLM을 적용하기 시작하려면 정확히 그들이 어떻게 작동하는지 알 필요가 없어요.\n\n다음 용어들을 이해해주세요:\n- 토큰\n- 문맥 창\n- 주의 메커니즘\n- 온도 (LLM에서)\n- 트랜스포머 아키텍처\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n처음으로 AI 프로젝트를 구축하는 데 필요한 기초를 제공해줄 거에요. 매 프로젝트마다 기초를 확장해나갈 거에요.\n\nLLM에 대해 배울 좋은 자료:\n\n- Andrej Karpathy의 YouTube 채널.\n- Sentdex의 YouTube 채널.\n- OpenAI Playground (실험용).\n\n# 3. 프롬프트 엔지니어링의 원리를 배우세요.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nAI 엔지니어들은 LLM과 잘 소통하는 방법을 알고 있어요.\n\n그래서 프롬프트 엔지니어링은 그들의 일부분이에요.\n\n그렇다면, 프롬프트를 배우는 데 너무 많은 시간을 쓰지 마세요. 너무 심화되지 않도록 해요.\n\n그냥:\n- 목표를 명확히 하는 법을 배워보세요\n- 맥락을 제공하는 법을 배워보세요\n- 구체적으로 표현하는 법을 배워보세요\n- 계속해서 프롬프트를 개선해 나가세요\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저는 미래의 기술로서 '프롬프트 엔지니어링'에 대해 의심합니다. 그 이유는 간단합니다: 프롬프트는 평범한 텍스트일 뿐이기 때문입니다. 이미 LLM 및 DSPy와 같은 프레임워크로 우수한 프롬프트 작성이 자동화되고 있습니다.\n\n그래서 여기서 너무 많은 시간을 투자하지 말 것을 권합니다. 하지만 기본기는 중요합니다.\n\n필요할 때만 고급 프롬프팅을 공부하세요 (자료는 풍부합니다).\n\n여기서 프롬프트 엔지니어링을 배울 수 있습니다:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 학습 유도 웹사이트.\n- OpenAI의 Prompt Engineering.\n- Lil’Log의 Prompt Engineering.\n\n## 4. AI 프로젝트 구축.\n\n“지식은 실행을 통해 가장 잘 활용됩니다.” Tiago Forte\nAI 프로젝트를 구축하는 것이 AI 엔지니어로 채용되는 유일한 요구 사항입니다.\n\n이론만으로는 충분하지 않습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n당신이 AI 엔지니어라고 자처하려면 AI와 코드를 사용하여 실제로 무언가를 만들 수 있다는 것을 보여주어야 해요.\n\n다행히도,  여러분은 이미 이 기사의 첫 3단계를 완료한 후 첫 번째 AI 프로젝트를 만들 준비가 되어 있어요.\n\n만약 여러분의 스타일이라면, 첫 3단계를 거치지 않고 바로 첫 번째 프로젝트에 도전해도 괜찮아요. 프로젝트를 만들며 3단계를 배울 거예요.\n\n첫 프로젝트는 간단하면서 좋아요.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n여기서는 \"only\" 2가지 목표가 있습니다:\n\n- 처음 3단계를 결합하는 것.\n- 이론뿐만 아니라 실전에서도 배우는 것.\n\n첫 프로젝트에서는 Python, LLMs 및 프롬프트를 사용할 것입니다. 이는 처음 3단계 모든 것을 결합한다는 뜻입니다. OpenAI API나 LangChain 또는 LlamaIndex와 같은 프레임워크 덕분에 간단합니다.\n\n실습을 통해 배우는 것에 대해 이야기해 봅시다...\n“지식이 실행을 앞서가지 않도록 항상 해라.” - Dan Koe\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n착하게 번역\n\n정말로 배우는 것은 응용해야만 합니다.\n\n정말로 강조할 수가 없어요.\n\n제가 수백 시간 (수천 시간일 수도 있음)을 이론만 배우는 데 낭비했어요. 오늘날 그 중 95%를 기억하고 있지 않아요.\n\n3년 전 (34세 때)부터는 아무것도 만들지 않고는 배우지 않기로 결심했어요.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n결과는?\n✔ 빠르게 배우게 됩니다.\n✔ 학습을 문서화합니다.\n✔ 거의 모든 것을 기억합니다.\n✔ 훨씬 더 깊은 이해를 얻게 됩니다.\n✔ 완료된 프로젝트 포트폴리오를 구축합니다.\n\n참고: 모든 결과를 향상시키는 추가적인 일을 하나 더합니다 (5단계에서 더 자세히 설명).\n\n그러니 제 실수를 반복하지 말아주세요.\n\n이론보다 실행에 우선순위를 두세요.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n프로젝트로 돌아가려고 합니다. 첫 번째로 권장하는 것은 ChatGPT \"복제본\"을 만드는 것입니다.\n\n복제본을 만들기 위해 필요한 것:\n✔ Python\n✔ OpenAI API\n✔ Streamlit(GUI용)\n✔ 약 40줄의 코드\n\n가능해 보이나요?\n\n이 프로젝트는 놀라운 것이 아닐 것입니다. 하지만 아래와 같은 이점이 있을 거예요:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- \"Creator mode\"으로 전환하겠습니다.\n- 단계 1, 2 및 3에서 얻은 지식을 확고히 할 수 있도록 도와 드리겠습니다.\n- LLM 매개변수 및 프롬프트를 실험해 볼 수 있도록 해 드릴게요.\n\n프로젝트에 도움이 필요하면 제 글을 참고해 주세요:\n\n# 5. 공개 학습하기.\n\nAustin Kleon의 \"Show Your Work\"은 2시간 정도의 읽을 거리입니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n책 덕분에 지금 이 글을 쓰고 있어요.\n\n이 책 덕분에 모두가 소중한 가치를 공유할 수 있다는 것을 이해했어요. 완전히 초보일지라도, 여러분은 대부분의 사람들보다 앞서 있는 거예요.\n\n증거가 필요하다고요?\n\n제가 AI 공부를 시작한 지 약 6주만에 쓴 글 링크를 공유했어요.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n무엇을 공유할까요?\n\n✔ 코드\n✔ 수업\n✔ 실수\n✔ 생각\n✔ 즐겨찾는 자료\n\n배운 모든 것을 공유하세요.\n\n많은 사람들에게 공개하는 것이 어려움을 알고 있어요 (제게도 무서웠어요).\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만 그것은 당신이 할 수 있는 최고의 장기 투자 중 하나가 될 것입니다. 크리에이터 경제가 번창하고 있습니다. 콘텐츠를 만드는 것은 시간 투자를 제외하고는 거의 단점이 없어요.\n\n다음을 얻게 될 거에요:\n✔ 신뢰 구축\n✔ 다른 사람들에게 영감 주기\n️✔ 신뢰성 확보\n✔ 진본성 확보\n✔ 성장 추적\n✔ 지식 고착화\n✔ 네트워크 확장\n✔ 가르치면서 더 깊이 배우기\n✔ 개인 브랜드 구축\n\n그리고 이 목록은 심지어 완전하지 않아요!\n\n저는 이것을 해봤고, 혜택을 확인해요.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n요렇게 생각하고 있어요:\n\n🌟 배운 것을 나누세요!\n\n# 6. 단계 4와 5를 반복하세요.\n\n계속해서 해나가요. 함께 공유해요.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음 프로젝트를 선택하는 방법?\n\n이건 당신이 다음 프로젝트에 뛰어들기 전에 꼭 물어봐야할 중요한 질문이에요.\n\n나는 프로젝트를 하나 끝마치는 데 몇 일밖에 없어서 엄청 큰 실수를 했어요. 매주 영상을 게시했기 때문에, 아래 과정을 거쳤죠:\n\n- 프로젝트 아이디어 찾기.\n- 사용할 기술 선택하기.\n- 프로젝트 만들기 (코드 작성).\n- 프로젝트에 대해 이야기하는 녹화.\n- 내 녹음물 편집하기 (너무 싫었어요).\n- 설명 준비하기.\n- 영상 업로드하기.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그럼, 저는 매체(Medium)를 위한 기사를 썼어요.\n\n모두 7일 안에.\n\n저는 실제 프로젝트를 만드는 대신 탐험하고 공유하는 데 시간을 보냈어요. 탐험 자체가 좋은 일이에요. 하지만 이 \"충격적인 진실\"을 빨리 알았다면 더 좋았을텐데요:\n\n탐험적 AI 프로젝트로는 취업이 어려워요!\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\"Benefits, not features\"는 판매에서의 주요 원칙 중 하나입니다.\n\n왜 AI 엔지니어링에 관한 기사에서 판매 원칙을 언급하는 걸까요? 많은 사람들이 AI가 과대포장되었다고 믿습니다.\n\nAI 뉴스는 멋진 AI 특징을 언급합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만 기업은 결과에만 관심이 있어요.\n안타깝게도, 기능은 대부분의 경우 기업에 실질적인 혜택을 제공하지 못해요.\n\n만약 여러분의 프로젝트가 두드러지게 하고 싶다면 혜택에 집중하세요.\n\n다음은 3가지 주요 혜택 카테고리에요:\n\n- 시간 절약\n- 비용 절감\n- 수익 증대\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n(힌트: AI는 시간을 절약하는 데 좋은 도구입니다.)\n\n프로젝트가 가치를 더하는지 확인하세요 (현실 세계에서 혜택을 제공).\n\n- 편안한 영역을 벗어나기\n- 새로운 것을 배울 것을 보장하기\n- 기존 기술 향상시키기\n- 실제 문제 해결하기\n\n각 프로젝트는 도전적이어야 하고 새로운 것을 배우도록 강제해야 합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n친구야, 여기 몇 가지 실용적인 주제가 있어:\n- RAG\n- LangChain\n- HuggingFace\n- Vector Databases\n- Vector Embedding\n- 오픈소스 LLMs\n- 이미지 생성기 (DALL-e 3)\n- 음성 모델 (Whisper와 TTS)\n\n나는 최근 AI 엔지니어링 직무에 지원해서 10회 이상의 면접을 봤어.\n\n내 경험상 회사들은 항상 RAG에 대해 물어봐. 그래서 나는 2번이나 3번째 프로젝트에서 RAG를 사용할 거야.\n\n조기에 시작할수록 좋아.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n표 태그를 Markdown 형식으로 변경하면서 더 나은 결과를 얻을 수 있습니다. \n\n**단계 1, 2, 3에서 기술을 향상**하여 프로젝트에서 더 많은 이점을 얻을 수 있습니다. \n\n- 코딩 기술 확장하기\n- LLMs 자세히 배우기\n- 프롬프트 탐색하기\n\n실제 문제를 해결하는 프로젝트를 개발하세요.\n\n# 7️. 멘토를 찾아보세요.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n멘토가 없다는 것이 내 진전을 엄청나게 늦춘 것 같아.\n\n내 여정에서는 글의 처음 6 단계를 모두 (어느 정도는) 수행했어.\n\n하지만 나는 멘토를 찾지 못한 채...\n\n내게는 이런 일을 할 수 있는 사람이 한 명도 없었어:\n- 어려운 시기를 함께 극복할 수 있는\n- 어떤 기술을 키워야 하는지 알려 줄 수 있는\n- 내 진로를 돕는\n- 자신의 네트워크에 소개해 주는\n- 성공을 함께 축하해 주는\n- 나에게 경력에 관한 조언을 해 주는\n- 피드백을 주는\n- 격려해 주는\n- 비판해 주는\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저의 실수를 하지 마세요!\n\n멘토를 찾아보세요.\n\n멘토가 여러분을 몇 달 동안 구해줄 거예요!\n\n참고: AI 엔지니어링 여정에서 멘토를 찾고 있다면 도와드릴게요. 매체(Medium) 출처라고 말씀하시고 LinkedIn에서 DM 보내주세요.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 결론\n\n아직 AI 엔지니어가 되기에는 매우 일찍입니다.\n\n이 역할은 아직 정의되지 않았지만 수요는 빠르게 증가하고 있습니다.\n\n이 기사를 읽은 후에는 AI 엔지니어가 되기 위한 제 경로를 엄청난 속도로 배웠습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n질문이 있으면 댓글에 적어주세요! ✍️\n\n🔔 제 이름은 Kris예요. 저는 기술 분야의 사람들이 AI 엔지니어가 되도록 도와드려요. \nMedium에서는 저의 경험과 새로운 AI 엔지니어가 되고 싶어하는 분들에게 가치 있는 교훈을 공유하고 있어요. 꼭 팔로우해주세요. 🔔","ogImage":{"url":"/TIL/assets/img/2024-07-12-IfIstartedlearningAIEngineeringin2024hereswhatIwoulddo_0.png"},"coverImage":"/TIL/assets/img/2024-07-12-IfIstartedlearningAIEngineeringin2024hereswhatIwoulddo_0.png","tag":["Tech"],"readingTime":16},{"title":"대수학자처럼 사고하는 법","description":"","date":"2024-07-12 20:29","slug":"2024-07-12-HowToThinkLikeAnAlgebraist","content":"\n\n![HowToThinkLikeAnAlgebraist](/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_0.png)\n\n30년이나 40년 전에 열정적이고 재능 있는 고등학교 수학 학생이었다면, 당신이 교과과정의 일환으로 기초 그룹 이론을 공부했을 확률은 거의 확실했습니다. 안타깝게도 이제는 이러한 경우가 많지 않습니다. 많은 고등학교 교육과정에서 그룹 이론이 제외되었으며, 오늘날 많은 사람들은 수학 관련 학과를 전공할 때 처음으로 마주치게 됩니다.\n\n이 점이 안타깝습니다. 그룹 이론은 대수학자처럼 생각하는 방법에 대한 훌륭한 입문입니다. 이러한 사고 방식을 통해 조직적이고 체계적이게 이산적이고 추상적인 객체를 처리할 수 있으며, 이는 고급 수학을 공부하는 데 필수적인 기술이자 삶 전반에서 극도로 유용한 기술입니다. 이러한 방식으로 생각하는 데는 익숙해지기에 시간이 필요하며, 학교에서 조금이라도 경험이 있는 사람들은 학위 수준에서 수학을 공부할 때 더 빨리 적응할 것입니다.\n\n저는 1988년 케임브리지 대학 입학 시험지에서 이 문제를 발견했으며, 이 문제가 내 주장을 훌륭하게 설명해 주는 사례라고 생각합니다. 만약 이 문제에 대해 적절한 순서와 논리적으로 생각할 수 있다면, 해결책은 깔끔하고 매우 우아할 것입니다. 그러나 대수학자처럼 생각하는 방법에 대한 훈련이 없다면, 이 문제에 대처하는 데 어려움을 겪을 것입니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n문제를 제시하기 전에 옛날에 고등학생들이 배우던 작은 양의 군 이론을 소개해드릴게요 😊\n\n## 그룹이란 무엇인가요?\n\n그룹은 수학에서 보는 일부 공통 구조를 추상화하고 일반화하는 방법입니다. 간단한 예로, 정수 집합을 살펴봅시다. 이 집합에서 덧셈 연산을 살펴보겠습니다. 정수와 이 연산에 대한 몇 가지 특성이 있습니다. 이러한 특성들은 당연하게 여기지만, 그들이 작동하고 유용성과 관련이 있습니다. 여기 몇 가지 특성이 있습니다:\n\n- 정수는 덧셈에 대해 닫혀 있습니다. 즉, 모든 두 정수를 더하면 다른 정수가 나옵니다.\n- 정수는 덧셈에 대해 결합적입니다. 즉, 세 개의 정수 a, b 및 c가 있을 때 a + (b + c) = (a + b) + c 가 성립합니다.\n- 항등원인 0(영)이 존재합니다. 이 정수는 다른 어떤 정수에 더해져도 같은 정수를 반환합니다. 따라서 모든 정수 a에 대해 a + 0 = 0 + a = a 가 성립합니다.\n- 임의의 정수 a에 대해 역원 -a가 존재합니다. 이는 정수와 그 역원을 아무 순서로 더하면 항등원 0이 나온다는 것을 의미합니다. 즉, 모든 정수 a에 대해 a + (-a) = (-a) + a = 0 가 성립합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이러한 속성들을 일반 대수 구조를 정의하는 추상화된 개념으로 정의할 수 있습니다.\n\n그룹은 다음과 같은 조건을 만족하는 객체 G로 이루어진 집합과 해당 객체들에 대한 연산인 *을 함께 하는 것으로 정의됩니다:\n\n- g, h가 G에 속한다면 g*h도 G에 속합니다.\n- 임의의 g, h, k가 G에 속할 때, g*(h*k) = (g*h)*k가 성립합니다.\n- G에는 항등원인 e가 존재하며, 모든 g ∈ G에 대해 g*e = e*g = g가 성립합니다.\n- 모든 g ∈ G에 대해 역원 g^(-1)이 존재하며, g * g^(-1) = g^(-1) * g = e가 성립합니다.\n\n그룹이라는 개념은 우리가 일상 생활에서 유용하게 활용하는 수학적 구조를 추상화하는 방법으로 개발되었지만, 이 정의가 소개된 이후 몇 세기 동안, 수학자들은 이러한 속성을 따르며 거대하고 완전히 믿기 어려운 방식으로 유한 그룹을 형성하는 놀라운 구조들을 발견해왔습니다. 예를 들어, 몬스터 그룹은 1970년대에 발견되었으며 808,017,424,794,512,875,886,459,904,961,710,757,005,754,368,000,000,000개의 원소로 구성되어 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 문제\n\n여기는 1988년 케임브리지 대학 입시시험의 한 문제입니다. 만약 원하신다면, 제가 풀이를 보여주기 전에 먼저 도전해보세요. 그룹에는 하나의 연산만 정의되어 있기 때문에 그룹에서 대수를 다룰 때 연산 표기를 생략하는 것이 일반적이며 편리합니다. 그렇기 때문에 g*h는 단순히 gh로 쓰이고 g*g는 g²로 쓰입니다.\n\n![Image](/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_1.png)\n\n## 나의 풀이 — 파트 (i)\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리는 o(g) = n이라고 말을 들었습니다, 따라서 g^k = e인 더 작은 양수 정수 k ` n은 없습니다. 따라서 g^N = e이면 N ≥ n이어야 합니다.\n\n우리는 숫자 N을 N = kn + j로 쓸 수 있습니다. 여기서 k ≥ 1이고 0 ≤ j ` n입니다. 그러면 다음을 말할 수 있습니다.\n\n\n| g^j      | g^(n+j)     | g^(2n+j)    | ... | g^((k-1)n+j) |\n|----------|-------------|--------------|-----|---------------|\n| g^0      | g^n         | g^(2n)       | ... | g^((k-1)n)   |\n\n\n그러나 우리는\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\n<img src=\"/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_3.png\" />\n\n그리고 0 ≤ j ≤ n이기 때문에 j = 0이어야 합니다. 왜냐하면 o(g) = n이기 때문이죠. 따라서 N = kn이고, 따라서 n은 N으로 나누어집니다.\n\n## 내 해답 — Part (ii)\n\n이 부분이 더욱 명확해지도록 우리의 연산 표기법을 사용해봅시다. 다음과 같이 말할 수 있습니다:\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\n![How to think like an algebraist](/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_4.png)\n\nWith the h element repeated m times. Now note that:\n\n![How to think like an algebraist](/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_5.png)\n\nSo this means we can make the following replacement:\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\\<img src=\"/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_6.png\" />\n\nh가 처음과 끝을 포함해 m번 나타납니다. 이제 우리가 이것을 원래 식에 넣으면\n\n\\<img src=\"/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_7.png\" />\n\n오른쪽에 m번 반복되어 있습니다. 따라서 필요한 결과를 얻을 수 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 나의 해결책 — Part (iii)\n\n주어진 정보를 사용하면:\n\n![이미지](/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_8.png)\n\n이제 우리가 이전 부분에서 얻은 결과를 사용해봅시다:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래와 같이 변경해주셨으면 좋겠습니다.\n\n\n![HowToThinkLikeAnAlgebraist_9](/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_9.png)\n\n아래의 내용대로 수정해 주십시오.\n\nTo find o(h), we can use the given fact that g⁵ = e. Consider the following logic:\n\n![HowToThinkLikeAnAlgebraist_10](/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_10.png)\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래와 같이 결과를 사용하여 이를 반복할 수 있습니다. \n\n\n![Part(ii)](/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_11.png)\n\n\n유사한 논리를 사용하여 계속하여 이 패턴을 유지하면 다음과 같은 결론을 내릴 수 있습니다: \n\n\n![Pattern Logic](/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_12.png)\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만 우리는 g⁵ = e임을 알고 있으므로, 다음과 같이도 말할 수 있습니다:\n\n![image](/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_13.png)\n\n따라서:\n\n![image](/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_14.png)\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그래서:\n\n![Image](/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_15.png)\n\n이제, Part (i)를 사용하면 h의 순서는 31로 나누어져야 합니다. 그런데 31은 소수입니다. 따라서 o(h) = 31입니다.\n\n그룹 이론에 대한 소개 어땠나요? 자유롭게 의견을 남겨주세요!","ogImage":{"url":"/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_0.png"},"coverImage":"/TIL/assets/img/2024-07-12-HowToThinkLikeAnAlgebraist_0.png","tag":["Tech"],"readingTime":8},{"title":"Python으로 웹사이트에서 데이터를 쉽게 추출하는 방법","description":"","date":"2024-07-12 20:27","slug":"2024-07-12-HowToEasilyExtractDataFromAWebsiteWithPython","content":"\n\n<table>\n<tr>\n  <th>Library</th>\n  <th>Purpose</th>\n</tr>\n<tr>\n  <td>Requests</td>\n  <td>For sending HTTP requests</td>\n</tr>\n<tr>\n  <td>BeautifulSoup</td>\n  <td>For parsing HTML and XML</td>\n</tr>\n<tr>\n  <td>Tkinter</td>\n  <td>For building the GUI</td>\n</tr>\n</table>\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- **Pandas**: 추출된 데이터를 위한 데이터베이스를 생성하는 데 사용됩니다.\n- **Requests**: 웹사이트에 접근 권한을 요청하는 데 사용됩니다.\n- **BeautifulSoup**: 웹상에서 데이터를 찾는 데 사용됩니다.\n\n# 작업: 이메일 목록 추출 및 CSV로 변환하기\n\n여러 주제에서 많은 이메일을 가져와야 했습니다.\n\n\"수동으로는 절대 할 수 없어\" 라고 생각했습니다. 그렇게 하면 시간이 많이 걸리고 지루할 것이라고 생. 따라서 나는 파이썬 기술을 사용하기로 결정했습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n웹 사이트에는 다음과 같은 데이터가 있습니다:\n\n![Data Table](/TIL/assets/img/2024-07-12-HowToEasilyExtractDataFromAWebsiteWithPython_1.png)\n\n네, 과목 이름과 이메일이 포함된 표가 있습니다.\n\n이 프로젝트의 목표는 이 데이터를 사용하여 CSV 파일을 생성하는 것입니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 단계 1. 모듈 가져오기\n\n먼저 사용할 Python 라이브러리를 가져와 봅시다:\n\n- pandas.\n- requests.\n- BeautifulSoup.\n\n```js\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 단계 2. 데이터 찾기\n\n## 2.1. 웹 스크래핑은 어떻게 작동되나요?\n\n웹에서 데이터를 추출하는 것이 가능한 이유는 무엇인가요?\n\n답은 HTML(HyperText Markup Language)에 달려 있습니다. HTML은 웹 브라우저에서 표시할 문서의 표준 마크업 언어입니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n웹사이트 어디에서든 마우스 오른쪽 버튼을 클릭하고 Inspect를 선택하면 웹의 코드가 오른쪽에 표시됩니다:\n\n![image](/TIL/assets/img/2024-07-12-HowToEasilyExtractDataFromAWebsiteWithPython_2.png)\n\n파이썬은 (일부 라이브러리와 함께) 이 HTML 코드를 \"읽고\" 원하는 데이터를 찾는 것입니다.\n\n더 자세한 내용은 향후 기사에서 확인해보세요.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 2.2. HTML 가져오기 함수\n\n우선, 웹 사이트에서 HTML 코드를 가져와야 합니다.\n\nURL을 매개변수로 하는 get_html 함수를 생성하는 방법은 다음과 같습니다:\n\n```js\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef get_html(url):\n\n    try:\n        response = requests.get(url) # 웹 사이트에서 HTML을 요청합니다\n        return response.text\n\n    except Exception as e: # 가능한 오류를 처리하기 위한 예외 처리\n        print(f\"웹 페이지를 가져오는 데 실패했습니다: {e}\")\n        return \"\"emails = set() # 중복을 피하기 위한 코드입니다\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 단계 3. 데이터 추출\n\n다음 단계는 우리가 원하는 데이터를 추출하는 것입니다.\n\n우리는 이전 함수에서 HTML 코드를 가져오는 extract_data 함수를 만들 수 있습니다. 이는 다음 단계를 포함합니다:\n\n- BeautifulSoup 클래스 초기화.\n- 테이블을 찾는 변수 설정.\n- 데이터를 수집할 빈 리스트.\n- 데이터를 검색하는 for 루프.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef get_html(url):\n\n    try:\n        response = requests.get(url) # 웹사이트로부터 HTML을 요청합니다.\n        return response.text\n\n    except Exception as e: # 가능한 오류를 처리하기 위한 부분\n        print(f\"웹 페이지를 가져오는 데 실패했습니다: {e}\")\n        return \"\" # 중복을 피하기 위해 이메일 집합으로 설정\n\ndef extract_data(html):\n\n    soup = BeautifulSoup(html, 'html.parser') # BeautifulSoup 클래스를 초기화\n    table = soup.find('table') # 테이블을 찾습니다.\n    data = [] # 데이터를 수집할 빈 리스트\n    \n    if table:\n\n        rows = table.find_all('tr') # 모든 테이블을 찾습니다.\n\n        for row in rows[1:]:  # 헤더 행을 건너 뜁니다.\n            cols = row.find_all('td') # 테이블에서 셀을 찾습니다.\n\n            if len(cols) == 4:  # 항상 4개의 열이 있다고 가정\n                catedra_name = cols[0].text.strip() # 과목 이름\n                email = cols[1].text.strip() # 이메일\n                data.append({'catedra': catedra_name, 'email': email}) # 데이터 리스트에 추가\n    \n    return data\n```\n\n# 단계 4. 함수 호출 및 데이터 CSV로 저장\n\n이제 모든 준비가 되었으므로 함수를 호출해야 합니다.\n\n```js\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef get_html(url):\n\n    try:\n        response = requests.get(url) # 웹사이트에서 HTML 가져오기\n        return response.text\n\n    except Exception as e: # 가능한 오류 처리\n        print(f\"웹 페이지를 가져오는 데 실패했습니다: {e}\")\n        return \"\" # 중복을 피하기 위해 이메일 집합\n\ndef extract_data(html):\n\n    soup = BeautifulSoup(html, 'html.parser') # BeautifulSoup 클래스를 초기화\n    table = soup.find('table') # 테이블 찾기\n    data = [] # 데이터 수집을 위한 빈 리스트\n    \n    if table:\n\n        rows = table.find_all('tr') # 모든 테이블 찾기\n\n        for row in rows[1:]:  # 헤더 행 건너 띄기\n            cols = row.find_all('td')\n\n            if len(cols) == 4:  # 항상 4개의 열\n                catedra_name = cols[0].text.strip() # 과목 이름\n                email = cols[1].text.strip() # 이메일\n                data.append({'catedra': catedra_name, 'email': email})\n    \n    return data\n\nurl = \"https://edipsicouba.net.ar/uncategorized/listado-mails-materias-electivas/\"  # 여러분의 링크 설정\n\nhtml = get_html(url) # get_html() 함수 호출하여 내용을 변수에 저장\ndata = extract_data(html) # extract_data() 함수 호출하여 결과를 변수에 저장\n\ndf = pd.DataFrame(data) # 데이터를 데이터프레임으로 변환\ndf.to_csv('mail_info.csv', index=False) # 데이터프레임을 CSV 파일로 저장\n\nprint(\"데이터가 성공적으로 추출되어 mail_info.csv로 저장되었습니다\")\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그게 다에요!\n\n이렇게 하면 웹사이트의 테이블 안에서 데이터를 수집할 수 있어요.\n\n또한, 이렇게 하면 Python을 사용하여 지루한 작업을 자동화할 수 있어요 😉\n\n다음 글에서는 데이터 분석 프로젝트를 위해 슈퍼마켓에서 데이터를 수집하는 방법을 보여드릴게요.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 도와드릴 수 있는 방법:\n\n- 새로운 무료 뉴스레터 'The Super Learning Lab'를 구독하세요.\n- 곧 무료 학습 이북과 이메일 코스가 출시될 예정입니다!\n\n![HowToEasilyExtractDataFromAWebsiteWithPython_3](/TIL/assets/img/2024-07-12-HowToEasilyExtractDataFromAWebsiteWithPython_3.png)\n\n## 내 최고의 학습 기사들:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n안녕하세요!\n\n가져와주셔서 감사합니다! 아래는 이번 주 발간물 내용입니다:\n\n- Ultralearning으로 무엇이든 배우기\n- 초안 속 9가지 Ultra-learning 원칙\n- Ultralearning을 활용하여 2개월 만에 무료로 독일어 배우기\n- 학습을 슈퍼파워로 만들기\n- 이것을 하지 않고 책을 읽는 것을 그만두세요\n\n만날 날을 기대하며,\n\nAxel\n\n# 간단하고 쉬운 용어로 🚀\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nIn Plain English 커뮤니티에 참여해 주셔서 감사합니다! 떠나시기 전에:\n\n- 작가에게 박수를 보내고 팔로우해주세요 👏️️\n- 팔로우하기: X | LinkedIn | YouTube | Discord | Newsletter\n- 다른 플랫폼 방문하기: CoFeed | Differ\n- PlainEnglish.io에서 더 많은 콘텐츠 확인하기","ogImage":{"url":"/TIL/assets/img/2024-07-12-HowToEasilyExtractDataFromAWebsiteWithPython_0.png"},"coverImage":"/TIL/assets/img/2024-07-12-HowToEasilyExtractDataFromAWebsiteWithPython_0.png","tag":["Tech"],"readingTime":9},{"title":"Apache Iceberg PySpark로 데이터 웨어하우스를 만드는 4가지 방법","description":"","date":"2024-07-12 20:24","slug":"2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark","content":"\n\n![image](/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_0.png)\n\n## 온디맨드 강좌| 추천\n\n몇몇 독자들이 데이터 엔지니어가 되는 데 도움이 되는 온디맨드 강좌를 요청했습니다. 이 중에서 제가 추천하는 3가지 좋은 자료들은 다음과 같습니다:\n\n- 데이터 엔지니어링 나노디그리 (UDACITY)\n- 아파치 카프카 및 아파치 스파크를 이용한 데이터 스트리밍 나노디그리 (UDACITY)\n- PySpark를 이용한 스파크 및 파이썬을 활용한 빅데이터 (UDEMY)\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아직 Medium 회원이 아니신가요? 매달 $5로 Medium의 모든 콘텐츠에 액세스할 수 있는 제 추천 링크로 가입해보세요!\n\n# 소개\n\nApache Iceberg은 데이터 레이크 내에서 매우 큰 데이터 세트를 효율적으로 저장하는 데 산업의 기준이 되어가고 있습니다.\n\n이 오픈 테이블 형식은 Parquet 또는 ORC 파일로 저장된 데이터 세트에 대해 작동할 수 있도록 해주며, 관계형 데이터베이스의 테이블과 정확히 같은 방식으로 작동합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아이스버그 데이터 웨어하우스를 올바르게 생성하는 것은 해당 형식의 모든 데이터를 저장하고 조회할 수 있는 기본적인 첫 번째 단계입니다. 그러나 아이스버그 문서는 종종 기술적인 내용이 부족하거나 관련 사용 사례를 다루지 않는 경우가 있습니다.\n\n현재 Spark가 아이스버그 테이블과 작업하기 위해 가장 잘 지원되는 컴퓨팅 엔진 중 하나이므로 이번 튜토리얼에서는 다음 질문에 대답할 것입니다:\n\n대규모 데이터 보관 솔루션을 개발하면서 얻은 실무 경험을 바탕으로, 본 튜토리얼은 데이터 엔지니어들에게 아이스버그 API를 PySpark에서 활용하기 위한 기본 단계를 보여주며, 연구하는 데 소요되는 시간(및 시행착오)을 감소시키는 등 지식 간극을 메우고자 합니다.\n\n# 방법론\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아이스버그 아키텍처는 세 개의 주요 레이어로 구성되어 있습니다:\n\n- 아이스버그 카탈로그 레이어: 각 테이블의 현재 메타데이터 파일의 정확한 위치를 저장합니다. 이 포인터는 Spark, Trino, Flink 등의 쿼리 엔진이 데이터를 읽거나 쓸 위치를 정확히 지정합니다.\n- 메타데이터 레이어: 아이스버그 테이블과 관련된 메타데이터를 저장합니다. 테이블 스키마, 파티셔닝 스키마 및 특정 시점의 테이블 스냅샷과 같은 정보를 포함합니다.\n- 데이터 레이어: 행 데이터를 컬럼 형식으로 저장하며, 파티션화된 디렉토리에 구성합니다.\n\n환경 및 특정 요구에 따라, 이러한 레이어는 동일한 저장소 내에 있거나 다른 기술을 활용할 수 있습니다.\n\n본 튜토리얼에서는 데이터 엔지니어가 로컬 환경에서 개발하는 동안 아이스버그 데이터 웨어하우스를 설정하는 세 가지 간단한 방법을 소개할 것입니다:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 방법 #1: 로컬에서 실행되는 Spark 세션으로, 메타데이터와 데이터 레이어는 MinIO 버킷에, 카탈로그 레이어는 PostGres DB에 저장되어 있습니다.\n- 방법 #2: 로컬에서 실행되는 Spark 세션으로, 모든 세 가지 레이어가 스테이징 AWS S3 버킷에 저장되어 있습니다(하둡 카탈로그 사용).\n- 방법 #3: EMR 클러스터에서 실행되는 Spark 세션으로, 수동 CLI 배포를 통해, 모든 세 가지 레이어가 스테이징 AWS S3 버킷에 저장되어 있습니다(하둡 카탈로그 사용).\n\n또한 상용 환경에서 동일한 결과를 얻는 데 일반적으로 사용되는 추가적인 방법을 설명할 것입니다:\n\n- 방법 #4: EMR 클러스터에서 실행되는 Spark 세션으로, 자동화된 Apache Airflow 배포를 통해, 모든 세 가지 레이어가 프로덕션 AWS S3 버킷에 저장되어 있습니다(하둡 카탈로그 사용).\n\n아래에 위에서 설명한 방법들의 시각적 표현을 찾을 수 있습니다:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\n![image](/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_1.png)\n\n# 환경 설정하기\n\n이 자습서에서 사용하는 자료는 GitHub에서 이용할 수 있습니다.\n\n따라서 따라하기 위해 단순히 원격 프로젝트 저장소를 로컬로 클론하고 spark_icb_warehouse 디렉터리로 이동한 후 docker compose up -d를 실행하면 됩니다.\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 명령은 다음 서비스를 실행합니다:\n\n```js\nspark_icb_warehouse % docker compose up -d\n[+] Running 6/6\n ⠿ Network spark_icb_warehouse_shared-network 생성됨                                                                                                         0.0초\n ⠿ Container postgres-db                       시작함                                                                                                         1.0초\n ⠿ Container minio                             시작함                                                                                                         1.0초\n ⠿ Container mc                                시작함                                                                                                         1.8초\n ⠿ Container airflow_webserver                 시작함                                                                                                         1.9초\n ⠿ Container airflow_scheduler                 시작함\n```\n\n- Postgres DB = 메소드 #1의 일환으로 사용되며 Iceberg 카탈로그를 저장하고, 메소드 #4의 일환으로 Airflow 메타데이터를 저장하기 위해 사용됩니다. 동일한 Postgres 서비스가 두 개의 데이터베이스를 호스트하는 효율적인 전략입니다. 즉, docker-compose.yml이 실행되면 생성된 airflow_metadata_db와 postgres-init 폴더의 init_dbs.sh 스크립트를 실행하여 생성된 iceberg_warehouse_pg 데이터베이스가 사용됩니다.\n- MinIO = 메소드 #1의 일환으로 Iceberg 데이터와 메타데이터 레이어를 저장하기 위해 사용됩니다. Docker를 통해 실행될 때 MinIO는 크라우드 스토리지 (예: S3 Bucket)의 기능을 무료로 제공하고 주요 설치나 유지 관리가 필요하지 않습니다. 따라서 PySpark 어플리케이션을 로컬로 테스트하는 동안 훌륭한 선택지입니다.\n- Apache Airflow (WebServer + Scheduler) = 메소드 #4의 일환으로 PySpark 어플리케이션을 EMR에 자동으로 배포하고, 프로덕션 환경에서의 일반적인 전략을 시뮬레이션하기 위해 사용됩니다. 이 튜토리얼에서는 Airflow 버전 2.5.2를 사용하고 UI를 8085 포트에서 사용할 수 있도록 설정했습니다.\n\n이 세 가지 서비스 외에도 동일한 지역(예: eu-central-1) 내에서 생성된 S3 Bucket과 기본 EMR 클러스터를 기다리는 상태로 설정한 AWS 계정이 있다고 가정합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만 PySpark은 어떻게 될까요? Iceberg 퀵스타트 가이드는 인기 있는 tabulario/spark-iceberg 이미지를 가져와 Docker에서 독립적으로 Spark를 실행하는 것을 권장합니다 – 온라인 가이드의 대부분과 마찬가지로 – 그러나 이 튜토리얼의 목표 중 하나는 실제로 이것이 필요하지 않다는 것을 보여주는 것입니다:\n\n- 메소드 #1 및 #2의 경우, 우리는 단순히 로컬로 pyspark (버전 3.3.1)을 설치할 수 있으며, 가상 환경의 일부로 코드를 Jupyter 노트북을 통해 컴파일할 수 있습니다.\n- 메소드 #3 및 #4의 경우, EMR 클러스터를 설정하는 동안 pyspark=3.3.1이 사전 설치되어 있으며 추가 단계가 필요하지 않습니다.\n\n# Iceberg Warehouse 생성하기\n\n이 섹션의 코드 조각은 GitHub 레포지토리의 Spark_Iceberg_WH Jupyter 노트북의 일부로 제공됩니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아이스버그 프레임워크가 Pyspark 위에서 작동하도록 하려면 스파크 세션을 생성할 때 구성값의 일부로 전달될 다음 패키지를 명시해야 합니다.\n\n```js\n# 종속성 정의 (Maven에서 다운로드할 패키지)\n# 1. Spark가 Iceberg 웨어하우스와 상호 작용하기 위해 필요\nDEPENDENCIES = \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.1\"\nDEPENDENCIES += \",software.amazon.awssdk:bundle:2.20.18\"\nDEPENDENCIES += \",com.amazonaws:aws-java-sdk-bundle:1.11.901\"\nDEPENDENCIES += \",org.apache.hadoop:hadoop-aws:3.3.4\"\n\n# 2. 카탈로그가 포스트그레스 DB에 저장된 경우에만 필요\nDEPENDENCIES += \",org.postgresql:postgresql:42.6.0\"\n```\n\n다음 사항을 유의해야 합니다:\n\n- 우리는 iceberg-spark 패키지 버전 1.3.1을 사용할 것입니다. 이 버전을 사용할 때 구성 오류를 피하기 위해 꼭 필요한 패키지는 awssdk-bundle, aws-sdk-bundle, hadoop-aws 세 개 뿐입니다.\n- Python에서 문자열을 연결하는 더 좋은 방법이 있지만, 위의 코드는 더 필요하지 않을 때 패키지에 주석을 달기 쉽게 만들어줍니다 (예를 들어 postgresql은 메소드 #1에만 필요합니다).\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 방법 #1 = `로컬 스파크 세션 + MinIO에 메타데이터/데이터 레이어 + 포스트그레스 DB에 카탈로그 레이어`\n\n파이스파크를 사용하여 아이스버그 데이터 웨어하우스를 만드는 첫 번째 방법은, 로컬 SparkSession을 실행하고 JDBC 카탈로그(포스트그레스 DB)를 사용하도록 구성하고 메타데이터 및 데이터 레이어를 MinIO 버킷에 저장하는 것입니다.\n\n이를 수행하는 코드는 아래에 제시되어 있습니다:\n\n```js\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\ndef spark_local_to_minio(icb_catalog_name,\n                         iceberg_warehouse,\n                         storage_type,\n                         pg_user,\n                         pg_password,\n                         minio_bucket,\n                         minio_access_key,\n                         minio_secret_key,\n                         minio_end_point):\n\n    conf = (\n            SparkConf()\n            .setAppName('spark_local_to_minio')\n            #Dependencies\n            .set('spark.jars.packages', DEPENDENCIES)\n            #SQL Extensions\n            .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n            #Catalog Configuration\n            .set(f'spark.sql.catalog.{icb_catalog_name}', 'org.apache.iceberg.spark.SparkCatalog')\n            .set(f'spark.sql.catalog.{icb_catalog_name}.catalog-impl', 'org.apache.iceberg.jdbc.JdbcCatalog')\n            .set(f'spark.sql.catalog.{icb_catalog_name}.uri', f'jdbc:postgresql://localhost:5439/{pg_db}')\n            .set(f'spark.sql.catalog.{icb_catalog_name}.jdbc.user', pg_user)\n            .set(f'spark.sql.catalog.{icb_catalog_name}.jdbc.password', pg_password)\n            .set(f'spark.sql.catalog.{icb_catalog_name}.jdbc.verifyServerCertificate', 'true')\n            .set(f'spark.sql.catalog.{icb_catalog_name}.jdbc.useSSL', 'true')\n            .set(f'spark.sql.defaultCatalog', icb_catalog_name)\n            .set(f'spark.sql.catalog.{icb_catalog_name}.warehouse', f's3a://{minio_bucket}/{iceberg_warehouse}/{storage_type}/')\n            # MinIO Configuration\n            .set('spark.hadoop.fs.s3a.access.key', minio_access_key)\n            .set('spark.hadoop.fs.s3a.secret.key', minio_secret_key)\n            .set(\"spark.hadoop.fs.s3a.endpoint\", minio_end_point)\n    )\n    \n    ## Start Spark Session\n    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n\n    print(\"Spark Session Running\")\n    \n    return spark\n\n############################################\nicb_catalog_name = 'pg_catalog'\niceberg_warehouse = 'iceberg-warehouse-pg'\nstorage_type = 'data-archives'\n\npg_db = 'iceberg_warehouse_pg'\npg_user = 'postgres'\npg_password = 'postgres'\n\nminio_bucket = 'iceberg-bucket'\nminio_access_key = 'admin'\nminio_secret_key = 'password'\nminio_end_point = 'http://127.0.0.1:9000'\n\nspark = spark_local_to_minio(icb_catalog_name,\n                             iceberg_warehouse,\n                             storage_type,\n                             pg_user,\n                             pg_password,\n                             minio_bucket,\n                             minio_access_key,\n                             minio_secret_key,\n                             minio_end_point)\r\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n`spark_local_to_minio()` 함수를 실행하면 SparkSession이 생성되어 iceberg_warehouse_pg(로컬에서 포트 5439로 사용 가능)와 MinIO의 iceberg-bucket을 가리키게 됩니다(docker-compose.yml을 실행하면서 생성됨, 로컬에서 127.0.0.1:9000으로 사용 가능).\n\n그러나 iceberg 웨어하우스는 SQL 명령을 실행하지 않으면 MinIO에 실질적으로 표시되지 않습니다(테스트 테이블처럼 객체를 생성하는 명령). 예를 들어:\n\n```js\n# iceberg-warehouse-pg 웨어하우스에 테스트 테이블(Within MinIO) 생성\n# *참고*: 이 작업을 실행하기 전에 UI에서 ICB WH를 볼 수 없습니다.\nspark.sql(f\"\"\"CREATE OR REPLACE TABLE {icb_catalog_name}.TEST_SCHEMA.TEST_TABLE_MINIO_PG (\n             FIELD_1 BIGINT,\n             FIELD_2 varchar(50),\n             FIELD_3 DATE,\n             FIELD_4 DOUBLE,\n             FIELD_5 TIMESTAMP\n             )\n             USING iceberg\n             \"\"\")\n\n# PG 카탈로그에 생성된 테이블 표시(TEST_SCHEMA)\nspark.sql(f'SHOW TABLES IN {icb_catalog_name}.TEST_SCHEMA').show(truncate=False)pyt\n```\n\n위와 같은 결과가 나타납니다:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\n|namespace  |tableName          |isTemporary|\n|-----------|-------------------|-----------|\n|TEST_SCHEMA|TEST_TABLE_MINIO_PG|false      |\n\n\n위의 표는 TEST_SCHEMA.TEST_TABLE_MINIO_PG가 카탈로그에 등록되었음을 의미합니다. 이 카탈로그는 iceberg_warehouse_pg 데이터베이스 위에 구축되었으며 MinIO 버킷에 메타데이터 파일이 저장되었습니다.\n\n위에 설명한 것이 실제로 발생했는지 확인하기 위해 두 가지 확인을 수행할 수 있습니다:\n\n- MinIO UI에 액세스하여(사용자: admin, 비밀번호: password) 메타데이터 파일로 이동합니다:\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n[Markdown 형식으로 변경한 텍스트]\n![이미지1](/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_2.png)\n\n![이미지2](/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_3.png)\n\n- 선호하는 DBMS를 통해 Postgres DB에 연결하고 iceberg_tables를 쿼리하세요:\n\n![이미지3](/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_4.png)\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## Method #2 - 로컬 Spark 세션 + S3 버킷의 모든 레이어\n\n일명 #Method 1과 마찬가지로, 데이터 엔지니어들은 로컬 개발 및 테스트를 지원하도록 정확히 설정된 스테이징 S3 버킷(프로덕션 버킷을 반영)에 액세스할 수 있습니다.\n\n만약 그렇다면, SparkSession은 로컬에서 실행되지만, 이번에는 모든 레이어가 S3 버킷 내에 저장될 수 있습니다. 특히, S3 저장소는 하둡 카탈로그(반면에 MinIO에는 누락된)를 내장하고 있어, Iceberg 데이터 웨어하우스를 생성하면서 설정할 수 있는 아마도 가장 쉬운 [외부] 카탈로그입니다.\n\n위에서 설명한 내용을 달성하는 코드는 다음과 같습니다:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nimport os\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\ndef spark_local_to_s3(icb_catalog_name,\n                      iceberg_warehouse,\n                      storage_type,\n                      s3_bucket,\n                      s3_access_key,\n                      s3_secret_key):\n    \n    \n    os.environ.update({'AWS_ACCESS_KEY_ID': s3_access_key,\n                       'AWS_SECRET_ACCESS_KEY': s3_secret_key\n                       #'AWS_SESSION_TOKEN': s3_session_token\n                      })\n    \n    conf = (\n            SparkConf()\n            .setAppName('spark_local_to_s3')\n            #packages\n            .set('spark.jars.packages', DEPENDENCIES)\n            #SQL Extensions\n            .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n            #Configuring Catalog\n            .set(f'spark.sql.catalog.{icb_catalog_name}', 'org.apache.iceberg.spark.SparkCatalog')\n            .set(f'spark.sql.catalog.{icb_catalog_name}.type', 'hadoop')\n            .set(f'spark.sql.catalog.{icb_catalog_name}.warehouse', f's3a://{s3_bucket}/{iceberg_warehouse}/{storage_type}/')\n            .set(f'spark.sql.catalog.{icb_catalog_name}.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n    )\n\n    # Start Spark Session\n    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n    \n    print(\"Spark 세션 실행 중\")\n    \n    return spark\n\nicb_catalog_name = 'hadoop_catalog'\niceberg_warehouse = 'iceberg-warehouse-dev-hdp'\nstorage_type = 'data-archives'\n\ns3_bucket = 'iceberg-bucket-9004'\ns3_access_key='XXXXX'\ns3_secret_key='XXXXX'\n\nspark = spark_local_to_s3(icb_catalog_name,\n                          iceberg_warehouse,\n                          storage_type,\n                          s3_bucket,\n                          s3_access_key,\n                          s3_secret_key)\r\n```\n\n한눈에 보기에 이 메서드는 #method 1보다 설정이 상당히 적게 필요함을 알 수 있습니다:\n\n- JDBC Catalog 구현이 `spark.sql.catalog.'icb_catalog_name'.type`, `hadoop`로 대체되었습니다.\n- MinIO 구성이 `spark.sql.catalog.'icb_catalog_name'.io-impl`, `org.apache.iceberg.aws.s3.S3FileIO`로 대체되었습니다.\n- 또한 S3 버킷의 access_key와 secret_key (가끔은 session_token이 필요할 수도 있음)이 환경 변수로 사용 가능하도록 만들었습니다. 환경 변수 사용으로 연결 오류가 발생하는 경우, 대안적으로 비밀을 default 프로필 아래 ~/.aws/credentials에 저장하는 방법이 있습니다.\n\n이번에는 spark_local_to_s3() 함수를 실행할 때, SparkSession을 생성하여 iceberg-bucket-9004를 가리키고 백그라운드에서 hadoop 카탈로그가 구성되었습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아마도 iceberg-warehouse-dev-hdp가 S3 계정 UI에 표시되지 않을 것입니다. 올바른 iceberg object를 생성하는 pyspark.sql 명령을 실행하지 않는 이상:\n\n```js\n# 메소드 1과 같이, 이 명령을 실행하지 않으면 ICB WH가 S3 Bucket 내부에 표시되지 않음\nspark.sql(f\"\"\"CREATE OR REPLACE TABLE {icb_catalog_name}.TEST_SCHEMA.TEST_TABLE_S3_HDP (\n             FIELD_1 BIGINT,\n             FIELD_2 varchar(50),\n             FIELD_3 DATE,\n             FIELD_4 DOUBLE,\n             FIELD_5 TIMESTAMP\n             )\n             USING iceberg\n             \"\"\")\n```\n\n실제로, 위의 명령은 데이터 웨어하우스와 TEST_TABLE_S3_HDP 테이블을 위한 전용 폴더를 생성합니다:\n\n![이미지](/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_5.png)\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\n<img src=\"/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_6.png\" />\n\n## Method #3 - Spark Session On EMR (via CLI) + All Layers On S3 Bucket\n\nIf AWS EMR is your go-to production framework for deploying PySpark applications, you probably have a staging cluster for development paired with a staging S3 bucket (similar to #method 2).\n\nIn this scenario, data engineers can submit PySpark applications to the EMR cluster through AWS CLI.\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nEMR 단계를 추가하는 명령어를 작성할 때는 클러스터 드라이버에서 SparkSession이 자동으로 실행될 것이므로 올바른 종속성, 구성 및 PySpark 스크립트를 제출하는 데 주의해야 합니다.\n\n이 방법을 사용하면 종속성이 Maven에서 자동으로 다운로드되는 대신 별도의 jars/ 폴더 내의 jar 파일로 S3에 사용 가능하게 만들어졌으며 PySpark 애플리케이션 자체도 scripts/ 폴더에 저장되었습니다.\n\n아래는 EMR에 create_iceberg_wh_app.py 애플리케이션을 제출하는 명령어의 예시입니다. 이 명령어는 실제로 해둡 카탈로그에 iceberg 웨어하우스를 설정하는 필요한 구성을 가지고 있습니다:\n\n```js\n# CLI를 통해 AWS EMR 클러스터에 단계 추가\n# 실제 클러스터 ID로 j-xxxxxxxxxxx를 대체하세요\naws emr add-steps --profile aws_personal --cluster-id j-xxxxxxxxxxx \\ \n--steps '[{\n  \"Args\":[\"spark-submit\",\n    \"--deploy-mode\",\"client\",\n    \"--jars\",\"s3://iceberg-bucket-9004/jars/hadoop-aws-3.3.4.jar,s3://iceberg-bucket-9004/jars/bundle-2.20.18.jar,s3://iceberg-bucket-9004/jars/aws-java-sdk-bundle-1.11.901.jar,s3://iceberg-bucket-9004/jars/iceberg-spark-runtime-3.3_2.12-1.3.1.jar\",\n    \"--conf\",\"spark.sql.catalog.hadoop_catalog=org.apache.iceberg.spark.SparkCatalog\",\n    \"--conf\",\"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n    \"--conf\",\"spark.sql.catalog.hadoop_catalog.type=hadoop\",\n    \"--conf\",\"spark.sql.catalog.hadoop_catalog.warehouse=s3://iceberg-bucket-9004/iceberg-warehouse-emr/data-archives/\",\n    \"--conf\",\"spark.sql.catalog.hadoop_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO\",\n    \"s3://iceberg-bucket-9004/scripts/create_iceberg_wh_app.py\"],\n  \"Type\":\"CUSTOM_JAR\",\n  \"ActionOnFailure\":\"CONTINUE\",\n  \"Jar\":\"command-runner.jar\",\n  \"Properties\":\"\",\n  \"Name\":\"create_iceberg_wh_via_emr_cli\"\n}]'\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n만약 명령이 성공적으로 실행되면, stepID가 반환됩니다:\n\n![StepID](/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_7.png)\n\nEMR 클러스터 UI에서도 동일한 stepID가 나타나며, 이는 PySpark 스크립트가 실행 준비가 된 것을 나타냅니다:\n\n![StepID](/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_8.png)\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n작업이 완료되면 클러스터에서 로그가 생성됩니다. 이 경우, 로그에서 TEST_TABLE_EMR_S3_HDP가 카탈로그에 실제로 생성되었음을 확인했습니다:\n\n```js\nINFO:root:Creating SPARK SESSION...\n\nINFO:root:SPARK SESSION created!\n\n+-----------+---------------------+-----------+\n|namespace  |tableName            |isTemporary|\n+-----------+---------------------+-----------+\n|TEST_SCHEMA|TEST_TABLE_EMR_S3_HDP|false      |\n+-----------+---------------------+-----------+\n\nINFO:root:Main APPLICATION was executed!\n```\n\n결과적으로, iceberg-bucket-9004 S3 버킷 내에서 add-steps 명령어의 일부로 지정된 이름인 iceberg-warehouse-emr가 생성되었습니다:\n\n![이미지](/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_9.png)\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## Method #4: Spark Session On EMR (Via Airflow) + All Layers On S3 Bucket\n\n일반적으로, 실제 환경에서는 Apache Airflow를 사용하여 작업을 조율하며 PySpark 응용 프로그램을 배포하기 위해 EMR에 단계를 자동으로 추가합니다.\n\nAWS EMR에서 Airflow를 사용하여 PySpark 파이프라인을 자동화하는 방법에 대해 자세히 알고 싶다면 이 문서를 참조하십시오.\n\n요약하면, 이 방법은 다음 네 가지 작업을 수행하는 전용 DAG(spark_create_iceberg_wh_dag.py)를 설정하는 것을 포함합니다:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- create_iceberg_wh_app.py 파일을 iceberg-bucket-9004의 scripts/ 폴더에 업로드합니다 (EMR이 배포할 수 있도록 함).\n- dag_params.json 파일에서 필요한 Spark 작업 구성을 구문 분석하고 EMR에 제출할 명령을 자동으로 빌드하는 데 사용합니다:\n\n```js\n{\n    \"local_conf\":{\n       \"local_sub_folder\":\"/assets/\",\n       \"files_to_upload\":[\"create_iceberg_wh_app.py\"]\n    },\n    \"s3_conf\":{\n       \"bucket_name\":\"iceberg-bucket-9004\",\n       \"s3_scripts_path\":\"scripts/\"\n    },\n    \"spark_submit_cmd\":{\n       \"cmd\":\"[\\\"spark-submit\\\", \\\"--deploy-mode\\\", \\\"client\\\"]\",\n       \"pyspark_exec\":\"scripts/create_iceberg_wh_app.py\"\n    },\n    \"spark_conf\":{\n         \"spark.sql.catalog.hadoop_catalog\": \"org.apache.iceberg.spark.SparkCatalog\",\n         \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n         \"spark.sql.catalog.hadoop_catalog.type\": \"hadoop\",\n         \"spark.sql.catalog.hadoop_catalog.warehouse\": \"s3a://iceberg-bucket-9004/iceberg-warehouse-prod-hdp/data-archives/\",\n         \"spark.sql.catalog.hadoop_catalog.io-impl\": \"org.apache.iceberg.aws.s3.S3FileIO\"\n    },\n    \"spark_jars_conf\":{\n       \"bucket_prefix\":\"s3://iceberg-bucket-9004/\",\n       \"bucket_subfolder\":\"jars/\"\n    },\n    \"spark_jars_conf_value\":[\n         \"hadoop-aws-3.3.4.jar\",\n         \"bundle-2.20.18.jar\",\n         \"aws-java-sdk-bundle-1.11.901.jar\",\n         \"iceberg-spark-runtime-3.3_2.12-1.3.1.jar\"\n    ]\n }\n```\n\n- EmrAddStepsOperator를 통해 PySpark 스크립트를 배포하고 실행할 단계를 EMR에 추가하도록 지시합니다.\n- EMRStepSensor를 통해 EMR 단계 상태를 지속적으로 모니터링합니다.\n\n<img src=\"/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_10.png\" />\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n#method3에 대해 이야기해보겠습니다. DAG가 트리거된 직후에 PySpark 스크립트를 실행하는 단계가 EMR UI의 Steps 섹션 아래에 나타납니다.\n\n![Step](/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_11.png)\n\n해당 단계가 완료 상태가 되면 S3 버킷 내에 새 Iceberg 웨어하우스인 \"iceberg-warehouse-prod-hdp\"가 생성된 것을 확인할 수 있습니다.\n\n![Iceberg Warehouse](/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_12.png)\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 결론\n\n이 튜토리얼에서는 PySpark를 사용하여 Apache Iceberg 데이터 웨어하우스를 로컬 또는 프로덕션 환경에서 생성하는 네 가지 방법을 소개했습니다.\n\n각 방법은 특정 사용 사례를 다루며 다른 기술 (Docker, MinIo, S3, EMR, Airflow) 및 다른 전략 (로컬에서 실행되는 SparkSession 대 EMR 클러스터에서 실행되는 SparkSession, 데이터베이스에 저장된 카탈로그 레이어 대 S3 버킷 등)을 사용합니다.\n\n현재 문서에 다양한 조합을 제시함으로써 데이터 엔지니어가 Iceberg 오픈 테이블 형식을 기반으로하는 PySpark 애플리케이션을 구축하면서 테스트하는 데 소요되는 시간을 절약하도록 돕는 것이 목표입니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 출처\n\n- Apache Iceberg 소개\n- Spark 및 Iceberg 빠른 시작\n- MinIO를 사용한 Apache Iceberg 개발자를 위한 소개\n- Iceberg 및 MinIO를 사용한 Lakehouse 아키텍처 완벽 가이드\n- Apache Iceberg 학습 — 카탈로그를 Postgres에 저장하기\n- Iceberg 항해: Pyspark로 iceberg 테이블 단위 테스트하기","ogImage":{"url":"/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_0.png"},"coverImage":"/TIL/assets/img/2024-07-12-ApacheIceberg4MethodsToCreateAWarehouseWithPySpark_0.png","tag":["Tech"],"readingTime":26},{"title":"Python 동시성 프로그래밍 Futures 사용법","description":"","date":"2024-07-12 20:22","slug":"2024-07-12-PythonConcurrencyProgrammingFutures","content":"\n\n\n![Image](/TIL/assets/img/2024-07-12-PythonConcurrencyProgrammingFutures_0.png)\n\n협업 프로그래밍을 올바르고 합리적으로 사용하면 우리 프로그램에 상당한 성능 향상을 가져다 줄 것입니다. 오늘의 글에서는 파이썬에서 동시성 프로그래밍 및 Future를 중점으로 하는 동시성 프로그래밍을 이해하고 적용하는 방법을 안내하겠습니다.\n\n# 동시성과 병렬성: 차이 이해하기\n\n동시성 프로그래밍에 대해 학습할 때 동시성과 병렬성이라는 용어를 함께 사용하는 경우가 많습니다. 이로 인해 많은 사람들이 두 용어가 동일한 것으로 생각되는 오해를 하게 됩니다. 그러나 이는 오인입니다.\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n먼저 일반적으로 오해되는 것을 명확하게 해 봅시다: 파이썬에서 동시성은 여러 작업(스레드 또는 태스크)이 동시에 동시에 발생한다는 것을 의미하지 않습니다. 대신, 한 번에 하나의 작업만 진행되도록 허용하고, 스레드 또는 태스크가 완료될 때까지 서로 전환됩니다. 아래 다이어그램을 살펴보겠습니다:\n\n![Concurrency Diagram](/TIL/assets/img/2024-07-12-PythonConcurrencyProgrammingFutures_1.png)\n\n이 다이어그램은 태스크 전환을 관리하는 두 가지 다른 방법을 보여줍니다. 이는 파이썬에서 동시성의 두 형태에 대응됩니다: 스레딩과 asyncio입니다.\n\n스레딩의 경우, 운영 체제는 각 스레드의 모든 세부 정보를 알고 있기 때문에 필요에 따라 스레드 전환을 처리합니다. 여기서의 장점은 프로그래머가 전환 프로세스를 관리할 필요가 없기 때문에 코드를 작성하기가 더 쉽다는 것입니다. 그러나 이는 레이스 컨디션과 같은 문제로 이어질 수 있습니다. 이는 스레드가 단일 명령문 실행 도중에 서로 간섭할 수 있는 상황을 말합니다(e.g., x += 1).\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그러나 asyncio를 사용할 경우, 주 프로그램이 작업을 전환할 수 있다는 것을 명시적으로 알려주어야 합니다. 이러한 설계는 asyncio 이벤트 루프에 의해 제어되는 방식으로 전환되므로 실행 중에 작업이 충돌하지 않도록 보장하여 경쟁 조건을 피할 수 있도록 도와줍니다.\n\n병렬성에 대해 이야기하면, 동시에 동시에 실행되는 작업을 가리킵니다. Python에서 이는 멀티 프로세싱을 통해 달성됩니다. 예를 들어, 컴퓨터가 6코어 프로세서를 사용한다면, 실행 속도를 높이기 위해 6개의 프로세스를 동시에 실행할 수 있습니다. 멀티 프로세싱의 기본 아이디어는 다음과 같이 나타낼 수 있습니다:\n\n![멀티 프로세싱](/TIL/assets/img/2024-07-12-PythonConcurrencyProgrammingFutures_2.png)\n\n두 가지를 비교해보면:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n동시성은 빈번한 I/O 작업이 발생하는 시나리오에서 자주 사용됩니다. 예를 들어, 웹 사이트에서 여러 파일을 다운로드할 때, I/O 작업에 소요되는 시간이 CPU 처리에 소요되는 시간보다 크게 늘어날 수 있습니다.\n\n반면에 병렬성은 CPU 집중적인 시나리오에 더 적합합니다. 예를 들어, MapReduce 병렬 계산에서 여러 대의 기계와 프로세서를 사용하여 작업을 여러 코어 또는 시스템에 분산시켜 실행 속도를 높이는 데 활용됩니다.\n\n# 동시성 프로그래밍의 Futures\n\n## 단일 스레드 및 다중 스레드 접근 방식의 성능 비교\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음으로, 코드 관점에서 Futures를 이해하고 단일 스레드 방식과 성능을 비교하기 위한 구체적인 예제를 사용해보겠습니다.\n\n여러 웹사이트에서 콘텐츠를 다운로드하고 결과를 출력하는 작업이 있다고 가정해봅시다. 단일 스레드 방식을 사용한다면, 코드 구현은 다음과 같이 보일 것입니다 (단순함을 위해 예외 처리는 간단히 생략했습니다):\n\n```js\nimport requests\nimport time\n \ndef download_one(url):\n    resp = requests.get(url)\n    print('{}에서 {}를 읽었습니다.'.format(url, len(resp.content)))\n    \ndef download_all(sites):\n    for site in sites:\n        download_one(site)\n \ndef main():\n    sites = [\n        'https://en.wikipedia.org/wiki/Portal:Arts',\n        'https://en.wikipedia.org/wiki/Portal:History',\n        'https://en.wikipedia.org/wiki/Portal:Society',\n        'https://en.wikipedia.org/wiki/Portal:Biography',\n        'https://en.wikipedia.org/wiki/Portal:Mathematics',\n        'https://en.wikipedia.org/wiki/Portal:Technology',\n        'https://en.wikipedia.org/wiki/Portal:Geography',\n        'https://en.wikipedia.org/wiki/Portal:Science',\n        'https://en.wikipedia.org/wiki/Computer_science',\n        'https://en.wikipedia.org/wiki/Python_(programming_language)',\n        'https://en.wikipedia.org/wiki/Java_(programming_language)',\n        'https://en.wikipedia.org/wiki/PHP',\n        'https://en.wikipedia.org/wiki/Node.js',\n        'https://en.wikipedia.org/wiki/The_C_Programming_Language',\n        'https://en.wikipedia.org/wiki/Go_(programming_language)'\n    ]\n    start_time = time.perf_counter()\n    download_all(sites)\n    end_time = time.perf_counter()\n    print('{}개 사이트를 {}초에 다운로드했습니다.'.format(len(sites), end_time - start_time))\n    \nif __name__ == '__main__':\n    main()\n```\n\n이 방식은 직접적이고 간단한 접근 방식입니다:\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n먼저 웹사이트 목록을 순회합니다. 그 후, 현재 웹사이트에 대한 다운로드 작업을 수행합니다. 현재 작업이 완료될 때까지 기다렸다가 다음 웹사이트로 넘어가며, 모든 작업이 완료될 때까지 이 과정을 계속합니다.\n\n총 소요 시간은 약 2.4초인 것을 확인할 수 있습니다. 단일 스레드 방식의 장점은 간단함에 있지만, 대부분의 시간이 I/O 작업을 기다리며 보내므로 효율적이지 않습니다. 프로그램은 각 웹사이트의 다운로드가 끝날 때까지 기다려야 다음 작업을 시작할 수 있습니다. 웹사이트 다운로드해야 하는 수가 수천 개에 달하는 현실적인 프로덕션 환경에서는 이 방식이 실행하기에 적합하지 않습니다.\n\n그 다음, 코드의 멀티 스레드 버전을 살펴봅시다:\n\n```js\nimport concurrent.futures\nimport requests\nimport threading\nimport time\n \ndef download_one(url):\n    resp = requests.get(url)\n    print('Read {} from {}'.format(len(resp.content), url))\n \ndef download_all(sites):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n        executor.map(download_one, sites)\n \ndef main():\n    sites = [\n        'https://en.wikipedia.org/wiki/Portal:Arts',\n        'https://en.wikipedia.org/wiki/Portal:History',\n        'https://en.wikipedia.org/wiki/Portal:Society',\n        'https://en.wikipedia.org/wiki/Portal:Biography',\n        'https://en.wikipedia.org/wiki/Portal:Mathematics',\n        'https://en.wikipedia.org/wiki/Portal:Technology',\n        'https://en.wikipedia.org/wiki/Portal:Geography',\n        'https://en.wikipedia.org/wiki/Portal:Science',\n        'https://en.wikipedia.org/wiki/Computer_science',\n        'https://en.wikipedia.org/wiki/Python_(programming_language)',\n        'https://en.wikipedia.org/wiki/Java_(programming_language)',\n        'https://en.wikipedia.org/wiki/PHP',\n        'https://en.wikipedia.org/wiki/Node.js',\n        'https://en.wikipedia.org/wiki/The_C_Programming_Language',\n        'https://en.wikipedia.org/wiki/Go_(programming_language)'\n    ]\n    start_time = time.perf_counter()\n    download_all(sites)\n    end_time = time.perf_counter()\n    print('Download {} sites in {} seconds'.format(len(sites), end_time - start_time))\n\nif __name__ == '__main__':\n    main()\n\n## Output\nRead 151021 from https://en.wikipedia.org/wiki/Portal:Mathematics\nRead 129886 from https://en.wikipedia.org/wiki/Portal:Arts\nRead 107637 from https://en.wikipedia.org/wiki/Portal:Biography\nRead 224118 from https://en.wikipedia.org/wiki/Portal:Society\nRead 184343 from https://en.wikipedia.org/wiki/Portal:History\nRead 167923 from https://en.wikipedia.org/wiki/Portal:Geography\nRead 157811 from https://en.wikipedia.org/wiki/Portal:Technology\nRead 91533 from https://en.wikipedia.org/wiki/Portal:Science\nRead 321352 from https://en.wikipedia.org/wiki/Computer_science\nRead 391905 from https://en.wikipedia.org/wiki/Python_(programming_language)\nRead 180298 from https://en.wikipedia.org/wiki/Node.js\nRead 56765 from https://en.wikipedia.org/wiki/The_C_Programming_Language\nRead 468461 from https://en.wikipedia.org/wiki/PHP\nRead 321417 from https://en.wikipedia.org/wiki/Java_(programming_language)\nRead 324039 from https://en.wikipedia.org/wiki/Go_(programming_language)\nDownload 15 sites in 0.19936635800002023 seconds\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n총 시간이 약 0.2초로 나타나는 것은 효율성이 10배 이상 향상되었다는 사실을 알 수 있어요.\n\n다중 스레드 버전과 단일 스레드 버전 사이의 주요 차이점을 살펴보겠어요:\n\n```js\nwith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n        executor.map(download_one, sites)\n```\n\n여기서는 5개의 스레드를 사용할 수 있는 스레드 풀을 생성했어요. executor.map()은 파이썬 내장 map() 함수와 유사하며, 각각의 urls 요소에 download_one() 함수를 동시에 적용해요.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n여담이지만, download_one() 함수에서 사용하는 requests.get() 메서드는 스레드 안전(thread-safe)하므로 경합 상태를 일으키지 않고 여러 스레드 환경에서 안전하게 사용할 수 있습니다.\n\n또한, 스레드의 개수는 사용자가 정의할 수 있지만, 더 많은 스레드가 항상 좋은 것은 아닙니다. 그 이유는 스레드를 생성, 유지 및 소멸하는 데 일정한 오버헤드가 발생하기 때문입니다. 스레드의 개수를 지나치게 높게 설정하면 오히려 성능이 떨어질 수 있습니다. 종종 실제 요구 사항에 기반하여 최적의 스레드 개수를 찾기 위해 몇 가지 테스트를 수행해야 합니다.\n\n물론 프로그램 효율성을 향상시키기 위해 병렬성(parallelism)을 사용할 수도 있습니다. 이를 위해 download_all() 함수에서 다음과 같이 변경할 수 있습니다:\n\n```js\nwith futures.ThreadPoolExecutor(workers) as executor\n=>\nwith futures.ProcessPoolExecutor() as executor:\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n코드를 변경해야 하는 부분에서는 ProcessPoolExecutor() 함수를 사용하여 병렬로 작업을 실행할 수 있는 프로세스 풀을 생성합니다. 그러나 여기서 worker 매개변수를 종종 생략하는데, 시스템이 사용 가능한 CPU 코어 수를 자동으로 프로세스로 활용할 수 있기 때문입니다.\n\n이전에 언급한 대로, 병렬성은 일반적으로 CPU 바운드 시나리오에서 사용됩니다. I/O 바운드 작업의 경우 대부분의 시간이 대기 상태에 소요되며, 여러 프로세스를 사용하는 것은 멀티 스레딩과 비교하여 효율성을 향상시키지 않습니다. 실제로 CPU 코어의 제한으로 인해 멀티 프로세싱의 성능이 종종 멀티 스레딩보다 나쁠 수 있습니다.\n\n# Futures란 정확히 무엇인가요?\n\n파이썬에서 Futures 모듈은 concurrent.futures와 asyncio에 위치하며, 둘 다 지연된 작업을 나타냅니다. Futures는 대기 상태에 있는 작업을 래핑하여 큐에 배치합니다. 이러한 작업의 상태는 언제든지 쿼리할 수 있으며, 작업이 완료된 후 결과 또는 예외를 검색할 수도 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n일반적으로 사용자들은 미래(Futures)를 생성하는 방법에 대해 걱정할 필요가 없습니다. 내부 메커니즘이 대신 처리해 주기 때문이죠. 실제로 해야 할 일은 이러한 Futures를 실행할 수 있도록 예약하는 것입니다.\n\n예를 들어, Futures 모듈에서 Executor 클래스를 사용하면 executor.submit(func) 메서드를 사용하여 작업을 제출할 수 있습니다. 이 메서드는 func() 함수를 실행할 수 있도록 예약하고 작업을 나타내는 Future 객체를 반환합니다. 그런 다음 이 Future 객체를 사용하여 작업의 상태를 조회하거나 결과를 검색할 수 있습니다.\n\nFutures 모듈에서 자주 사용되는 몇 가지 메서드는 다음과 같습니다:\n\n- done(): 해당 작업이 완료되었는지 확인하는 메서드입니다. True는 작업이 완료되었음을 나타내고, False는 아직 진행 중이라는 뜻입니다. done()은 비차단 방식으로 동작하며 결과를 즉시 반환합니다.\n- add_done_callback(fn): 이 메서드는 Future가 완료되면 실행될 콜백 함수 fn을 등록합니다. 콜백 함수 fn은 Future가 작업을 완료한 후에 알림을 받고 호출됩니다.\n- result(): 이 메서드는 Future의 결과 또는 예외를 반환합니다. Future가 완료되었을 때 호출합니다. Future가 예외를 만났다면 result()는 해당 예외를 발생시킵니다.\n- as_completed(fs): 이 함수는 Futures fs의 반복자를 가져와 완료될 때마다 Futures를 생성하는 반복자를 반환합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그래서, 앞서 언급한 예제는 다음과 같은 형식으로도 작성할 수 있습니다:\n\n```js\nimport concurrent.futures\nimport requests\nimport time\n\ndef download_one(url):\n    resp = requests.get(url)\n    print('Read {} from {}'.format(len(resp.content), url))\n\ndef download_all(sites):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n        to_do = []\n        for site in sites:\n            future = executor.submit(download_one, site)\n            to_do.append(future)\n\n        for future in concurrent.futures.as_completed(to_do):\n            future.result()\n\ndef main():\n    sites = [\n        'https://en.wikipedia.org/wiki/Portal:Arts',\n        'https://en.wikipedia.org/wiki/Portal:History',\n        'https://en.wikipedia.org/wiki/Portal:Society',\n        'https://en.wikipedia.org/wiki/Portal:Biography',\n        'https://en.wikipedia.org/wiki/Portal:Mathematics',\n        'https://en.wikipedia.org/wiki/Portal:Technology',\n        'https://en.wikipedia.org/wiki/Portal:Geography',\n        'https://en.wikipedia.org/wiki/Portal:Science',\n        'https://en.wikipedia.org/wiki/Computer_science',\n        'https://en.wikipedia.org/wiki/Python_(programming_language)',\n        'https://en.wikipedia.org/wiki/Java_(programming_language)',\n        'https://en.wikipedia.org/wiki/PHP',\n        'https://en.wikipedia.org/wiki/Node.js',\n        'https://en.wikipedia.org/wiki/The_C_Programming_Language',\n        'https://en.wikipedia.org/wiki/Go_(programming_language)'\n    ]\n    start_time = time.perf_counter()\n    download_all(sites)\n    end_time = time.perf_counter()\n    print('Download {} sites in {} seconds'.format(len(sites), end_time - start_time))\n\nif __name__ == '__main__':\n    main()\n\n## 결과\nhttps://en.wikipedia.org/wiki/Portal:Arts에서 129,886을 읽음\nhttps://en.wikipedia.org/wiki/Portal:Biography에서 107,634을 읽음\nhttps://en.wikipedia.org/wiki/Portal:Society에서 224,118을 읽음\nhttps://en.wikipedia.org/wiki/Portal:Mathematics에서 158,984을 읽음\nhttps://en.wikipedia.org/wiki/Portal:History에서 184,343을 읽음\nhttps://en.wikipedia.org/wiki/Portal:Technology에서 157,949을 읽음\nhttps://en.wikipedia.org/wiki/Portal:Geography에서 167,923을 읽음\nhttps://en.wikipedia.org/wiki/Portal:Science에서 94,228을 읽음\nhttps://en.wikipedia.org/wiki/Python_(programming_language)에서 391,905을 읽음\nhttps://en.wikipedia.org/wiki/Computer_science에서 321,352을 읽음\nhttps://en.wikipedia.org/wiki/Node.js에서 180,298을 읽음\nhttps://en.wikipedia.org/wiki/Java_(programming_language)에서 321,417을 읽음\nhttps://en.wikipedia.org/wiki/PHP에서 468,421을 읽음\nhttps://en.wikipedia.org/wiki/The_C_Programming_Language에서 56,765을 읽음\nhttps://en.wikipedia.org/wiki/Go_(programming_language)에서 324,039을 읽음\n15개 사이트를 0.21698231499976828초에 다운로드함\n```\n\n여기서, 우리는 먼저 executor.submit()를 호출하여 각 웹사이트에서 내용을 다운로드하는 작업을 to_do라는 미래 큐에 넣고 실행을 대기시킵니다. 그런 다음 as_completed() 함수를 사용하여 각 미래가 완료될 때마다 결과를 출력합니다.\n\n그러나 미래가 완료되는 순서가 목록의 순서와 일치하는 것은 아님을 유의해야 합니다. 완료 순서는 시스템 스케줄링 및 각 미래의 실행 시간에 따라 달라집니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 왜 멀티스레딩에서 한 번에 하나의 스레드만 실행될 수 있을까요?\n\n이전에 파이썬에서 메인 프로그램에서 한 번에 하나의 스레드만 실행될 수 있다고 언급했습니다. 이게 왜 그런 걸까요?\n\n나중에 더 자세히 배울 Global Interpreter Lock (GIL) 개념을 간단히 소개해 드리겠습니다.\n\n사실 파이썬 인터프리터는 스레드로부터 안전하지 않습니다. 동시 스레드로 인한 레이스 컨디션과 같은 문제를 해결하기 위해 파이썬은 Global Interpreter Lock (GIL)를 도입했습니다. 이 잠금은 한 번에 한 스레드만 파이썬 바이트코드를 실행할 수 있도록 보장합니다. 그러나 스레드가 I/O 작업으로 인해 차단될 때 GIL이 해제되어 다른 스레드가 계속해서 실행될 수 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 결론\n\n이 글에서는 파이썬에서 동시성과 병렬성의 개념과 차이점을 먼저 살펴보았습니다.\n\n동시성은 스레드와 작업 간에 전환을 통해 달성되지만, 어떤 순간에는 한 스레드 또는 작업만 실행될 수 있습니다. 반면에 병렬성은 여러 프로세스가 동시에 실행되는 것을 의미합니다.\n\n동시성은 일반적으로 빈번한 I/O 작업이 있는 시나리오에서 사용되고, 병렬성은 CPU 집약적인 작업에 더 적합합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리는 웹 사이트 콘텐츠를 다운로드하는 예제를 통해 단일 스레드 방식과 Futures를 사용한 멀티 스레딩 버전 사이의 성능 차이를 비교했습니다. 잘 구현된 멀티 스레딩 접근 방식이 프로그램의 효율성을 크게 향상시킬 수 있다는 것이 명백합니다.\n\n또한 Futures의 구체적인 원리를 탐구했습니다. done(), result(), as_completed()과 같은 일반적인 함수들을 다루며, 예제를 통해 그 사용법을 설명했습니다.\n\n기억해야 할 중요한 점은 Python이 글로벌 인터프리터 락(GIL) 때문에 한 번에 하나의 스레드만 실행되도록 한다는 것입니다. 그러나 I/O 작업에 대해서는 스레드가 차단될 때 GIL이 해제되어 다른 스레드가 계속 실행될 수 있습니다.\n\n<img src=\"/TIL/assets/img/2024-07-12-PythonConcurrencyProgrammingFutures_3.png\" />","ogImage":{"url":"/TIL/assets/img/2024-07-12-PythonConcurrencyProgrammingFutures_0.png"},"coverImage":"/TIL/assets/img/2024-07-12-PythonConcurrencyProgrammingFutures_0.png","tag":["Tech"],"readingTime":17},{"title":"스태킹 앙상블 기법 이해하고 활용하는 방법","description":"","date":"2024-07-12 20:20","slug":"2024-07-12-TheStackingEnsembleMethod","content":"\n\n기계 학습에서 쌓기의 힘을 발견해보세요. 이 기술은 여러 모델을 하나로 결합하여 강력한 예측기로 만드는 기법입니다. 이 기사에서는 쌓기의 기초부터 고급 기술까지 살펴보고, 다양한 모델의 장점을 결합하여 정확성을 향상시키는 방법을 소개합니다. 쌓기에 익숙하지 않은 분들이나 최적화 전략을 찾고 계신 분들을 위해, scikit-learn을 활용하여 예측 모델링을 높이는 실용적인 통찰과 팁을 제공합니다.\n\n이 기사는 scikit-learn을 바탕으로 작성되었지만, scikit-learn의 쌓기 모델을 구현하고 모방하는 순수한 Python 클래스를 제공합니다. 이 순수한 Python 구현을 검토하는 것은 여러분의 이해도를 확인하고 테스트하는 데 훌륭한 방법입니다.\n\n이 게시물에서는 다음을 확인하게 됩니다:\n\n- 쌓기가 머신러닝의 앙상블 기법 중 일부인지\n- 쌓기가 예측을 제공하기 위해 내부적으로 어떻게 작동하는지\n- 쌓기가 어떻게 fitting되는지\n- “리스택”이 무엇인지\n- 다층 쌓기를 어떻게 생성할 수 있는지\n- 왜 기본 모델의 성능을 검사해야 하는지\n- 스택 모델의 사용을 조정하고 최적화하는 방법\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\"테이블\" 태그를 마크다운 형식으로 변경해주세요.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\"’베이스 모델’이라 함은 만날 수 있는 모든 전통적인 모델을 의미합니다 - scikit-learn에서 직접 가져와 학습하고 예측할 수 있는 모델입니다. 이러한 베이스 모델은 예를 들어 다음과 같습니다:\n\n- 선형 회귀나 로지스틱 회귀 (및 이들의 변형인 LASSO 또는 리지)\n- 서포트 벡터 회귀 또는 분류기\n- 의사결정 트리 회귀 또는 분류기\n- K-최근접 이웃 회귀 또는 분류기\n\n각각의 이 모델들은 장단점이 있고, 하이퍼파라미터들이 있으며, ‘편안한’ 영역이 있습니다. 따라서, 이 모델들은 여러분의 머신러닝 문제에서 서로 다르게 수행할 수 있습니다: 데이터셋 전체에서 다른 점수를 가질 뿐만 아니라, 데이터셋의 일부 샘플/영역에서는 더 나은 성능을 발휘할 수도 있고, 다른 곳에서는 더 나쁠 수도 있습니다.\n\n앙상블 방법의 일반적인 아이디어는 여러 이러한 베이스 모델을 하나의 더 나은 모델로 결합하는 것입니다. 스태킹 기술에서는 이러한 모델들을 건물의 벽돌로 사용하고 예측을 결합하는 수단으로 사용할 것입니다.\"\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n일반적인 앙상블 모델 기술 중 몇 가지를 빠르게 살펴봅시다:\n\n- 투표: 투표 앙상블은 일련의 기본 모델에서 생성되며, 최종 예측은 기본 모델의 예측을 평균 (회귀의 경우)하거나 가장 예측 클래스 (분류의 경우)로 간단히 계산하여 얻습니다. 이것은 가장 간단한 앙상블 기술 중 하나이며 이해하고 설명하기 가장 쉽습니다.\n- 부스팅: 일련의 약한 학습자들의 (거의 더미 모델) 예측이 순환적으로 가중치를 사용하여 조정됩니다. 이 기술은 Adaboost 알고리즘과 GradientBoosting으로 이어지며 성능이 우수한 모델 중 하나입니다.\n- 배깅: 부트스트랩 집계라고도 하는 병렬 앙상블 기술로, 여러 개별 모델이 전체 데이터 세트의 다른 하위 집합에 적합됩니다. 이러한 모델의 예측은 그런 다음 (투표/평균 사용) 결합됩니다. 개별 모델이 결정 트리인 경우, 이 기술은 랜덤 포레스트 알고리즘으로 이어집니다.\n- 스태킹: 이 게시물이 설명하는 앙상블 기술입니다. 계속 읽어 보세요!\n\n그래서 스태킹은 앙상블 기술 중 하나일 뿐입니다. 스태킹은 위의 앙상블 기술들과 많은 면에서 다릅니다:\n\n- 기본 모델을 순차적으로 결합하지 않습니다.\n- 모델의 가중치를 순환적으로 업데이트하지 않습니다.\n- 기본 모델의 예측을 단순히 평균화하지 않습니다.\n- 다양한 종류의 모델 (LinReg, KNN, SVM 등) 모음과 함께 작동합니다. 반면에 배깅과 부스팅은 일반적으로 동일한 기본 모델 (보통 결정 트리)의 모음을 사용합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 스태킹 작동 방식\n\n스태킹 방법의 가장 큰 차이점과 특징 중 하나는 기본 모델의 구조와 최종 예측을 계산하는 방식입니다. 스태킹 앙상블의 구조는 다음과 같습니다:\n\n- 일련의 기본 모델이 앙상블을 이룸: 이들은 독립적으로 훈련되고 예측합니다. 이러한 모델은 \"기본 레이어\" 또는 스택 구조의 \"0층\"이라고도 불립니다. 이러한 기본 모델은 부스팅/배깅 컨텍스트에서와 같이 \"약 학습기(weak learners)\"로 불릴 수도 있지만, 엄격히 말하면 약 학습기는 아닙니다.\n- 또 다른 단일 모델이 기본 모델의 예측을 감싸 최종 예측을 계산합니다. 이 모델은 \"최종 레이어\" 또는 \"1층\"이라고도 불립니다.\n\n다음과 같이 도식화됩니다 (우리는 지금 예측 과정에 초점을 맞춥니다):\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\n![Image](/TIL/assets/img/2024-07-12-TheStackingEnsembleMethod_1.png)\n\n스태킹의 특징 중요한 것은 다음과 같습니다: 내부적으로 스택의 최종 모델은 일반적인 입력 데이터 집합 X와 함께 작동하지 않고, 대신에 베이스 모델의 예측값에서 학습/예측합니다. 아이디어는 모델을 사용하여 기초 모델이 출력 y를 예측하는 방법을 학습하고, 이러한 예측을 실제 참값 y_true와 비교하는 것입니다.\n\n모델이 적합화된 후, 예측 프로세스는 간단히 다음과 같이 이루어집니다: 새로운 X 데이터 집합(또는 단일 샘플 x)에 대해\n\n- 모든 베이스 모델 예측이 독립적으로 계산됩니다: 모델로 기초 모델에 대해 y_pred=model.predict(X)를 수행합니다.\n- 이러한 y_pred 벡터들은 가로로 연결(concatenated)되어 새로운 2D 데이터 집합을 만들게 되는데, 이 과정에 각 열은 특정 기초 모델에 대한 예측을 나타내고 이 행렬을 X_final이라고 부릅니다. 이것은 최종 추정기의 2D 입력 데이터 집합을 나타냅니다.\n- 이 X_final 2D 데이터 집합은 최종 추정기에 의해 실제 최종 예측을 예측하는 데 사용됩니다: final.predict(X_final)\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/TIL/assets/img/2024-07-12-TheStackingEnsembleMethod_2.png\" />\n\n다시 말해, 스태킹은 기본 모델의 y-예측으로 구성된 내부 2D 데이터셋 행렬이 생성되고, 이 행렬이 최종 모델의 입력으로 사용된다는 것을 의미합니다. 따라서 최종 모델은 y-공간 값의 2D 행렬을 입력으로 받아 실제 1D 예측인 y-공간 값의 출력을 생성합니다.\n\n스태킹을 보는 다른 방법은 간단히 투표 모델로 볼 수 있지만, 평균값/가장 많이 투표된 예측을 가져오는 것이 아니라 이러한 예측값을 다른 모델에 공급하는 것입니다. 다시 말해, 스태킹을 사용하여 투표를 흉내 내기 위해 최종 모델로 평균화/argmax 모델을 사용할 수 있습니다.\n\n일반적으로 회귀 문제의 경우 최종 모델로는 간단한 LinearRegression을, 분류 문제의 경우 LogisticRegression을 사용하며, 이러한 모델은 scikit-learn의 StackingClassifier 및 StackingRegressor의 기본값입니다. 최종 추정기에 의해 기본 모델의 예측을 결합하는 단계는 종종 \"블렌딩\"이라고 불리며, 모든 모델의 예측이 단일 예측으로 혼합됩니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 스택 모델을 적절하게 적합시키는 방법\n\n스택 모델이 작동하는 구체적인 방식 때문에 특정한 적합 절차를 사용해야 합니다.\n\nscikit-learn에서는 어떤 추정기든 자동으로 이 작업이 처리됩니다. 따라서 스택 모델을 사용하기 위해 이 적합 절차를 이해할 필요는 절대적으로 없습니다. 한편, 모델이 어떻게 작동하는지 이해하는 것은 항상 좋은 일이죠. 또한 새로운 것을 배우는 것은 굉장히 재밌고 만족스러운 경험입니다!\n\n스택을 적합시키는 알고리즘은 2단계로 나눌 수 있습니다. 첫 번째는 매우 간단한데, 두 번째는 조금 더 설명이 필요합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 단계 1: 모든 기본 모델(레이어 0에서 온 모델)은 완전히 독립적으로 피팅되며 전체 X/y 데이터셋을 사용합니다. 다시 말해, 첫 번째 단계는 아래와 같습니다: base_models의 각 모델에 대해 model.fit(X, y)를 실행하는 것과 동일합니다.\n- 단계 2: 최종 추정기는 피팅되지 않은 기본 모델을 사용하여 교차 검증 체계를 활용하여 피팅됩니다.\n\n첫 번째 단계는 상당히 명확합니다: 스택 모델인 stack.fit(X, y)에 대해 fit 메서드가 호출되면, X와 y가 각기의 기본 모델로 전달되어 서로 독립적으로 피팅이 진행됩니다. 이 단계는 base_models의 각 모델에 대해 model.fit(X, y)를 실행하는 것과 동일합니다. 이러한 피팅된 추정기는 stack 모델의 stack.estimators_ 속성에 저장됩니다.\n\n그런 다음 최종 추정기를 훈련시키기 위해 다른 방법이 사용됩니다. 이미 해당 피팅을 위한 대상 출력 y가 있지만, 여전히 기본 모델들의 예측으로 생성된 중간 X_final 데이터셋을 생성해야 합니다. 이미 피팅된 추정기들에 .predict 메서드를 사용하는 것은 입력 X가 이미 기본 추정기에 의해 보고되고 훈련되었기 때문에 어떤 종류의 오버피팅을 초래할 수 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/TIL/assets/img/2024-07-12-TheStackingEnsembleMethod_4.png\" />\n\n다시 말해 이미 학습된 기본 추정기를 사용하여 입력 데이터 세트를 최종 추정기에 맞출 수 없습니다. 대신, 각 추정기는 동일한 교차 검증 폴드로 cross_val_predict 함수에 공급되어 모든 기본 추정기에 대한 y_pred를 생성합니다. 이러한 기본 예측값은 X_final 훈련 데이터 세트에 수평으로 연결되어 최종 추정기가 X_final, y로 적합하게 됩니다.\n\n<img src=\"/TIL/assets/img/2024-07-12-TheStackingEnsembleMethod_5.png\" />\n\n이제 문제를 아래와 같이 분해하는 방법을 알게 되었습니다: \"최종 추정기가 대상 y에 적합하도록 사용하는 X 데이터 세트는 모든 추정기에 의해 제공된 예측 값들을 연결하여 생성됩니다. (모든 추정기에 대해 동일한 cv 전략을 사용하여, 모든 기본 추정기가 항상 동일한 입력/출력을 보는 사실을 고려하기 위함). 이는 첫 번째 단계에서 적합된 기본 모델들(전체 X 데이터 세트로 적합된 모델들)이 최종 추정기를 적합하기 위해 사용되지 않는다는 것을 의미합니다. 대신, 각 폴드에 대해 적합되지 않은 기본 모델의 사본이 사용되며, 각 폴드에 대한 예측값이 연결되어 X_final을 생성합니다. (각 모델에 대해 수직으로, 그리고 수평으로 X_final을 만들기 위해)\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n\n![image](/TIL/assets/img/2024-07-12-TheStackingEnsembleMethod_6.png)\n\n이 방법을 통해 최종 추정기는 베이스 모델의 예측값이 이미 확인되지 않은 샘플에 적합됩니다.\n\n이 적합 절차를 보는 또 다른 방법은 교차 검증 데이터 폴드 분해에서 중간 2D 데이터 세트 X_final을 표현하는 것입니다: X 데이터 세트는 N 개의 폴드(X_train, X_test, y_train, y_test)로 분할됩니다. 폴드는 각 개별 샘플이 테스트 세트로 한 번만 사용되는 \"분할\" 기준을 준수해야 합니다. 이 분할의 각각에 대해 미적합 기본 모델의 복사본이 .fit(X_train, y_train)을 사용하여 적합되고, 해당 예측값이 .predict(X_test)을 사용하여 계산됩니다. 이렇게 하면 모든 베이스 모델에 대한 모든 분할에 대해 y_pred를 얻을 수 있습니다. 최종 X_train은 모든 분할의 예측값을 수직으로 연결한 것입니다.\n\n# 리스태킹\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n스태킹 아키텍처는 모델의 추가적인 사용자 정의를 가능하게 합니다. 이는 모델이 작동하는 방식을 조정할 수 있는 하이퍼파라미터로 생각할 수 있습니다. 아이디어는 정말 간단합니다: 베이스 모델의 수평으로 쌓인 예측뿐만 아니라 원본 입력 X 데이터 집합도 마지막 추정기의 입력에 추가됩니다. 이렇게 하면 최종 추정기가 베이스 모델의 예측과 원본 데이터를 모두 포함하여 데이터에서 더 많은 정보를 찾을 수 있습니다. 다시 말해, 스태킹 모델의 복잡성은 마지막 모델의 입력 특성의 수를 늘림으로써 증가됩니다.\n\n사이킷런에서는 이를 스택 모델의 passthrough=True 매개변수를 통해 수행합니다.\n\n![Stacking Ensemble Method](/TIL/assets/img/2024-07-12-TheStackingEnsembleMethod_7.png)\n\n물론 X 행렬은 학습 시간과 예측 시간 모두 X_final에 추가됩니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 다층\n\n스태킹 모델의 디자인 방식을 통해 추가적인 스택을 간단히 만들 수 있어 더 복잡한 모델을 만들 수 있습니다. 이는 새로운 스택을 중첩하여 수행될 수 있는데, 새로운 스택은 이전 스택의 최종 에스티메이터로 설정됩니다. 이 작업은 모델의 복잡성을 높이지만, 학습/예측 시간과 과적합 가능성이 증가할 수 있다는 점을 유의해야 합니다.\n\n![이미지](/TIL/assets/img/2024-07-12-TheStackingEnsembleMethod_8.png)\n\n# 스택 모델 튜닝\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이미 언급한 대로, 스태킹은 각 베이스 모델의 최상의 부분을 유지하려는 목적으로, 서로의 약점을 보완할 수 있도록 하는 것을 목표로 합니다. 즉, 모든 모델이 동일한 샘플에서 동일한 성능을 발휘한다면, 스택 모델은 어떤 개선도 이루어지지 않을 것입니다. 따라서 다양한 데이터셋의 위상 영역에서 이상적으로 성능을 발휘하는 서로 다르게 작용하는 베이스 모델을 사용하여 스택 모델을 최적화할 수 있습니다.\n\n따라서 베이스 모델이 어떻게 성능을 발휘하고, 그들의 예측 오류가 서로 상관 관계가 있는지 여부를 검토하는 것이 좋은 아이디어일 수 있습니다. 두 모델이 거의 동일한 오류를 공유한다면, 한 모델은 아마 버릴 수 있을 것입니다 - 전반적인 성능을 유지하면서 연산 복잡성을 줄일 수 있습니다.\n\n스택은 모든 베이스 모델과 최종 모델을 포함하므로, 모든 하이퍼파라미터를 상속합니다. 스택의 성능에 관심을 가지기 때문에 베이스 모델의 성능에 엄격하게 초점을 맞추는 것이 아니라, 그들의 하이퍼파라미터는 스택에서 튜닝되어야 합니다. 다시 말해, 베이스 모델은 하이퍼파라미터 A로 혼자서 더 나은 성능을 발휘할 수 있지만, 스택의 성능은 하이퍼파라미터 B로 더 나을 수 있습니다.\n\n마지막으로, passthrough=True 또는 False를 사용하여 스택 모델을 조정하는 또 다른 방법이 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n일반적으로, 스택이 있는지 기본 모델과 동일한 성능을 발휘하는지 항상 확인해야 합니다. 그렇다면, 스택을 버리거나 조정해야 합니다. 높은 계산 복잡성은 상당한 성능 향상과 함께 제공된다는 것을 명심해야 합니다.\n\n스태킹은 앙상블 모델의 한 형태이므로, 개별 모델이 완벽하지 않다고 해서 항상 안 좋은 것은 아닙니다. 목표는 모델의 성능이 서로 보완될 수 있도록 하는 것입니다 — 마치 약한 학습자들이 결합되어 더 나은 모델이 만들어지는 것처럼요.\n\n실제로, 스택 예측기는 기본 계층의 최고 예측기만큼 잘 예측하며 때로는 이러한 예측기의 서로 다른 강점을 결합함으로써 이를 앞지르기도 합니다. 그러나 스택 예측기로 훈련과 예측을 하는 것은 계산적으로 비용이 많이 드는 일입니다.\n\n# scikit-learn을 사용한 스태킹\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 우리는 스태킹에 대해 모두 알았으니, scikit-learn과 함께 어떻게 사용하는지 살펴봅시다. 기억해야 할 유일한 것은 scikit-learn에서 스태킹이 모델/메타-모델로 구현된다는 것입니다:\n\n- .fit 및 .predict 메서드를 가진 모델이다.\n- 다른 모델을 입력으로 받는 메타-모델이다.\n\nScikit-learn은 앙상블 모듈에서 분류 및 회귀용 스태킹 모델을 제공합니다. 회귀용 매우 간단한 예제를 살펴보겠습니다:\n\n```python\n%matplotlib qt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris, make_moons, make_circles\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom sklearn.ensemble import StackingClassifier\n\nX, y = make_circles(n_samples=500, noise=0.3, factor=0.5, random_state=1, shuffle=True)\n\n# 3개의 기본 모델을 사용합니다.\nbase_models = [\n    ('knn', KNeighborsClassifier(n_neighbors=3)),\n    ('logreg', LogisticRegression(max_iter=1000)),\n    ('svc', SVC(C=0.05)),\n]\n\n# 기본 모델을 사용하여 스택을 만들고 최종 예측기로 LogisticRegression을 사용합니다.\nstack = StackingClassifier(\n    estimators=base_models,\n    final_estimator=LogisticRegressionCV(max_iter=1000)\n)\n\nrestack = StackingClassifier(\n    estimators=base_models,\n    final_estimator=LogisticRegressionCV(max_iter=1000),\n    passthrough=True,\n)\n\n# 그런 다음 모든 모델을 비교할 수 있는 멋진 목록을 만듭니다.\nmodels = base_models + [('stack', stack), ('restack', restack)]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n\nfor name, model in models:\n    model.fit(X_train, y_train)\n\n\n# stacking classifier의 결정 경계\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.2),\n                     np.arange(y_min, y_max, 0.2))\n\nfig, axes = plt.subplots(1, len(models), sharex=True, sharey=True)\nfor ax, (name, model) in zip(axes, models):\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    ax.contourf(xx, yy, Z, alpha=0.8)\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k')\n    ax.set_title(f\"{name} 점수={model.score(X_test, y_test):.2f}\")\n    ax.set_xlabel(\"특성 1\")\n    ax.set_ylabel(\"특성 2\")\nfig.tight_layout()\r\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n완벽을 위해 stack의 다시 쌓은 버전을 추가했습니다. 플롯을 보면 기본 모델을 결합하면 stack의 성능이 기본 모델 중 가장 좋은 성능을 조금 능가한다는 것을 알 수 있습니다. 다시 쌓기를 사용하면 성능이 더 조금 향상됩니다.\n\n위에서 설명한 것은 모델 스택을 만드는 방법을 보여줄 뿐, 실제 성능 추정을 위해서는 튜닝과 교차 검증이 필요합니다.\n\n## \"처음부터\" 스택 구성하기\n\n이제 scikit-learn API를 사용하여 스태킹의 간단한 구현을 살펴보겠습니다. 이것은 스태킹이 그렇게 복잡하지 않다는 것을 보여주는 장난감 예제에 불과합니다. 학습/설명 목적 이외에는 scikit-learn의 구현을 사용해야 합니다!\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nimport numpy as np\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin, clone\nfrom sklearn.model_selection import cross_val_predict\n\nclass HomeMadeStackingRegressor(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_estimators, meta_estimator):\n        self.base_estimators = base_estimators\n        self.meta_estimator = meta_estimator\n\n    def fit(self, X, y):\n        self.base_estimators_ = [clone(est) for est in self.base_estimators]\n        self.meta_estimator_ = clone(self.meta_estimator)\n\n        # Fit base estimators independently\n        for est in self.base_estimators_:\n            est.fit(X, y)\n\n        # Generate X_final feature dataset for the final estimator\n        # as the concatenation of the cross_val_predict results of the base estimators\n        X_final = np.array([cross_val_predict(est, X, y, cv=5) for est in self.base_estimators_]).T\n\n        # Fit final estimator\n        self.meta_estimator_.fit(X_final, y)\n        return self\n\n    def predict(self, X):\n        meta_features = np.array([est.predict(X) for est in self.base_estimators_]).T\n        return self.meta_estimator_.predict(meta_features)\n\n\nboston = load_diabetes()\nX, y = boston.data, boston.target\n\nbase_estimators = [RandomForestRegressor(n_estimators=20, max_depth=3, random_state=1), Ridge()]\nmeta_estimator = Ridge()\nstack = HomeMadeStackingRegressor(base_estimators, meta_estimator)\n\nmodels = list(zip(['rf', 'ridge'], base_estimators)) + [('stack', stack)]\n\nresults = []\nfor name, basemodel in models:\n    cv_results = cross_validate(basemodel, X, y, return_estimator=True, return_train_score=True)\n    df = pd.DataFrame.from_dict(cv_results)\n    df.reset_index(names=\"run\", inplace=True)\n    df[\"model\"] = name\n    results.append(df)\n\ncv_results = pd.concat(results).reset_index(drop=True)\ncv_results_melt = cv_results.melt(id_vars=[\"model\", \"run\"], value_vars=[\"test_score\", \"train_score\", \"fit_time\", \"score_time\"], var_name=\"metric\", value_name='value')\nsns.catplot(cv_results_melt, y=\"model\", x=\"value\", kind=\"bar\", hue=\"model\", col=\"metric\", sharex=False)\n```\n\n스태킹의 테스트 점수가 기본 모델보다 우수함을 확인할 수 있습니다. 그러나 더 높은 학습 시간을 필요로 합니다.\n\n# 요약\n\n스태킹에 대해 기억해야 할 것들입니다:\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 앙상블 기법이며, 최종 모델을 사용하여 기본 모델의 예측을 결합합니다.\n- 최종 모델은 기본 모델의 예측을 수평 스택으로 사용합니다.\n- 기본 모델이 서로 다르고 서로의 약점을 보완할 때 가장 잘 작동합니다. 최소한 기본 모델 중 가장 우수한 것보다는 성능이 우수해야 합니다.\n- 기본 및 최종 모델의 하이퍼파라미터와 통과 매개변수를 사용하여 조정할 수 있습니다. 다시 쌓을 수 있도록 하거나 아니면 그렇지 않도록 설정할 수 있습니다.\n- 적합화 과정에는 기본 추정기와 동일하게 독립적으로 적합된 최종 추정기를 적합화하기 위해 교차 검증 체계를 사용합니다.\n\n이 글이 마음에 드셨다면 다른 글도 읽어보세요:","ogImage":{"url":"/TIL/assets/img/2024-07-12-TheStackingEnsembleMethod_0.png"},"coverImage":"/TIL/assets/img/2024-07-12-TheStackingEnsembleMethod_0.png","tag":["Tech"],"readingTime":19}],"page":"11","totalPageCount":33,"totalPageGroupCount":2,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true}