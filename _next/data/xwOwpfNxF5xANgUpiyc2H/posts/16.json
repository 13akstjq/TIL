{"pageProps":{"posts":[{"title":"Scikit-learn 2024 시각화 가이드 모델을 이해하기 쉽게 만드는 방법","description":"","date":"2024-07-09 20:32","slug":"2024-07-09-Scikit-learnVisualizationGuideMakingModelsSpeak","content":"\n아래는 제가 요청하신 테이블을 Markdown 형식으로 변경한 것입니다.\n\n| Header One  | Header Two  |\n| ----------- | ----------- |\n| Content One | Content Two |\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Scikit-learn Visualization Guide Making Models Speak](/TIL/assets/img/2024-07-09-Scikit-learnVisualizationGuideMakingModelsSpeak_1.png)\n\n이 그래프를 보면 동일한 데이터 세트에서 오른쪽 모델이 더 일반화하는 데 더 좋다는 것을 알 수 있습니다.\n\n대부분의 머신러닝 서적은 시각화에 대해 matplotlib 코드를 사용하기를 선호합니다. 이는 다음과 같은 문제를 야기합니다:\n\n- Matplotlib로 그리기에 대해 많은 내용을 배워야 합니다.\n- 플로팅 코드가 노트북을 가득 채우므로 읽기 어려워집니다.\n- 때로는 비즈니스 환경에서 이상적이지 않은 타사 라이브러리가 필요할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n좋은 소식이에요! Scikit-learn은 이제 Display 클래스를 제공하며 from_estimator 및 from_predictions과 같은 메소드를 사용하여 다양한 상황에서 그래프를 그리기가 훨씬 쉬워졌어요.\n\n궁금하신가요? 이 멋진 API를 보여드릴게요.\n\n# Scikit-learn Display API 소개\n\n## 사용 가능한 API를 찾으려면 utils.discovery.all_displays를 사용하세요\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nScikit-learn (sklearn)은 항상 새 릴리스에서 Display API를 추가하기 때문에 당신의 버전에서 무엇을 사용할 수 있는지 알아두는 것이 중요합니다.\n\nSklearn의 utils.discovery.all_displays를 사용하면 사용할 수 있는 클래스들을 볼 수 있습니다.\n\n```python\nfrom sklearn.utils.discovery import all_displays\n\ndisplays = all_displays()\ndisplays\n```\n\n예를 들어, 내 Scikit-learn 1.4.0에서 이러한 클래스들을 사용할 수 있습니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n[\n  (\"CalibrationDisplay\", sklearn.calibration.CalibrationDisplay),\n  (\"ConfusionMatrixDisplay\", sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay),\n  (\"DecisionBoundaryDisplay\", sklearn.inspection._plot.decision_boundary.DecisionBoundaryDisplay),\n  (\"DetCurveDisplay\", sklearn.metrics._plot.det_curve.DetCurveDisplay),\n  (\"LearningCurveDisplay\", sklearn.model_selection._plot.LearningCurveDisplay),\n  (\"PartialDependenceDisplay\", sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay),\n  (\"PrecisionRecallDisplay\", sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay),\n  (\"PredictionErrorDisplay\", sklearn.metrics._plot.regression.PredictionErrorDisplay),\n  (\"RocCurveDisplay\", sklearn.metrics._plot.roc_curve.RocCurveDisplay),\n  (\"ValidationCurveDisplay\", sklearn.model_selection._plot.ValidationCurveDisplay),\n];\n```\n\n## decision_boundaries를 위해 inspection.DecisionBoundaryDisplay 사용하기\n\ndecision boundaries로 시작해보죠.\n\nmatplotlib를 사용하여 draw 하는 경우, 번거롭습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- np.linspace을 사용하여 좌표 범위 설정;\n- plt.meshgrid를 사용하여 그리드 계산;\n- plt.contourf를 사용하여 결정 경계를 채우기;\n- 그런 다음 plt.scatter를 사용하여 데이터 포인트를 플로팅합니다.\n\n이제 inspection.DecisionBoundaryDisplay를 사용하여이 프로세스를 간소화 할 수 있습니다:\n\n```js\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.datasets import load_iris\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\niris = load_iris(as_frame=True)\nX = iris.data[['petal length (cm)', 'petal width (cm)']]\ny = iris.target\n\n\nsvc_clf = make_pipeline(StandardScaler(),\n                        SVC(kernel='linear', C=1))\nsvc_clf.fit(X, y)\n\ndisplay = DecisionBoundaryDisplay.from_estimator(svc_clf, X,\n                                                 grid_resolution=1000,\n                                                 xlabel=\"Petal length (cm)\",\n                                                 ylabel=\"Petal width (cm)\")\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, edgecolors='w')\nplt.title(\"Decision Boundary\")\nplt.show()\n```\n\n이 그림에서 최종 효과를 확인하십시오:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/TIL/assets/img/2024-07-09-Scikit-learnVisualizationGuideMakingModelsSpeak_2.png\" />\n\n기억해 주세요. Display 기능은 2D만 그릴 수 있으므로 데이터가 두 개의 특성 또는 축소된 차원만 가지고 있는지 확인하세요.\n\n## 확률 보정을 위해 calibration.CalibrationDisplay 사용\n\n분류 모델을 비교하기 위해 확률 보정 곡선은 모델이 예측에 자신감을 가지고 있는지를 보여줍니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nCalibrationDisplay는 모델의 predict_proba를 사용합니다. Support Vector Machine을 사용하는 경우 확률을 True로 설정해주세요:\n\n```js\nfrom sklearn.calibration import CalibrationDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nX, y = make_classification(n_samples=1000,\n                           n_classes=2, n_features=5,\n                           random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                            test_size=0.3, random_state=42)\nproba_clf = make_pipeline(StandardScaler(),\n                          SVC(kernel=\"rbf\", gamma=\"auto\",\n                              C=10, probability=True))\nproba_clf.fit(X_train, y_train)\n\nCalibrationDisplay.from_estimator(proba_clf,\n                                            X_test, y_test)\n\nhist_clf = HistGradientBoostingClassifier()\nhist_clf.fit(X_train, y_train)\n\nax = plt.gca()\nCalibrationDisplay.from_estimator(hist_clf,\n                                  X_test, y_test,\n                                  ax=ax)\nplt.show()\n```\n\n<img src=\"/TIL/assets/img/2024-07-09-Scikit-learnVisualizationGuideMakingModelsSpeak_3.png\" />\n\n## 혼동 행렬에 대한 metrics.ConfusionMatrixDisplay 사용하기\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n분류 모델을 평가하고 불균형 데이터를 다룰 때, 우리는 정밀도와 재현율을 살펴봅니다.\n\n이들은 TP, FP, TN, FN으로 나뉘며, 혼동 행렬을 구성합니다.\n\n혼동 행렬을 그리려면 metrics.ConfusionMatrixDisplay를 사용하세요. 이렇게 그림을 그릴 수 있고요, 이미 잘 알려져 있어서 자세하게 설명은 생략할게요.\n\n```js\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ndigits = fetch_openml('mnist_784', version=1)\nX, y = digits.data, digits.target\nrf_clf = RandomForestClassifier(max_depth=5, random_state=42)\nrf_clf.fit(X, y)\n\nConfusionMatrixDisplay.from_estimator(rf_clf, X, y)\nplt.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 이미지 링크입니다:\n\n![Scikit-learn Visualization Guide](/TIL/assets/img/2024-07-09-Scikit-learnVisualizationGuideMakingModelsSpeak_4.png)\n\n## metrics.RocCurveDisplay 및 metrics.DetCurveDisplay\n\n이 두 가지가 함께 소개되는 이유는 종종 측정할 때 함께 사용하기 때문입니다.\n\nRocCurveDisplay는 모델의 TPR 및 FPR을 비교합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이진 분류의 경우, 낮은 FPR과 높은 TPR을 원하기 때문에 좌상단이 가장 좋습니다. Roc 곡선은 이 쪽으로 휘어집니다.\n\nRoc 곡선이 좌상단에 근접하여 유지되기 때문에 우하단이 비어 있는데, 이는 모델 간 차이를 파악하기 어렵게 만듭니다.\n\n그래서 우리는 또한 FNR과 FPR로 Det 곡선을 그리는 DetCurveDisplay를 사용합니다. 이는 Roc 곡선보다 명확하게 만들어주는 데 더 많은 공간을 사용합니다.\n\nDet 곡선의 완벽한 지점은 좌하단입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn.metrics import DetCurveDisplay\n\nX, y = make_classification(n_samples=10_000, n_features=5,\n                           n_classes=2, n_informative=2)\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                             test_size=0.3, random_state=42,\n                                                   stratify=y)\n\n\nclassifiers = {\n    \"SVC\": make_pipeline(StandardScaler(),\n                        SVC(kernel=\"linear\", C=0.1, random_state=42)),\n    \"Random Forest\": RandomForestClassifier(max_depth=5, random_state=42)\n}\n\nfig, [ax_roc, ax_det] = plt.subplots(1, 2, figsize=(10, 4))\nfor name, clf in classifiers.items():\n    clf.fit(X_train, y_train)\n\n    RocCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_roc, name=name)\n    DetCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_det, name=name)\n```\n\n<img src=\"/TIL/assets/img/2024-07-09-Scikit-learnVisualizationGuideMakingModelsSpeak_5.png\" />\n\n## Using metrics.PrecisionRecallDisplay to adjust thresholds\n\nWith imbalanced data, you might want to shift recall and precision.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 이메일 사기를 방지하려면 고정도가 필요합니다.\n- 질병 선별을 위해서는 더 많은 사례를 포착하기 위해 고회수가 필요합니다.\n\n임계값을 조정할 수 있지만, 적절한 양이 무엇인지 궁금하신가요?\n\n여기서 metrics.PrecisionRecallDisplay가 도움이 될 수 있습니다.\n\n```js\nfrom xgboost import XGBClassifier\nfrom sklearn.datasets import load_wine\nfrom sklearn.metrics import PrecisionRecallDisplay\n\nwine = load_wine()\nX, y = wine.data[wine.target<=1], wine.target[wine.target<=1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n\nxgb_clf = XGBClassifier()\nxgb_clf.fit(X_train, y_train)\n\nPrecisionRecallDisplay.from_estimator(xgb_clf, X_test, y_test)\nplt.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/TIL/assets/img/2024-07-09-Scikit-learnVisualizationGuideMakingModelsSpeak_6.png\" />\n\nScikit-learn의 디자인을 따르는 모델은 여기처럼 그릴 수 있습니다. 편리하죠?\n\n## 회귀 모델에 metrics.PredictionErrorDisplay 사용하기\n\n우리는 분류에 대해 이야기했었는데, 이제 회귀에 대해 이야기해볼게요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n사이킷런의 metrics.PredictionErrorDisplay는 회귀 모델을 평가하는 데 도움이 됩니다.\n\n```python\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import PredictionErrorDisplay\n\nrng = np.random.default_rng(42)\nX = rng.random(size=(200, 2)) * 10\ny = X[:, 0]**2 + 5 * X[:, 1] + 10 + rng.normal(loc=0.0, scale=0.1, size=(200,))\n\nreg = make_pipeline(StandardScaler(), SVR(kernel='linear', C=10))\nreg.fit(X, y)\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\nPredictionErrorDisplay.from_estimator(reg, X, y, ax=axes[0], kind=\"actual_vs_predicted\")\nPredictionErrorDisplay.from_estimator(reg, X, y, ax=axes[1], kind=\"residual_vs_predicted\")\nplt.show()\n```\n\n![Image](/TIL/assets/img/2024-07-09-Scikit-learnVisualizationGuideMakingModelsSpeak_7.png)\n\n그림에서와 같이 두 종류의 그래프를 그릴 수 있습니다. 왼쪽 그래프는 예측 대 실제 값 비교를 보여주며, 선형 회귀 분석에 적합합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만 모든 데이터가 완벽하게 선형적이지는 않습니다. 그럴 때는 적절한 그래프를 사용하세요.\n\n실제 대 예측 차이인 잔차 도표를 그려보세요.\n\n이 도표의 바나나 모양은 데이터가 선형 회귀에 맞지 않을 수 있다는 것을 시사합니다.\n\n선형에서 rbf 커널로 전환하는 것이 도움이 될 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nreg = make_pipeline(StandardScaler(), SVR((kernel = \"rbf\"), (C = 10)));\n```\n\n![Image](/TIL/assets/img/2024-07-09-Scikit-learnVisualizationGuideMakingModelsSpeak_8.png)\n\n이렇게 rbf를 사용하면 잔차 플롯이 더 나아 보여요.\n\n## 학습 곡선에 model_selection.LearningCurveDisplay 사용하기\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n성능을 평가한 후 LearningCurveDisplay를 사용하여 최적화를 살펴봅시다.\n\n첫 번째로, 학습 곡선을 확인해봅시다. 이 모델이 다양한 학습 및 테스트 데이터로 얼마나 일반화되며, 과적합 또는 편향 문제가 있는지 살펴봅니다.\n\n아래에서는 DecisionTreeClassifier와 GradientBoostingClassifier를 비교하여 학습 데이터가 변할 때 어떻게 작동하는지 살펴봅니다.\n\n```python\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import LearningCurveDisplay\n\nX, y = make_classification(n_samples=1000, n_classes=2, n_features=10,\n                           n_informative=2, n_redundant=0, n_repeated=0)\n\ntree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\ngb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=3, tol=1e-3)\n\ntrain_sizes = np.linspace(0.4, 1.0, 10)\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\nLearningCurveDisplay.from_estimator(tree_clf, X, y,\n                                    train_sizes=train_sizes,\n                                    ax=axes[0],\n                                    scoring='accuracy')\naxes[0].set_title('DecisionTreeClassifier')\nLearningCurveDisplay.from_estimator(gb_clf, X, y,\n                                    train_sizes=train_sizes,\n                                    ax=axes[1],\n                                    scoring='accuracy')\naxes[1].set_title('GradientBoostingClassifier')\nplt.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/TIL/assets/img/2024-07-09-Scikit-learnVisualizationGuideMakingModelsSpeak_9.png\" />\n\n해당 그래프는 트리 기반 GradientBoostingClassifier가 훈련 데이터에서 높은 정확도를 유지하더라도, 테스트 데이터에서는 DecisionTreeClassifier와 비교하여 상당한 장점이 없다는 것을 보여줍니다.\n\n## 매개변수 튜닝 시 시각화를 위해 model_selection.ValidationCurveDisplay 사용\n\n그러므로, 다른 부분에 대해 일반화되지 않는 모델의 경우, 모델의 정규화 매개변수를 조정하여 성능을 미세 조정해 볼 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n전통적인 방법은 GridSearchCV 또는 Optuna와 같은 도구를 사용하여 모델을 조정하는 것이었지만, 이러한 방법은 전반적으로 성능이 가장 좋은 모델만 제공하며 조정 과정이 그다지 직관적이지 않습니다.\n\n특정 매개변수를 조정하여 모델의 영향을 테스트하고 싶은 시나리오의 경우, model_selection.ValidationCurveDisplay를 사용하여 매개변수가 변경됨에 따라 모델이 어떻게 수행되는지 시각화하는 것을 권장합니다.\n\n```js\nfrom sklearn.model_selection import ValidationCurveDisplay\nfrom sklearn.linear_model import LogisticRegression\n\nparam_name, param_range = \"C\", np.logspace(-8, 3, 10)\nlr_clf = LogisticRegression()\n\nValidationCurveDisplay.from_estimator(lr_clf, X, y,\n                                      param_name=param_name,\n                                      param_range=param_range,\n                                      scoring='f1_weighted',\n                                      cv=5, n_jobs=-1)\nplt.show()\n```\n\n<img src=\"/TIL/assets/img/2024-07-09-Scikit-learnVisualizationGuideMakingModelsSpeak_10.png\" />\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 아쉬운 점\n\n이러한 표시물을 모두 시도해본 후 몇 가지 아쉬운 점을 인정해야 합니다:\n\n- 가장 큰 아쉬움은 이러한 API의 대부분이 자세한 튜토리얼을 제공하지 않는다는 것입니다. 이것이 Scikit-learn의 철저한 문서와 비교되어 잘 알려지지 않은 이유일 것입니다.\n- 이러한 API는 다양한 패키지에 흩어져 있어 한 곳에서 참조하기 어렵습니다.\n- 코드는 여전히 매우 기본적입니다. 종종 Matplotlib의 API와 함께 사용하여 작업을 완료해야 합니다. 전형적인 예는 DecisionBoundaryDisplay인데, 결정 경계를 플로팅한 후에도 데이터 분포를 플로팅하기 위해 Matplotlib이 필요합니다.\n- 확장하기 어렵습니다. 몇 가지 매개변수를 확인하는 메서드 외에 도구나 방법으로 내 모델 시각화 과정을 간단하게 하는 것은 힘듭니다. 많은 부분을 다시 작성해야 합니다.\n\n이러한 API들이 더 많은 관심을 받고, 버전이 업그레이드되는 과정에서 시각화 API를 사용하기가 더욱 쉬워지기를 희망합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 결론\n\n머신 러닝 여정에서 모델을 시각화로 설명하는 것은 그들을 훈련시키는 것만큼 중요합니다.\n\n본문에서는 현재 버전의 scikit-learn에서 다양한 플로팅 API를 소개했습니다.\n\n이러한 API를 사용하면 Matplotlib 코드를 간소화하고 학습 곡선을 완화시키며 모델 평가 프로세스를 간소화할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nAPI에 대해 상세히 다루지 않아서 죄송합니다. 자세한 내용을 원하시면 관련 공식 문서를 확인해보세요.\n\n이제 당신 차례입니다. 기계 학습 방법을 시각화하는 데 기대하는 점이 무엇인가요? 의견을 자유롭게 남겨 주세요.\n\n이 글을 즐겼다면, 더 많은 첨단 데이터 과학 팁을 받고 싶다면 지금 구독하세요! 피드백과 질문은 언제나 환영합니다. 아래 댓글에서 토론해요!\n\n이 기사는 원문이 Data Leads Future에 게재되었습니다.\n","ogImage":{"url":"/assets/img/2024-07-09-Scikit-learnVisualizationGuideMakingModelsSpeak_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-Scikit-learnVisualizationGuideMakingModelsSpeak_0.png","tag":["Tech"],"readingTime":19},{"title":"GraphQL API에서 인증 및 권한 관리하는 방법","description":"","date":"2024-07-09 20:30","slug":"2024-07-09-HowtohandleauthenticationandauthorizationinGraphQLAPI","content":"\n아래는 작성한 표의 내용이에요.\n\n| 제목                             | 링크                                                                                           |\n| -------------------------------- | ---------------------------------------------------------------------------------------------- |\n| Authentication and authorization | [여기](/TIL/assets/img/2024-07-09-HowtohandleauthenticationandauthorizationinGraphQLAPI_0.png) |\n\n인증(Authentication)과 권한 부여(authorization)는 종종 혼동되지만, 이러한 개념들은 서로 다른 프로세스를 담당하고 있어요. '인증'은 사용자 식별을 결정하며(사용자가 시스템에 로그인되어 있는지 여부), '권한 부여'는 인증된 사용자가 특정 리소스에 액세스할 수 있는지 여부를 나타냅니다. 그래서 보통 인증 단계가 권한 부여 단계를 선행해요. GraphQL에서 인증과 권한 부여는 도전적일 수 있는데 이는 하나의 노출된 HTTP 엔드포인트 (예: /graphql)만 있기 때문이에요. 이 엔드포인트 진입점에서 사용자를 인증할 수는 있지만, 그 구현에서 일부 리소스에 대한 공개 접근 옵션을 포기해야 할 수 있어요. 이 유일한 엔드포인트 진입에서 권한을 부여하는 것은 불가능해요. 왜냐하면 어떤 리소스가 쿼리될 지 모르기 때문이에요.\n\n이 게시물의 영감은 해당 주제에 대한 답변을 찾는 스택오버플로우 질문에서 얻은 거예요. # 애플리케이션 설정\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저의 텍스트는 Python에서 GraphQL와 REST의 예제 구현을 비교한 내용에 대한 후속 게시물입니다. 따라서 애플리케이션을 설정하는 데 필요한 요구 사항을 찾을 수 있습니다.\n\n# 회원 가입 / 로그인\n\n우선 이메일 및 해싱된 패스워드 속성을 가진 간단한 사용자 모델(User)부터 시작합니다.\n\n```js\nfrom sqlalchemy import Column, Integer, String\nfrom sqlalchemy.orm import relationship\nfrom db import Base\n\nclass User(Base):\n    __tablename__ = \"user\"\n\n    id = Column(Integer, primary_key=True, autoincrement=True)\n    email = Column(String, unique=True)\n    password = Column(String)\n    table_bookings = relationship(\"TableBooking\")\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n사용자 등록은 이메일과 비밀번호 매개변수를 받아들이고 데이터베이스에 사용자 레코드를 생성하는 SignUp 뮤테이션을 통해 제공됩니다.\n\n```js\n# api/graphql.py\nimport graphene\nfrom service import sign_up\n\nclass SignUp(graphene.Mutation):\n    class Arguments:\n        email = graphene.String(required=True)\n        password = graphene.String(required=True)\n\n   user = graphene.Field(UserNode)\n\n   def mutate(self, info, email: str, password: str):\n        session = info.context[\"session\"]\n        user = sign_up(session, email, password)\n        return SignUp(user=user)\n\nclass Mutation(graphene.ObjectType):\n    sign_up = SignUp.Field()\n\n# service.py\nfrom auth import generate_password_hash\n\nclass UserAlreadyExist(Exception):\n    pass\n\ndef sign_up(session: Session, email: str, password) -> User:\n    if session.query(User).filter_by(email=email).first():\n        raise UserAlreadyExist()\n    user = User(email=email, password=generate_password_hash(password))\n    session.add(user)\n    session.commit()\n    return user\n\n# auth.py\nimport hashlib\n\nSALT = \"STRONg@Salt\"\n\ndef generate_password_hash(password: str) -> str:\n    h = hashlib.md5(f\"{password}{SALT}\".encode())\n    return h.hexdigest()\n```\n\n뮤테이션은 /graphql 엔드포인트에서 POST 요청을 통해 실행됩니다. GraphQL에 대한 이전 게시물과 같이 insomnia를 사용하여 HTTP 요청을 수행합니다.\n\n![이미지](/TIL/assets/img/2024-07-09-HowtohandleauthenticationandauthorizationinGraphQLAPI_1.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n사용자 인스턴스가 생성되면 올바른 자격 증명이 전달되면 사용자 인증 JWT 토큰을 생성하는 SignIn 뮤테이션이 필요합니다.\n\n```js\n# api/graphql.py\nimport graphene\nfrom service import sign_in\n\nclass SignIn(graphene.Mutation):\n    class Arguments:\n        email = graphene.String(required=True)\n        password = graphene.String(required=True)\n\n   token = graphene.String()\n\n   def mutate(self, info, email: str, password: str):\n        session = info.context[\"session\"]\n        token = sign_in(session, email, password)\n        return SignIn(token=token)\n\n   class Mutation(graphene.ObjectType):\n        sign_in = SignIn.Field()\n\n# service.py\nfrom sqlalchemy.orm import Session\nfrom auth import generate_token, verify_password\nfrom models import User\n\nclass UserAuthenticationError(Exception):\n    pass\n\ndef sign_in(session: Session, email: str, password) -> str:\n    user = session.query(User).filter_by(email=email).first()\n    if not user:\n        raise UserAuthenticationError()\n    if not verify_password(user, password):\n        raise UserAuthenticationError()\n    return generate_token(user)\n\n# auth.py\nimport hashlib\nfrom itsdangerous import TimedJSONWebSignatureSerializer as Serializer\nfrom models import User\n\nSALT = \"STRONg@Salt\"\nSECRET_KEY = \"!SECRET!\"\nTOKEN_EXPIRES_IN = 3600 * 24 * 30\n\ndef generate_password_hash(password: str) -> str:\n    h = hashlib.md5(f\"{password}{SALT}\".encode())\n    return h.hexdigest()\n\ndef verify_password(user: User, password: str) -> bool:\n    return user.password == generate_password_hash(password)\n\ndef generate_token(user: User) -> str:\n    serializer = Serializer(SECRET_KEY, expires_in=TOKEN_EXPIRES_IN)\n    return serializer.dumps({\"user_id\": user.id}).decode(\"utf-8\")\n```\n\nSignIn 뮤테이션을 위해 이메일과 비밀번호를 전달하고 인증이 필요한 요청에서 사용할 수 있는 토큰을 페이로드로 받습니다.\n\n![이미지](/TIL/assets/img/2024-07-09-HowtohandleauthenticationandauthorizationinGraphQLAPI_2.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 인증\n\nSignIn 단계에서 생성된 토큰이 있으므로 \"Bearer Authentication\" 프로세스에서 사용할 수 있습니다. 이러한 유형의 인증에서는 유효한 토큰(베어러)을 가진 모든 사용자가 해당 토큰에 해당하는 사용자로 인식될 수 있습니다. 각 GraphQL 필드 리졸버에 사용할 수 있는 sign_in_required 데코레이터를 정의합니다. 이 데코레이터는 \"Authorization\" 요청 헤더에서 토큰을 가져와 해독하여 user_id를 얻은 다음, user_id에 해당하는 User가 있는지 확인합니다. 성공적으로 완료되면 인증된 User가 됩니다.\n\n```js\n# api/graphql.py\n\nimport graphene\nfrom api.auth import sign_in_required\n\nclass Query(graphene.ObjectType):\n    up = graphene.Boolean()\n    restaurants = graphene.relay.ConnectionField(\n        RestaurantConnection, q=graphene.String()\n    )\n    me = graphene.Field(UserNode)\n\n    def resolve_up(root, info, **kwargs):\n        return True\n\n    @sign_in_required()\n    def resolve_restaurants(root, info, **kwargs):\n        query = get_restaurants(\n            info.context[\"session\"], search=kwargs.get(\"q\"), limit=kwargs.get(\"first\")\n        )\n        return [RestaurantNode(id=r.id, name=r.name) for r in query]\n\n    @sign_in_required()\n    def resolve_me(root, info, **kwargs):\n        return kwargs[\"current_user\"]\n\n# api/auth.py\nfrom functools import wraps\nfrom auth import get_user_by_token\nfrom models import User\n\nclass UnauthenticatedUser(Exception):\n    pass\n\ndef sign_in_required():\n    def decorator(func):\n        @wraps(func)\n        def wrapper(root, info, *args, **kwargs):\n            kwargs[\"current_user\"] = get_current_user(info.context)\n            return func(root, info, *args, **kwargs)\n        return wrapper\n    return decorator\n\ndef get_current_user(context) -> User:\n    try:\n        token = get_token_from_request(context[\"request\"])\n        user = get_user_by_token(context[\"session\"], token)\n        if not user:\n            raise UnauthenticatedUser(\"UnauthenticatedUser\")\n        return user\n    except KeyError:\n        raise UnauthenticatedUser(\"UnauthenticatedUser\")\n\ndef get_token_from_request(request) -> str:\n    header = request.headers[\"Authorization\"]\n    token = header.replace(\"Bearer \", \"\", 1)\n    return token\n\n# auth.py\nimport hashlib\nfrom typing import Optional\nfrom itsdangerous import TimedJSONWebSignatureSerializer as Serializer\nfrom sqlalchemy.orm import Session\nfrom models import User\n\nSECRET_KEY = \"!SECRET!\"\nTOKEN_EXPIRES_IN = 3600 * 24 * 30\n\ndef get_user_by_token(session: Session, token: str) -> Optional[User]:\n    serializer = Serializer(SECRET_KEY, expires_in=TOKEN_EXPIRES_IN)\n    data = serializer.loads(token)\n    return session.query(User).get(data[\"user_id\"])\n```\n\nup 필드는 공개 액세스이므로 쿼리를 위해 자격 증명을 전달할 필요가 없습니다. 한편, me 필드는 sign_in_required로 데코레이트되었으므로 적절한 토큰을 전달해야 해결할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/TIL/assets/img/2024-07-09-HowtohandleauthenticationandauthorizationinGraphQLAPI_3.png)\n\n![이미지](/TIL/assets/img/2024-07-09-HowtohandleauthenticationandauthorizationinGraphQLAPI_4.png)\n\n\"Authorization\" 헤더에 토큰을 전달하지 않고 sign_in_required로 표시된 필드에 접근하면 UnauthenticatedUser 예외가 발생합니다.\n\n![이미지](/TIL/assets/img/2024-07-09-HowtohandleauthenticationandauthorizationinGraphQLAPI_5.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 인가\n\n쿼리를 인증하여 로그인한 사용자만 액세스할 수 있도록 제한하는 방법을 살펴보았습니다. 그러나 사용자가 로그인했지만 수행하는 작업이 허용되지 않는 경우는 어떻게 할까요? 예를 들어, 예약한 레스토랑 테이블이 있으며 사용자가 인증되었을 때 허용되어야 하는 경우와 테이블 예약을 취소해야 하는 경우와 같이 사용자가 이전에 생성한 예약만 취소할 수 있는 경우가 있습니다.\n\n우리는 두 가지 작업을 구현했습니다:\n\n- 사용자가 인증되었을 때 허용되어야 하는 BookRestaurantTable 뮤테이션,\n- 취소되어야 하는 TableBooking을 취소하는 CancelTableBooking 뮤테이션.\n\n이를 위해 BookRestaurantTable은 sign_in_required로 데코레이트된 mutate 메서드를 사용하고, CancelTableBooking은 authorize_required로 데코레이트된 새로운 메서드를 사용합니다. 이 데코레이터는 사용자가 인증되었는지 확인하고, table_booking_gid(인스턴스의 전역 ID를 나타내는 값)가 인증된 사용자에 의해 생성된 TableBooking 인스턴스와 일치하는지 확인합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nclass BookRestaurantTable(graphene.Mutation):\n    class Arguments:\n        restaurant_gid = graphene.ID(required=True)\n        persons = graphene.Int(required=True)\n\n    table_booking = graphene.Field(TableBookingNode)\n\n    @sign_in_required()\n    def mutate(self, info, restaurant_gid: str, persons: int, **kwargs):\n        session = info.context[\"session\"]\n        current_user = kwargs[\"current_user\"]\n        _, restaurant_id = from_global_id(restaurant_gid)\n        table_booking = book_restaurant_table(\n            session, restaurant_id, current_user.email, persons\n        )\n        return BookRestaurantTable(\n            table_booking=TableBookingNode(\n                id=table_booking.id,\n                is_active=table_booking.is_active,\n            )\n        )\n\nclass CancelTableBooking(graphene.Mutation):\n    class Arguments:\n        table_booking_gid = graphene.ID(required=True)\n\n    table_booking = graphene.Field(TableBookingNode)\n\n    @authorize_required(TableBooking)\n    def mutate(self, info, table_booking_gid: str, **kwargs):\n        session = info.context[\"session\"]\n        table_booking = kwargs[\"instance\"]\n        cancel_table_booking(session, table_booking)\n        return CancelTableBooking(\n            table_booking=TableBookingNode(\n                id=table_booking.id,\n                is_active=table_booking.is_active,\n            )\n        )\n\nclass Mutation(graphene.ObjectType):\n    book_restaurant_table = BookRestaurantTable.Field()\n    cancel_table_booking = CancelTableBooking.Field()\n\n# api/auth.py\nimport re\nfrom functools import wraps\nfrom graphene.relay.node import from_global_id\nfrom auth import authorize\nfrom models import User\n\ndef camel_to_snake(name: str) -> str:\n    \"\"\"CamelCase -> camel_case\"\"\"\n    return re.sub(r\"(?<!^)(?=[A-Z])\", \"_\", name).lower()\n\nclass UnauthorizedAccess(Exception):\n    pass\n\nclass InstanceNotExist(Exception):\n    pass\n\ndef authorize_required(model):\n    \"\"\"\n    We assume that the global id field name of a resource\n    follow convention like:\n    model_name: `TableBooking`\n    global id field name: `table_booking_gid`\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(root, info, *args, **kwargs):\n            kwargs[\"current_user\"] = get_current_user(info.context)\n            model_name = model.__name__\n            gid_field_name = f\"{camel_to_snake(model_name)}_gid\"\n            instance_gid = kwargs[gid_field_name]\n            instance_model_name, instance_id = from_global_id(instance_gid)\n            if instance_model_name != f\"{model_name}Node\":\n                raise UnauthorizedAccess(\"UnauthorizedAccess\")\n            instance = info.context[\"session\"].query(model).get(instance_id)\n            if not instance:\n                InstanceNotExist()\n            kwargs[\"instance\"] = instance\n            if not authorize(instance, kwargs[\"current_user\"]):\n                raise UnauthorizedAccess(\"UnauthorizedAccess\")\n            return func(root, info, *args, **kwargs)\n        return wrapper\n    return decorator\n\n# auth.py\nfrom functools import singledispatch\nfrom models import TableBooking, User\n\n@singledispatch\ndef authorize(instance, current_user: User) -> bool:\n    raise NotImplementedError\n\n@authorize.register(TableBooking)\ndef _authorize(instance: TableBooking, current_user: User) -> bool:\n    return instance.user_id == current_user.id\n```\n\nBookRestaurantTable을 실행하기 위해 restaurant_gid 및 persons라는 두 가지 필수 인수를 전달해야 합니다. \"Authorization\" 헤더에 토큰을 추가해야 합니다. Mutation 응답에서는 TableBooking.id를 얻습니다.\n\n이미지가 포함된 파일경로: `/assets/img/2024-07-09-HowtohandleauthenticationandauthorizationinGraphQLAPI_6.png`\n\nCancelTableBooking은 BookRestaurantTable 페이로드(TableBooking.id)에서 가져올 수 있는 table_booking_gid만 필요합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/TIL/assets/img/2024-07-09-HowtohandleauthenticationandauthorizationinGraphQLAPI_7.png\" />\n\n만약 토큰이 주어진 테이블 예약의 소유자와 일치하지 않는 경우, 동작을 수행할 수 없으며, 권한이 없음 예외가 발생합니다.\n\n<img src=\"/TIL/assets/img/2024-07-09-HowtohandleauthenticationandauthorizationinGraphQLAPI_8.png\" />\n\n# 결론\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n인증 및 권한 부여 단계를 쿼리와 뮤테이션 모두 위한 방법을 소개했어요. 이 구현은 매우 범용적이며 어떤 Python GraphQL 프로젝트에도 쉽게 통합할 수 있어요. 전체 소스 코드는 여기에서 확인할 수 있어요: https://github.com/jorzel/service-layer/tree/auth.\n","ogImage":{"url":"/assets/img/2024-07-09-HowtohandleauthenticationandauthorizationinGraphQLAPI_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-HowtohandleauthenticationandauthorizationinGraphQLAPI_0.png","tag":["Tech"],"readingTime":15},{"title":"모든 개발자가 알아야 할 Top 10 Python 인터뷰 질문","description":"","date":"2024-07-09 20:28","slug":"2024-07-09-Top10PythonInterviewQuestionsEveryDeveloperMustKnow","content":"\n\n## 파이썬 인터뷰 질문\n\n전 세계에는 800만 명 이상의 파이썬 개발자가 있습니다. 매일 수천 명의 새로운 학습자가 파이썬 커뮤니티에 가입합니다. 혹독한 진실은, 그 중에서도 단 10~20%만이 좋은 개발자가 되어 좋은 직장을 찾을 수 있습니다. 이유는 고급 인터뷰 질문을 해결하지 못하기 때문입니다. 이제 10가지 중요하고 흔한 파이썬 인터뷰 질문을 공유하겠습니다.\n\n# `.py` 파일과 `.pyc` 파일의 차이는 무엇인가요?\n\n`.py` 파일은 프로그램의 소스 코드입니다. `.pyc` 파일은 컴파일된 바이트 코드입니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n파이썬은 `.py` 파일을 컴파일하여 `.pyc` 파일로 저장하고, 이 파일은 파이썬 가상 머신에서 실행됩니다.\n\n주 소스 코드를 실행하기 전에 파이썬은 컴파일된 버전인 `.pyc` 파일을 찾습니다. 찾았다면 해당 파일을 가상 머신으로 실행합니다. 찾지 못했다면 `.py` 파일을 컴파일하고 실행합니다. 요컨대, `.pyc` 파일은 이미 컴파일된 코드를 재사용하여 컴파일 시간을 단축해줍니다.\n\n# 추상화란 무엇인가요? 파이썬에서는 어떻게 추상화를 구현할 수 있나요?\n\n추상화는 내부 함수를 사용자로부터 숨기는 것을 말합니다. 사용자는 결과가 어떻게 생성되었는지 알지 못한 채 함수와 상호 작용합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n간단히 말하면, 추상화는 프로그램 복잡성을 줄이기 위해 관련 없는 데이터를 숨기는 것을 의미합니다. 파이썬에서는 추상화를 달성하기 위해 ABC 모듈을 사용합니다.\n\n추상 클래스는 다른 클래스의 기반으로 사용할 수 있습니다. 추상 클래스의 객체는 생성할 수 없으므로 요소에 액세스하는 유일한 방법은 상속을 통해서입니다.\n\n```python\nfrom abc import ABC, abstractmethod\n```\n\n```python\nclass Parent(ABC):\n    @abstractmethod\n    def show(self):\n        pass\n  \nclass child(Parent):\n    def show(self):\n        print(\"Child Class\")\nobj = child() # 추상 클래스는 인스턴스화 할 수 없습니다\nobj.show() # Child Class\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# FrozenSet이란 무엇인가요? 예시와 함께 중요성을 설명해주세요.\n\nFrozenSets는 set과 비슷하지만 변경할 수 없습니다.\n\n`set` 요소를 언제든지 수정할 수 있지만, 생성된 후에는 `frozenset`을 변경할 수 없습니다.\n\n생성 후에는 요소를 추가, 삭제 또는 업데이트할 수 없습니다. `frozenset`은 반복 가능한 항목을 입력으로 받아들이고 변경할 수 없게 만듭니다. 변경할 수 없기 때문에 딕셔너리의 키로 사용할 수 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ndata = {\"Name\": \"Roger\", \"Pin\": 3056, \"ActNo\":9892345112234565}\nfSet = frozenset(data)\nprint(fSet) # frozenset({'Name', 'Pin', 'ActNo'})\n```\n\n**얕은 복사와 깊은 복사의 차이를 설명해보겠습니다.**\n\n`얕은 복사`는 객체에 대한 참조를 새로운 메모리 위치에 저장합니다. 새로운 위치에서의 변경 사항은 이전 위치에 반영됩니다. 깊은 복사보다 더 빠릅니다.\n\n`깊은 복사`는 객체의 값을 새로운 위치에 저장합니다. 새 위치에서의 변경 사항이 이전 위치에 영향을 미치지 않습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n`id` 는 객체의 메모리 주소를 보는 데 사용됩니다. 예시에서의 주소는 여러분의 컴퓨터에서 다를 수 있습니다.\n\n```js\n## 얕은 복사\ndata = [1, 2, 3, 4, 5]\nupdated_data = data\nupdated_data.append(6)\nprint(updated_data) # [1, 2, 3, 4, 5, 6]\nprint(data) # [1, 2, 3, 4, 5, 6]\nprint(id(data)) # 16777216\nprint(id(updated_data)) # 16777216\n```\n\n```js\n## 깊은 복사\nimport copy\ndata = [1, 2, 3, 4, 5]\nupdated_data = copy.deepcopy(data)\nupdated_data.append(6)\nprint(updated_data) # [1, 2, 3, 4, 5, 6]\nprint(data) # [1, 2, 3, 4, 5]\nprint(id(updated_data)) # 16777216\nprint(id(data)) # 14020317\n```\n\n# pickle을 사용하는 방법은요?\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n`피클링(Pickling)`은 Python 객체를 바이트 스트림으로 변환하여 직렬화하는 것을 말합니다.\n\n`언피클링(Unpickling)`은 그 반대로, 바이트 스트림을 다시 Python 객체로 변환하여 역직렬화하는 것을 의미합니다.\n\nPython에서는 직렬화와 역직렬화를 위해 `pickle.dump`와 `pickle.load`를 사용합니다.\n\n```python\n## Pickling\nimport pickle\ndata =  {\n    'Names': [\"Karl\",\"Robin\",\"Lary\"],\n    'Id': ('G770531','G770532','G770533'),\n    'Salary':[55600,88900,76000]\n    }\noutput = open('./data.pkl', 'wb')\npickle.dump(data, output)\noutput.close()\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n## 피클링\nimport pickle\nstream = open('./data.pkl', 'rb')\ndata = pickle.load(stream)\nprint(data) # {'Names': ['Karl', 'Robin', 'Lary'], 'Id': ('G770531', 'G770532', 'G770533'), 'Salary': [55600, 88900, 76000]}\nstream.close()\n```\n\n# *args와 **kwargs가 무엇인가요?\n\n*args와 **kwargs는 함수에 가변 개수의 인수를 전달하는 것을 가능하게 합니다. 전달되는 인수가 몇 개인지 확신이 없을 때 사용합니다.\n\n*args는 함수에 가변 개수의 인수를 전달하는 것을 가능하게 합니다.\n\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\ndef addNumbers(*numbers):\n    sum = 0\n    for number in numbers:\n        sum = sum + number\n    print(\"Sum: \", sum)\naddNumbers(3,5) # Sum: 8\naddNumbers(5,6,7) # Sum: 18\n```\n\n**kwargs는 함수에 변수 수의 키워드 인수를 전달하는 데 사용됩니다.\n\n```python\ndef addNumbers(**data):\n    sum = 0\n    for key, value in data.items():\n        sum = sum + value\n    print(\"Sum: \", sum)\n    \naddNumbers(a=5, b=6) # Sum: 11\naddNumbers(a=5, b=8, c=10) # Sum: 23\n```\n\n# 파이썬에서 컨텍스트 매니저란 무엇인가요?\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n컨텍스트 관리자는 리소스를 관리합니다. 필요에 따라 리소스를 할당하고 해제할 수 있게 해줍니다.\n\n가장 일반적인 예시는 `with` 문입니다.\n\n주로 파일을 열고 닫는 데 사용됩니다.\n\n`with`를 사용하면 코드 한 줄에서 문제가 발생해도 파일이 올바르게 닫힙니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nwith open('./data.txt', 'w') as f:\n    f.write(\"Hello\")\n```\n\n# 파이썬에서 인스턴스 메소드, 클래스 메소드 및 정적 메소드란 무엇인가요?\n\n파이썬에서는 세 가지 종류의 메소드가 있습니다: 인스턴스 메소드, 클래스 메소드 및 정적 메소드.\n\n- 인스턴스 메소드: 클래스 내에 만드는 일반적인 메소드로, 객체와 관련이 있습니다. 인스턴스를 가리키기 위해 `self`를 사용합니다.\n- 클래스 메소드: 객체가 아닌 클래스에 바운드된 메소드로, 클래스 수준의 작업을 수행하며 클래스 상태를 변경할 수 있습니다. `@classmethod` 데코레이터를 사용합니다.\n- 정적 메소드: 클래스 내에 정의되어 있지만 클래스 자체와 관련이 없는 논리를 명확하게 하기 위해 사용합니다. `@staticmethod` 데코레이터를 사용합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# nonlocal과 global 변수란 무엇인가요?\n\n이들은 변수의 범위를 정의합니다. Global 변수는 함수 외부에서 정의됩니다.\n\n```js\npi = 3.14  ## Global 변수\ndef circle(radius):\n    area_of_circle = pi * (radius) ** 2\n    print(\"원의 면적은: \", area_of_circle)\ncircle(7) # 원의 면적은: 153.86\n```\n\n이들의 값은 코드 전체에서 동일하며 프로그램의 어디에서든 사용할 수 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n중첩 함수에서 정의된 로컬 스코프 없이 비로컬 변수가 사용됩니다. 비로컬 변수의 값을 변경하면 로컬 변수의 값도 변경됩니다.\n\n```js\ndef outer_function():\n    x = \"로컬 변수\"\n    def inner_function():\n        nonlocal x\n        x = \"비로컬 변수\"\n        print(\"내부 함수:\", x)\n    inner_function()\n    print(\"외부 함수:\", x)\nouter_function()\n# 내부 함수: 비로컬 변수\n# 외부 함수: 비로컬 변수\n```\n\n# 예제와 함께 제너레이터에 대해 설명해주세요.\n\n제너레이터(generator)는 순회 가능한 객체(iterable objects)를 반환하는 함수입니다. 하나 이상의 `yield` 문을 포함해야 합니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n`yield`은 함수의 현재 상태나 로컬 변수 참조를 잃지 않고 값을 반환하는 키워드입니다. `yield`가 포함된 함수를 제너레이터라고 부릅니다.\n\n제너레이터는 필요할 때만 항목을 생성하여 메모리를 효율적으로 사용합니다.\n\n초보자들에게 `yield`를 함수를 중지하지 않고 값을 반환하는 `return`으로 생각해보세요.\n\n```js\ndef fibon(limit):\n    a,b = 0,1\n    while a < limit:\n        yield a\n        a, b = b, a + b\n        \nfor x in fibon(10):\n    print(x) # 1 2 3 5 8 13 21 34 55 89\n```\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 결론\n\n저는 10가지 흔한 Python 면접 질문을 공유했습니다. 이 질문들이 직장을 바꾸거나 직장을 찾을 때 도움이 되길 바라요!\n\n저는 “Medium에서 빠르게 팔로워를 얻는 방법”에 대한 eBook을 쓰고 있어요. 왜냐하면 저는 최고의 증거이기 때문이죠 — 딱 한 달 만에 5,000명 이상의 팔로워를 얻었어요. 기대해주세요!\n\n저는 Substack에서 \"GPT 소개\" 시리즈를 쓰고 있어요. 관심 있으시면 팔로우 해주세요!\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n10개의 글 중 10번째가 이미 완료되었습니다!\n\n읽어주셔서 감사합니다📖, 강조해주셔서 감사합니다🖍️, 박수를 보내주셔서 감사합니다👏, 댓글을 달아주셔서 감사합니다💬, 그리고 공유해주셔서 감사합니다🗣️. \"미디움의 친구\"로써, 저는 매일 동료 작가들에게 제 게시물을 봐주며 보답하려 노력합니다.\n\n최신 AI 이야기에 대한 소식을 받으려면 Substack에서 저희와 연락을 유지하세요. 함께 AI의 미래를 함께 만들어 봅시다!","ogImage":{"url":"/TIL/assets/no-image.jpg"},"coverImage":"/TIL/assets/no-image.jpg","tag":["Tech"],"readingTime":10},{"title":"탐험을 위한 최고의 소프트웨어 10가지","description":"","date":"2024-07-09 20:26","slug":"2024-07-09-Softwareforexploration","content":"\n## 연구 소프트웨어에서 빠른 길을 선택해야 하는 때\n\n![2024-07-09-Softwareforexploration_0](/TIL/assets/img/2024-07-09-Softwareforexploration_0.png)\n\n내 인생 동안 연구에 종사해왔기 때문에 연구자들이 못생긴 코드를 작성한다는 편견을 알고 있어요 (예: 여기, 여기, 또는 여기를 참고하세요). 하지만, 이를 해결할 수 있다고 생각했어요. 그래서 여러 번 멋진 연구 프레임워크를 디자인하려고 노력했어요. 좋아하는 소프트웨어 엔지니어링 책과 블로그를 참고하여 인터페이스를 도입하고 좋은 추상화를 만들려고 노력했죠.\n\n그러나 그 모든 노력이 물거품으로 돌아갔어요. 내가 작업한 대부분의 연구 소프트웨어가 실제 운영에 적용되지 않았거든요 (일부는 적용되기는 했지만). 단순한 진실을 알려준 사람이 있었다면, 내 정신건강에는 참 좋았을 텐데요: 죽어가는 연구 코드는 실제로 일어나야 하는 일이에요. 연구자들은 처음부터 그것을 엔지니어링하는 데 많은 시간을 들이지 않아도 되요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n전문 소프트웨어 엔지니어들은 항상 가장 좋은 소프트웨어 관행을 사용하지 않는 연구자들을 무시하는 경향이 있습니다. 연구 코드의 수준을 높이려는 노력이 여러 개 있습니다 (예: 이 훌륭한 게시물 및 연구 코드 핸드북). 그러나 이 게시물은 다른 방향으로 나아가서 최고의 소프트웨어 관행을 지나치게 적용하지 않는 방법을 주장하며 빠른 탐구에만 주력하는 것이 중요하다고 주장합니다. 이는 빠르게 여러 아이디어를 시도해보는 것이 목표인 연구 지향적 기업을 대상으로 합니다.\n\n# 1. 몇몇 전략적 기술 부채를 맡아보세요\n\n회사의 성공적인 연구 프로젝트에는 두 가지 단계가 있습니다: 탐색과 활용. \"탐색\" 단계에서는 가능한 다양한 경로를 시도해보려고 합니다. \"활용\" 단계에서는 가장 좋은 해결책을 견고하게 만들어 유용한 제품으로 전환하려고 합니다.\n\n![Software for exploration](/TIL/assets/img/2024-07-09-Softwareforexploration_1.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n두 가지 사이에서 최적의 소프트웨어 관행은 꽤 다릅니다. 그래서 회사들은 종종 별도의 연구 및 제품 부서를 갖고 있습니다. 소프트웨어 디자인에 관한 일반적으로 읽을 수 있는 모든 책들은 주로 두 번째 \"활용\" 단계에 대해 설명하고 있습니다. 이 단계에서는 확장 가능한 제품을 위한 기반을 구축합니다. 여기서 모든 디자인 패턴이 나타납니다: 좋은 API, 로깅, 오류 처리 등이 있습니다.\n\n그러나 첫 번째 \"탐색\" 단계에서는 영원히 존속할 기반이 아닙니다. 실제로 대부분의 노력이 남아 있다면, 그것은 다양성을 탐색하지 않았다는 뜻입니다.\n\n이 글에서 소개한 많은 관행은 일반적으로 \"기술적 부채\"가 되는 예시입니다. 깨끗하고 재사용 가능하며 잘 추상화된 코드를 작성하지 않아서 생기는 결과입니다. 부채는 항상 나쁜 것일까요? 대출이나 모기지를 받지 않는 것이 좋겠지만, 돈을 빌리는 것은 종종 인생에서 좋은 전략입니다. 빠르게 움직이고 후에 수익을 얻기 위해 부채를 불러들이는 것은 괜찮습니다.\n\n![이미지](/TIL/assets/img/2024-07-09-Softwareforexploration_2.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n비슷하게, 기술적 부채를 감당하지 않으면 연구 속도가 느려질 수 있습니다. 좋은 소식은 대부분의 경우에는 다시 상환할 필요가 없다는 것입니다. 대부분의 연구 코드는 결국 사라질 것이기 때문에 평균적으로 가져간 전체 기술적 부채로 인해 고통받지 않을 것입니다.\n\n## 코드 재사용에 반대하는 사례\n\n많은 소프트웨어 아키텍처 및 리팩토링 기술은 코드의 재사용성을 향상시키기 위해 특별히 설계되었습니다. 코드 재사용에는 일반적인 부작용이 있습니다. 그러나 실제 운영에서는 이미 잘 알려진 혜택들이 그것을 상쇄시킵니다 (예를 들어, 이러한 전형적인 글을 참조하세요). 연구 프로젝트에서 대부분의 코드는 무시될 운명입니다. 코드 재사용을 추구하는 것이 실제로 연구 속도를 늦출 수 있습니다.\n\n코드 재사용을 제한하는 것은 연구에서 받아들이는 종류의 기술적 부채입니다. 추가적인 불필요한 종속성 추가, 코드 복사 붙여넣기, 큰 양의 공유 연구 코드 유지, 조기 설계 투자 등 여러 가지 코드 재사용 패턴에 대해 논의하고 싶습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 새로운 것을 가져오기 전에 한 번 더 생각해보세요\n\n성능을 향상시켜 줄 잘 유지되는 버전 관리된 라이브러리가 있다면 채택해 보세요! 하지만 새로운 의존성을 추가하기 전에 그것이 그만한 가치가 있는지 판단해 보세요. 추가되는 각각의 의존성은 의존성 지옥에 더 가까워지게 합니다. 학습과 문제 해결에 시간 투자하게 만듭니다. 이 간결한 포스트에서 의존성의 함정을 더 자세히 살펴보세요.\n\n아마도 다음과 같은 경우에는 안심할 수 있을 것입니다:\n\n- 이미 사용하고 있고, 배울 것이 많지 않고, 큰 커뮤니티와 좋은 문서 및 테스트가 있는 경우\n- 버전이 명시되어 있고, 쉽게 설치할 수 있는 경우\n- 마지막으로, 직접 구현할 수 있는 방법이 없는 경우.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n하지만 만약에 이러한 상황에 의심스러운 의존성이 있다면:\n\n- 어떻게 빠르게 사용해야 하는지 알 수 없거나 매우 새로운 것 (또는 매우 오래된 것) 이거나 아무도 알지 못하는 경우; 문서나 테스트가 없는 경우\n- 모노 레포에서 다른 팀들에 의해 계속 바뀌고 있는 경우\n- 많은 다른 의존성과 도구들을 가져오거나 설치하기 어려운 경우\n- 마지막으로, 여러분이 (또는 몇몇 LLM이) 몇 시간 안에 이 코드를 짤 수 있다고 느낀다면\n\n명시적인 의존성 대신, 좋은 Go 속담을 따를 수도 있습니다: “좀 카피하는 것은 좀 의존하는 것보다 낫다”, 이것이 우리의 다음 주제입니다.\n\n## 복사 붙여넣기는 실험의 자유를 줍니다\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Software Exploration](/TIL/assets/img/2024-07-09-Softwareforexploration_3.png)\n\n\"복사 붙여 넣기는 불법이어야 한다\"는 사람들이 있다고 해요. 하지만 제 놀람은 자주 그것을 옹호하는 데 주장을 하게 된다는 것이었어요. 탐색 단계에서 복사 붙여 넣기가 최적의 선택일 수도 있어요.\n\n코드베이스 다른 부분에서 크게 사용되는 함수에 의존한다면 쉽게 변경할 수 없을 거예요. 누군가를 위해 무언가를 망가뜨릴 가능성이 높고 소중한 시간을 코드 검토와 수정에 소비해야 할 수도 있어요. 그러나 필요한 코드를 폴더에 복사 붙여 넣기하면 자유롭게 원하는 대로 할 수 있어요. 이는 실험이 규칙이 아닌 예외일 때 연구 프로젝트에서 중요한 일이에요. 특히 모든 사람에게 유용할지 확실하지 않을 때요.\n\n저는 딥러닝 코드베이스가 대부분 복사 붙여 넣기에 적합하다고 생각해요. 일반적으로 모델과 그 학습을 설명하는 데 필요한 코드 양이 크지 않을 수도 있어요. 그러나 동시에 매우 미묘하고 일반화하기 어려울 수도 있어요. 공유 가능한 학습 스크립트는 관리하기 힘든 크기로 커지곤 해요: 예를 들어 Hugging Face transformers Trainer는 4천 줄을 넘었어요. 재미있는 사실은 transformers가 모델 수준에서 복사 붙여 넣기를 선택했다는 것이에요. 그들의 \"단일 파일 모델\" 정책 뒤에 이유에 대한 포스트를 확인해주세요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n대안으로 복사 및 붙여넣기 대신 브랜치를 유지하는 것이 있습니다. 하지만 팀원들과 협업에 너무 많은 부담을 주는 것 같아요. 게다가, 복사 붙여넣기의 매력에 관한 몇 가지 게시물을 더 발견했어요. \"자료\"에서 확인해보세요.\n\n## 공유된 연구 코드 유지 관리는 어려워요\n\n많이 사용되는 공유 코드를 유지보수하는 것은 많은 작업이 필요해요. torch.nn.Module의 파일 라인 수를 Pytorch 버전에 대해 그래프로 나타낸 것을 살펴봐요. 심지어 가장 선물된 연구 팀도 복잡성을 유지하기 어렵다는 것을 알 수 있어요.\n\n![Softwareforexploration_4](/TIL/assets/img/2024-07-09-Softwareforexploration_4.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n큰 공동 연구 코드를 유지하는 데 필요한 시간과 리소스를 과소평가하지 마세요. 연구 라이브러리가 더 많이 사용될수록 더 복잡해집니다. 일반 라이브러리보다 빠르게 발생하는 이유는 각 연구 방향이 약간 다른 사용 사례를 갖고 있기 때문입니다. 기여할 수 있는 내용에 대한 매우 엄격한 규칙을 정립하세요. 그렇지 않으면 공동 코드는 취약해지고 옵션을 가득 채우고 버그가 많은 최적화와 예외 사항으로 넘치게 됩니다. 대부분의 연구 코드가 사라지기 때문에 이 모든 추가 복잡성은 다시 사용되지 않을 것입니다. 일부 공동 코드를 제거하면 실제 연구를 진행할 시간을 확보할 수 있습니다.\n\n## 탐험을 위해 설계하고 코드 재사용을 위해 설계하지 마세요\n\n코드를 너무 많이 미래지향적으로 만들고 싶지 않은 것도 약간 사실입니다. 요구 사항을 충족하는 가장 간단한 솔루션을 구현하려 노력하세요. 그러나 프로덕션 코드에는 항상 유지 관리 가능성을 고려해야 합니다. 예를 들어, 오류 처리, 속도, 로깅, 모듈화는 일반적으로 고려해야 할 사항입니다.\n\n연구 코드에는 그런 것이 중요하지 않습니다. 단지 아이디어가 좋은지 나쁜지를 최대한 빠르게 증명하고 넘어가려 합니다. 따라서 모듈이나 API 없이 해당 목적을 달성하는 더러운 단순성은 전혀 문제가 되지 않습니다!\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n가치 있는 시간을 낭비하지 마세요. 이른 시점에 소프트웨어 투자를 하지 마십시오:\n\n- 프로젝트 초반에 컴포넌트 인터페이스를 생성하는 것. 자체 제작한 인위적인 제약 조건에 맞추느라 너무 많은 시간을 소비하게 됩니다.\n- 딥 러닝 솔루션을 확정하기 전에 딥 러닝 훈련 인프라를 최적화하는 것.\n- 프로토타이핑 중에는 프로덕션 구성/팩토리/직렬화 시스템이나 베이스 클래스를 사용하지 않는 것. 종종 이 기능이 필요하지 않을 수 있습니다.\n- 지나치게 엄격한 린팅 및 타입 체크 시스템. 빠르게 변화하는 일회용 연구 코드를 늦추는 이유가 없습니다.\n\n# 2. 빠른 탐색에 투자하기\n\n연구 프로젝트의 목표는 혁신적인 솔루션을 찾는 것입니다. 아무도 (정의상) 그 모습을 모릅니다. 제한된 정보가 있는 복잡한 연구 환경에서 최적화 프로세스와 유사합니다. 좋은 최솟값을 찾기 위해 많은 경로를 시도하고, 좋은 경로와 나쁜 경로를 인식하고 국소 최솟값에 갇히지 않아야 합니다. 모든 것을 빠르게 수행하려면 기술적 부채를 갚는 대신 소프트웨어 투자를 할 필요가 있을 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 공통 경로 가속화\n\n![Screenshot](/TIL/assets/img/2024-07-09-Softwareforexploration_5.png)\n\n시도하고 싶은 여러 가지 연구 경로가 있습니다. 대부분의 경로에서 시간을 단축시킬 수 있는 디자인, 라이브러리 또는 최적화가 있습니까? 시도할 아이디어를 항상 알 수 없으므로 너무 많은 것을 과도하게 설계하지는 마십시오. 이는 각 프로젝트마다 맞춤적이지만, 여기에 일부 예시가 있습니다:\n\n- 딥 네트워크를 학습한다면, 학습 인프라에 투자하십시오. 학습 중 빠르고 신뢰성 있게 수렴할 수 있는 하이퍼파라미터를 찾으세요.\n- 모든 실험이 각기 다른 모델을 사용해야 한다면, 어떻게 빠르게 교체할 수 있는지 파악하세요 (예: 단순한 팩토리 시스템 또는 단순 복사 붙여넣기).\n- 모든 실험이 매개변수가 너무 많아 관리가 어렵다면, 좋은 구성 라이브러리에 투자하십시오.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 빠르게 분기 내리기\n\n![image](/TIL/assets/img/2024-07-09-Softwareforexploration_6.png)\n\n연구자들은 빠르게 새로운 다양한 아이디어를 시작할 수 있어야 합니다. 프로젝트 초반에는 쉬워 보이지만, 사람들이 자신이 선호하는 연구 경로에 빠지면서 점차 더 어려워지는 경향이 있습니다. 이를 해결하기 위해 문화적인 변화와 조직적인 변화가 필요합니다. 비영리적인 연구가 너무 많은 비용과 감정을 들이기 전에 중단시킬 수 있는 프로세스가 있어야 합니다. 정기적인 데모 데이와 기술 동료 검토는 이를 위한 효과적인 전략으로 기능할 수 있습니다. 또한 새롭고 반짝거리는 아이디어에 뛰어드는 사람들과 현재 프로젝트를 적절하게 마무리하는 사람들 사이의 균형을 찾는 것이 중요합니다.\n\n그러나 이것은 소프트웨어 게시물이므로 새로운 프로젝트를 분기 내리기 쉽게 하는 몇 가지 실천 방법을 소개합니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 알고리즘에서 평가 코드를 분리하여 유지하세요. 평가는 일반적으로 연구 방향보다 안정적입니다.\n- 새로운 프로젝트를 빈 화면에서 시작하는 것을 환영하지만, 재사용되는 구성 요소에 주의하세요. 모듈화하고 정리하는 것이 좋은 투자입니다.\n- 새로운 연구 프로젝트에서 가장 혁신적이고 위험한 구성 요소를 먼저 구현하세요. 이렇게 함으로써 대부분의 병목 현상을 확인하고 미래 소프트웨어 디자인을 안내할 수 있습니다.\n\n## 신호 대 잡음 비율 증가\n\n![](/TIL/assets/img/2024-07-09-Softwareforexploration_7.png)\n\n잡음이 많고 버그가 있는 코드는 결과를 너무 모호하고 결론이 나지 않게 만들어 전체 프로젝트가 시간 낭비가 될 수 있습니다. 여러분이 과도하게 엔지니어링을 하지 않아도 되지만, 코드를 지저분하게 만들지 않으려면 다음과 같은 간단한 지침을 쉽게 따를 수 있습니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 부작용이 있는 코드를 피하세요.\n- 클래스보다는 함수를 기본으로 사용하고, 클래스를 사용할 때에는 상속 대신 캡슐화를 선호하세요.\n- 함수/클래스/모듈의 길이를 최소화하고, if 문의 개수를 최소화하세요.\n- 파이썬을 잘 알지만 간단한 기술을 사용하세요. 메타클래스, 데코레이터, 함수형 프로그래밍의 복잡한 부분에 빠지지 말아주세요.\n\n서로 다른 실행에서 다른 결과를 내는 소프트웨어는 작업하기 까다로울 수 있습니다. 불운한 시드를 기반으로 한 중요하지만 잘못된 결정을 내렸을 경우, 많은 시간을 소비하게 될 수도 있습니다. 비결정적 소프트웨어를 다룰 때 몇 가지 팁을 안내드립니다:\n\n- 알고리즘에서 나오는 노이즈와 해당 평가에서 나오는 노이즈를 구분하세요. 노이즈 원천들이 쌓이며 완전히 결정론적인 평가를 지향해야 합니다.\n- 모든 랜덤 시드를 찾을 때까지 무작위성 원천을 찾지 마세요. 모든 무작위 시드를 찾은 후에도 노이즈는 데이터나 부작용이 있는 일반 함수에서 나올 수 있습니다.\n- 시드를 변경하고 결과의 기본 분산을 결정하세요. 통계적으로 유의미하지 않은 결과에 기반한 결정을 내리지 마세요.\n\n# 결론\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n위 텍스트를 친근한 톤으로 한국어로 번역해드릴게요:\n\n연구 코드에 관한 이 게시물에서 이 유머가 나왔어요:\n\n훌륭한 코딩 기초가 중요한 것은 사실이지만, 결국 중요한 것은 탐구와 실제로 유용한 제품입니다. 연구에 너무 많은 제품 소프트웨어를 사용하면 새로운 것을 발견하는 데 필요한 시간을 낭비하게 됩니다. 대신, 탐구 과정을 늦추는 요소를 찾아내세요. 빠른 브랜칭, 결과 도출 시간 및 깔끔한 무소음 코드에 투자하여 연구 경로를 가속화하세요.\n\n코드 재사용을 완전히 부정하는 것은 어리석은 일일 것입니다. 제가 말하고 싶은 건 코드 재사용이 균형있게 이뤄져야 한다는 점입니다. 연구에서는 일회성 코드의 비율이 제품 개발보다 더 큽니다. 그래서 재사용에 대한 균형은 더욱 무게가 실립니다. 코드 재사용의 함정에 대한 더 많은 정보를 아래에서 확인해보세요.\n\n읽어 주셔서 감사합니다! LinkedIn이나 Twitter에서 저를 만날 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 자료\n\n코드 재사용에 관한 더 많은 게시물:\n\n- 재사용 가능한 코드: 좋은 점, 나쁜 점, 그리고 못생긴 점\n- 반복하는 것이 맞는 경우\n- 재사용의 장단점이 포함된 균형 StackExchange\n\n복사 및 붙여넣기 실천 방법을 주장하는 더 많은 게시물:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 복사 붙여넣기가 정말 문제일까요?\n- 코드를 복사 붙여넣기해야 하는 때는 언제인가요?\n- 복사 붙여넣기 프로그래밍의 장단점\n- \"5년 경력의 시니어로써 저는 내가 어떤 코드를 복사해야 하는지 아는 것이 내 급여의 일부일 것 같아요 lol\"\n\n원문은 https://sinavski.com에서 확인할 수 있습니다.\n","ogImage":{"url":"/assets/img/2024-07-09-Softwareforexploration_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-Softwareforexploration_0.png","tag":["Tech"],"readingTime":13},{"title":"FastAPI Apppy에서 모듈러 아키텍처로 전환하는 방법","description":"","date":"2024-07-09 20:24","slug":"2024-07-09-FastAPIFromApppytoaModularArchitecture","content":"\n<img src=\"/TIL/assets/img/2024-07-09-FastAPIFromApppytoaModularArchitecture_0.png\" />\n\n패스트API를 사용하여 백엔드를 구축할 때, 일반적으로 하나의 app.py 파일로 시작하는 것이 흔합니다. 이 접근 방식은 작은 프로젝트에 적합하지만, 응용 프로그램이 성장함에 따라 유지 보수와 확장이 어려워집니다.\n\n이 블로그 포스트에서는 Routers, Controllers, Services 및 Repositories로 구성된 구조화된 아키텍처를 사용하여 FastAPI 애플리케이션을 모놀리식 app.py 파일에서 리팩토링하는 방법을 살펴보겠습니다.\n\n## 소개: 우리의 할 일 API\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n리팩터링 프로세스에 뛰어들기 전에 함께 작업할 API를 살펴보겠습니다. 다음과 같은 엔드포인트를 가진 간단한 할 일 애플리케이션을 구축 중입니다.\n\n![API Image](/TIL/assets/img/2024-07-09-FastAPIFromApppytoaModularArchitecture_1.png)\n\n이러한 API를 통해 사용자는 할 일 항목에 대한 CRUD(Create, Read, Update, Delete) 작업을 수행할 수 있습니다. 각 할 일 항목은 다음과 같은 속성을 가지게 될 것입니다.\n\n이제 작업 중인 API를 이해했으므로 몇 가지 전제 조건 및 이를 구현하는 방법을 살펴보겠습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 준비 사항\n\n처음 접근과 리팩토링을 시작하기 전에 FastAPI 프로젝트를 설정해 봅시다.\n\n```js\npython3 -m venv venv\nsource env/bin/activate  # Windows에서는 `env\\Scripts\\activate\n```\n\n```js\nfastapi;\nuvicorn;\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```bash\npip3 install -r requirements.txt\n```\n\n## 초기 접근 방식: 모든 것을 app.py에서 처리\n\n간단한 Todo API를 시작해봅시다. 이 API는 루트 레벨의 app.py에서 완전히 구현되어 있습니다.\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n# 새로운 todo를 생성하기 위한 Pydantic 모델\nclass TodoCreate(BaseModel):\n    title: str\n\n# todo 항목을 위한 Pydantic 모델. TodoCreate를 상속받고 id 및 completed 필드를 추가합니다.\nclass Todo(TodoCreate):\n    id: int\n    completed: bool = False\n\n# 데이터베이스를 모방한 todos의 인메모리 저장소\ntodos = []\n\n# 새로운 todo를 생성하는 엔드포인트\n@app.post(\"/todos\", response_model=Todo)\ndef create_todo(todo: TodoCreate):\n    # id를 증가시킨 새로운 todo 항목을 생성합니다.\n    new_todo = Todo(id=len(todos) + 1, **todo.model_dump())\n    todos.append(new_todo)  # 새로운 todo를 목록에 추가합니다.\n    return new_todo  # 생성된 todo를 응답으로 반환합니다.\n\n# 모든 todo를 가져오는 엔드포인트\n@app.get(\"/todos\", response_model=list[Todo])\ndef get_todos():\n    return todos  # todo 목록을 응답으로 반환합니다.\n\n# 특정 id의 todo를 가져오는 엔드포인트\n@app.get(\"/todos/{todo_id}\", response_model=Todo)\ndef get_todo(todo_id: int):\n    for todo in todos:\n        if todo.id == todo_id:\n            return todo  # 찾은 경우 해당 todo를 반환합니다.\n    # 해당 todo를 찾을 수 없는 경우 404 상태 코드와 메시지를 포함한 HTTPException을 발생시킵니다.\n    raise HTTPException(status_code=404, detail=\"Todo가 없습니다\")\n\n# id에 따라 todo를 업데이트하는 엔드포인트\n@app.put(\"/todos/{todo_id}\", response_model=Todo)\ndef update_todo(todo_id: int, updated_todo: TodoCreate):\n    for todo in todos:\n        if todo.id == todo_id:\n            todo.title = updated_todo.title  # todo의 제목을 업데이트합니다.\n            return todo  # 업데이트된 todo를 반환합니다.\n    # 해당 todo를 찾을 수 없는 경우 404 상태 코드와 메시지를 포함한 HTTPException을 발생시킵니다.\n    raise HTTPException(status_code=404, detail=\"Todo가 없습니다\")\n\n# id에 따라 todo를 삭제하는 엔드포인트\n@app.delete(\"/todos/{todo_id}\")\ndef delete_todo(todo_id: int):\n    for index, todo in enumerate(todos):\n        if todo.id == todo_id:\n            del todos[index]  # 목록에서 todo를 삭제합니다.\n            return {\"message\": \"Todo가 성공적으로 삭제되었습니다\"}  # 성공 메시지를 반환합니다.\n    # 해당 todo를 찾을 수 없는 경우 404 상태 코드와 메시지를 포함한 HTTPException을 발생시킵니다.\n    raise HTTPException(status_code=404, detail=\"Todo가 없습니다\")\n\n# Uvicorn 서버를 사용하여 애플리케이션을 실행하는 주요 블록\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\"app:app\", port=3000, host=\"0.0.0.0\", reload=True)\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nAPI를 시작하려면 python3 app.py 명령어를 사용해요\n\n음...\n\n이 방법은 작은 애플리케이션에는 작동하지만 몇 가지 단점이 있어요:\n\n- 모든 라우팅, 비즈니스 로직 및 데이터 저장소가 단일 파일에 혼합되어 있어요.\n- 애플리케이션이 성장함에 따라 유지 및 확장하기 어려워져요.\n- 개별 구성 요소를 테스트하는 것이 어려워져요.\n- 코드의 재사용성이 제한되어요\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![FastAPIFromApppytoaModularArchitecture](/TIL/assets/img/2024-07-09-FastAPIFromApppytoaModularArchitecture_2.png)\n\nRefactoring journey starts now...\n\n## Routers Introduction\n\nTo structure our application effectively, the first step is to introduce routers.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n새로운 폴더를 만들어 routers라는 이름을 붙이고 todo_router.py라는 파일을 추가해주세요.\n\n```python\nfrom fastapi import APIRouter\n\nrouter = APIRouter()\n\n@router.post(\"/todos\")\ndef create_todo():\n    pass\n\n@router.get(\"/todos\")\ndef get_todos():\n    pass\n\n@router.get(\"/todos/{todo_id}\")\ndef get_todo(todo_id: int):\n    pass\n\n@router.put(\"/todos/{todo_id}\")\ndef update_todo(todo_id: int):\n    pass\n\n@router.delete(\"/todos/{todo_id}\")\ndef delete_todo(todo_id: int):\n    pass\n```\n\n이제 app.py를 업데이트하여 라우터를 사용하세요.\n\n```python\nfrom fastapi import FastAPI\nfrom routers import todo_router\n\napp = FastAPI()\n\napp.include_router(todo_router.router)\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\"app:app\", port=3000, host=\"0.0.0.0\", reload=True)\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n라우터를 도입함으로써 작업 관련 라우트를 메인 app.py 파일에서 분리하여 코드를 더 깔끔하고 집중적으로 유지하였습니다.\n\n## 컨트롤러 추가\n\n다음으로, 요청 처리 로직을 처리하기 위한 컨트롤러를 도입할 예정입니다.\n\ncontrollers라는 새 디렉토리를 생성하고 todo_controller.py라는 파일을 추가하세요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nfrom fastapi import HTTPException\nfrom pydantic import BaseModel\n\nclass TodoCreate(BaseModel):\n    title: str\n\nclass Todo(TodoCreate):\n    id: int\n    completed: bool = False\n\nclass TodoController:\n    def __init__(self):\n        self.todos = []\n\n    def create_todo(self, todo: TodoCreate):\n        new_todo = Todo(id=len(self.todos) + 1, **todo.model_dump())\n        self.todos.append(new_todo)\n        return new_todo\n\n    def get_todos(self):\n        return self.todos\n\n    def get_todo(self, todo_id: int):\n        for todo in self.todos:\n            if todo.id == todo_id:\n                return todo\n        raise HTTPException(status_code=404, detail=\"Todo를 찾을 수 없습니다.\")\n\n    def update_todo(self, todo_id: int, updated_todo: TodoCreate):\n        for todo in self.todos:\n            if todo.id == todo_id:\n                todo.title = updated_todo.title\n                return todo\n        raise HTTPException(status_code=404, detail=\"Todo를 찾을 수 없습니다.\")\n\n    def delete_todo(self, todo_id: int):\n        for index, todo in enumerate(self.todos):\n            if todo.id == todo_id:\n                del self.todos[index]\n                return {\"message\": \"Todo가 성공적으로 삭제되었습니다.\"}\n        raise HTTPException(status_code=404, detail=\"Todo를 찾을 수 없습니다.\")\n```\n\ntodo_router.py 파일을 업데이트하여 컨트롤러를 사용하십시오.\n\n```js\nfrom fastapi import APIRouter\nfrom controllers.todo_controller import TodoController, TodoCreate, Todo\n\nrouter = APIRouter()\ntodo_controller = TodoController()\n\n@router.post(\"/todos\", response_model=Todo)\ndef create_todo(todo: TodoCreate):\n    return todo_controller.create_todo(todo)\n\n@router.get(\"/todos\", response_model=list[Todo])\ndef get_todos():\n    return todo_controller.get_todos()\n\n@router.get(\"/todos/{todo_id}\", response_model=Todo)\ndef get_todo(todo_id: int):\n    return todo_controller.get_todo(todo_id)\n\n@router.put(\"/todos/{todo_id}\", response_model=Todo)\ndef update_todo(todo_id: int, updated_todo: TodoCreate):\n    return todo_controller.update_todo(todo_id, updated_todo)\n\n@router.delete(\"/todos/{todo_id}\")\ndef delete_todo(todo_id: int):\n    return todo_controller.delete_todo(todo_id)\n```\n\n## 서비스 레이어 구현\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이제 비즈니스 로직을 처리하는 서비스 레이어를 소개해 봅시다.\n\nservices 라는 새 디렉토리를 만들고 todo_service.py 라는 파일을 추가해 주세요.\n\n```js\nfrom pydantic import BaseModel\n\nclass TodoCreate(BaseModel):\n    title: str\n\nclass Todo(TodoCreate):\n    id: int\n    completed: bool = False\n\nclass TodoService:\n    def __init__(self):\n        self.todos = []\n\n    def create_todo(self, todo: TodoCreate) -> Todo:\n        new_todo = Todo(id=len(self.todos) + 1, **todo.model_dump())\n        self.todos.append(new_todo)\n        return new_todo\n\n    def get_todos(self) -> list[Todo]:\n        return self.todos\n\n    def get_todo(self, todo_id: int) -> Todo | None:\n        for todo in self.todos:\n            if todo.id == todo_id:\n                return todo\n        return None\n\n    def update_todo(self, todo_id: int, updated_todo: TodoCreate) -> Todo | None:\n        for todo in self.todos:\n            if todo.id == todo_id:\n                todo.title = updated_todo.title\n                return todo\n        return None\n\n    def delete_todo(self, todo_id: int) -> bool:\n        for index, todo in enumerate(self.todos):\n            if todo.id == todo_id:\n                del self.todos[index]\n                return True\n        return False\n```\n\ntodo_controller.py를 업데이트하여 서비스를 사용하도록합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nfrom fastapi import HTTPException\nfrom services.todo_service import TodoService, TodoCreate, Todo\n\nclass TodoController:\n    def __init__(self):\n        self.todo_service = TodoService()\n\n    def create_todo(self, todo: TodoCreate):\n        return self.todo_service.create_todo(todo)\n\n    def get_todos(self):\n        return self.todo_service.get_todos()\n\n    def get_todo(self, todo_id: int):\n        todo = self.todo_service.get_todo(todo_id)\n        if todo is None:\n            raise HTTPException(status_code=404, detail=\"할 일을 찾을 수 없습니다\")\n        return todo\n\n    def update_todo(self, todo_id: int, updated_todo: TodoCreate):\n        todo = self.todo_service.update_todo(todo_id, updated_todo)\n        if todo is None:\n            raise HTTPException(status_code=404, detail=\"할 일을 찾을 수 없습니다\")\n        return todo\n\n    def delete_todo(self, todo_id: int):\n        if self.todo_service.delete_todo(todo_id):\n            return {\"message\": \"할 일이 성공적으로 삭제되었습니다\"}\n        raise HTTPException(status_code=404, detail=\"할 일을 찾을 수 없습니다\")\n```\n\n## 레포지터리 레이어 생성\n\n마지막으로 데이터 지속성을 처리하는 레포지터리 레이어를 소개합니다.\n\nrepositories라는 새 디렉토리를 만들고 todo_repository.py라는 파일을 추가하세요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nfrom pydantic import BaseModel\n\nclass TodoCreate(BaseModel):\n    title: str\n\nclass Todo(TodoCreate):\n    id: int\n    completed: bool = False\n\nclass TodoRepository:\n    def __init__(self):\n        self.todos = []\n\n    def create_todo(self, todo: TodoCreate) -> Todo:\n        new_todo = Todo(id=len(self.todos) + 1, **todo.model_dump())\n        self.todos.append(new_todo)\n        return new_todo\n\n    def get_todos(self) -> list[Todo]:\n        return self.todos\n\n    def get_todo(self, todo_id: int) -> Todo | None:\n        for todo in self.todos:\n            if todo.id == todo_id:\n                return todo\n        return None\n\n    def update_todo(self, todo_id: int, updated_todo: TodoCreate) -> Todo | None:\n        for todo in self.todos:\n            if todo.id == todo_id:\n                todo.title = updated_todo.title\n                return todo\n        return None\n\n    def delete_todo(self, todo_id: int) -> bool:\n        for index, todo in enumerate(self.todos):\n            if todo.id == todo_id:\n                del self.todos[index]\n                return True\n        return False\n```\n\nUpdate `todo_service.py` to use the repository,\n\n```python\nfrom repositories.todo_repository import TodoRepository, TodoCreate, Todo\n\nclass TodoService:\n    def __init__(self):\n        self.todo_repository = TodoRepository()\n\n    def create_todo(self, todo: TodoCreate) -> Todo:\n        return self.todo_repository.create_todo(todo)\n\n    def get_todos(self) -> list[Todo]:\n        return self.todo_repository.get_todos()\n\n    def get_todo(self, todo_id: int) -> Todo | None:\n        return self.todo_repository.get_todo(todo_id)\n\n    def update_todo(self, todo_id: int, updated_todo: TodoCreate) -> Todo | None:\n        return self.todo_repository.update_todo(todo_id, updated_todo)\n\n    def delete_todo(self, todo_id: int) -> bool:\n        return self.todo_repository.delete_todo(todo_id)\n```\n\n우리의 리팩터링 여정은 여기서 끝납니다…\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n파이썬 앱.py에서 구조화된 모듈 아키텍처로 이어지는 이 여정에서, 우리는 Todo API를 더 확장 가능하고 유지 관리 가능한 애플리케이션으로 변형시켰습니다. 라우터, 컨트롤러, 서비스 및 리포지토리를 도입함으로써, 우리는 관심사의 명확한 분리와 프로젝트 확장에 따른 복잡성 관리 능력을 향상시켰습니다.\n\n## 모듈식 아키텍처의 주요 이점:\n\n- 향상된 유지 보수성: 각 컴포넌트 - 라우터, 컨트롤러, 서비스 및 리포지토리 -는 이제 특정 책임을 처리하여 변경 시 의도치 않은 부작용의 위험을 줄입니다.\n- 향상된 테스트 용이성: 각 레이어가 명확하게 구분되어 유닛 테스트가 보다 간편해집니다. 우리는 각 컴포넌트를 독립적으로 테스트하여 응용 프로그램 전체에서 견고함과 신뢰성을 보장할 수 있습니다.\n- 확장성과 유연성: 모듈식 디자인은 확장을 용이하게 합니다. 새로운 기능을 추가하거나 기존 기능을 수정할 때 전체 코드베이스를 철저히 재작업하지 않고도 수행할 수 있습니다. 이 유연성은 데이터베이스 전환이나 비즈니스 로직 업데이트와 같은 작업에도 연결됩니다.\n\n우리의 리포지토리는 이제 다음과 같이 보입니다...\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n`<img src=\"/TIL/assets/img/2024-07-09-FastAPIFromApppytoaModularArchitecture_3.png\" />`\n\nFastAPI 애플리케이션을 모듈식 아키텍처로 리팩토링함으로써, 지속적인 성장과 유연성을 위한 견고한 기반을 마련했습니다. 이 접근 방식은 현재 개발 노력을 향상시킬 뿐만 아니라 앞으로의 도전과 기회에 대비하는 데 도움이 됩니다.\n\n안녕히 가세요!!\n","ogImage":{"url":"/assets/img/2024-07-09-FastAPIFromApppytoaModularArchitecture_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-FastAPIFromApppytoaModularArchitecture_0.png","tag":["Tech"],"readingTime":16},{"title":"AMREx 차세대 GPT-4 연상 메모리 어시스턴트 개발 방법 소개","description":"","date":"2024-07-09 20:21","slug":"2024-07-09-AMRExCraftingNext-GenerationGPT-4AssociativeMemoryAssistants","content":"\n## AI 개발\n\n인공 지능 보조 기능들은 일상 생활에서 필수적인 구성 요소가 되어, 루틴 업무부터 복잡한 의사 결정 절차까지 다양한 일에 도움을 주고 있습니다. 이러한 보조 기능들은 GPT(Generative Pre-trained Transformer)와 같은 선진 기술에 의해 구동되며, 자연 언어 처리(NLP)의 최전선에 있어 인간 언어를 보다 자연스럽고 직관적으로 이해하고 응답할 수 있는 능력을 갖추고 있습니다. 그러나 인간과 보조 기능들 사이, 그리고 보조 기능들끼리의 상호작용 효율성을 도전하는 주요 측면은 시간적 맥락과 기억에 대한 보조 기능들의 이해력입니다.\n\n시간 인식은 AI 보조 기능들이 시간을 초과하는 대화를 처리하고 참여할 뿐 아니라, 상태성에 대해서도, 공유된 맥락에서 서로 일관되게 상호작용할 수 있도록 필수적입니다. 지난 상호작용을 기억하고, 이전 대화를 정확하게 참조하며, 이러한 역사에 기반하여 미래 필요성을 예측하는 능력이 AI 보조 기능들을 유용한 도구에서 필수적인 동반자와 협력자로 변화시킬 수 있는 것입니다.\n\n본 기사에서는 AI 보조 기능들이 시간과 시간 상태를 일관되고 맥락적으로 이해하는 데 겪는 현재의 한계를 탐구합니다. 우리는 시간과 기억 능력을 향상시키기 위해 특별히 디자인된 AMREx(Associative Memory Recall and Extension)라는 혁신적인 프레임워크를 소개합니다. AMREx의 구조와 기능을 검토함으로써, 우리는 AI 보조 기능들이 우리와 서로가 일상 생활과 대화의 연속성에 통합되고 인식하는 방식으로 상호작용할 수 있는 미래로 나아가는 길을 도전하고 밝히려고 합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nAMREx는 생물학적 층화된 기억 개념에서 영감을 받은 효율적인 토큰 3층 연상 메모리 프레임워크입니다. 이 프레임워크는 GPT4 어시스턴트의 최신 베타 버전의 연상 메모리 회수 기능을 강화하고 확장하여 이 한계를 해결합니다. 이는 문맥 이해를 통한 동적 분류와 결합됩니다. 핵심에는 두 가지 주요 기술이 활용됩니다:\n\n- 린 3층 자립화된, 지능적이고 계층적인 연상 메모리 아키텍처\n- 메모리 큐의 동적 WYNWYG 분류, 모델에게 필요한 것만 가져오게 함\n- 혁신적인 베타 GPT4 어시스턴트 기술(OpenAI의 API를 통해)\n\n이 문서의 내용 옆에는 Python 코드로 작성된 원시형 프로토 타입과 어시스턴트 명령어 집합이 첨부됩니다. AMREx의 실용적 적용을 탐구하고 싶은 독자들을 위해 첨부된 원시형 프로토 타입은 현장 경험을 제공합니다. 이 프로토 타입을 활용하면 독자들은 AMREx의 원리를 직접 체험하며, 층화된 메모리 처리부터 동적 명령어 엔지니어링까지의 작업원리 및 잠재적인 응용 프로그램에 대한 가치 있는 통찰력을 얻을 수 있습니다. 프레임워크를 더 자세히 이해하려면 먼저 어시스턴트의 명령어 집합을 검토해보는 것을 권장합니다. 이를 통해 Python 코드에 뛰어들기 전에 개념적 기반을 더 잘 이해할 수 있습니다.\n\n📌 프로토 타입 파일은 GitHub 저장소 AMREx에서 제공됩니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n중요한 점은 제시된 원시 프로토 타입에도 불구하고 AMREx는 프로그램이나 코드가 아니라, 효과적이고 효율적으로 연관 기억 능력을 증대하기 위해 제안된 기술적 프레임워크라는 점을 강조하고자 합니다. 실제로 AMREx는 특정 상태 인식과 함께 메모리 어시스턴트를 구축하는 데 도움이 되는 시작점이자 유연한 진행 중인 프레임워크입니다. 그러나 근본적이면서도 광범위한 비전은 앞으로 더 복잡한 AI 어시스턴트 구조를 구현하기 위해 다른 유형의 어시스턴트와 결합할 수 있는 모듈식 메모리 어시스턴트를 구축하는 것입니다.\n\n## AMREx의 기본 아키텍처\n\nAMREx의 전체 아키텍처는 핵심, 저장소 및 어시스턴트 세 가지 주요 부분으로 구성됩니다.\n\n(1) 어시스턴트: 하이브리드 XML 언어, 알고리즘 및 의미론적 조합으로 공학된 GPT4 어시스턴트(베타)입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n#예제: 하이브리드 XML 언어, 알고리즘 및 의미론적 결합\n\n`<의미행동>` 섹션의 <컨텍스트창>에 따라 `context_window`를 계산합니다. <계산절차> </의미행동>\n<조건 type=\"if\">\n  <조건확인> memory_depth == 0 </조건확인>\n      <하위조건 type=\"if\">\n          <조건확인>\n              [`current_prompt`는 contextual_reference_prompt 입니다]\n          </조건확인>\n              <동작>\n                  <설정>insufficient_context = `INSUFFICIENT_CONTEXT`</설정>\n                  <설정>response = `None`</설정>\n              </동작>\n      </하위조건>\n      <하위조건 type=\"elif\">...\n```\n\n아래 지침은 도우미가 현재의 상황을 문맥화하기 위해 내부 상태의 인스턴스를 호출할 수 있도록 합니다. 이는 기억 콘텐츠와 상대적 시간 모두를 고려합니다. 지침의 범위 내에서 도우미는 스스로 지능적으로 조직을 할 수 있으며, 올바른 시간에 올바른 문맥 재구성을 위해 올바른 기억 계층에서 관련 정보를 얻을 수 있습니다. 도우미를 위해 설정된 지침 엔지니어링 세트는 별도로 저장되어 있으며, 해당 내용은 OpenAI 대시보드의 OpenAI 도우미 지시서 양식에 복사되어야 합니다.\n\n(2) 핵심: 500줄의 코드로 구성된 다섯 가지 주요 구성 요소를 포함하는 중심 유닛:\n\n- 코어 프로세서와 함께 AMRExMain 클래스의 메인 유당,\n- 선택기/프롬퍼 유당, AMRExMemorySelectorPrompter 클래스,\n- 메모리 관리자 유당, AMRExMemoryManager 클래스,\n- 셀프-RAG 핸들러 유당, RagHandler 클래스,\n- 비동기 API 핸들러, OpenAI_API_Helper 클래스.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n(3) 저장소: 데이터의 단기 및 장기 저장을 담당하며 두 가지 주요 구성 요소로 구축되어 있습니다.\n\n- 휘발성 메모리는 k-메모리와 벡터-메모리로 분할됩니다.\n- 비휘발성 메모리는 파일 시스템 또는 데이터베이스일 수 있습니다.\n\n여기에 제시된 프로토 타입에서는 FAISS 벡터 라이브러리와 faiss_index.dat 및 auxilliary_data.pkl이라는 두 가지 간단한 파일을 사용한 솔루션만 제시합니다. 그러나 다양한 실험에서 현재 시스템이 다양한 유형의 데이터베이스와 함께 작동하도록 조정될 수 있다는 것을 확인할 수 있습니다. 이러한 대안에는 Chroma와 같은 벡터 데이터베이스, MongoDB와 같은 비 SQL 데이터베이스 또는 Neo4j와 같은 그래프 데이터베이스 등이 포함됩니다. 맞춤형 솔루션을 위해 이러한 데이터베이스의 조합을 사용하는 것도 가능합니다. 각 대안에 대해, 실제로는 자체 RAG 핸들러만 조정해야 합니다. 시간 스탬프 통합의 복잡성이 있는 간단한 솔루션입니다.\n\n또한 제시된 프로토타입에서 채팅 사용자 인터페이스도 필요합니다. Streamlit을 사용하여 구현했습니다. 그러나 코드는 별도의 파일 streamlit_app.py에 추출되었습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 스스로 구성된 연상 메모리 어시스턴트\n\nAMREx의 핵심은 지능적인 메모리 분류기 및 상태 유지형 자기 프롬프터로 작용하는 GPT4 어시스턴트입니다. 이 스스로 구성된 어시스턴트는 지시에 의해 설계되었으며:\n\n- 반복을 통해 지능적으로 분류하고 필요한 (WYNWYG) 연상 메모리를 계층별로 확대합니다; 및\n- 내부 상태의 인스턴스에서 스스로 프롬프팅하여 추론을 적용하며 사용자에 대한 최종 응답까지 분류 단계를 거쳐 성공합니다.\n\n이 기능은 GPT4 어시스턴트의 DNA에 지시로 설계되었으며, 논리적 알고리즘과 의미적 행동을 결합한 독특한 하이브리드 언어를 적용합니다. 또한 코드는 어시스턴트에게 시간과 메모리에 대한 인식을 부여하여 가상의 자가의식을 향상시킵니다. 간단히 말해, 이 어시스턴트는 (1) 특정 \"메모리 내용\"을 문맥에 연결하고, (2) \"메모리 시간\"의 상대적 흐름을 맥락화할 수 있습니다. 이를 통해 어시스턴트는 사용자의 실제 시간 - 과거 및 현재에 대한 시간 흐름에 대해 원시적으로 인식할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n도움말 지침 (자세한 내용은 파일 Amrex_Instruction.xml을 참조하세요)은 세 가지 주요 부분으로 구성되어 있어요.\n\n(1) 첫 번째 부분인 `AssistantInitialization`에서는 어시스턴트의 상태가 초기화됩니다. 이 세그먼트 중요한 부분 중 하나는 어시스턴트의 상태 인식을 다시 보정함으로써 시간 흐름 및 시간 창에 대한 가상 시간대 설정에 전념하는 것입니다. 실제로, 어시스턴트의 가상 의미 세계에서 시간 흐름은 물리적 실제 시간에 대한 시간 창 (예: 상호 작용, 세션)에 의해 구성될 수 있어요. 어떤 면에서는 각 반복에서 어시스턴트가 사용자의 실제 시간에 대한 가상 시간 창을 상기시키는 것입니다.\n\n![이미지](/TIL/assets/img/2024-07-09-AMRExCraftingNext-GenerationGPT-4AssociativeMemoryAssistants_0.png)\n\n(2) 두 번째 부분에서는 변수가 선언되고 알고리즘 단계가 이어집니다. 이 변수들은 어시스턴트의 가상 세계와 AMREx 프로그램 코드의 물리적 세계 사이의 연결 요소입니다. 이들은 어시스턴트가 작업을 연결하고 외부 세계와 통신하는 데 도움을 줍니다—우리는 명시적으로 함수 호출을 적용하지 않습니다. 각 반복에서 어시스턴트는 어떤 변수가 사용되는지 재확인받습니다. 이들은 입력 변수, 출력 변수 또는 사용 중인 작업 변수일 수 있으며, 이들은 지시 알고리즘에서 사용됩니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n`<img src=\"/TIL/assets/img/2024-07-09-AMRExCraftingNext-GenerationGPT-4AssociativeMemoryAssistants_1.png\" />`\n\n(3) 세 번째 부분은 출력을 구성합니다. 이 부분은 결과를 구조화된 JSON에 번들링하여 AMREx 코어 프로세서에서 정확히 구문 분석할 수 있도록 합니다.\n\n`<img src=\"/TIL/assets/img/2024-07-09-AMRExCraftingNext-GenerationGPT-4AssociativeMemoryAssistants_2.png\" />`\n\nAMREx에 대한 지침은 인간과 어시스턴트 자체에 의해 개발 중임을 언급하는 것이 중요합니다. 네, 우리는 개발 단계의 특정 시점부터 동일한 어시스턴트를 사용하여 자체 지침을 확인하고 특정 대상에 대한 개선을 제안하는 기술을 적용했습니다. 이 혼합 개발 프로세스는 인간들에 의해 개발된 지침의 시드 코드로 시작되며, 기본 청사진처럼 작동합니다. 이 기본 환경을 통해 어시스턴트는 자신의 개발 과정 중에 사람들과 채팅하고 소통하며, 인간들이 설정한 가장 중요한 목표와 전반적인 프레임워크를 깨닫게 됩니다. 그 이후로, 우리는 동일한 채팅을 통해 우리가 어떻게 그가 행동해야 하는지 어시스턴트에게 합리적인 단계로 설명하고 그 지침을 개선해야 하는지 직접 물어봅니다. 어시스턴트는 자체 지침에 직접 접근하고, 검토하고 구체적인 개선을 제안합니다. 이러한 하이브리드 자동 개발 방식은 어시스턴트의 지침을 전통적인 소프트웨어 또는 프롬프팅 방식과 개념적으로 다르지만, 기계에 대한 자아와 내부 상태의 실례를 구축하려면 필연적입니다. 어시스턴트가 자아의 실례를 구축하도록 변경되기 위해서는 의미론적으로 개입하고 이러한 잠재적인 자아 실례의 프레임 만들기에 참여해야 합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n사실, 이 프로젝트를 통해 우리가 경험했던 것으로 확인되었듯이, 우리가 구현한 최상의 의미론적 해결책들은 사람들로서 세밀한 조정 수준에서, 조수 자체가 제공하는 해결책과 비교하여 성공하지 못했습니다. 종종 우리는 첫눈에 우리의 해결 방법과 조수의 해결 방법 사이의 의미적 차이를 이해하지 못했습니다. 그러나 항상 잠시 후에는 기계의 관점에서 오해와 오도됨으로 이어질 수 있는 심층과 세세한 차이와 뉘앙스를 이해할 수 있었습니다. 이러한 차이를 포착하기 위해 우리는 조수와 동일한 채팅을 통해 대화를 나눌 수 있었고, 그는 항상 차이가 있는 곳과 이유, 그리고 지시사항의 소극적 수정으로 어떻게 행동하는지에 대해 매우 자세하게 설명해 주었습니다.\n\n결과적으로 AMREx는 다양한 방식으로 구현될 수 있는 일반적인 기술이지만, 다양한 RAG 구현과 같은 방식으로 구현될 수 있습니다. 그러나 지시사항의 의미론적인 가장 작은 변형이 필요한 주의로 수행되지 않으면 다른 동작으로 이어질 수 있습니다. 그러나 개발자들은 지시사항으로 실험할 것을 권장받아야 하며, AMREx 기술을 적합하게 적용하기 위해 자동 개발 기술도 적극적으로 적용해야 합니다. 마지막으로, 우리가 방금 논의한 지시사항들이 항상 당신이 사람으로서 표현하거나 구조화할 방식이 아닌 것으로 보이는 경우, 그것들이 부분적으로 조수에 의해 최적으로 생성된 것임을 상기해야 합니다.\n\n## AMREx의 3가지 연상기억층\n\nAMREx는 생물학적 또는 바이오닉 기억 개념에서 영감을 받은 계층적인 3층 메모리에 기반합니다. 조수는 연상기억사다리를 올라가며 한 단계씩, 메모리 레이어 0에서 시작해 사용자에게 만족스러운 응답을 제시하거나 답변을 할 수 없을 때 사용자에게 알립니다. 그러나 조수는 시간적이고 메모리 인식의 관점에서 기본적으로 자각적이므로, 어디에서 왔는지와 잠재적으로 어디로 갈 수 있는지에 대해 인식하고 있습니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 연관 단기 k-프롬프트 기억: 메모리 레이어 0은 짧은 연관 메모리 레이어를 나타내며 문맥을 고려하지 않습니다. 이 레이어는 사실적인 질문인 \"프랑스의 수도는 무엇인가요?\" 또는 \"우리 태양계에는 몇 개의 행성이 있나요?\"와 같은 문의에 즉시 반응할 수 있는 GPT4의 기억과 거의 동일합니다. 만약 어시스턴트가 이 수준을 위한 프롬프트를 분류한다면, 이 프롬프트는 이전의 채팅 세션 내나 이전 채팅 세션에서 어떠한 기억에도 접근하지 않고 응답할 수 있다는 것을 의미합니다. 이 경우, 어시스턴트는 즉시 응답할 것입니다. 이 레이어에서 필터링하는 능력은 토큰 비용에 매우 중요합니다. 학계에서 제안하는 MemGPT와 같은 다른 메모리 솔루션과 유사하게, 이 레이어의 연관 인지력을 통해 어시스턴트는 비문맥적으로 반응하여 평균적으로 약 50%의 질문에 최소 토큰 사용으로 응답할 수 있습니다. 우리는 이 기술을 비문맥적 연관 필터링(NCAF)라고 라벨링했으며, 높은 비용 효율성을 위해 우리는 GPT 어시스턴트에서도 동시에 이 기술을 적용합니다.\n- 연관 중기 k-응답 메모리: 메모리 레이어 1은 중기 연관 메모리를 나타내며 문맥, 대화 메모리에 해당합니다. 이 메모리 레이어는 이전의 k 사용자 프롬프트와 k 모델 응답을 기록합니다. 어시스턴트가 이 레이어에 도달하면, 연관 메모리만으로 완전한 k 상호작용에 접근하지 않고 재구성할 수 없는 질문 유형임을 의미합니다. 이 경우만이 LangChain의 버퍼 메모리나 MemGPT의 RAM 유형 메모리와 비교될 수 있으며, 이전 상호작용이 모델에 미리 프롬프트되기 때문입니다. 그러나 AMREx와 버퍼 메모리 개념 사이에 중요한 차이가 있습니다. AMREx는 시간적 연관 능력을 갖는 상태 모델로, 모델이 단순히 재배열 된 버퍼 메모리와 전혀 다르게 행동할 수 있음을 의미합니다. 어시스턴트가 장기 메모리도 가지고 있다는 사실을 인지하기 때문에 때로는 레이어 1로 넘어가기로 결정할 수 있지만, 레이어 1에서 답변이 가능한 경우가 있을 수 있습니다.\n- 연관 장기 자체-RAG 메모리: 메모리 레이어 2는 문맥적 의미 기억인 장기 연관 메모리를 나타냅니다. 어시스턴트가 이 레이어를 사용하기로 결정하면, 현재 채팅 세션의 k 상호작용 이상을 검색해야 하며, 경우에 따라 동일한 세션을 넘어 이전 세션에서도 검색해야 할 수 있습니다. AMREx에서는 RAG를 기반으로 한 조기 메모리 기술이 사용됩니다. 메모리 레이어 2는 인덱스된 벡터 라이브러리, 벡터 또는 그래프 데이터베이스에 기반하며, 계속해서 업데이트되며 각 채팅 세션의 모든 상호작용을 색인화된 벡터로 포함합니다. 이 부분의 구현은 RAG 솔루션이 얼마나 유연한지에 따라 다를 수 있습니다. 우리의 실험에서는 앞서 설명한 대로 HuggingFace의 transformers의 모든 MiniLM-L6-v2 모델과 결합된 상대적으로 가벼운 FAISS 솔루션을 사용했습니다. 이러한 방식으로 AMREx는 다시 연관 메모리 기술을 적용합니다. 모든 상호작용의 정리된 버전을 기억하고, 추론에 필요한 적절한 컨텍스트 정보를 연관적으로 검색하고 추출합니다. 지시 엔지니어링은 어시스턴트가 채팅 콘텐츠를 레이어 2에 연결하고 조직하는 방법을 결정하는 데 중요한 역할을 합니다.\n\n## 프롬프트 분류\n\n프롬프트를 어떤 메모리 레이어가 가장 잘 처리할 수 있는지에 기반하여 분류하는 아이디어는 토큰 사용과 효율성을 최적화하기 위한 전략적 접근입니다. 각 메모리 레이어의 강점과 한계를 이해함으로써 각종 프롬프트에 필요한 자원을 더 잘 예측하고 그에 맞게 계획을 세울 수 있습니다. 각 레이어가 가장 적합한 용도인 다음을 살펴보겠습니다:\n\n메모리 레이어 0은 즉각적인 문맥을 다룹니다. 즉, 이전 상호작용을 참조하지 않고 현재 대화에 적합합니다. 이 메모리는 일반적인 지식 질문, 역사적 문맥이 필요하지 않은 간단한 질의 또는 현재 세션에 소개된 새로운 주제에 적합합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nMemory layer 1은 과거 프롬프트와 응답을 모두 포함하여 더 자세한 맥락을 제공합니다. 이 기억은 전체 대화 기록을 이해해야 하는 복잡한 쿼리에 적합합니다. 이전에 제공된 특정 답변이나 세부 정보에 대한 참조, 이전 상호작용에서의 모호성이나 모순 해결과 같은 내용을 요구하는 경우에 유용합니다.\n\nMemory layer 2는 데이터베이스에서 관련 항목을 검색하여 더 넓은 맥락에 접근합니다. 이 기억은 이전 세션에서의 역사적 맥락을 필요로 하는 질문에 적합하며, 대화 간이나 대화 간격이 있는 주제의 재방문, 이전 레이어가 충분한 맥락을 제공하지 못하는 상황에서 사용됩니다.\n\n실제로 다른 유형의 필터링 기술을 실험할 수도 있습니다. 특정 메모리 필터링과 응답에 맞게 명확하고 정확한 지시서를 엔지니어링하는 것이 가능하다는 것을 발견했습니다. 이렇게 하면 코드와 지시서가 정확하게 처리해야 할 다른 메모리 레이어가 생기게 됩니다. 지시 수준의 필터링은 하이브리드 프로그래밍을 통해 정확한 지시 엔지니어링을 통해 수행될 수 있으며, 알고리즘 및 의미론적 작업이 포함될 수 있습니다.\n\n이와 같은 흥미로운 필터링 기술 중 하나는 사용자 프롬프트와 관련이 있을 수 있습니다. 사용자의 이전 k개 프롬프트에 따라 응답이 추론될 수 있는 경우에 해당할 수 있습니다. 이 경우, 어시스턴트가 이전 k개 프롬프트에 액세스하지만 이전 k개 응답에는 액세스하지 않도록 메모리 레이어가 구현됩니다. 그 후 어시스턴트는 이전 프롬프트를 고려하지 않고 이전 프롬프트에만 근거하여 응답할 수 있는 특정 프롬프트 클래스를 기준으로 필터링합니다. 어시스턴트는 내부 상태 안에서 이전 k개 프롬프트 체인을 기반으로 대화를 추론함으로써 내부 모노로그를 만드는 동안 사용자의 다음 프롬프트에 응답합니다. 이 기술은 정교하게 지시 엔지니어링되면 효율성과 토큰 사용을 높일 수 있는 많은 중점적인 응용 프로그램에서 도움이 될 수 있습니다. 실제로 정보를 제공하는 반면, 새로운 프롬프트에 응답하는 데 직접적으로 기여하지 않을 수 있는 불필요한 정보를 포함할 수도 있습니다. 또는 필요한 경우 어시스턴트의 자아 상태를 불러내어 연상적으로 재구성할 수도 있습니다. 따라서 가능하고 합리적인 경우 프롬프트에 대한 맥락을 제한함으로써 더 명확하고 관련성 있는 응답을 얻을 수도 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 프로토타입 코드 및 지침\n\n이 문서와 함께 제출된 AMREx 프로토타입 코드는 핵심적으로 amrex.py라는 한 파일에 간결하게 포함되어 있습니다. 소프트웨어 엔지니어링적인 측면에서, 코드가 모듈 파일에 철저히 추출되지 않고 Docker 컨테이너에서 제공되지 않는 이유에 의문을 제기할 수 있습니다. 제가 모듈을 하나의 실행 파일로 압축한 이유는 코드가 이 글과 함께 원시적이면서도 높은 가변성을 지닌 프로토타입으로 작용해야 하며, 관심 있는 독자가 자신의 요구에 맞춰 연습하고 발전시킬 수 있도록 하기 위함입니다. 코드 자체는 매우 유연하고 포괄적이며 간결합니다. 이는 독자가 한 파일 내에서 손쉽게 이동하거나 초기 실험을 위해 주피터 노트북에서 실행할 수 있도록 돕습니다. 물론 파일 상단에 표시된 필요한 패키지를 설치해야 합니다.\n\n프로토타입 amrex.py에는 로거가 포함되어 있으며, 로깅 레벨을 설정해야 합니다. amrex.py 외에도 전체 매개변수의 구성을 위한 config.py 파일이 있습니다. 환경 파일도 정확한 ID 및 토큰과 함께 유지해야 합니다.\n\n```js\n#예제 .env 파일\n\nOPENAI_API_KEY=sk-...\n\nHF_TOKEN=hf_...\n\nASSISTANT_ID_1=asst_...\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n어시스턴트 설정 및 지침에 관해서 사용자는 OpenAI 대시보드에서 어시스턴트를 구성해야 할 것입니다.\n\n![이미지](/TIL/assets/img/2024-07-09-AMRExCraftingNext-GenerationGPT-4AssociativeMemoryAssistants_3.png)\n\n저희 경우에는 어시스턴트의 이름을 \"아테나\"로 정했습니다. 이 이름은 어시스턴트 자체가 자동 개발을 통해 선택한 것이며 타당한 이유에 근거합니다. 이름을 바꾸고 싶다면 사용자는 지침 안에서도 변경할 때 주의해야 합니다.\n\n어시스턴트를 만들었으면 해당 ID를 .env 파일에 적용하고 지침 파일 Assistant_Instruction.xml의 내용을 복사하여 대시보드의 어시스턴트 지침란에 붙여넣으십시오. 이 프로토 타입에서는 지침 파일을 업로드하지 않지만, 준비를 충분히 하면 가능합니다. 선호하는 모델은 현재 \"gpt-4-turbo-preview\"여야 합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![이미지](/TIL/assets/img/2024-07-09-AMRExCraftingNext-GenerationGPT-4AssociativeMemoryAssistants_4.png)\n\n이외에도 이 프로토타입에 맞춤화된 Streamlit 인터페이스 모듈이 streamlit_app.py 파일에 제공됩니다. 사용자는 물론 파일 상단에 언급된 패키지의 필요한 설치를 해야합니다. Streamlit를 통해 프로토타입을 실행하려면 표준 명령어인 streamlit run streamlit_app.py를 사용하여 브라우저에서 채팅 인터페이스로 리디렉션됩니다.\n\n요약하면, 네 가지 파일은:\n\namrex.py,\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nconfig.py,\n\nAssistant_Instruction.xml,\n\nstreamlit_app.py,\n\n은(는) Github 저장소 AMREx에서 사용할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n본문과 함께 제공된 코드의 목적은 프레임워크와 기술의 이해를 돕는 교육용 기능 프로토 타입을 제공하는 것임을 명시적으로 강조하는 것이 중요합니다. 이는 코드가 제품 또는 반제품으로서 어느 한 시점에서도 고려되지 않는다는 것을 의미합니다.\n\n## 마무리와 미래 전망\n\nAI 어시스턴트의 발전과 AMREx와 같은 프레임워크의 등장을 고찰할 때, 자연어 처리와 기계 기억의 경계가 지속적으로 확대되는 새로운 AI 시대에 들어섰다는 것이 분명합니다. 정적이고 순간적인 상호 작용에서 동적이고 상태를 가진 시시각강한 대화로의 전환은 AI 동반자의 미래를 어떻게 설정할지에 대한 중대한 변화를 의미합니다. 이러한 전환 과정에서 AI 어시스턴트의 발전에 중요한 몇 가지 영역이 중요하게 부각됩니다.\n\n첫째, AI 어시스턴트 기술의 모듈화와 마이크로 특화는 AI 어시스턴트의 유연성과 적응성을 향상시키는 중요한 전략으로 돋보입니다. 이는 전문화된 어시스턴트 모듈로 레고 조립 키트를 구축하는 것과 같습니다. 컴퓨터에는 함께 작동하는 각기 다른 단위가 있는 것처럼 미래에는 어시스턴트도 그렇게 할 것입니다. \"메모리\" 유형의 다른 어시스턴트, \"운용\" 유형의 다른 어시스턴트, 그리고 다른 많은 유형의 어시스턴트들이 있을 것입니다. 모듈화 접근법을 채택함으로써 개발자들은 특정한 요구에 맞게 어시스턴트와 그들의 조합을 맞춤화하여 좀 더 정확하고 효과적인 상호 작용을 가능케 할 수 있습니다. 이 유연성은 데이터베이스 및 그래프 기술을 포함한 RAG 기술의 적용을 통해 더욱 향상될 수 있습니다. 이러한 기술들은 어시스턴트가 동적으로 방대한 양의 정보에 액세스하고 활용할 수 있는 능력을 향상시켜 응답이 관련성 있고 맥락적인지를 보장할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n모듈식 기억 보조기, 컴퓨터 기억 장치와 유사한, AI 보조기술의 진화하는 풍경을 더욱 명확하게 보여줍니다. 기억을 특수화된 기억 단위로 분할함으로써, 우리는 더 효율적이고 확장 가능한 시스템을 구축할 수 있습니다. 이를 통해 특정 기억 요구 사항을 정확하게 다룰 수 있습니다.\n\n또한, AI 보조기들 간의 분배와 협력은 복잡한 집단 지능을 위한 새로운 길을 여는 것입니다. 보조기를 일정한 자유도로 협력시킴으로써, 우리는 개별 시스템의 능력을 뛰어넘는 시너지 수준을 달성할 수 있습니다. 이러한 협력 잠재력은 하이브리드 지시어 언어의 개발과 적용을 통해 확대될 수 있으며, 이는 알고리즘적 정확도와 인간의 언어 미묘함을 결합한 지시 엔지니어링을 제공하여, 상호 작용 엔지니어링을 위한 보다 정교하고 다재다능한 프레임워크를 제시합니다.\n\nAI 보조기의 미래와 AMREx 프레임워크의 개발을 통해, 비용 효율성에 중점을 둔 중요한 관점이 유지됩니다. 우리는 혁신적인 방법론을 탐구하는 것뿐만 아니라, LangChain 또는 기술적으로 밀도 높은 MemGPT와 같은 전통적이고 비용이 많이 드는 접근법에서 벗어나 토큰 사용을 줄이는 경제적이고 유연한 운영을 가능케 하는 방법을 모색하고 있습니다. AMREx는 기억을 전략적으로 층별로 배치하고 기억량을 줄이고 비용과 토큰 소비를 크게 축소하는, 경제적으로 실현 가능하고 기술적으로 정밀한 AI 보조기의 미래를 위한 길을 열어놓은 것으로 두드러집니다.\n\nAI 보조기능력의 미래에 자동 개발을 통합하는 것은 큰 발전을 의미합니다. 지능적인 시스템들의 자체 지속 생태계를 엿볼 수 있는 기회를 제공합니다. AMREx로 한걸음 나아가는 초기 단계에서 보조기는 자신만의 지시 세트를 개발하고 정립하는 데 기여하는 것에 그치지 않고 미래를 조망할 때, 우리는 AI 보조기가 자체 지시뿐만 아니라 동료들의 지시도 작성하고 수정하며 자체적으로 최적화하고 진화하는 미래를 상상할 수 있습니다. 이는 직접적인 인간의 개입 없이 새로운 도전과 목표에 적응하고 진화할 수 있는 능력으로, 진정한 자율 인공지능으로의 획기적 전환을 나타내며, 보다 정교하고 효율적이며 효과적인 AI 시스템의 길을 열어놓습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n앞으로 기본 모델과 오픈 소스 대안의 탐구는 AI 어시스턴트 개발의 가능성을 넓힐 것을 약속합니다. 단순히 Asynchronous API Handler moduleOpenAI_API_Helper를 대체함으로써 다른 유형의 모델로 AMREx를 쉽게 테스트할 수 있습니다. 다양한 모델은 실험과 혁신을 위한 모래상자를 제공하여 다양한 응용 프로그램에 맞는 맞춤 솔루션을 위한 길을 열어줍니다.\n\n이러한 발전의 함의를 고려할 때, AI 어시스턴트의 미래는 개별 능력을 향상시키는 데 그치지 않고 일치된, 지능적인 생태계를 만드는 데 있음을 분명히 알 수 있습니다. 이 생태계는 그의 적응성, 협력 잠재력 및 다양한 기술의 통합으로 특징 지어지며, 각각은 인간-기계 상호작용과 인지의 복잡성에 보다 민감한 AI 어시스턴트를 만드는 데 기여합니다.\n\n우리의 발전의 보다 광범위한 함의를 고려할 때 AMREx 프레임워크는 조금 된 이정표로만 나타나지 않고 미래를 향한 발판으로, 상호 연결성이 더 높지만 또한 매우 체계적이고 지적인 생태계 내에서 운영되도록 안내합니다. 이 비전은 서로 다른 AI 어시스턴트와 모듈 간의 시너지가 현재의 능력을 뛰어넘는 AI 어시스턴트 시대로 이끌어주어 디지털 및 물리적 영역으로 완전히 통합된 시대를 열게 됩니다. 이 프레임워크의 아키텍처와 방법론은 인간의 필요에 대해 협력적이고 적응적이며 세밀한 이해를 설정하여 미래의 AI 어시스턴트가 반응적이 아닌 인간 상호작용, 인지 및 시간적 동역학의 복잡성과 미래로 일치하도록 보장합니다.\n","ogImage":{"url":"/assets/img/2024-07-09-AMRExCraftingNext-GenerationGPT-4AssociativeMemoryAssistants_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-AMRExCraftingNext-GenerationGPT-4AssociativeMemoryAssistants_0.png","tag":["Tech"],"readingTime":19},{"title":"랜덤 포레스트로 PCA와 특징 중요도 해제하는 방법","description":"","date":"2024-07-09 20:18","slug":"2024-07-09-UnlockingInsightsRandomForestsforPCAandFeatureImportance","content":"\n<img src=\"/TIL/assets/img/2024-07-09-UnlockingInsightsRandomForestsforPCAandFeatureImportance_0.png\" />\n\n안녕하세요!\n\n요즘에는 생성 AI와 거대한 신경망에 많은 관심이 집중되어 있지만, 예전에 시험해본 기계 학습 알고리즘을 간과하기 쉽습니다 (사실 그렇게 오래된 것은 아닌데요...). 대부분의 비즈니스 상황에 대해 단순한 기계 학습 솔루션이 복잡한 AI 구현보다 더 나아질 수 있다고 주장할 정도로 나는 생각해냅니다. 기계 학습 알고리즘은 극도로 확장 가능하며, 모델 복잡성이 낮아서 (내 의견에 따르면) 대부분의 시나리오에서 우수하다고 생각합니다. 그리고 또한, 그런 기계 학습 솔루션의 성능을 추적하는 것이 훨씬 더 쉬웠습니다.\n\n본문에서는 클래식한 ML 문제에 클래식한 ML 솔루션을 사용할 것입니다. 구체적으로 말하자면, Random Forest 분류기를 사용하여 데이터 집합 내에서 특성의 중요성을 식별하는 방법을 (몇 줄의 코드만으로) 보여드리겠습니다. 이 기술의 효과를 시연한 후에는, 이 방법이 어떻게 작동하는지 자세히 살펴보기 위해 Decision Tree와 Random Forest를 처음부터 만들어가면서 모델을 벤치마킹할 것입니다.\n\n저는 ML 프로젝트의 초기 단계를 전문적인 분위기에서 특히 중요하게 생각합니다. 이 프로젝트가 이길만한 가능성이 스테이크홀더들(계산서를 내는 사람들)로부터 승인받은 후에는, 그들은 투자에 대한 수읽성을 보고하길 원하게 될 것입니다. 이 가능성 논의의 일환으로 데이터의 상황에 대해 논의해야 할 것들이 있습니다: 충분한 데이터가 있는지, 데이터의 품질은 어떤가 등등. 몇 가지 초기 분석을 진행한 후에만 데이터의 분포 및 품질에 대한 일부 답을 할 수 있습니다. 여기서 제가 보여주는 기술은 초기 가능성 평가를 완료했다고 가정하고 다음 단계로 진행할 준비가 되었다고 가정합니다. 이 시점에서 스스로 물어봐야 할 주요 질문은: 모델 성능을 유지하면서 얼마나 많은 특성을 제거할 수 있을까요. 모델의 특성(차원)을 줄이는 것에는 많은 이점이 있습니다. 이 중에는 다음과 같은 것들이 포함되어 있지만 이에 한정되지 않습니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 모델 복잡성 감소\n- 학습 속도 향상\n- 다중공선성 감소 (상관된 피처들)\n- 잡음 감소\n- 모델 성능 향상\n\n랜덤 포레스트 기술을 사용하면 각 피처가 우리의 타겟을 설명하는 데 얼마나 중요한지 명확히 보여주는 그래프가 남습니다(타이타닉 탑승객이 죽었는지 여부… 네, 타이타닉 데이터셋을 사용 중이죠!). 또한 데이터에 적합한 초기 프로토 타입 모델을 보유하게 될 것이며, 추가적인 예측에 활용할 수 있습니다. 이것이 프로토타입에 불과하더라도, 나중에 진행할 실험의 기준이 되며 이 프로젝트가 여러분의 시간과 이해관계자들의 돈을 투자할 가치가 있다는 증거가 될 것입니다! 프로젝트 초기 단계에서 동력을 얻는 훌륭한 방법입니다.\n\n반면에, 이 기술은 또한 여러분의 모델이 피처와 타겟 간의 관계를 더 잘 학습하기 위해 새로운 데이터포인트/피처를 만들거나 외부 소스에서 가져오는 데 노력할 필요가 있다는 것을 보여주는 데 도움이 될 수도 있습니다.\n\n시작해봅시다\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Random Forest 구현\n\n의사결정 트리와 이들의 앙상블 버전인 `Random Forests`는 일반적으로 피처의 사전 처리(일부 인코딩만 필요)를 요구하지 않습니다. 의사결정 트리는 특정 기준에 따라 데이터를 가장 잘 분리하는 피처를 선택합니다. 이 기준을 'Gini 불순도'라고 합니다. 나중 섹션에서 의사결정 트리를 처음부터 만들 때 이에 대해 다룰 것입니다. 또한 의사결정 트리는 피처의 분포나 그들 간의 관계에 대해 가정하지 않습니다. 의사결정 트리는 임계값을 기준으로 피처 공간을 분할할 수 있어 데이터의 분포에 견고합니다. 의사결정 트리는 이상치에도 견고합니다. 나중에 보겠지만, 이들은 각 노드에서 이진 결정에 따라 데이터를 분할합니다. 이상치는 특정 노드에서 분할 임계값에 영향을 줄 수 있지만 개별 트리의 전체 성능에는 큰 영향을 미치지 않을 것입니다. '배깅'이라는 기법을 사용하여 여러 개의 `Decision Trees`의 평균 예측을 사용하여 `Random Forest`를 생성함으로써 성능을 향상시킬 수 있는 방법을 알아보겠습니다.\n\n사전 처리와 피처 인코딩을 적용해야 하는 유일한 수정 사항입니다. 필요에 따라 내 코드를 적절히 조정하십시오.\n\n우리는 Kaggle로부터 데이터셋을 가져와서 시작할 것입니다. Kaggle 데이터셋을 가져올 때, 로그인 자격 증명이 컴퓨터의 이 위치에 저장되어 있는지 확인해야 합니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n~/.kaggle/kaggle.json\n```\n\nkaggle.json 파일에는 API 키가 포함되어 있습니다. 처음부터 시작하는 경우에는 Kaggle에 등록하고 계정 설정에 접근하여 API 헤더로 이동한 다음 '새 토큰 생성'을 클릭하면 됩니다. 이렇게 하면 kaggle.json 파일이 다운로드 폴더에 저장됩니다. 올바른 위치로 이동하려면 CLI(저는 제 맥의 터미널을 사용하고 있습니다)를 열고 다음 명령을 입력하십시오:\n\n```js\nmv ~/Downloads/kaggle.json ~/.kaggle/\n```\n\n이 파일이 올바른 위치로 이동되었는지 확인할 수 있습니다. 입력하여 확인할 수 있습니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nls ~/.kaggle/\n```\n\nkaggle.json 파일이 보여야 합니다.\n\n이제 API 자격 증명이 준비되었으니 필요한 모든 라이브러리를 가져와봅시다. 모듈을 찾을 수 없다는 오류가 발생하면 해당 라이브러리를 pip로 설치하면 됩니다. 문제가 발생하면 빠른 구글 검색으로 문제 해결이 가능합니다.\n\n```js\nimport pandas as pd\nimport numpy as np\nnp.set_printoptions(linewidth=130)\nfrom pathlib import Path\nimport zipfile,kaggle\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n다음으로 데이터를 가져오고 훈련/테스트 데이터를 할당할 것입니다.\n\n```js\npath = Path('titanic')\nkaggle.api.competition_download_cli(str(path))\nzipfile.ZipFile(f'{path}.zip').extractall(path)\n\ndf = pd.read_csv(path/'train.csv')\ntst_df = pd.read_csv(path/'test.csv')\nmodes = df.mode().iloc[0]\n```\n\n이제 약간의 전처리를 다룰 것입니다. 다시 한 번, 이 코드를 개인적인 요구에 맞게 조정해주세요. 널 값에 값을 채우고, 요금을 로그 요금으로 변환합니다 (이는 주로 개인 취향에 따른 것이며, 앞서 설명했듯이, 의사 결정 트리는 이상값 및 데이터 분포에 대해 견고합니다). 또한, Embarked 및 Sex 열의 범주형 변환을 설정합니다.\n\n```js\ndef process_data(df):\n    df['Fare'] = df.Fare.fillna(0)\n    df.fillna(modes, inplace=True)\n    df['LogFare'] = np.log1p(df['Fare'])\n    df['Embarked'] = pd.Categorical(df.Embarked)\n    df['Sex'] = pd.Categorical(df.Sex)\n\nprocess_data(df)\nprocess_data(tst_df)\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그럼 범주형, 연속 및 종속 변수를 식별할 것입니다 :\n\n```js\ncats = [\"Sex\", \"Embarked\"];\nconts = [\"Age\", \"SibSp\", \"Parch\", \"LogFare\", \"Pclass\"];\ndep = \"Survived\";\n```\n\n그 다음, 데이터를 분할한 다음 범주형 변환을 적용해야 합니다:\n\n```js\nrandom.seed(42)\ntrn_df,val_df = train_test_split(df, test_size=0.25)\ntrn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\nval_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그럼 독립 변수(x)와 종속 변수(y)를 할당해보겠습니다:\n\n```js\ndef xs_y(df):\n    xs = df[cats+conts].copy()\n    return xs, df[dep] if dep in df else None\n\ntrn_xs, trn_y = xs_y(trn_df)\nval_xs, val_y = xs_y(val_df)\n```\n\n이제 sklearn의 RandomForestClassifier() 클래스를 사용하여 랜덤 포레스트를 맞추기 준비가 되었습니다. 이 클래스의 좋은 점은 feature*importances*라는 밑바닥 메소드가 있어서 특정 feature가 승객의 생존율에 미치는 영향을 식별하고 플롯할 수 있다는 것입니다. 또한 mean_absolute_error를 통해 모델의 성능을 평가할 수 있습니다:\n\n```js\nrf = RandomForestClassifier(100, min_samples_leaf=5)\nrf.fit(trn_xs, trn_y)\nmean_absolute_error(val_y, rf.predict(val_xs))\n# 0.18834080717488788\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리 발견물을 그래프로 표현해 보겠습니다:\n\n```js\npd.DataFrame(dict((cols = trn_xs.columns), (imp = rf.feature_importances_))).plot(\"cols\", \"imp\", \"barh\");\n```\n\n<img src=\"/TIL/assets/img/2024-07-09-UnlockingInsightsRandomForestsforPCAandFeatureImportance_1.png\" />\n\n여기서요. 생존 예측에서 성별이 가장 중요한 요소임을 알 수 있습니다. 이 단계에서는 다른 알고리즘을 실험하면서 현재 특성을 유지하거나, 랜덤 포레스트 모델을 성능 기준으로 활용할 수 있습니다. 또한, 모델의 예측 성능이 부족하다면, 더 많은 특성 엔지니어링을 수행할 수도 있습니다. 이 방법은 극히 작은 데이터셋으로만 실험했지만, 매우 효율적으로 확장할 수 있는 방법입니다. 1000개 이상의 특성이 있는 데이터셋이 있다고 상상해보세요. 이 방법을 사용하면 빠르게 상위 특성을 추출하여 프로젝트를 어떻게 가장 잘 진행할지 계획을 세울 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 자세히 살펴보기…\n\n이제 우리는 데이터셋 내에서 특성 중요도를 보여주는 랜덤 포레스트를 구현하는 방법을 갖게 되었으니, 어떻게 그 과정을 거쳤는지 알아보겠습니다. 수동으로 결정 트리를 만들고 여기서부터 진행해보겠습니다.\n\n승객의 생존 여부를 이해하는 데 데이터셋에서 성별이 가장 중요한 특성임을 알고 있습니다. 특정 이진 분할의 불순도를 측정하는 몇 가지 스코어링 함수를 만들어서 이를 수동으로 테스트할 수 있습니다. 불순도는 특정 특성(예: 성별)에 대한 분할이 각 그룹 내의 행이 서로 유사하거나 다른 정도를 나타냅니다. 목표는 특성에 대한 분할을 만들어 이 불순도를 줄이는 것이며, 이를 통해 우리의 특성과 목표 간의 관계를 가장 잘 설명하는 특성에 대한 분할을 생성하는 것입니다. 이전의 랜덤 포레스트 예제에서, 개별 결정 트리는 \"성별\" 열을 분할하도록 선택한 이유는 그 열이 (생존 및 비생존) 클래스 간에 가장 작은 혼합을 만들어줘서 결과를 예측하는 데 불확실성(불순도)을 줄이기 때문입니다. 이진 분할은 의사 결정 트리의 첫 번째 구성 요소이며, 여기서 각 특성마다 이진 분할이 이뤄집니다.\n\n그룹 내 행의 유사성을 측정하기 위해 종속 변수의 표준 편차를 취할 것입니다. 표준 편차가 높을수록 행들 사이에 차이가 더 크다는 뜻입니다. 그런 다음 이 값을 행 수로 곱할 것입니다. 왜냐하면 값들이 더 큰 그룹이 더 작은 그룹보다 더 큰 영향을 미치기 때문입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ndef side_score(side, y):\n    tot = side.sum()\n    if tot <= 1:\n        return 0\n    return y[side].std() * tot\n```\n\n이제 좌측과 우측의 점수를 더하여 분할에 대한 점수를 계산할 수 있습니다:\n\n```js\ndef score(col, y, split):\n    lhs = col <= split\n    return (side_score(lhs, y) + side_score(~lhs, y)) / len(y)\n```\n\n0.5로 임계값을 설정하여 성별 열의 불순도 점수를 확인할 수 있습니다. 데이터 내에서 여성 승객은 0으로 표현되고, 남성은 1로 표현됩니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nscore(trn_xs[\"Sex\"], trn_y, 0.5)\n# 0.40787530982063946\n```\n\n다른 요소들에 대해서는 임계값이 명확하지 않습니다. 다른 범주형 또는 상수 변수에 대한 실험을 설정하여 데이터의 불순도에 어떤 영향을 미치는지 확인할 수 있습니다. 각 분할에서 불순도를 줄이고 데이터의 순도를 높이기를 원합니다:\n\n```js\ndef iscore(nm, split):\n    col = trn_xs[nm]\n    return score(col, trn_y, split)\n\nfrom ipywidgets import interact\ninteract(nm=conts, split=15.5)(iscore);\n```\n\n이제 슬라이더를 사용해보세요. 연속 변수에만 적용했지만 범주형 변수에도 테스트할 수 있습니다. 모든 기능에 대해 이 작업을 수행하면 다소 시간이 소요됩니다. 열의 최적 분할점을 찾을 수 있는 함수를 작성해 봅시다. 해당 필드의 모든 가능한 분할점(해당 필드의 고유한 값) 목록을 만들고 score()가 가장 낮은 지점을 찾아야 합니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\ndef min_col(df, nm):\n    col,y = df[nm],df[dep]\n    unq = col.dropna().unique()\n    scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\n    idx = scores.argmin()\n    return unq[idx],scores[idx]\n\nmin_col(trn_df, \"나이\")\n# (6.0, 0.478316717508991)\n```\n\n좋아요. 우리의 훈련 세트의 \"나이\" 열에서 최적 분할이 6인 것을 찾았고, 불순도 점수는 0.478입니다.\n\n모든 열에 대해 이 아이디어를 구현해 봅시다:\n\n```python\ncols = cats + conts\n{o: min_col(trn_df, o) for o in cols}\n# {'성별': (0, 0.40787530982063946),\n# '승선항': (0, 0.47883342573147836),\n# '나이': (6.0, 0.478316717508991),\n# '형제_배우자': (4, 0.4783740258817434),\n# '부모_자녀': (0, 0.4805296527841601),\n# '로그요금': (2.4390808375825834, 0.4620823937736597),\n# '선실등급': (2, 0.46048261885806596)}\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n그래서, Sex`=0이 최적의 분할이라고 하는 것은 우리의 불순도 점수가 가장 낮을 때입니다. 이 결과들이 초기 랜덤 포레스트 예제와 정확히 일치하지는 않지만 (데이터의 작은 하위 집합만 사용하고 앙상블 방법을 사용하지 않기 때문에 이해하기 쉽다), 여전히 올바른 방향으로 나아가고 있다는 것을 보여줍니다. 본질적으로 OneR 분류기의 기본 버전을 재현한 것입니다.\n\n# 의사 결정 트리 탐색\n\n여기서부터 진행하려면 Sex 열을 초기 최적 분할로 선택하는 것이 중요합니다. 다음 단계로 수동으로 이동하여 데이터를 남성/여성으로 분할한 후 각 그룹에 대한 다음 최적 분할이 무엇인지 결정합니다. 이것이 어디에 향하고 있는지 볼 수 있습니다. 우리는 다음 단계로 이동하여 의사 결정 트리의 구성 요소를 모아가고 있습니다. 그러기 위해서, 가능한 분할 목록에서 Sex를 제거해야 합니다. 그런 다음 데이터를 남성 또는 여성으로 분할하고 각 그룹의 최적 분할을 찾아야 합니다 (불순도 점수가 가장 낮은 분할). 시작해 봅시다.\n\nSex 열을 제거하고 데이터를 분할합니다. 이것은 사실상 의사 결정 트리 내에서 첫 번째 이진 분할입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ncols.remove(\"Sex\");\nismale = trn_df.Sex == 1;\nmales, (females = trn_df[ismale]), trn_df[~ismale];\n```\n\n이제 우리는 남성들에 대한 최적의 분할을 찾습니다:\n\n```js\n{o:min_col(males, o) for o in cols}\n# {'Embarked': (0, 0.3875581870410906),\n# 'Age': (6.0, 0.3739828371010595),\n# 'SibSp': (4, 0.3875864227586273),\n# 'Parch': (0, 0.3874704821461959),\n# 'LogFare': (2.803360380906535, 0.3804856231758151),\n# 'Pclass': (1, 0.38155442004360934)}\n```\n\n그리고 여성들에 대한 최적의 분할:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n{cols의 o에 대해 females 최소 열계수(females, o) for o in cols}\n# {'Embarked': (0, 0.4295252982857327),\n# 'Age': (50.0, 0.4225927658431649),\n# 'SibSp': (4, 0.42319212059713535),\n# 'Parch': (3, 0.4193314500446158),\n# 'LogFare': (4.256321678298823, 0.41350598332911376),\n# 'Pclass': (2, 0.3335388911567601)}\n```\n\n남성의 경우, 다음 최적의 이진 분할은 Age`=6이고, 여성의 경우는 Pclass`=2입니다. 여기서 불순도 점수가 가장 낮았습니다.\n\n여러분은 손으로 첫 번째 의사 결정 트리를 만들었습니다. 우리는 지금 생성한 네 개의 하위 그룹마다 추가적인 규칙을 만들어 이 과정을 반복할 수 있습니다. 그러나 우리가 바퀴를 재발명할 필요는 없습니다. 우리 대신에 많은 오픈 소스 라이브러리가 중요한 작업을 대신 해줍니다. 동일한 프로세스를 반복하되, 기존 라이브러리를 사용하고 결과 의사 결정 트리를 출력하여 우리의 발견을 비교해 보겠습니다:\n\n```js\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nimport matplotlib.pyplot as plt\n\nmodel = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y)\nplt.figure(figsize=(20, 10))\nplot_tree(model, feature_names=trn_xs.columns, filled=True, max_depth=3, rounded=True, precision=2)\nplt.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/TIL/assets/img/2024-07-09-UnlockingInsightsRandomForestsforPCAandFeatureImportance_2.png)\n\n와우, 수동 분리 결과와 동일한 결과를 얻었네요. 이 모델의 성능을 측정해 봅시다:\n\n```js\nmean_absolute_error(val_y, model.predict(val_xs))\n# 0.2242152466367713\n```\n\n예상대로, 이 모델은 처음에 생성한 랜덤 포레스트 앙상블 모델보다 성능이 낮습니다. 다이어그램의 각 노드는 특정 규칙 집합과 일치하는 행/샘플이 몇 개인지 및 생존 또는 사망한 승객이 몇 명인지를 보여줍니다. Gini 점수는 이전에 만든 점수 기능과 매우 유사합니다. 다음과 같이 정의됩니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ndef gini(cond):\n    act = df.loc[cond, dep]\n    return 1 - act.mean()**2 - (1-act).mean()**2\n```\n\n이 함수는 조건에 따라 지니 불순도를 계산합니다. 여기서, 먼저 사용자가 선택한 두 행이 각각 \"Survived\" 결과가 같을 확률을 계산합니다. 그룹이 모두 같은 경우, 확률은 1.0입니다. 모두 다른 경우에는 0.0의 결과가 나옵니다.\n\n# 더 큰 의사 결정 트리…\n\n더 큰 의사 결정 트리를 만들어 성능에 어떤 영향을 미치는지 살펴봅시다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nmodel = DecisionTreeClassifier(min_samples_leaf=50).fit(trn_xs, trn_y)\nplt.figure(figsize=(20, 10))\nplot_tree(model, feature_names=trn_xs.columns, filled=True, rounded=True, precision=2)\nplt.show()\n```\n\n![image](/TIL/assets/img/2024-07-09-UnlockingInsightsRandomForestsforPCAandFeatureImportance_3.png)\n\n이제 큰 모델의 성능을 측정해 보겠습니다:\n\n```python\nmean_absolute_error(val_y, model.predict(val_xs))\n# 0.18385650224215247\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n이 모델은 초기 모델보다 우수한 성능을 보입니다. 그러나 데이터셋이 매우 작기 때문에 이 점을 고려해야 할 것 같아요.\n\n# 손수 랜덤 포레스트…\n\n마지막으로, 직접 랜덤 포레스트 분류기를 만들어보겠습니다. 우리는 sklearn의 학습 방법을 사용하여 많은 개별 의사 결정 트리를 만들 것입니다. 그리고 각 개별 의사 결정 트리의 출력의 평균을 취할 것입니다. 여기서 아이디어는 상관 관계가 없는 모델의 예측을 평균 내어 예측 오차를 줄인다는 점입니다. 여기서 중요한 것은 '상관 관계가 없는'입니다. 우리의 각 의사 결정 트리가 데이터의 고유한 하위 집합에서 트레이닝을 수행하도록 보장해야 합니다. 따라서 각 의사 결정 트리의 성능은 개별적으로 평균보다 조금 더 나을 것입니다. 각각은 너무 높게 또는 너무 낮게 예측할 것입니다. 여러 개의 개별, 편향되지 않은 상관 관계가 없는 의사 결정 트리의 예측을 평균 내어 정확하게 참 값을 얻을 수 있습니다. 이것은 상관 관계가 없는 무작위 오류의 평균은 0이기 때문입니다. 상당히 멋지죠. 이 기술은 배깅이라고 알려져 있습니다. 이를 코드로 구현해봅시다.\n\n먼저, 데이터의 새로운 무작위 하위 집합에 대한 의사 결정 트리 생성을 다룹니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\ndef get_tree(prop=0.75):\n    n = len(trn_y)\n    idxs = random.choice(n, int(n*prop))\n    return DecisionTreeClassifier(min_samples_leaf=5).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs])\n```\n\n이제 원하는 만큼의 트리를 생성합니다:\n\n```python\ntrees = [get_tree() for t in range(100)]\n```\n\n이제 모든 트리의 평균 예측값을 얻습니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```python\nall_probs = [t.predict(val_xs) for t in trees]\navg_probs = np.stack(all_probs).mean(0)\n\nmean_absolute_error(val_y, avg_probs)\n# 0.22524663677130047\n```\n\n이 방법론은 이 글 초반에서 설명한 랜덤 포레스트 분류기에서 사용하는 방식과 거의 동일합니다. 유일한 차이는 sklearn에서 각 분할마다 무작위로 열의 부분 집합을 선택한다는 것뿐입니다.\n\n# 결론\n\n요령이다. 데이터셋 내에서 가장 중요한 기능을 이해하는 데 도움이 되는 기본 요소를 다루었습니다. 이 방법론을 적용하여 데이터 과학 프로젝트에서 신속히 진전할 수 있기를 바라겠습니다. 앞서 말했듯이, 이 글에 소개된 각 모델의 성능 기준은 우리의 데이터셋이 매우 작았기 때문에 준중요하게 여겨야 합니다. 그럼에도 불구하고, 이 방법은 놀랍도록 잘 확장되며 설명 가능한 기준을 설정하는 좋은 방법입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n언제든지 궁금한 점이 있거나 기사에서 언급된 내용에 대해 논의를 원하시면 언제든지 말씀해 주세요.\n\n건배하세요!\n\n모든 이미지는 명시되지 않는 한 작성자에게 속합니다.\n","ogImage":{"url":"/assets/img/2024-07-09-UnlockingInsightsRandomForestsforPCAandFeatureImportance_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-UnlockingInsightsRandomForestsforPCAandFeatureImportance_0.png","tag":["Tech"],"readingTime":19},{"title":"스파크 최적화 집중 강좌 최고의 성능을 위한 단계별 가이드","description":"","date":"2024-07-09 20:14","slug":"2024-07-09-IntensiveSparkOptimizationCourse","content":"\n![Intensive Spark Optimization Course](/TIL/assets/img/2024-07-09-IntensiveSparkOptimizationCourse_0.png)\n\n# 로컬에서 플레이그라운드 설정하기\n\n- Docker Desktop을 설치합니다.\n- `docker run -p 8888:8888 jupyter/pyspark-notebook`을 실행합니다.\n- 다음 메시지가 표시되면 브라우저에서 주피터 랩을 열기 위해 URL 중 하나를 붙여넣습니다.\n\n```js\n서버에 액세스하려면 브라우저에서 이 파일을 엽니다:\n    file:///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html\n또는 다음 URL 중 하나를 복사하여 붙여넣습니다:\n    http://3c331b638888:8888/lab?token=a88888b6aa6620fc976588ba58817f3b14ea0674bdc77f72\n    http://127.0.0.1:8888/lab?token=a88888b6aa6620fc976588ba58817f3b14ea0674bdc77f72\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# SparkSession 초기화하기\n\n```js\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"Spark Test\").getOrCreate()\n```\n\n# 데이터프레임\n\n## 1. 데이터프레임 생성하기\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 컬럼 사용\n\n```js\nfrom pyspark.sql.types import StructType, IntegerType, StringType\n\n# 데이터를 튜플의 리스트로 정의\ndata = [(\"James\", 34), (\"Anna\", 20), (\"Lee\", 30)]\n\n# 컬럼 사용\ncolumns = [\"Name\", \"Age\"]\ndf = spark.createDataFrame(data, schema=columns)\n```\n\n- 스키마 사용\n\n```js\nfrom pyspark.sql.types import StructType, IntegerType, StringType\n\n# 데이터를 튜플의 리스트로 정의\ndata = [(\"James\", 34), (\"Anna\", 20), (\"Lee\", 30)]\n\n# 스키마 사용\nschema = StructType([\n    StructField(\"Name\", StringType(), True),\n    StructField(\"Age\", IntegerType(), True)\n])\ndf = spark.createDataFrame(data, schema=schema)\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- RDD 사용\n\n```js\nfrom pyspark.sql.types import StructType, IntegerType, StringType\n\n# 데이터를 튜플의 리스트로 준비\ndata = [(\"James\", 34), (\"Anna\", 20), (\"Lee\", 30)]\n\n# RDD 사용\nrdd = spark.sparkContext.parallelize(data)\nschema = StructType([\n    StructField(\"이름\", StringType(), True),\n    StructField(\"나이\", IntegerType(), True)\n])\ndf = spark.createDataFrame(rdd, schema=schema)\n```\n\n```js\ndf.show()\n\n# 출력\n+-----+---+\n| 이름|나이|\n+-----+---+\n|James| 34|\n| Anna| 20|\n|  Lee| 30|\n+-----+---+\n```\n\n## 2. 데이터프레임 표시\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ndf.printSchema();\nprint(df.schema);\nprint(df.columns);\ndf.describe().show();\n```\n\n```js\n#결과\n\n## df.printSchema()\nroot\n |-- Name: string (nullable = true)\n |-- Age: long (nullable = true)\n\n## print(df.schema)\nStructType([\n  StructField(‘Name’, StringType(), True),\n  StructField(‘Age’, LongType(), True)\n])\n\n## print(df.columns)\n[‘Name’, ‘Age’]\n\n## df.describe().show()\n+-------+----+-----------------+\n|summary|Name|              Age|\n+-------+----+-----------------+\n|  count|   3|                3|\n|   mean|NULL|             28.0|\n| stddev|NULL|7.211102550927978|\n|    min|Anna|               20|\n|    max| Lee|               34|\n+-------+----+-----------------+\n```\n\n## 3. 컬럼 선택\n\n```js\ndf.select(df[0]).show();\ndf.select(df.Name).show();\ndf.select(df[\"Name\"]).show();\ndf.select(\"Name\").show();\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n#Output\n+-----+\n| Name|\n+-----+\n|James|\n| Anna|\n|  Lee|\n+-----+\n```\n\n## 4. 데이터 필터링\n\n```js\n# 데이터 필터링\ndf.filter(df[1] > 25).show()\ndf.filter(df.Age > 25).show()\ndf.filter(df[\"Age\"] > 25).show()\n```\n\n```js\n#Output\n+-----+---+\n| Name|Age|\n+-----+---+\n|James| 34|\n|  Lee| 30|\n+-----+---+\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 5. 파일에 DataFrame 작성하기\n\n```js\n# JSON 파일 작성\ndf.write.json(\"test123.json\")\n\n# Parquet 파일 작성\ndf.write.parquet(\"test123.parquet\")\n```\n\n## 6. 파일을 DataFrame으로 읽기\n\n```js\n# JSON 파일 읽기\ndf_json = spark.read.json(\"test123.json\")\n# Parquet 파일 읽기\ndf_parquet = spark.read.parquet(\"test123.parquet\")\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 7. 새로운 복합 데이터 유형을 포함한 새로운 열 추가\n\n```js\nfrom pyspark.sql.functions import struct\ndf2 = df.withColumn(\"NameAndAge\", struct(df.Name, df.Age))\ndf2.show()\ndf2.printSchema()\n```\n\n```js\n# 출력\n+-----+---+-----------+\n| Name|Age| NameAndAge|\n+-----+---+-----------+\n|James| 34|{James, 34}|\n| Anna| 20| {Anna, 20}|\n|  Lee| 30|  {Lee, 30}|\n+-----+---+-----------+\n\n# 스키마 출력\nroot\n |-- Name: string (nullable = true)\n |-- Age: long (nullable = true)\n |-- NameAndAge: struct (nullable = false)\n |    |-- Name: string (nullable = true)\n |    |-- Age: long (nullable = true)\n```\n\n# 쿼리: 그룹화 및 집계\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 1. count()\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import count\n\n# 스파크 세션 초기화\nspark = SparkSession.builder.appName(\"그루핑 및 집계\").getOrCreate()\n\n# 데이터프레임 생성\ndata = [(\"James\", \"Sales\", 3000),\n        (\"Michael\", \"Sales\", 4600),\n        (\"Robert\", \"Sales\", 4100),\n        (\"Maria\", \"Finance\", 3000),\n        (\"James\", \"Sales\", 3000),\n        (\"Scott\", \"Finance\", 3300),\n        (\"Jen\", \"Finance\", 3900),\n        (\"Jeff\", \"Marketing\", 3000),\n        (\"Kumar\", \"Marketing\", 2000),\n        (\"Saif\", \"Sales\", 4100)]\ncolumns = [\"employee_name\", \"department\", \"salary\"]\ndf = spark.createDataFrame(data, schema=columns)\n\n# 그룹화 및 count 수행\ngrouped_df = df.groupBy(\"department\").count()\ngrouped_df.show()\n```\n\n```python\n# 결과\n\n+----------+-----+\n|department|count|\n+----------+-----+\n|     Sales|    5|\n|   Finance|    3|\n| Marketing|    2|\n+----------+-----+\n```\n\n## 2. max(), min(), avg(), sum()\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 그룹별로 그룹화 및 최댓값 찾기\nmax_df = df.groupBy(\"department\").max(\"salary\").alias(\"max_salary\")\nmax_df.show()\n```\n\n```js\n# 결과\n\n+----------+-----------+\n|department|max(salary)|\n+----------+-----------+\n|     Sales|       4600|\n|   Finance|       3900|\n| Marketing|       3000|\n+----------+-----------+\n```\n\n## 3. agg() + F.max(), F.count() 등…\n\n```js\nfrom pyspark.sql import functions as F\n\n# 여러 가지 집계 동작 수행\nagg_df = df.groupBy(\"department\").agg(\n    F.count(\"salary\").alias(\"count\"),\n    F.max(\"salary\").alias(\"max_salary\"),\n    F.min(\"salary\").alias(\"min_salary\"),\n    F.sum(\"salary\").alias(\"total_salary\"),\n    F.avg(\"salary\").alias(\"average_salary\")\n)\nagg_df.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 결과\n+----------+-----+----------+----------+------------+--------------+\n|department|count|max_salary|min_salary|total_salary|average_salary|\n+----------+-----+----------+----------+------------+--------------+\n|     Sales|    5|      4600|      3000|       18800|        3760.0|\n|   Finance|    3|      3900|      3000|       10200|        3400.0|\n| Marketing|    2|      3000|      2000|        5000|        2500.0|\n+----------+-----+----------+----------+------------+--------------+\n```\n\n## 4. agg() + collect_list() 및 collect_set()\n\n```js\nfrom pyspark.sql.functions import collect_list\n\n# GroupBy 및 리스트 수집 수행\ncollected_list_df = df.groupBy(\"department\").agg(\n  collect_list(\"salary\"),\n  collect_set(\"salary\")\n)\ncollected_list_df.show(truncate=False)\n```\n\n```js\n# 결과\n+----------+------------------------------+-------------------+\n|department|collect_list(salary)          |collect_set(salary)|\n+----------+------------------------------+-------------------+\n|Sales     |[3000, 4600, 4100, 3000, 4100]|[4600, 3000, 4100] |\n|Finance   |[3000, 3300, 3900]            |[3900, 3000, 3300] |\n|Marketing |[3000, 2000]                  |[3000, 2000]       |\n+----------+------------------------------+-------------------+\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 5. agg() + 사용자 정의 집계 함수 (UDAF)\n\n- 때로는 내장 함수만으로 복잡한 집계를 수행하기에 충분하지 않을 수 있습니다. Spark를 사용하면 사용자 정의 집계 함수를 만들 수 있습니다.\n\n```js\n# 초기 데이터\n|부서     |직원 이름        |급여    |\n|----------|-------------|------|\n|Sales    |James        |3000  |\n|Sales    |Michael      |4600  |\n|Sales    |Robert       |4100  |\n|Finance  |Maria        |3000  |\n|Sales    |James        |3000  |\n|Finance  |Scott        |3300  |\n|Finance  |Jen          |3900  |\n|Marketing|Jeff         |3000  |\n|Marketing|Kumar        |2000  |\n|Sales    |Saif         |4100  |\n```\n\n```js\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\nfrom pandas import DataFrame\n\n@pandas_udf(\"double\")\ndef mean_salary(s: pd.Series) -> float:\n return s.mean()\nudaf_df = df.groupBy(\"department\").agg(\n  mean_salary(df[\"salary\"]).alias(\"average_salary\")\n)\nudaf_df.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 결과\n+----------+--------------+\n|department|average_salary|\n+----------+--------------+\n|   Finance|        3400.0|\n| Marketing|        2500.0|\n|     Sales|        3760.0|\n+----------+--------------+\n\n## 6. agg() + 복잡한 조건: when()\n\n- 때로는 조건에 따른 합계나 평균과 같은 복잡한 조건이 집계 중에 필요할 수 있습니다.\n\n# 초기 데이터\n+----------+-------------+------+\n|department|employee_name|salary|\n+----------+-------------+------+\n|     Sales|        James|  3000|\n|     Sales|      Michael|  4600|\n|     Sales|       Robert|  4100|\n|   Finance|        Maria|  3000|\n|     Sales|        James|  3000|\n|   Finance|        Scott|  3300|\n|   Finance|          Jen|  3900|\n| Marketing|         Jeff|  3000|\n| Marketing|        Kumar|  2000|\n|     Sales|         Saif|  4100|\n+----------+-------------+------+\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nfrom pyspark.sql.functions import when\n\n# 조건부 집계\nconditional_agg_df = df.groupBy(\"department\").agg(\n    sum(when(df[\"salary\"] > 3000, df[\"salary\"])).alias(\"sum_high_salaries\")\n)\nconditional_agg_df.show()\n\n# 결과\n\n+----------+-----------------+\n|department|sum_high_salaries|\n+----------+-----------------+\n|     Sales|            12800|\n|   Finance|             7200|\n| Marketing|             NULL|\n+----------+-----------------+\n\n## 6. agg() 이후 GroupBy에서 RDD Map 함수 사용하기\n\n- 경우에 따라 매핑 함수를 GroupBy와 결합하여 집단화된 데이터에 대한 직접적인 집계가 아닌 작업을 수행할 수 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```\n\n# 데이터\n\n| department | employee_name | salary |\n| ---------- | ------------- | ------ |\n| Sales      | James         | 3000   |\n| Sales      | Michael       | 4600   |\n| Sales      | Robert        | 4100   |\n| Finance    | Maria         | 3000   |\n| Sales      | James         | 3000   |\n| Finance    | Scott         | 3300   |\n| Finance    | Jen           | 3900   |\n| Marketing  | Jeff          | 3000   |\n| Marketing  | Kumar         | 2000   |\n| Sales      | Saif          | 4100   |\n\n# GroupBy 후 map 작업 적용\n\n```python\nresult_rdd = df.groupBy(\"department\").agg(\n  collect_list(\"salary\")\n).rdd.map(\n  lambda x: (x[0], max(x[1]))\n)\n\nresult_df = spark.createDataFrame(result_rdd, [\"department\", \"max_salary\"])\nresult_df.show()\n```\n\n# 결과\n\n| department | max_salary |\n| ---------- | ---------- |\n| Sales      | 4600       |\n| Finance    | 3900       |\n| Marketing  | 3000       |\n\n# 조회: 다른 것\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 1. rollup()과 cube()\n\n- Rollup()은 다차원 집계를 생성하고 Excel의 소계와 유사한 계층적 요약을 제공합니다.\n\n```js\n# 초기 데이터\n|department|employee_name|salary|\n|----------|-------------|------|\n| Sales    | James       | 3000 |\n| Sales    | Michael     | 4600 |\n| Sales    | Robert      | 4100 |\n| Finance  | Maria       | 3000 |\n| Sales    | James       | 3000 |\n| Finance  | Scott       | 3300 |\n| Finance  | Jen         | 3900 |\n| Marketing| Jeff        | 3000 |\n| Marketing| Kumar       | 2000 |\n| Sales    | Saif        | 4100 |\n```\n\n```js\nfrom pyspark.sql.functions import sum\n\n# Rollup 예제\nrollup_df = df.rollup(\"department\", \"employee_name\").sum(\"salary\")\nrollup_df.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 결과\n+----------+-------------+-----------+\n|department|employee_name|sum(salary)|\n+----------+-------------+-----------+\n|     Sales|        James|       6000|\n|      NULL|         NULL|      34000|\n|     Sales|         NULL|      18800|\n|     Sales|      Michael|       4600|\n|     Sales|       Robert|       4100|\n|   Finance|         NULL|      10200|\n|   Finance|        Maria|       3000|\n|   Finance|        Scott|       3300|\n|   Finance|          Jen|       3900|\n| Marketing|         NULL|       5000|\n| Marketing|         Jeff|       3000|\n| Marketing|        Kumar|       2000|\n|     Sales|         Saif|       4100|\n+----------+-------------+-----------+\n```\n\n- Cube(): Cube는 다차원 집계를 생성하고 지정된 그룹화 열의 다중 조합을 통해 통찰을 제공합니다.\n\n```js\n# 초기 데이터\n+----------+-------------+------+\n|department|employee_name|salary|\n+----------+-------------+------+\n|     Sales|        James|  3000|\n|     Sales|      Michael|  4600|\n|     Sales|       Robert|  4100|\n|   Finance|        Maria|  3000|\n|     Sales|        James|  3000|\n|   Finance|        Scott|  3300|\n|   Finance|          Jen|  3900|\n| Marketing|         Jeff|  3000|\n| Marketing|        Kumar|  2000|\n|     Sales|         Saif|  4100|\n+----------+-------------+------+\n```\n\n```js\nfrom pyspark.sql.functions import sum\n\n# Cube 예시\ncube_df = df.cube(\"department\", \"employee_name\").sum(\"salary\")\ncube_df.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 결과\n\n+----------+-------------+-----------+\n|부서      |직원명         |급여합계   |\n+----------+-------------+-----------+\n|      NULL|        James|       6000|\n|     판매 |        James|       6000|\n|      NULL|         NULL|      34000|\n|     판매 |         NULL|      18800|\n|      NULL|      Michael|       4600|\n|     판매 |      Michael|       4600|\n|     판매 |       Robert|       4100|\n|      NULL|       Robert|       4100|\n|   재무   |         NULL|      10200|\n|   재무   |        Maria|       3000|\n|      NULL|        Maria|       3000|\n|      NULL|        Scott|       3300|\n|   재무   |        Scott|       3300|\n|      NULL|          Jen|       3900|\n|   재무   |          Jen|       3900|\n| 마케팅  |         NULL|       5000|\n| 마케팅  |         Jeff|       3000|\n|      NULL|         Jeff|       3000|\n| 마케팅  |        Kumar|       2000|\n|      NULL|        Kumar|       2000|\n+----------+-------------+-----------+\n상위 20개 행만 표시\n```\n\n## 2. groupBy() + pivot()\n\n- Pivoting을 사용하면 행을 열로 변환하여 피벗 테이블과 유사한 방식으로 데이터를 요약할 수 있습니다. 종종 두 열 간의 관계를 이해하는 데 사용됩니다.\n\n```js\n# 초기 데이터\n+----------+-------------+------+\n|부서     |직원명        |급여   |\n+----------+-------------+------+\n|     판매 |        James|  3000|\n|     판매 |      Michael|  4600|\n|     판매 |       Robert|  4100|\n|   재무  |        Maria|  3000|\n|     판매 |        James|  3000|\n|   재무  |        Scott|  3300|\n|   재무  |          Jen|  3900|\n| 마케팅  |         Jeff|  3000|\n| 마케팅  |        Kumar|  2000|\n|     판매 |         Saif|  4100|\n+----------+-------------+------+\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# Pivot 예시\npivot_df = df.groupBy(\"department\").pivot(\"employee_name\").sum(\"salary\")\npivot_df.show()\n```\n\n```js\n# 결과\n+----------+-----+----+----+-----+-----+-------+------+----+-----+\n|department|James|Jeff| Jen|Kumar|Maria|Michael|Robert|Saif|Scott|\n+----------+-----+----+----+-----+-----+-------+------+----+-----+\n|     Sales| 6000|NULL|NULL| NULL| NULL|   4600|  4100|4100| NULL|\n|   Finance| NULL|NULL|3900| NULL| 3000|   NULL|  NULL|NULL| 3300|\n| Marketing| NULL|3000|NULL| 2000| NULL|   NULL|  NULL|NULL| NULL|\n+----------+-----+----+----+-----+-----+-------+------+----+-----+\n```\n\n## 3. 윈도우 함수: partitionBy() + row_number()/rank().over(w)\n\n- 윈도우 함수는 현재 행과 관련된 \"윈도우\"에 대해 계산을 수행할 수 있어 전통적인 group-by 작업보다 더 유연성을 제공합니다. 이는 러닝 토탈, 이동 평균 또는 이전 및 다음 행에 액세스하는 데 특히 유용합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 초기 데이터\n| 부서      | 직원 이름       | 급여  |\n|----------|---------------|------|\n| Sales    | James         | 3000 |\n| Sales    | Michael       | 4600 |\n| Sales    | Robert        | 4100 |\n| Finance  | Maria         | 3000 |\n| Sales    | James         | 3000 |\n| Finance  | Scott         | 3300 |\n| Finance  | Jen           | 3900 |\n| Marketing| Jeff          | 3000 |\n| Marketing| Kumar         | 2000 |\n| Sales    | Saif          | 4100 |\n\n```\n\n```js\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import col, row_number\n\nwindowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").asc())\ndf_with_row_number = df.withColumn(\"row_number\", row_number().over(windowSpec))\ndf_with_row_number.show()\n```\n\n```js\n# 결과\n| 직원 이름      | 부서        | 급여  | row_number |\n|---------------|------------|------|------------|\n| Maria         | Finance    | 3000 |      1     |\n| Scott         | Finance    | 3300 |      2     |\n| Jen           | Finance    | 3900 |      3     |\n| Kumar         | Marketing  | 2000 |      1     |\n| Jeff          | Marketing  | 3000 |      2     |\n| James         | Sales      | 3000 |      1     |\n| James         | Sales      | 3000 |      2     |\n| Robert        | Sales      | 4100 |      3     |\n| Saif          | Sales      | 4100 |      4     |\n| Michael       | Sales      | 4600 |      5     |\n\n```\n\n- Rank() 함수를 위해\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import rank, col\n\nwindowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\ndf_with_rank = df.withColumn(\"rank\", rank().over(windowSpec))\ndf_with_rank.show()\n```\n\n```js\n# 출력\n+-------------+----------+------+----------+\n|employee_name|department|salary|      rank|\n+-------------+----------+------+----------+\n|          Jen|   Finance|  3900|         1|\n|        Scott|   Finance|  3300|         2|\n|        Maria|   Finance|  3000|         3|\n|         Jeff| Marketing|  3000|         1|\n|        Kumar| Marketing|  2000|         2|\n|      Michael|     Sales|  4600|         1|\n|       Robert|     Sales|  4100|         2|\n|         Saif|     Sales|  4100|         2|\n|        James|     Sales|  3000|         3|\n|        James|     Sales|  3000|         3|\n+-------------+----------+------+----------+\n```\n\n# 최적화 I: 무게 감소\n\n## 0. 불필요한 원시 데이터 제거\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 1. DataFrame이 여러 번 액세스 될 때 캐시합니다.\n\n```js\ndf.cache();\ndf.count();\n```\n\n```js\n#출력\n3\n```\n\n## 2. 적절한 파일 형식 사용하기\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 압축 파일은 파일의 입출력 및 메모리를 절약할 수 있어요.\n- 압축 해제된 파일은 CPU를 절약할 수 있어요.\n\n```js\ndf.write.parquet(\"output.parquet\");\n```\n\n## 3. 스키마 수동 지정하기\n\n```js\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\nschema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", IntegerType(), True)\n])\ndf = spark.read.schema(schema).csv(\"path/to/file.csv\")\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 4. 조기에 필요한 열 선택하기\n\n데이터 처리 파이프라인에서 메모리 사용량을 줄이기 위해 필요한 열만 미리 선택하세요.\n\n```js\ndf.select(\"dept_name\", \"name\").filter(\"dept_id >= 102\").show();\ndf.select(\"dept_name\", \"name\")\n  .filter(df.dept_id >= 102)\n  .show();\n```\n\n```js\n#출력\n+---------+----+\n|dept_name|name|\n+---------+----+\n|Marketing|Jane|\n|  Finance| Joe|\n+---------+----+\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 5. 필터를 조인이나 집계 전에 빠르게 적용하세요.\n\n```js\ndf.filter(\"age > 25\").join(df_other, \"id\").show();\n```\n\n## 6. 큰 데이터셋 수집 방지를 위해 limit() 사용하기\n\n- 큰 데이터셋에 collect()를 사용하지 않도록 주의하여 메모리 부족 오류를 방지하세요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ndf.filter(\"age > 30\").limit(100).collect();\n```\n\n## 7. Using spark.sql(): Catalyst optimizer for Complex Queries\n\n- Leverage Spark SQL for complex queries, which might be more readable and can benefit from the Catalyst optimizer.\n\n```js\ndf.createOrReplaceTempView(\"table\");\nspark.sql(\"SELECT id, sum(value) FROM table GROUP BY id\").show();\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 8. RDD 사용: reduceByKey를 사용한 집계\n\n- 집계 작업을 수행할 때 reduceByKey를 사용하는 것이 groupBy보다 더 효율적일 수 있습니다.\n\n```python\nrdd = df.rdd.map(lambda x: (x[0], x[1]))\nreduced = rdd.reduceByKey(lambda a, b: a + b)\nreduced.toDF([\"key\", \"value\"]).show()\n```\n\n# 최적화 II: 파티션 없이 병렬화 없음\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 1. 테이블 분할: partitionBy()\n\n- 데이터프레임을 디스크에 저장할 때 빠른 후속 읽기를 위해 분할을 사용하세요.\n\n```js\ndf.write.partitionBy(\"year\", \"month\").parquet(\"path/to/output\");\n```\n\n## 2. 스튜 관리를 위한 Salting 키\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 조인 연산 중 데이터 스큐가 발생하는 경우, 하나 이상의 키 값이 다른 것들보다 훨씬 더 많은 데이터를 가지고 있을 때 발생합니다.\n- 예를 들어 \"customer_id\"를 기준으로 조인을 수행하고, 대부분의 거래가 소수의 고객에 속해 있다면 이러한 소수의 키는 다른 키들에 비해 훨씬 많은 양의 데이터를 가지고 있을 것입니다. 이로 인해 일부 작업(큰 키를 처리하는 작업)이 훨씬 더 오랜 시간이 걸리고 병목 현상이 발생할 수 있습니다.\n- 이 문제를 해결하는 방법은 skewed 데이터를 관리하기 위해 키에 임의의 접두사를 추가하는 것입니다.\n\n```js\nfrom pyspark.sql.functions import monotonically_increasing_id, expr\ndf.withColumn(\"salted_key\",\n    expr(\"concat(name, '_', (monotonically_increasing_id() % 10))\")\n).groupBy(\"salted_key\").count().select(sum(\"count\")).show()\n```\n\n```js\n# 결과\n+----------+\n|sum(count)|\n+----------+\n|         3|\n+----------+\n```\n\n- 데이터 로딩의 균형을 어떻게 맞출까요?\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nfrom pyspark.sql.functions import monotonically_increasing_id, expr\ndf.withColumn(\"salted_key\",\n    expr(\"concat(name, '_', (monotonically_increasing_id() % 10))\")\n).groupBy(\"salted_key\").count().show()\n```\n\n```js\n# 결과\n+----------+-----+\n|salted_key|count|\n+----------+-----+\n|   James_6|    1|\n|   James_4|    1|\n|   James_0|    1|\n+----------+-----+\n```\n\n# 최적화 III: Shuffling을 최소화하는 전략\n\n```js\n### Shuffling 최소화 전략\n- **Broadcast 변수 사용**\n  - 데이터셋이 작은 경우 Shuffling을 피하기 위해 모든 노드에 브로드캐스트합니다.\n- **파티션 튜닝**\n  - 작업 및 데이터 규모에 맞게 파티션 수를 조정합니다.\n- **변환 최적화**\n  - Shuffling을 필요로 하는 넓은 변환을 최소화하기 위해 작업을 계획합니다.\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 섞기\n\n- 섞기는 데이터가 서로 다른 파티션에 재분배되는 과정입니다.\n- 이는 데이터를 실행자 간이나 심지어 기계 간에 이동하는 것을 포함합니다.\n- 네트워크 및 디스크 I/O 측면에서 가장 비용이 많이 드는 작업 중 하나입니다.\n\n## 섞기의 목적\n\n- 데이터 재분배: 조인, 그룹화, 집계 및 재분할과 같은 넓은 변환을 용이하게 합니다.\n- 부하 분산: 클러스터 전체에 데이터와 작업 부담을 균등하게 분배합니다.\n- 동시성: 병렬 처리를 강화하고 리소스 활용을 최적화합니다.\n- 데이터 지역성 최적화: 데이터가 처리될 위치에 가까이 이동하도록 합니다. 네트워크 트래픽을 줄입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 셔플링의 고민거리\n\n- 자원 소모가 많음: 상당한 네트워크 대역폭과 디스크 I/O를 사용합니다.\n- 지연 시간 증가: 특히 대량의 데이터셋인 경우 처리 시간이 상당히 증가합니다.\n- 병목 현상 발생 가능성: 적절히 관리되지 않으면 전체 시스템 성능을 느리게 만들 수 있습니다.\n\n## 1. 작은 DataFrame과 큰 DataFrame을 조인할 때 데이터 셔플링을 최소화하기 위해 브로드캐스트 조인을 사용합니다.\n\n```js\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import broadcast\n\n# 스파크 세션 초기화\nspark = SparkSession.builder.appName(\"브로드캐스트 조인 예제\").getOrCreate()\n# 직원용 큰 DataFrame 생성\ndata_employees = [(1, \"John\", 101),\n                  (2, \"Jane\", 102),\n                  (3, \"Joe\", 103),\n                  (4, \"Jill\", 101),\n                  # 더 많은 레코드가 있다고 가정\n                  ]\ncolumns_employees = [\"emp_id\", \"name\", \"dept_id\"]\ndf_employees = spark.createDataFrame(data_employees, columns_employees)\n# 부서용 작은 DataFrame 생성\ndata_departments = [(101, \"인사\"),\n                    (102, \"마케팅\"),\n                    (103, \"금융\"),\n                    (104, \"IT\"),\n                    (105, \"지원\")\n                    ]\ncolumns_departments = [\"dept_id\", \"dept_name\"]\ndf_departments = spark.createDataFrame(data_departments, columns_departments)\n# 브로드캐스트 조인 수행\ndf_joined = df_employees.join(broadcast(df_departments), \"dept_id\")\ndf_joined.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n#출력\n+-------+------+----+---------+\n|dept_id|emp_id|  이름|dept_name|\n+-------+------+-----+--------+\n|    101|     1| 존|       인사|\n|    102|     2| 제인|     마케팅|\n|    103|     3| 조|     금융|\n|    101|     4| 질|       인사|\n+-------+------+-----+--------+\n```\n\n- 직원 — 직원 세부 정보를 담은 작은 데이터셋입니다.\n- 부서 — 부서 세부 정보를 담은 큰 데이터셋입니다.\n\n두 데이터셋을 부서 ID를 기준으로 조인하되, 부서 데이터셋을 크게 섞지 않도록 하는 것이 목표입니다.\n\n## 2. 파티션 조정: 병렬성 증가를 위한 다시 분할\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 데이터 프레임의 파티션을 재분할하여 병렬성을 높이거나 셔플 비용을 줄일 수 있습니다.\n- 그러나 여전히 전체 셔플을 유발할 수 있습니다.\n\n```js\n# 병렬성을 높이기 위한 재분할 예제\ndf = spark.createDataFrame([\n  (1, 'foo'), (2, 'bar'), (3, 'baz'), (4, 'qux')\n], [\"id\", \"value\"])\ndf_repartitioned = df.repartition(10)  # 파티션 수 증가\n```\n\n## 3. 파티션 튜닝: 파티션 감소를 위한 Coalesce\n\n- 전체 셔플 피하기: coalesce는 대규모 데이터 세트를 필터링한 후 파티션 수를 줄이고 싶을 때 셔플 비용을 피해야 할 때 최적입니다.\n- 전형적인 사용 사례: 많은 파티션이 부분적으로 채워지거나 비어있는 상태로 남을 수 있는 대규모 DataFrame을 필터링한 후 사용됩니다. coalesce는 네트워크 오버헤드를 줄이고 비용 효율적으로 리소스를 관리하는 데 도움이 됩니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 대규모 셔플링 없이 파티션 수를 줄이기 위한 코어스 예시\ndf_filtered = df.filter(\"id > 1\")\ndf_coalesced = df_filtered.coalesce(2)  # 파티션 수 줄이기\n\n```\n\n## 4. 변환 최적화를 통해 데이터 셔플링 최소화하기\n\n- 최적화된 변환을 통해 Apache Spark에서 셔플링을 최소화하는 것은 Spark 애플리케이션의 성능을 향상시키는 중요한 측면입니다.\n- 변환 최적화는 데이터 처리 작업을 구조화하여 클러스터 전체에서 불필요한 데이터 이동을 줄이는 것을 포함하며, 이는 리소스를 많이 사용하고 실행 속도를 늦출 수 있습니다.\n- 이를 달성하는 방법을 보여주는 몇 가지 전략과 코드 예제는 다음과 같습니다:\n\n## 4–1. 일찍 필터링하기\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 데이터 처리 파이프라인에서 조인 또는 집계와 같은 후속 작업에서 나중에 섞어야 하는 데이터 양을 줄이기 위해 필터를 가능한 한 빨리 적용하세요.\n\n```python\nfrom pyspark.sql import SparkSession\n\n# 스파크 세션 초기화\nspark = SparkSession.builder.appName(\"Shuffling 최소화\").getOrCreate()\n\n# DataFrame 생성\ndata = [(\"John\", \"금융\", 3000), (\"Jane\", \"마케팅\", 4000), (\"Joe\", \"마케팅\", 2800), (\"Jill\", \"금융\", 3900)]\ncolumns = [\"이름\", \"부서\", \"연봉\"]\ndf = spark.createDataFrame(data, schema=columns)\n\n# 넓은 변환 전에 미리 필터링\nfiltered_df = df.filter(df[\"연봉\"] > 3000)\n\n# 이제 집계 수행\naggregated_df = filtered_df.groupBy(\"부서\").avg(\"연봉\")\naggregated_df.show()\n```\n\n```python\n# 출력\n+----------+-----------+\n|부서      |avg(연봉)  |\n+----------+-----------+\n| 마케팅  |     4000.0|\n| 금융    |     3900.0|\n+----------+-----------+\n```\n\n## 4-2. 가능한 경우 RDD/넓은 변환 사용하기\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 좁은 변환, 예를 들어 `map` 및 `filter`와 같은 작업은 개별 파티션에서 작동하며 데이터 셔플을 필요로하지 않습니다. 가능한 경우 이러한 작업을 넓은 변환 대신 사용하십시오.\n\n```js\n# 셔플을 발생시키지 않고 새로운 열을 만들기 위해 map을 사용합니다\nrdd = df.rdd.map(lambda x: (x.Name, x.Salary * 1.1))\nupdated_salaries_df = spark.createDataFrame(\n  rdd, schema=[\"Name\", \"UpdatedSalary\"]\n)\nupdated_salaries_df.show()\n```\n\n```js\n# 결과\n+----+------------------+\n|Name|     UpdatedSalary|\n+----+------------------+\n|John|3300.0000000000005|\n|Jane|            4400.0|\n| Joe|3080.0000000000005|\n|Jill|            4290.0|\n+----+------------------+\n```\n\n## 4-3. Boardcasting join으로 불필요한 셔플을 피하세요\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 조인 작업시, 한 데이터셋이 다른 데이터셋보다 현저히 작을 때 브로드캐스트 조인을 사용하여 큰 데이터셋을 셔플링하지 않도록 합니다.\n\n```js\nfrom pyspark.sql.functions import broadcast\n# df_small이 df_large보다 훨씬 작다고 가정합니다\ndf_small = spark.createDataFrame(\n  [(1, \"HR\"), (2, \"마케팅\")], [\"id\", \"부서\"]\n)\ndf_large = spark.createDataFrame(\n  [(1, \"존\"), (2, \"제인\"), (1, \"조\"), (2, \"질\")],\n  [\"부서ID\", \"이름\"]\n)\n# 조인 최적화를 위해 작은 DataFrame을 브로드캐스트합니다\noptimized_join_df = df_large.join(broadcast(df_small), df_large.부서ID == df_small.id)\noptimized_join_df.show()\n```\n\n```js\n# 결과\n\n+------+----+---+------+\n|부서ID|이름| id|  부서|\n+------+----+---+------+\n|     1| 존|  1|   HR|\n|     2|제인|  2|마케팅|\n|     1| 조|  1|   HR|\n|     2| 질|  2|마케팅|\n+------+----+---+------+\n```\n\n## 4-4. 전략적으로 파티션 나누기\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 만약 넓은 변환을 사용해야 한다면, 나중에 조인 또는 집계할 키를 기반으로 데이터를 파티션으로 나누세요. 이 전략을 사용하면 동일한 키를 가진 행을 동일한 파티션에 함께 두어 셔플링을 줄일 수 있습니다.\n\n```js\n# 셔플링 최소화를 위해 집계 전에 파티션 재분배\nrepartitioned_df = df.repartition(\"Department\")\naggregated_df = repartitioned_df.groupBy(\"Department\").avg(\"Salary\")\naggregated_df.show()\n```\n\n```js\n# 결과\n+----------+-----------+\n|Department|avg(Salary)|\n+----------+-----------+\n|   Finance|     3450.0|\n| Marketing|     3400.0|\n+----------+-----------+\n```\n\n# 성능 모니터링 및 세부 조정\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 1. 메모리 관리\n\n```js\nspark.conf.set(“spark.executor.memory”, “4g”)\nspark.conf.set(“spark.driver.memory”, “2g”)\n```\n\n## 2. 작업 및 스테이지 모니터링\n\n- Spark UI를 사용하여 응용 프로그램 내의 작업 및 스테이지의 성능을 모니터링합니다.\n- Spark UI에 액세스하려면 다음으로 이동하십시오: http://[your-spark-driver-host]:4040\n- Executor 메트릭 분석: 각 executor의 메트릭을 모니터링하여 메모리 사용, 디스크 스피릴 및 가비지 수집에 대한 통찰을 얻을 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 자세한 실행자 지표를 수집하도록 Spark를 구성합니다\nspark.conf.set(\"spark.executor.metrics.pollingInterval\", \"5000\")\n```\n\n## 3. SQL 성능 튜닝\n\n- SQL 실행 계획을 이해하고 최적화하기 위해 `EXPLAIN` 계획을 활용하세요.\n\n```js\ndf.explain(“formatted”)\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n== 물리적인 계획 ==\n* 기존 RDD 스캔 (1)\n\n\n(1) 기존 RDD 스캔 [코드 생성 ID : 1]\n출력 [3]: [이름 #4628, 부서 #4629, 급여 #4630L]\n인수: [이름 #4628, 부서 #4629, 급여 #4630L],\n applySchemaToPythonRDD에 있는 MapPartitionsRDD[693]에서\n                       at <알 수 없음>:0, ExistingRDD, UnknownPartitioning(0)\n```\n\n## 4. 동적 할당\n\n- 워크로드에 따라 스파크가 실행자 수를 동적으로 조정할 수 있도록 동적 할당을 활성화합니다.\n\n```js\nspark.conf.set(\"spark.dynamicAllocation.enabled\", \"true\");\nspark.conf.set(\"spark.dynamicAllocation.minExecutors\", \"1\");\nspark.conf.set(\"spark.dynamicAllocation.maxExecutors\", \"20\");\nspark.conf.set(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\");\nspark.conf.set(\"spark.shuffle.service.enabled\", \"true\");\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 5. 데이터 지역성\n\n- 저장 및 처리 장치 간에 데이터가 이동해야 하는 거리를 최소화하여 데이터 지역성을 최적화합니다.\n\n```js\nspark.conf.set(\"spark.locality.wait\", \"300ms\");\n```\n\n## 6. Garbage Collection Tuning\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n- 가비지 컬렉터 설정을 조정하여 메모리 관리를 최적화하고 일시 중지 시간을 줄일 수 있습니다.\n\n# 더 나은 지연 시간을 위해 G1GC 사용\nspark.conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\")\n# 짧은 일시 중지를 위해 명시적인 GC 설정 구성\nspark.conf.set(\"spark.executor.extraJavaOptions\", \"-XX:MaxGCPauseMillis=100\")\n\n## 7. 데이터 직렬화 세부 조정\n\n- 데이터 직렬화는 분산 애플리케이션의 성능에 중요한 역할을 합니다. Spark는 두 가지 직렬화 도구를 지원합니다:\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 더 나은 성능과 효율성을 위해 Kryo 직렬화 프로그램 사용\nspark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\nspark.conf.set(\"spark.kryo.registrationRequired\", \"true\")\n\n# Kryo와 사용자 정의 클래스 등록\nclass MyClass:\n    def __init__(self, name, id):\n        self.name = name\n        self.id = id\nspark.sparkContext.getConf().registerKryoClasses([MyClass])\n```\n\n## 8. 네트워크 구성 최적화\n\n- 네트워크 설정은 특히 대규모 배포에서 성능에 중대한 영향을 미칠 수 있습니다:\n\n```js\n# 네트워크 타임아웃 설정을 조정하여 대규모 클러스터에서 불필요한 작업 실패를 피하십시오\nspark.conf.set(\"spark.network.timeout\", \"800s\")\nspark.conf.set(\"spark.core.connection.ack.wait.timeout\", \"600s\")\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 9. 고급 Spark SQL 튜닝\n\n- Catalyst 옵티마이저 및 Tungsten 실행 엔진을 활용하면 Spark SQL의 성능을 향상시킬 수 있습니다:\n\n```js\n# 직렬 처리를 위한 전체 단계 코드 생성 활성화\nspark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n\n# 조인 최적화에 유용한 테이블 브로드캐스트를 위한 최대 바이트 수 증가\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10 MB\n```\n\n## 10. 데이터 파티셔닝 최적화\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n데이터 분배를 세밀하게 조정하여 쿼리 성능을 향상시키고 셔플 오버헤드를 줄일 수 있어요:\n\n```js\n# 데이터 크기 및 작업을 기준으로 수동으로 셔플 파티션 수 설정\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n# 클러스터 크기 및 데이터에 맞게 조정하세요\n```\n\n## 11. 적응형 쿼리 실행 활성화\n\n- 적응형 쿼리 실행 (AQE)는 실행 중에 쿼리 계획을 조정함으로써 Spark SQL 쿼리를 더 빠르고 데이터 스쿠 및 기타 이슈에 더 강건하게 만드는 기능이에요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 쿼리 실행을 적응적으로 조정하는 AQE를 활성화합니다. 이는 구성을 간소화하고 성능을 향상시킬 수 있습니다.\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n```\n\n- AQE는 실제 데이터에 적응해 셔플 파티셔닝을 조정하고, 불균형 조인을 처리하며, 정렬을 최적화할 수 있습니다.\n\n## 12. 메모리 관리 지정\n\n- 적절한 메모리 관리는 메모리 집약적인 작업에서 특히 효과적인 성능 개선을 위해 스파이지를 방지할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# RDD 저장소에 예약된 메모리 분수를 구성합니다.\n\nspark.conf.set(“spark.memory.fraction”, “0.6”)\nspark.conf.set(“spark.memory.storageFraction”, “0.5”)\n\n이러한 설정은 실행 메모리와 저장소 메모리 사이의 균형을 맞추어 셔플 및 캐싱 중 디스크 스파일을 줄이는 데 도움이 됩니다.\n\n# 읽어 주셔서 감사합니다\n\n이 글이 마음에 드시면:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 👏 여러 번 박수로 지지를 보여주세요!\n- 이 안내서를 친구들과 공유해도 좋아요.\n- 여러분의 피드백은 소중합니다. 앞으로의 글에 영감을 주고 안내해 줍니다.\n- 또는 메시지를 남겨주세요: https://www.linkedin.com/in/kevinchwong\n","ogImage":{"url":"/assets/img/2024-07-09-IntensiveSparkOptimizationCourse_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-IntensiveSparkOptimizationCourse_0.png","tag":["Tech"],"readingTime":43},{"title":"Python으로 XGBoost를 사용한 모노토닉 시계열 예측 실습","description":"","date":"2024-07-09 20:12","slug":"2024-07-09-HandsOnMonotonicTimeSeriesForecastingwithXGBoostusingPython","content":"\n![이미지](/TIL/assets/img/2024-07-09-HandsOnMonotonicTimeSeriesForecastingwithXGBoostusingPython_0.png)\n\n몇 달 전에 리서치 프로젝트를 진행하면서 시계열을 다루는 문제를 해결해야 했어요.\n\n이 문제는 상당히 간단했어요:\n\nMachine Learning 애호가들에게는 \"Hello World\"를 작성하는 것과 같은 느낌이죠. 이 문제는 \"forecasting\"이라는 이름으로 커뮤니티에서 매우 잘 알려진 문제입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n기계 학습 커뮤니티는 시계열 데이터의 다음 값을 예측하는 데 사용할 수 있는 많은 기술을 개발했습니다. 일부 전통적인 방법에는 ARIMA/SARIMA 또는 푸리에 변환 분석과 같은 알고리즘이 포함되어 있으며, 더 복잡한 알고리즘에는 컨볼루션/순환 신경망 또는 슈퍼 유명한 \"Transformer\" (ChatGPT의 T는 transformers를 나타냅니다)이 있습니다.\n\n예측 문제는 매우 잘 알려진 문제이지만, 제약 조건이 있는 예측 문제에 대해 다루는 것은 덜 흔한 것일 수 있습니다.\n무엇을 의미하는지 설명해 드릴게요.\n\n일련의 매개변수 X와 시간 단계 t가 있는 시계열 데이터가 있다고 가정해 봅시다.\n표준 시간 예측 문제는 다음과 같습니다:\n\n![image](/TIL/assets/img/2024-07-09-HandsOnMonotonicTimeSeriesForecastingwithXGBoostusingPython_1.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n우리가 직면한 문제는 다음과 같습니다:\n\n![이미지](/TIL/assets/img/2024-07-09-HandsOnMonotonicTimeSeriesForecastingwithXGBoostusingPython_2.png)\n\n따라서 입력 매개변수가 d 차원이라고 가정할 때, 저는 차원 1을 위한 함수가 단조적이 되기를 원합니다. 그렇다면 어떻게 처리해야 할까요? 어떻게 \"단조적\" 시계열을 예측할 수 있을까요? 이 문제에 대한 설명은 XGBoost를 사용할 것입니다.\n\n이 블로그 포스트의 구조는 다음과 같습니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- XGBoost에 대해: XGBoost가 무엇에 대해, 기본 아이디어가 무엇인지, 장단점은 무엇인지 몇 줄로 설명하겠습니다.\n- XGBoost 예제: XGBoost 코드를 설명하겠습니다. Python 설명부터 장난감 예제까지를 포함하여요.\n- XGBoost의 명확성을 갖춘 예제: XGBoost를 실제 예제로 테스트하겠습니다.\n- 결론: 이 블로그 포스트에서 언급된 내용에 대한 요약을 제시하겠습니다.\n\n# 1. XGBoost에 대해\n\n## 1.1 XGBoost의 아이디어\n\nXGBoost의 XG는 extreme gradient(부스팅)을 의미합니다.\n\"gradient boosting\" 알고리즘은 \"예측자 체인\"을 사용하려고 합니다.\n입력 행렬 X 및 해당 출력 y가 주어지면, 아이디어는 여러 예측자가 있습니다. 첫 번째 예측자는 입력 X로부터 직접 해당 출력 y를 찾으려고 합니다. 이야기의 끝. 아니요, 농담이에요 🤣\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n첫 번째 예측기는 “약한 예측기”라고 불리는 것을 목표로 합니다. 이는 예측된 y1과 실제 출력 y 사이에 무시할 수 없는 차이가 있는 것을 의미합니다. 두 번째 예측기는 첫 번째 예측의 오류를 보정하는 것을 목표로 하므로 X에서 y로 이동하는 것이 아니라 y2 = y-y1로 이동하는 것으로 훈련됩니다. 이 작업은 예측기의 수인 N번 반복되며, 아래 이미지에 나타난 바와 같습니다:\n\n![image](/TIL/assets/img/2024-07-09-HandsOnMonotonicTimeSeriesForecastingwithXGBoostusingPython_3.png)\n\n각 예측기는 의사 결정 트리(decision tree)입니다. 의사 결정 트리에 대한 설명을 할 때마다, 나는 그것을 게임 “guess who”에 비유하여 설명합니다. 그 게임은 다음과 같이 진행됩니다.\n\n각 플레이어가 일부 캐릭터 얼굴로 가득 찬 게시판을 가진 “guess who”의 클래식 게임을 하는 상황을 상상해보세요. 각 캐릭터는 머리카락 색상, 눈동자 색상, 안경, 모자 등과 같은 구별 가능한 특징을 가지고 있습니다. 목표는 이러한 특징들에 대한 예/아니오 질문을 통해 상대방의 비밀 캐릭터를 추측하는 것입니다. 각 질문은 답변과 맞지 않는 후보를 없애줄 뿐 아니라, 가능성을 좁히며 마침내 확신을 갖고 비밀 캐릭터를 추측할 수 있도록 도와줍니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Hands on Monotonic Time Series Forecasting with XGBoost using Python](/TIL/assets/img/2024-07-09-HandsOnMonotonicTimeSeriesForecastingwithXGBoostusingPython_4.png)\n\nAs you can see, the structure resembles an upside-down tree, starting from the leaves (bottom) and extending to the root (top).\n\nEach predictor corresponds to one of these trees. If you are pondering the difference between classification and regression, you are correct. The example I showcased, for simplicity, focuses on a classification problem. However, if you substitute \"Kristen\" with a real number like \"0.47462\", you will shift to a regression problem. It's that straightforward.\n\nThe XGBoost algorithm cleverly utilizes all these decision trees to \"boost\" the prediction of the preceding tree. It's called \"extreme\" because a plethora of intermediate optimization steps have been undertaken by the talented scientists who developed the algorithm, which you can explore [here](link).\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n학습 파트는 항상 손실 함수 최소화 + 오버피팅 방지를 위한 정규화 항이 포함됩니다. 파라미터로는 잎의 수, 트리의 깊이 및 트리의 분할 지점이 있습니다. \"트리의 분할 지점\"이란 무엇을 의미하냐면요. 위 예제에서 \"이 사람이 아이인가요?\"와 같은 질문이 있습니다. 실제로는 연속적인 특성을 갖게 되어 \"x_1이 분할 지점인가요?\"와 같이 더 많이 사용하게 될 수 있습니다. 이는 분할 지점이 하나의 파라미터로 작용하게 되는 것입니다.\n\n## 1.1 XGBoost 단조성\n\n이제, XGBoost 알고리즘은 제공할 것이 많습니다:\n\n- 오버피팅에 대한 일반적으로 강한 내구성을 보여줍니다. 의사결정 트리는 오버피팅 문제가 잘 알려져 있으며 이러한 앙상블 방법은 이를 극복하는 데 좋습니다.\n- 저렴한 계산 복잡성을 다룰 수 있습니다. 그것을 하면서 오버피팅을 방지하는 것이 특히 명확하지 않습니다.\n- 특성의 중요성 덕분에 여전히 설명 가능성을 제공합니다. 이를 통해 어떤 X의 파라미터가 예측에 중요한지 이해할 수 있습니다.\n- 특정 특성에 대해 단조 함수로 응답을 하도록 선택할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n예를 들어, 다음과 같은 함수가 있다고 가정해 봅시다:\n\n![Alt text](/TIL/assets/img/2024-07-09-HandsOnMonotonicTimeSeriesForecastingwithXGBoostusingPython_5.png)\n\n이 함수는 x2에 대해 단조적입니다. x1과 x2가 주어졌을 때 XGBoost를 사용하여 f(x1, x2)의 결과를 예측하려고 한다고 상상해 봅니다. 이제 현실에서 f(x1, x2)는 알려지지 않았습니다 (그렇지 않으면 기계 학습을 하지 않고 휴가를 가기위해 플로리다로 비행을 갈 것입니다), 하지만 x2에 대해 단조적이라는 점을 알거나 원할 수 있습니다. 실제로 저는 다른 양과 관련하여 단조적임을 알고 있던 물리량이 있었던 적이 있습니다. XGBoost의 구조를 수정하여 해당 특징에 대한 요구 사항을 충족시킬 수 있습니다. 우리의 경우, x2에 대한 단조적 행동을 강제하는 예측을 할 수 있습니다.\n\n좋아요. 이제 이걸 버텨내셨으면 좋겠네요. 이제 재미있는 부분으로 넘어갑시다. 😅\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 2. 코딩\n\n## 2.1 장난감 데이터셋\n\n결정 트리가 나를 많이 성장시키고, 나의 첫 번째 머신러닝 코드 중 하나였으니까 많은 감정을 불러일으켰어요. 조금 바보 같게 들릴지 몰라도, 조금은 감정적이 느껴져요 ❤️\n\n하지만 솔직히 이 코드는 굉장히 간단합니다. 그러니 더 이상 말이 필요 없이, 바로 시작해 봅시다. 두 가지 예제를 보여 드릴 건데, 첫 번째 예제는 1차원으로, 이 모든 것이 어떻게 작동하는지 설명하는 겁니다. 이것은 우리가 XGBoost를 사용해 예측하고자 하는 대상 함수 f입니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![Image](/TIL/assets/img/2024-07-09-HandsOnMonotonicTimeSeriesForecastingwithXGBoostusingPython_6.png)\n\n이것이 플롯입니다.\n\n이제 XGBoost의 실용적인 구현이 문서에 단어별로 자세히 나와 있습니다. 제가 모든 부분을 알려드릴 필요는 없지만, 이 경우에 어떻게 작동하는지 보여드리고 싶습니다. 이는 온라인에서 찾을 수 있는 어떤 SkLearn 모델과 다를 바 없습니다.\n\n쉽죠. 이제 실제 테스트를 해봅시다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 2.2 실제 데이터셋\n\n위의 예제는 xgboost 코드의 구조를 보여줍니다. 그러나, 우리가 구축한 장난감 데이터셋에는 몇 가지 제한 사항이 있습니다:\n\n- 너무 간단하다 (1 차원) 그리고 완전히 근거없이 만들어진 데이터셋이다.\n- 우리는 데이터셋을 섞었기 때문에 예측적으로 사용하지 않았다.\n- 우리는 모노토닉 제약 조건을 사용하지 않았다 (내가 약속한 것).\n\n그래서, 실제 비즈니스를 시작해 볼까요?\n저는 델리 데이터의 데이터셋인 이 데이터셋을 사용했습니다. 이 데이터셋은 두 개의 csv 테이블 (DailyDelhiClimateTrain.csv와 DailyDelhiClimateTest.csv)으로 구성되어 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nPandas를 사용하여 데이터를 가져와서 5개의 행을 표시했습니다. 날짜, 습도, 풍속, 평균 기압 및 평균 온도와 같이 5개의 열이 있습니다. 하나의 합리적인 문제는 다음과 같을 수 있습니다:\n\n그리고 이 문제에 대해 잘 다루는 블로그 포스트가 있습니다. 우리는 한 단계 더 나아가려고 합니다. 우리는 이 다른 열인 \"City_Index\"를 추가할 것입니다. 이 City_Index는 뉴델리보다 더 덥거나 더 추운 다른 도시가 있다는 사실을 모방할 것입니다. 세계의 다른 부분별로 도시 지수를 그룹화하는 것처럼\\*:\n\n![이미지](/TIL/assets/img/2024-07-09-HandsOnMonotonicTimeSeriesForecastingwithXGBoostusingPython_7.png)\n\n이제 City_Index=1이 mean_temp의 기본 값이 되도록 만들 것입니다. City_Index = 2는 목표 값으로 2*mean_temp를 가져오도록 만들 것이고, City_Index=9는 목표 값으로 9*mean_temp를 가져오도록 할 것입니다. 이는 다른 변수를 모두 고정시킨다면 City_Index가 단조 변수가 될 수 있다는 것을 의미합니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/TIL/assets/img/2024-07-09-HandsOnMonotonicTimeSeriesForecastingwithXGBoostusingPython_8.png\" />\n\n실제로 이것은 로스엔젤레스, 캘리포니아의 온도가 앵코리지, 알래스카의 온도보다 항상 높다는 것을 의미합니다 (알래스카에 있는 도시를 몰라서 구글링했어요).\n\n우리는 \"City_Index\" 변수와 이 두 함수를 통해 이를 할 것입니다:\n\n이제 우리는 이를 할 것입니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 새로운 City_Index 변수를 사용하여 증강된 데이터 세트를 생성합니다.\n- 데이터를 훈련 및 검증 세트로 분할합니다.\n- City_Index에서 단조성을 강제하여 시계열 XGBoost 예측기를 훈련하는 데 훈련 데이터를 사용하고 훈련된 모델을 사용하여 다음 값을 예측합니다.\n\n그리고 이렇게 테스트해 봅니다:\n\n그리고 이것이 우리의 예측 결과입니다:\n\n![image](/TIL/assets/img/2024-07-09-HandsOnMonotonicTimeSeriesForecastingwithXGBoostusingPython_9.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n몬토닉성은 params_constraints에서 확인할 수 있습니다. Params constraints 벡터는 우리가 몬토닉성을 유지하고 싶은 특성을 제외한 모든 특성을 0으로 설정한 벡터입니다.\n이제 알 수 있듯이, 다른 모든 특성을 고정하고 City_Index를 변경하면 몬토닉한 동작을 볼 수 있습니다:\n\n완벽하게 몬토닉합니다. 거의 선형적이기도 하죠 (우리가 사전에 특정한 것). 이것은 XGBoost 대 ARIMA, SARIMA 또는 다항 회귀를 사용하는 차이를 보여줍니다: XGBoost를 사용하면 명시적인 몬토닉 동작을 부여할 수 있습니다.\n\n# 3. 결론\n\n여기에서 우리가 한 작업을 요약해 드릴게요:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 저희는 예보 작업을 수행하되 보존하고 싶은 일부 단조 특성이 있는 문제를 소개했습니다. 이러한 경우에는 ARIMA나 RNN과 같은 고급 방법이 단조성을 깨뜨릴 수 있습니다.\n\n- 간단히 말해, \"추측 누구?\" 예제를 사용하여 XGBoost 아이디어를 소개했고, 부스팅 알고리즘이 선택할 수 있는 의사 결정 트리를 수정하여 단조성을 강제할 수 있다는 점을 확인했습니다.\n\n- 우리는 XGBoost를 장난감 예제에서 사용하여 구문이 작동하는 방식을 기본적으로 이해했습니다.\n\n- 기후변화 데이터를 사용하여 예보 연구를 수행했으며, 기본적인 예보 이상의 작업을 수행했습니다. 'City_Index'라는 새로운 특성에 단조적으로 증가하는 값을 부과함으로써, City_Index 특성에 대해 예측된 온도가 단조적이어야 한다는 가정에 기반하여 온도를 예측했습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# 4. 저에 대해!\n\n다시 한 번 시간 내주셔서 감사합니다. 정말 감사드려요 ❤\n\n내 이름은 Piero Paialunga이고 난 이 사람이야:\n\n![이미지](/TIL/assets/img/2024-07-09-HandsOnMonotonicTimeSeriesForecastingwithXGBoostusingPython_10.png)\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n저는 시내티 대학교 항공우주공학부 박사 후보이자 Gen Nine의 머신러닝 엔지니어입니다. 블로그 글과 Linkedin에서 AI 및 머신러닝에 대해 이야기합니다. 만약 글이 마음에 드시고 머신러닝에 대해 더 알고 싶으시다면:\n\nA. 제가 모든 이야기를 게시하는 Linkedin에서 팔로우하세요.\nB. 뉴스레터를 구독하세요. 새로운 이야기에 대한 업데이트를 제공하며 의문이나 궁금증이 있을 때 연락해 모든 수정을 받아볼 수 있습니다.\nC. 추천 회원이 되어 \"월간 최대 이야기 수\" 제한 없이 저 (그리고 수천 명의 다른 머신러닝 및 데이터 과학 최고 작가)가 제공하는 최신 기술에 대해 읽을 수 있습니다.\n\n질문이 있거나 협업을 시작하려면 여기에 메시지를 남겨주세요:\n\npiero.paialunga@hotmail.com\n","ogImage":{"url":"/assets/img/2024-07-09-HandsOnMonotonicTimeSeriesForecastingwithXGBoostusingPython_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-HandsOnMonotonicTimeSeriesForecastingwithXGBoostusingPython_0.png","tag":["Tech"],"readingTime":13},{"title":"파이썬 데이터 시각화 Seaborn 라이브러리 사용법","description":"","date":"2024-07-09 20:10","slug":"2024-07-09-DataVisualizationinPythonSeabornLibrary","content":"\n시각화는 데이터에서 통찰을 전달하는 강력한 방법입니다. 파이썬의 Seaborn 라이브러리는 Matplotlib을 기반으로 한, 시각적으로 매력적이고 정보를 제공하는 높은 수준의 인터페이스를 제공합니다. 이 기사에서는 Seaborn 라이브러리를 자세히 살펴보며 그 기능을 탐구하고 다양성을 보여주는 실용적인 코드 예제를 제시할 것입니다.\n\n![image](/TIL/assets/img/2024-07-09-DataVisualizationinPythonSeabornLibrary_0.png)\n\n## Seaborn이란\n\nSeaborn은 복잡하고 아름다운 시각화를 만드는 과정을 단순화하는 데이터 시각화 라이브러리입니다. 시각적 경험을 향상하기 위해 내장된 테마와 색 팔레트를 제공합니다. Seaborn은 통계적 시각화를 생성하는 데 특히 적합하며 변수 간의 관계를 시각화하는 데 자주 사용됩니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# Seaborn을 이용한 시각화 생성\n\nSeaborn의 주요 기능과 기능을 몇 가지 코드 예제와 함께 살펴보겠습니다.\n\n## 1. 산점도\n\n- a) 기본 산점도\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n\n```js\n# 샘플 데이터\nx = [1, 2, 3, 4, 5]\ny = [3, 5, 8, 6, 7]\n```\n\n```js\n# Seaborn을 사용하여 기본 산점도 그리기\nsns.scatterplot(x=x, y=y)\nplt.xlabel('X축')\nplt.ylabel('Y축')\nplt.title('Seaborn을 사용한 기본 산점도')\nplt.show()\n```\n\n<img src=\"/TIL/assets/img/2024-07-09-DataVisualizationinPythonSeabornLibrary_1.png\" />\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- b) 색상과 색조를 가진 산점도\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n\n```python\n# 샘플 데이터\nx = [1, 2, 3, 4, 5]\ny = [3, 5, 8, 6, 7]\ncategories = ['A', 'B', 'A', 'B', 'A']\n```\n\n```python\n# Seaborn을 사용하여 색상과 색조를 가진 산점도 생성\nsns.scatterplot(x=x, y=y, hue=categories, palette='Set1')\nplt.xlabel('X-축')\nplt.ylabel('Y-축')\nplt.title('색상과 색조를 가진 산점도')\nplt.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/TIL/assets/img/2024-07-09-DataVisualizationinPythonSeabornLibrary_2.png\" />\n\n## 2. 상자 그림\n\n- a) 기본 상자 그림\n\n```js\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 샘플 데이터\ndata = sns.load_dataset('iris')\n```\n\n```js\n# Seaborn을 사용하여 기본 상자 그림 생성\nsns.boxplot(x='species', y='sepal_length', data=data)\nplt.xlabel('종류')\nplt.ylabel('꽃 받침 길이')\nplt.title('Seaborn을 사용한 기본 상자 그림')\nplt.show()\n```\n\n<img src=\"/TIL/assets/img/2024-07-09-DataVisualizationinPythonSeabornLibrary_3.png\" />\n\n- b) 색상 팔레트를 사용한 가로 상자 그림\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n\n```js\n# 샘플 데이터\ndata = sns.load_dataset('titanic')\n```\n\n```js\n# Seaborn을 사용하여 색상 팔레트를 이용한 수평 상자 그림 생성\nsns.boxplot(x='age', y='class', data=data, orient='h', palette='Set2')\nplt.xlabel('나이')\nplt.ylabel('등급')\nplt.title('색상 팔레트를 이용한 수평 상자 그림')\nplt.show()\n```\n\n<img src=\"/TIL/assets/img/2024-07-09-DataVisualizationinPythonSeabornLibrary_4.png\" />\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- c) 그룹화된 상자 그림\n\n```js\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n\n```js\n# 샘플 데이터\ndata = sns.load_dataset('tips')\n```\n\n```js\n# Seaborn을 사용하여 그룹화된 상자 그림 생성\nsns.boxplot(x='day', y='total_bill', data=data, hue='sex', palette='Set3')\nplt.xlabel('요일')\nplt.ylabel('총 계산')\nplt.title('그룹화된 상자 그림')\nplt.legend(title='성별')\nplt.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n<img src=\"/TIL/assets/img/2024-07-09-DataVisualizationinPythonSeabornLibrary_5.png\" />\n\n- d) Notched Box Plot\n\n```js\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n\n```js\n# Sample data\ndata = sns.load_dataset('diamonds')\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# Seaborn을 사용하여 notch가 있는 상자 그림 만들기\nsns.boxplot(x='cut', y='price', data=data, notch=True, palette='pastel')\nplt.xlabel('Cut')\nplt.ylabel('Price')\nplt.title('Notched Box Plot')\nplt.show()\n```\n\n<img src=\"/TIL/assets/img/2024-07-09-DataVisualizationinPythonSeabornLibrary_6.png\" />\n\n- e) 사용자 정의 상자 그림\n\n```js\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 샘플 데이터\ndata = sns.load_dataset('mpg')\n```\n\n```js\n# Seaborn을 사용하여 사용자 정의 상자 그림 만들기\nsns.boxplot(x='origin', y='mpg', data=data, hue='cylinders', palette='Set2')\nplt.xlabel('Origin')\nplt.ylabel('Miles per Gallon')\nplt.title('Customized Box Plot')\nplt.legend(title='Cylinders')\nplt.show()\n```\n\n<img src=\"/TIL/assets/img/2024-07-09-DataVisualizationinPythonSeabornLibrary_7.png\" />\n\n## 3. Pair Plot\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- a) 기본 Pair Plot\n\n```js\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n\n```js\n# 샘플 데이터\ndata = sns.load_dataset('iris')\n```\n\n```js\n# Seaborn을 사용하여 기본 Pair Plot 생성\nsns.pairplot(data, hue='species')\nplt.title('Seaborn을 사용한 기본 Pair Plot')\nplt.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n![image](/TIL/assets/img/2024-07-09-DataVisualizationinPythonSeabornLibrary_8.png)\n\n- b) Pair Plot with Custom Color Palette\n\n```js\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n\n```js\n# Sample data\ndata = sns.load_dataset('tips')\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# Seaborn을 사용하여 사용자 정의 색상 팔레트로 pair plot 만들기\nsns.pairplot(data, hue='sex', palette='Set2')\nplt.title('사용자 정의 색상 팔레트로 Pair Plot 만들기')\nplt.show()\n```\n\n<img src=\"/TIL/assets/img/2024-07-09-DataVisualizationinPythonSeabornLibrary_9.png\" />\n\n- c) 다른 플롯 유형을 사용한 Pair Plot\n\n```js\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 샘플 데이터\ndata = sns.load_dataset('penguins')\n```\n\n```js\n# Seaborn을 사용하여 다른 종류의 플롯을 사용하여 페어 플롯 생성\ng = sns.PairGrid(data)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.histplot, kde_kws={'color': 'k'})\nplt.title('다른 플롯 유형을 사용한 페어 플롯')\nplt.show()\n```\n\n<img src=\"/TIL/assets/img/2024-07-09-DataVisualizationinPythonSeabornLibrary_10.png\" />\n\n# 결론\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n시본은 파이썬에서 정보를 전달하고 미적으로 매력적인 데이터 시각화를 생성하는 강력한 라이브러리입니다. 산점도, 상자 그림, 쌍 플롯 또는 더 복잡한 시각화를 만들 때 시본의 우아한 구문과 내장 테마가 과정을 단순화합니다.\n\n# 파이썬 기초\n\n소중한 시간 내어 주셔서 감사합니다! 🚀\n더 많은 콘텐츠는 \"Python Fundamentals\"에서 찾아보실 수 있어요! 💫\n","ogImage":{"url":"/assets/img/2024-07-09-DataVisualizationinPythonSeabornLibrary_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-DataVisualizationinPythonSeabornLibrary_0.png","tag":["Tech"],"readingTime":10}],"page":"16","totalPageCount":33,"totalPageGroupCount":2,"lastPageGroup":20,"currentPageGroup":0},"__N_SSG":true}