{"pageProps":{"post":{"title":"스파크 최적화 집중 강좌 최고의 성능을 위한 단계별 가이드","description":"","date":"2024-07-09 20:14","slug":"2024-07-09-IntensiveSparkOptimizationCourse","content":"\n![Intensive Spark Optimization Course](/TIL/assets/img/2024-07-09-IntensiveSparkOptimizationCourse_0.png)\n\n# 로컬에서 플레이그라운드 설정하기\n\n- Docker Desktop을 설치합니다.\n- `docker run -p 8888:8888 jupyter/pyspark-notebook`을 실행합니다.\n- 다음 메시지가 표시되면 브라우저에서 주피터 랩을 열기 위해 URL 중 하나를 붙여넣습니다.\n\n```js\n서버에 액세스하려면 브라우저에서 이 파일을 엽니다:\n    file:///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html\n또는 다음 URL 중 하나를 복사하여 붙여넣습니다:\n    http://3c331b638888:8888/lab?token=a88888b6aa6620fc976588ba58817f3b14ea0674bdc77f72\n    http://127.0.0.1:8888/lab?token=a88888b6aa6620fc976588ba58817f3b14ea0674bdc77f72\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# SparkSession 초기화하기\n\n```js\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"Spark Test\").getOrCreate()\n```\n\n# 데이터프레임\n\n## 1. 데이터프레임 생성하기\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 컬럼 사용\n\n```js\nfrom pyspark.sql.types import StructType, IntegerType, StringType\n\n# 데이터를 튜플의 리스트로 정의\ndata = [(\"James\", 34), (\"Anna\", 20), (\"Lee\", 30)]\n\n# 컬럼 사용\ncolumns = [\"Name\", \"Age\"]\ndf = spark.createDataFrame(data, schema=columns)\n```\n\n- 스키마 사용\n\n```js\nfrom pyspark.sql.types import StructType, IntegerType, StringType\n\n# 데이터를 튜플의 리스트로 정의\ndata = [(\"James\", 34), (\"Anna\", 20), (\"Lee\", 30)]\n\n# 스키마 사용\nschema = StructType([\n    StructField(\"Name\", StringType(), True),\n    StructField(\"Age\", IntegerType(), True)\n])\ndf = spark.createDataFrame(data, schema=schema)\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- RDD 사용\n\n```js\nfrom pyspark.sql.types import StructType, IntegerType, StringType\n\n# 데이터를 튜플의 리스트로 준비\ndata = [(\"James\", 34), (\"Anna\", 20), (\"Lee\", 30)]\n\n# RDD 사용\nrdd = spark.sparkContext.parallelize(data)\nschema = StructType([\n    StructField(\"이름\", StringType(), True),\n    StructField(\"나이\", IntegerType(), True)\n])\ndf = spark.createDataFrame(rdd, schema=schema)\n```\n\n```js\ndf.show()\n\n# 출력\n+-----+---+\n| 이름|나이|\n+-----+---+\n|James| 34|\n| Anna| 20|\n|  Lee| 30|\n+-----+---+\n```\n\n## 2. 데이터프레임 표시\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ndf.printSchema();\nprint(df.schema);\nprint(df.columns);\ndf.describe().show();\n```\n\n```js\n#결과\n\n## df.printSchema()\nroot\n |-- Name: string (nullable = true)\n |-- Age: long (nullable = true)\n\n## print(df.schema)\nStructType([\n  StructField(‘Name’, StringType(), True),\n  StructField(‘Age’, LongType(), True)\n])\n\n## print(df.columns)\n[‘Name’, ‘Age’]\n\n## df.describe().show()\n+-------+----+-----------------+\n|summary|Name|              Age|\n+-------+----+-----------------+\n|  count|   3|                3|\n|   mean|NULL|             28.0|\n| stddev|NULL|7.211102550927978|\n|    min|Anna|               20|\n|    max| Lee|               34|\n+-------+----+-----------------+\n```\n\n## 3. 컬럼 선택\n\n```js\ndf.select(df[0]).show();\ndf.select(df.Name).show();\ndf.select(df[\"Name\"]).show();\ndf.select(\"Name\").show();\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n#Output\n+-----+\n| Name|\n+-----+\n|James|\n| Anna|\n|  Lee|\n+-----+\n```\n\n## 4. 데이터 필터링\n\n```js\n# 데이터 필터링\ndf.filter(df[1] > 25).show()\ndf.filter(df.Age > 25).show()\ndf.filter(df[\"Age\"] > 25).show()\n```\n\n```js\n#Output\n+-----+---+\n| Name|Age|\n+-----+---+\n|James| 34|\n|  Lee| 30|\n+-----+---+\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 5. 파일에 DataFrame 작성하기\n\n```js\n# JSON 파일 작성\ndf.write.json(\"test123.json\")\n\n# Parquet 파일 작성\ndf.write.parquet(\"test123.parquet\")\n```\n\n## 6. 파일을 DataFrame으로 읽기\n\n```js\n# JSON 파일 읽기\ndf_json = spark.read.json(\"test123.json\")\n# Parquet 파일 읽기\ndf_parquet = spark.read.parquet(\"test123.parquet\")\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 7. 새로운 복합 데이터 유형을 포함한 새로운 열 추가\n\n```js\nfrom pyspark.sql.functions import struct\ndf2 = df.withColumn(\"NameAndAge\", struct(df.Name, df.Age))\ndf2.show()\ndf2.printSchema()\n```\n\n```js\n# 출력\n+-----+---+-----------+\n| Name|Age| NameAndAge|\n+-----+---+-----------+\n|James| 34|{James, 34}|\n| Anna| 20| {Anna, 20}|\n|  Lee| 30|  {Lee, 30}|\n+-----+---+-----------+\n\n# 스키마 출력\nroot\n |-- Name: string (nullable = true)\n |-- Age: long (nullable = true)\n |-- NameAndAge: struct (nullable = false)\n |    |-- Name: string (nullable = true)\n |    |-- Age: long (nullable = true)\n```\n\n# 쿼리: 그룹화 및 집계\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 1. count()\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import count\n\n# 스파크 세션 초기화\nspark = SparkSession.builder.appName(\"그루핑 및 집계\").getOrCreate()\n\n# 데이터프레임 생성\ndata = [(\"James\", \"Sales\", 3000),\n        (\"Michael\", \"Sales\", 4600),\n        (\"Robert\", \"Sales\", 4100),\n        (\"Maria\", \"Finance\", 3000),\n        (\"James\", \"Sales\", 3000),\n        (\"Scott\", \"Finance\", 3300),\n        (\"Jen\", \"Finance\", 3900),\n        (\"Jeff\", \"Marketing\", 3000),\n        (\"Kumar\", \"Marketing\", 2000),\n        (\"Saif\", \"Sales\", 4100)]\ncolumns = [\"employee_name\", \"department\", \"salary\"]\ndf = spark.createDataFrame(data, schema=columns)\n\n# 그룹화 및 count 수행\ngrouped_df = df.groupBy(\"department\").count()\ngrouped_df.show()\n```\n\n```python\n# 결과\n\n+----------+-----+\n|department|count|\n+----------+-----+\n|     Sales|    5|\n|   Finance|    3|\n| Marketing|    2|\n+----------+-----+\n```\n\n## 2. max(), min(), avg(), sum()\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 그룹별로 그룹화 및 최댓값 찾기\nmax_df = df.groupBy(\"department\").max(\"salary\").alias(\"max_salary\")\nmax_df.show()\n```\n\n```js\n# 결과\n\n+----------+-----------+\n|department|max(salary)|\n+----------+-----------+\n|     Sales|       4600|\n|   Finance|       3900|\n| Marketing|       3000|\n+----------+-----------+\n```\n\n## 3. agg() + F.max(), F.count() 등…\n\n```js\nfrom pyspark.sql import functions as F\n\n# 여러 가지 집계 동작 수행\nagg_df = df.groupBy(\"department\").agg(\n    F.count(\"salary\").alias(\"count\"),\n    F.max(\"salary\").alias(\"max_salary\"),\n    F.min(\"salary\").alias(\"min_salary\"),\n    F.sum(\"salary\").alias(\"total_salary\"),\n    F.avg(\"salary\").alias(\"average_salary\")\n)\nagg_df.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 결과\n+----------+-----+----------+----------+------------+--------------+\n|department|count|max_salary|min_salary|total_salary|average_salary|\n+----------+-----+----------+----------+------------+--------------+\n|     Sales|    5|      4600|      3000|       18800|        3760.0|\n|   Finance|    3|      3900|      3000|       10200|        3400.0|\n| Marketing|    2|      3000|      2000|        5000|        2500.0|\n+----------+-----+----------+----------+------------+--------------+\n```\n\n## 4. agg() + collect_list() 및 collect_set()\n\n```js\nfrom pyspark.sql.functions import collect_list\n\n# GroupBy 및 리스트 수집 수행\ncollected_list_df = df.groupBy(\"department\").agg(\n  collect_list(\"salary\"),\n  collect_set(\"salary\")\n)\ncollected_list_df.show(truncate=False)\n```\n\n```js\n# 결과\n+----------+------------------------------+-------------------+\n|department|collect_list(salary)          |collect_set(salary)|\n+----------+------------------------------+-------------------+\n|Sales     |[3000, 4600, 4100, 3000, 4100]|[4600, 3000, 4100] |\n|Finance   |[3000, 3300, 3900]            |[3900, 3000, 3300] |\n|Marketing |[3000, 2000]                  |[3000, 2000]       |\n+----------+------------------------------+-------------------+\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 5. agg() + 사용자 정의 집계 함수 (UDAF)\n\n- 때로는 내장 함수만으로 복잡한 집계를 수행하기에 충분하지 않을 수 있습니다. Spark를 사용하면 사용자 정의 집계 함수를 만들 수 있습니다.\n\n```js\n# 초기 데이터\n|부서     |직원 이름        |급여    |\n|----------|-------------|------|\n|Sales    |James        |3000  |\n|Sales    |Michael      |4600  |\n|Sales    |Robert       |4100  |\n|Finance  |Maria        |3000  |\n|Sales    |James        |3000  |\n|Finance  |Scott        |3300  |\n|Finance  |Jen          |3900  |\n|Marketing|Jeff         |3000  |\n|Marketing|Kumar        |2000  |\n|Sales    |Saif         |4100  |\n```\n\n```js\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\nfrom pandas import DataFrame\n\n@pandas_udf(\"double\")\ndef mean_salary(s: pd.Series) -> float:\n return s.mean()\nudaf_df = df.groupBy(\"department\").agg(\n  mean_salary(df[\"salary\"]).alias(\"average_salary\")\n)\nudaf_df.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 결과\n+----------+--------------+\n|department|average_salary|\n+----------+--------------+\n|   Finance|        3400.0|\n| Marketing|        2500.0|\n|     Sales|        3760.0|\n+----------+--------------+\n\n## 6. agg() + 복잡한 조건: when()\n\n- 때로는 조건에 따른 합계나 평균과 같은 복잡한 조건이 집계 중에 필요할 수 있습니다.\n\n# 초기 데이터\n+----------+-------------+------+\n|department|employee_name|salary|\n+----------+-------------+------+\n|     Sales|        James|  3000|\n|     Sales|      Michael|  4600|\n|     Sales|       Robert|  4100|\n|   Finance|        Maria|  3000|\n|     Sales|        James|  3000|\n|   Finance|        Scott|  3300|\n|   Finance|          Jen|  3900|\n| Marketing|         Jeff|  3000|\n| Marketing|        Kumar|  2000|\n|     Sales|         Saif|  4100|\n+----------+-------------+------+\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\nfrom pyspark.sql.functions import when\n\n# 조건부 집계\nconditional_agg_df = df.groupBy(\"department\").agg(\n    sum(when(df[\"salary\"] > 3000, df[\"salary\"])).alias(\"sum_high_salaries\")\n)\nconditional_agg_df.show()\n\n# 결과\n\n+----------+-----------------+\n|department|sum_high_salaries|\n+----------+-----------------+\n|     Sales|            12800|\n|   Finance|             7200|\n| Marketing|             NULL|\n+----------+-----------------+\n\n## 6. agg() 이후 GroupBy에서 RDD Map 함수 사용하기\n\n- 경우에 따라 매핑 함수를 GroupBy와 결합하여 집단화된 데이터에 대한 직접적인 집계가 아닌 작업을 수행할 수 있습니다.\n\n<!-- TIL 수평 -->\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```\n\n# 데이터\n\n| department | employee_name | salary |\n| ---------- | ------------- | ------ |\n| Sales      | James         | 3000   |\n| Sales      | Michael       | 4600   |\n| Sales      | Robert        | 4100   |\n| Finance    | Maria         | 3000   |\n| Sales      | James         | 3000   |\n| Finance    | Scott         | 3300   |\n| Finance    | Jen           | 3900   |\n| Marketing  | Jeff          | 3000   |\n| Marketing  | Kumar         | 2000   |\n| Sales      | Saif          | 4100   |\n\n# GroupBy 후 map 작업 적용\n\n```python\nresult_rdd = df.groupBy(\"department\").agg(\n  collect_list(\"salary\")\n).rdd.map(\n  lambda x: (x[0], max(x[1]))\n)\n\nresult_df = spark.createDataFrame(result_rdd, [\"department\", \"max_salary\"])\nresult_df.show()\n```\n\n# 결과\n\n| department | max_salary |\n| ---------- | ---------- |\n| Sales      | 4600       |\n| Finance    | 3900       |\n| Marketing  | 3000       |\n\n# 조회: 다른 것\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 1. rollup()과 cube()\n\n- Rollup()은 다차원 집계를 생성하고 Excel의 소계와 유사한 계층적 요약을 제공합니다.\n\n```js\n# 초기 데이터\n|department|employee_name|salary|\n|----------|-------------|------|\n| Sales    | James       | 3000 |\n| Sales    | Michael     | 4600 |\n| Sales    | Robert      | 4100 |\n| Finance  | Maria       | 3000 |\n| Sales    | James       | 3000 |\n| Finance  | Scott       | 3300 |\n| Finance  | Jen         | 3900 |\n| Marketing| Jeff        | 3000 |\n| Marketing| Kumar       | 2000 |\n| Sales    | Saif        | 4100 |\n```\n\n```js\nfrom pyspark.sql.functions import sum\n\n# Rollup 예제\nrollup_df = df.rollup(\"department\", \"employee_name\").sum(\"salary\")\nrollup_df.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 결과\n+----------+-------------+-----------+\n|department|employee_name|sum(salary)|\n+----------+-------------+-----------+\n|     Sales|        James|       6000|\n|      NULL|         NULL|      34000|\n|     Sales|         NULL|      18800|\n|     Sales|      Michael|       4600|\n|     Sales|       Robert|       4100|\n|   Finance|         NULL|      10200|\n|   Finance|        Maria|       3000|\n|   Finance|        Scott|       3300|\n|   Finance|          Jen|       3900|\n| Marketing|         NULL|       5000|\n| Marketing|         Jeff|       3000|\n| Marketing|        Kumar|       2000|\n|     Sales|         Saif|       4100|\n+----------+-------------+-----------+\n```\n\n- Cube(): Cube는 다차원 집계를 생성하고 지정된 그룹화 열의 다중 조합을 통해 통찰을 제공합니다.\n\n```js\n# 초기 데이터\n+----------+-------------+------+\n|department|employee_name|salary|\n+----------+-------------+------+\n|     Sales|        James|  3000|\n|     Sales|      Michael|  4600|\n|     Sales|       Robert|  4100|\n|   Finance|        Maria|  3000|\n|     Sales|        James|  3000|\n|   Finance|        Scott|  3300|\n|   Finance|          Jen|  3900|\n| Marketing|         Jeff|  3000|\n| Marketing|        Kumar|  2000|\n|     Sales|         Saif|  4100|\n+----------+-------------+------+\n```\n\n```js\nfrom pyspark.sql.functions import sum\n\n# Cube 예시\ncube_df = df.cube(\"department\", \"employee_name\").sum(\"salary\")\ncube_df.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 결과\n\n+----------+-------------+-----------+\n|부서      |직원명         |급여합계   |\n+----------+-------------+-----------+\n|      NULL|        James|       6000|\n|     판매 |        James|       6000|\n|      NULL|         NULL|      34000|\n|     판매 |         NULL|      18800|\n|      NULL|      Michael|       4600|\n|     판매 |      Michael|       4600|\n|     판매 |       Robert|       4100|\n|      NULL|       Robert|       4100|\n|   재무   |         NULL|      10200|\n|   재무   |        Maria|       3000|\n|      NULL|        Maria|       3000|\n|      NULL|        Scott|       3300|\n|   재무   |        Scott|       3300|\n|      NULL|          Jen|       3900|\n|   재무   |          Jen|       3900|\n| 마케팅  |         NULL|       5000|\n| 마케팅  |         Jeff|       3000|\n|      NULL|         Jeff|       3000|\n| 마케팅  |        Kumar|       2000|\n|      NULL|        Kumar|       2000|\n+----------+-------------+-----------+\n상위 20개 행만 표시\n```\n\n## 2. groupBy() + pivot()\n\n- Pivoting을 사용하면 행을 열로 변환하여 피벗 테이블과 유사한 방식으로 데이터를 요약할 수 있습니다. 종종 두 열 간의 관계를 이해하는 데 사용됩니다.\n\n```js\n# 초기 데이터\n+----------+-------------+------+\n|부서     |직원명        |급여   |\n+----------+-------------+------+\n|     판매 |        James|  3000|\n|     판매 |      Michael|  4600|\n|     판매 |       Robert|  4100|\n|   재무  |        Maria|  3000|\n|     판매 |        James|  3000|\n|   재무  |        Scott|  3300|\n|   재무  |          Jen|  3900|\n| 마케팅  |         Jeff|  3000|\n| 마케팅  |        Kumar|  2000|\n|     판매 |         Saif|  4100|\n+----------+-------------+------+\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# Pivot 예시\npivot_df = df.groupBy(\"department\").pivot(\"employee_name\").sum(\"salary\")\npivot_df.show()\n```\n\n```js\n# 결과\n+----------+-----+----+----+-----+-----+-------+------+----+-----+\n|department|James|Jeff| Jen|Kumar|Maria|Michael|Robert|Saif|Scott|\n+----------+-----+----+----+-----+-----+-------+------+----+-----+\n|     Sales| 6000|NULL|NULL| NULL| NULL|   4600|  4100|4100| NULL|\n|   Finance| NULL|NULL|3900| NULL| 3000|   NULL|  NULL|NULL| 3300|\n| Marketing| NULL|3000|NULL| 2000| NULL|   NULL|  NULL|NULL| NULL|\n+----------+-----+----+----+-----+-----+-------+------+----+-----+\n```\n\n## 3. 윈도우 함수: partitionBy() + row_number()/rank().over(w)\n\n- 윈도우 함수는 현재 행과 관련된 \"윈도우\"에 대해 계산을 수행할 수 있어 전통적인 group-by 작업보다 더 유연성을 제공합니다. 이는 러닝 토탈, 이동 평균 또는 이전 및 다음 행에 액세스하는 데 특히 유용합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 초기 데이터\n| 부서      | 직원 이름       | 급여  |\n|----------|---------------|------|\n| Sales    | James         | 3000 |\n| Sales    | Michael       | 4600 |\n| Sales    | Robert        | 4100 |\n| Finance  | Maria         | 3000 |\n| Sales    | James         | 3000 |\n| Finance  | Scott         | 3300 |\n| Finance  | Jen           | 3900 |\n| Marketing| Jeff          | 3000 |\n| Marketing| Kumar         | 2000 |\n| Sales    | Saif          | 4100 |\n\n```\n\n```js\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import col, row_number\n\nwindowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").asc())\ndf_with_row_number = df.withColumn(\"row_number\", row_number().over(windowSpec))\ndf_with_row_number.show()\n```\n\n```js\n# 결과\n| 직원 이름      | 부서        | 급여  | row_number |\n|---------------|------------|------|------------|\n| Maria         | Finance    | 3000 |      1     |\n| Scott         | Finance    | 3300 |      2     |\n| Jen           | Finance    | 3900 |      3     |\n| Kumar         | Marketing  | 2000 |      1     |\n| Jeff          | Marketing  | 3000 |      2     |\n| James         | Sales      | 3000 |      1     |\n| James         | Sales      | 3000 |      2     |\n| Robert        | Sales      | 4100 |      3     |\n| Saif          | Sales      | 4100 |      4     |\n| Michael       | Sales      | 4600 |      5     |\n\n```\n\n- Rank() 함수를 위해\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import rank, col\n\nwindowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\ndf_with_rank = df.withColumn(\"rank\", rank().over(windowSpec))\ndf_with_rank.show()\n```\n\n```js\n# 출력\n+-------------+----------+------+----------+\n|employee_name|department|salary|      rank|\n+-------------+----------+------+----------+\n|          Jen|   Finance|  3900|         1|\n|        Scott|   Finance|  3300|         2|\n|        Maria|   Finance|  3000|         3|\n|         Jeff| Marketing|  3000|         1|\n|        Kumar| Marketing|  2000|         2|\n|      Michael|     Sales|  4600|         1|\n|       Robert|     Sales|  4100|         2|\n|         Saif|     Sales|  4100|         2|\n|        James|     Sales|  3000|         3|\n|        James|     Sales|  3000|         3|\n+-------------+----------+------+----------+\n```\n\n# 최적화 I: 무게 감소\n\n## 0. 불필요한 원시 데이터 제거\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 1. DataFrame이 여러 번 액세스 될 때 캐시합니다.\n\n```js\ndf.cache();\ndf.count();\n```\n\n```js\n#출력\n3\n```\n\n## 2. 적절한 파일 형식 사용하기\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 압축 파일은 파일의 입출력 및 메모리를 절약할 수 있어요.\n- 압축 해제된 파일은 CPU를 절약할 수 있어요.\n\n```js\ndf.write.parquet(\"output.parquet\");\n```\n\n## 3. 스키마 수동 지정하기\n\n```js\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\nschema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", IntegerType(), True)\n])\ndf = spark.read.schema(schema).csv(\"path/to/file.csv\")\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 4. 조기에 필요한 열 선택하기\n\n데이터 처리 파이프라인에서 메모리 사용량을 줄이기 위해 필요한 열만 미리 선택하세요.\n\n```js\ndf.select(\"dept_name\", \"name\").filter(\"dept_id >= 102\").show();\ndf.select(\"dept_name\", \"name\")\n  .filter(df.dept_id >= 102)\n  .show();\n```\n\n```js\n#출력\n+---------+----+\n|dept_name|name|\n+---------+----+\n|Marketing|Jane|\n|  Finance| Joe|\n+---------+----+\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 5. 필터를 조인이나 집계 전에 빠르게 적용하세요.\n\n```js\ndf.filter(\"age > 25\").join(df_other, \"id\").show();\n```\n\n## 6. 큰 데이터셋 수집 방지를 위해 limit() 사용하기\n\n- 큰 데이터셋에 collect()를 사용하지 않도록 주의하여 메모리 부족 오류를 방지하세요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\ndf.filter(\"age > 30\").limit(100).collect();\n```\n\n## 7. Using spark.sql(): Catalyst optimizer for Complex Queries\n\n- Leverage Spark SQL for complex queries, which might be more readable and can benefit from the Catalyst optimizer.\n\n```js\ndf.createOrReplaceTempView(\"table\");\nspark.sql(\"SELECT id, sum(value) FROM table GROUP BY id\").show();\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 8. RDD 사용: reduceByKey를 사용한 집계\n\n- 집계 작업을 수행할 때 reduceByKey를 사용하는 것이 groupBy보다 더 효율적일 수 있습니다.\n\n```python\nrdd = df.rdd.map(lambda x: (x[0], x[1]))\nreduced = rdd.reduceByKey(lambda a, b: a + b)\nreduced.toDF([\"key\", \"value\"]).show()\n```\n\n# 최적화 II: 파티션 없이 병렬화 없음\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 1. 테이블 분할: partitionBy()\n\n- 데이터프레임을 디스크에 저장할 때 빠른 후속 읽기를 위해 분할을 사용하세요.\n\n```js\ndf.write.partitionBy(\"year\", \"month\").parquet(\"path/to/output\");\n```\n\n## 2. 스튜 관리를 위한 Salting 키\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 조인 연산 중 데이터 스큐가 발생하는 경우, 하나 이상의 키 값이 다른 것들보다 훨씬 더 많은 데이터를 가지고 있을 때 발생합니다.\n- 예를 들어 \"customer_id\"를 기준으로 조인을 수행하고, 대부분의 거래가 소수의 고객에 속해 있다면 이러한 소수의 키는 다른 키들에 비해 훨씬 많은 양의 데이터를 가지고 있을 것입니다. 이로 인해 일부 작업(큰 키를 처리하는 작업)이 훨씬 더 오랜 시간이 걸리고 병목 현상이 발생할 수 있습니다.\n- 이 문제를 해결하는 방법은 skewed 데이터를 관리하기 위해 키에 임의의 접두사를 추가하는 것입니다.\n\n```js\nfrom pyspark.sql.functions import monotonically_increasing_id, expr\ndf.withColumn(\"salted_key\",\n    expr(\"concat(name, '_', (monotonically_increasing_id() % 10))\")\n).groupBy(\"salted_key\").count().select(sum(\"count\")).show()\n```\n\n```js\n# 결과\n+----------+\n|sum(count)|\n+----------+\n|         3|\n+----------+\n```\n\n- 데이터 로딩의 균형을 어떻게 맞출까요?\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\nfrom pyspark.sql.functions import monotonically_increasing_id, expr\ndf.withColumn(\"salted_key\",\n    expr(\"concat(name, '_', (monotonically_increasing_id() % 10))\")\n).groupBy(\"salted_key\").count().show()\n```\n\n```js\n# 결과\n+----------+-----+\n|salted_key|count|\n+----------+-----+\n|   James_6|    1|\n|   James_4|    1|\n|   James_0|    1|\n+----------+-----+\n```\n\n# 최적화 III: Shuffling을 최소화하는 전략\n\n```js\n### Shuffling 최소화 전략\n- **Broadcast 변수 사용**\n  - 데이터셋이 작은 경우 Shuffling을 피하기 위해 모든 노드에 브로드캐스트합니다.\n- **파티션 튜닝**\n  - 작업 및 데이터 규모에 맞게 파티션 수를 조정합니다.\n- **변환 최적화**\n  - Shuffling을 필요로 하는 넓은 변환을 최소화하기 위해 작업을 계획합니다.\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 섞기\n\n- 섞기는 데이터가 서로 다른 파티션에 재분배되는 과정입니다.\n- 이는 데이터를 실행자 간이나 심지어 기계 간에 이동하는 것을 포함합니다.\n- 네트워크 및 디스크 I/O 측면에서 가장 비용이 많이 드는 작업 중 하나입니다.\n\n## 섞기의 목적\n\n- 데이터 재분배: 조인, 그룹화, 집계 및 재분할과 같은 넓은 변환을 용이하게 합니다.\n- 부하 분산: 클러스터 전체에 데이터와 작업 부담을 균등하게 분배합니다.\n- 동시성: 병렬 처리를 강화하고 리소스 활용을 최적화합니다.\n- 데이터 지역성 최적화: 데이터가 처리될 위치에 가까이 이동하도록 합니다. 네트워크 트래픽을 줄입니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 셔플링의 고민거리\n\n- 자원 소모가 많음: 상당한 네트워크 대역폭과 디스크 I/O를 사용합니다.\n- 지연 시간 증가: 특히 대량의 데이터셋인 경우 처리 시간이 상당히 증가합니다.\n- 병목 현상 발생 가능성: 적절히 관리되지 않으면 전체 시스템 성능을 느리게 만들 수 있습니다.\n\n## 1. 작은 DataFrame과 큰 DataFrame을 조인할 때 데이터 셔플링을 최소화하기 위해 브로드캐스트 조인을 사용합니다.\n\n```js\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import broadcast\n\n# 스파크 세션 초기화\nspark = SparkSession.builder.appName(\"브로드캐스트 조인 예제\").getOrCreate()\n# 직원용 큰 DataFrame 생성\ndata_employees = [(1, \"John\", 101),\n                  (2, \"Jane\", 102),\n                  (3, \"Joe\", 103),\n                  (4, \"Jill\", 101),\n                  # 더 많은 레코드가 있다고 가정\n                  ]\ncolumns_employees = [\"emp_id\", \"name\", \"dept_id\"]\ndf_employees = spark.createDataFrame(data_employees, columns_employees)\n# 부서용 작은 DataFrame 생성\ndata_departments = [(101, \"인사\"),\n                    (102, \"마케팅\"),\n                    (103, \"금융\"),\n                    (104, \"IT\"),\n                    (105, \"지원\")\n                    ]\ncolumns_departments = [\"dept_id\", \"dept_name\"]\ndf_departments = spark.createDataFrame(data_departments, columns_departments)\n# 브로드캐스트 조인 수행\ndf_joined = df_employees.join(broadcast(df_departments), \"dept_id\")\ndf_joined.show()\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n#출력\n+-------+------+----+---------+\n|dept_id|emp_id|  이름|dept_name|\n+-------+------+-----+--------+\n|    101|     1| 존|       인사|\n|    102|     2| 제인|     마케팅|\n|    103|     3| 조|     금융|\n|    101|     4| 질|       인사|\n+-------+------+-----+--------+\n```\n\n- 직원 — 직원 세부 정보를 담은 작은 데이터셋입니다.\n- 부서 — 부서 세부 정보를 담은 큰 데이터셋입니다.\n\n두 데이터셋을 부서 ID를 기준으로 조인하되, 부서 데이터셋을 크게 섞지 않도록 하는 것이 목표입니다.\n\n## 2. 파티션 조정: 병렬성 증가를 위한 다시 분할\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 데이터 프레임의 파티션을 재분할하여 병렬성을 높이거나 셔플 비용을 줄일 수 있습니다.\n- 그러나 여전히 전체 셔플을 유발할 수 있습니다.\n\n```js\n# 병렬성을 높이기 위한 재분할 예제\ndf = spark.createDataFrame([\n  (1, 'foo'), (2, 'bar'), (3, 'baz'), (4, 'qux')\n], [\"id\", \"value\"])\ndf_repartitioned = df.repartition(10)  # 파티션 수 증가\n```\n\n## 3. 파티션 튜닝: 파티션 감소를 위한 Coalesce\n\n- 전체 셔플 피하기: coalesce는 대규모 데이터 세트를 필터링한 후 파티션 수를 줄이고 싶을 때 셔플 비용을 피해야 할 때 최적입니다.\n- 전형적인 사용 사례: 많은 파티션이 부분적으로 채워지거나 비어있는 상태로 남을 수 있는 대규모 DataFrame을 필터링한 후 사용됩니다. coalesce는 네트워크 오버헤드를 줄이고 비용 효율적으로 리소스를 관리하는 데 도움이 됩니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 대규모 셔플링 없이 파티션 수를 줄이기 위한 코어스 예시\ndf_filtered = df.filter(\"id > 1\")\ndf_coalesced = df_filtered.coalesce(2)  # 파티션 수 줄이기\n\n```\n\n## 4. 변환 최적화를 통해 데이터 셔플링 최소화하기\n\n- 최적화된 변환을 통해 Apache Spark에서 셔플링을 최소화하는 것은 Spark 애플리케이션의 성능을 향상시키는 중요한 측면입니다.\n- 변환 최적화는 데이터 처리 작업을 구조화하여 클러스터 전체에서 불필요한 데이터 이동을 줄이는 것을 포함하며, 이는 리소스를 많이 사용하고 실행 속도를 늦출 수 있습니다.\n- 이를 달성하는 방법을 보여주는 몇 가지 전략과 코드 예제는 다음과 같습니다:\n\n## 4–1. 일찍 필터링하기\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 데이터 처리 파이프라인에서 조인 또는 집계와 같은 후속 작업에서 나중에 섞어야 하는 데이터 양을 줄이기 위해 필터를 가능한 한 빨리 적용하세요.\n\n```python\nfrom pyspark.sql import SparkSession\n\n# 스파크 세션 초기화\nspark = SparkSession.builder.appName(\"Shuffling 최소화\").getOrCreate()\n\n# DataFrame 생성\ndata = [(\"John\", \"금융\", 3000), (\"Jane\", \"마케팅\", 4000), (\"Joe\", \"마케팅\", 2800), (\"Jill\", \"금융\", 3900)]\ncolumns = [\"이름\", \"부서\", \"연봉\"]\ndf = spark.createDataFrame(data, schema=columns)\n\n# 넓은 변환 전에 미리 필터링\nfiltered_df = df.filter(df[\"연봉\"] > 3000)\n\n# 이제 집계 수행\naggregated_df = filtered_df.groupBy(\"부서\").avg(\"연봉\")\naggregated_df.show()\n```\n\n```python\n# 출력\n+----------+-----------+\n|부서      |avg(연봉)  |\n+----------+-----------+\n| 마케팅  |     4000.0|\n| 금융    |     3900.0|\n+----------+-----------+\n```\n\n## 4-2. 가능한 경우 RDD/넓은 변환 사용하기\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 좁은 변환, 예를 들어 `map` 및 `filter`와 같은 작업은 개별 파티션에서 작동하며 데이터 셔플을 필요로하지 않습니다. 가능한 경우 이러한 작업을 넓은 변환 대신 사용하십시오.\n\n```js\n# 셔플을 발생시키지 않고 새로운 열을 만들기 위해 map을 사용합니다\nrdd = df.rdd.map(lambda x: (x.Name, x.Salary * 1.1))\nupdated_salaries_df = spark.createDataFrame(\n  rdd, schema=[\"Name\", \"UpdatedSalary\"]\n)\nupdated_salaries_df.show()\n```\n\n```js\n# 결과\n+----+------------------+\n|Name|     UpdatedSalary|\n+----+------------------+\n|John|3300.0000000000005|\n|Jane|            4400.0|\n| Joe|3080.0000000000005|\n|Jill|            4290.0|\n+----+------------------+\n```\n\n## 4-3. Boardcasting join으로 불필요한 셔플을 피하세요\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 조인 작업시, 한 데이터셋이 다른 데이터셋보다 현저히 작을 때 브로드캐스트 조인을 사용하여 큰 데이터셋을 셔플링하지 않도록 합니다.\n\n```js\nfrom pyspark.sql.functions import broadcast\n# df_small이 df_large보다 훨씬 작다고 가정합니다\ndf_small = spark.createDataFrame(\n  [(1, \"HR\"), (2, \"마케팅\")], [\"id\", \"부서\"]\n)\ndf_large = spark.createDataFrame(\n  [(1, \"존\"), (2, \"제인\"), (1, \"조\"), (2, \"질\")],\n  [\"부서ID\", \"이름\"]\n)\n# 조인 최적화를 위해 작은 DataFrame을 브로드캐스트합니다\noptimized_join_df = df_large.join(broadcast(df_small), df_large.부서ID == df_small.id)\noptimized_join_df.show()\n```\n\n```js\n# 결과\n\n+------+----+---+------+\n|부서ID|이름| id|  부서|\n+------+----+---+------+\n|     1| 존|  1|   HR|\n|     2|제인|  2|마케팅|\n|     1| 조|  1|   HR|\n|     2| 질|  2|마케팅|\n+------+----+---+------+\n```\n\n## 4-4. 전략적으로 파티션 나누기\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 만약 넓은 변환을 사용해야 한다면, 나중에 조인 또는 집계할 키를 기반으로 데이터를 파티션으로 나누세요. 이 전략을 사용하면 동일한 키를 가진 행을 동일한 파티션에 함께 두어 셔플링을 줄일 수 있습니다.\n\n```js\n# 셔플링 최소화를 위해 집계 전에 파티션 재분배\nrepartitioned_df = df.repartition(\"Department\")\naggregated_df = repartitioned_df.groupBy(\"Department\").avg(\"Salary\")\naggregated_df.show()\n```\n\n```js\n# 결과\n+----------+-----------+\n|Department|avg(Salary)|\n+----------+-----------+\n|   Finance|     3450.0|\n| Marketing|     3400.0|\n+----------+-----------+\n```\n\n# 성능 모니터링 및 세부 조정\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 1. 메모리 관리\n\n```js\nspark.conf.set(“spark.executor.memory”, “4g”)\nspark.conf.set(“spark.driver.memory”, “2g”)\n```\n\n## 2. 작업 및 스테이지 모니터링\n\n- Spark UI를 사용하여 응용 프로그램 내의 작업 및 스테이지의 성능을 모니터링합니다.\n- Spark UI에 액세스하려면 다음으로 이동하십시오: http://[your-spark-driver-host]:4040\n- Executor 메트릭 분석: 각 executor의 메트릭을 모니터링하여 메모리 사용, 디스크 스피릴 및 가비지 수집에 대한 통찰을 얻을 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 자세한 실행자 지표를 수집하도록 Spark를 구성합니다\nspark.conf.set(\"spark.executor.metrics.pollingInterval\", \"5000\")\n```\n\n## 3. SQL 성능 튜닝\n\n- SQL 실행 계획을 이해하고 최적화하기 위해 `EXPLAIN` 계획을 활용하세요.\n\n```js\ndf.explain(“formatted”)\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n== 물리적인 계획 ==\n* 기존 RDD 스캔 (1)\n\n\n(1) 기존 RDD 스캔 [코드 생성 ID : 1]\n출력 [3]: [이름 #4628, 부서 #4629, 급여 #4630L]\n인수: [이름 #4628, 부서 #4629, 급여 #4630L],\n applySchemaToPythonRDD에 있는 MapPartitionsRDD[693]에서\n                       at <알 수 없음>:0, ExistingRDD, UnknownPartitioning(0)\n```\n\n## 4. 동적 할당\n\n- 워크로드에 따라 스파크가 실행자 수를 동적으로 조정할 수 있도록 동적 할당을 활성화합니다.\n\n```js\nspark.conf.set(\"spark.dynamicAllocation.enabled\", \"true\");\nspark.conf.set(\"spark.dynamicAllocation.minExecutors\", \"1\");\nspark.conf.set(\"spark.dynamicAllocation.maxExecutors\", \"20\");\nspark.conf.set(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\");\nspark.conf.set(\"spark.shuffle.service.enabled\", \"true\");\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 5. 데이터 지역성\n\n- 저장 및 처리 장치 간에 데이터가 이동해야 하는 거리를 최소화하여 데이터 지역성을 최적화합니다.\n\n```js\nspark.conf.set(\"spark.locality.wait\", \"300ms\");\n```\n\n## 6. Garbage Collection Tuning\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n- 가비지 컬렉터 설정을 조정하여 메모리 관리를 최적화하고 일시 중지 시간을 줄일 수 있습니다.\n\n# 더 나은 지연 시간을 위해 G1GC 사용\nspark.conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\")\n# 짧은 일시 중지를 위해 명시적인 GC 설정 구성\nspark.conf.set(\"spark.executor.extraJavaOptions\", \"-XX:MaxGCPauseMillis=100\")\n\n## 7. 데이터 직렬화 세부 조정\n\n- 데이터 직렬화는 분산 애플리케이션의 성능에 중요한 역할을 합니다. Spark는 두 가지 직렬화 도구를 지원합니다:\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 더 나은 성능과 효율성을 위해 Kryo 직렬화 프로그램 사용\nspark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\nspark.conf.set(\"spark.kryo.registrationRequired\", \"true\")\n\n# Kryo와 사용자 정의 클래스 등록\nclass MyClass:\n    def __init__(self, name, id):\n        self.name = name\n        self.id = id\nspark.sparkContext.getConf().registerKryoClasses([MyClass])\n```\n\n## 8. 네트워크 구성 최적화\n\n- 네트워크 설정은 특히 대규모 배포에서 성능에 중대한 영향을 미칠 수 있습니다:\n\n```js\n# 네트워크 타임아웃 설정을 조정하여 대규모 클러스터에서 불필요한 작업 실패를 피하십시오\nspark.conf.set(\"spark.network.timeout\", \"800s\")\nspark.conf.set(\"spark.core.connection.ack.wait.timeout\", \"600s\")\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n## 9. 고급 Spark SQL 튜닝\n\n- Catalyst 옵티마이저 및 Tungsten 실행 엔진을 활용하면 Spark SQL의 성능을 향상시킬 수 있습니다:\n\n```js\n# 직렬 처리를 위한 전체 단계 코드 생성 활성화\nspark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n\n# 조인 최적화에 유용한 테이블 브로드캐스트를 위한 최대 바이트 수 증가\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10 MB\n```\n\n## 10. 데이터 파티셔닝 최적화\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n데이터 분배를 세밀하게 조정하여 쿼리 성능을 향상시키고 셔플 오버헤드를 줄일 수 있어요:\n\n```js\n# 데이터 크기 및 작업을 기준으로 수동으로 셔플 파티션 수 설정\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n# 클러스터 크기 및 데이터에 맞게 조정하세요\n```\n\n## 11. 적응형 쿼리 실행 활성화\n\n- 적응형 쿼리 실행 (AQE)는 실행 중에 쿼리 계획을 조정함으로써 Spark SQL 쿼리를 더 빠르고 데이터 스쿠 및 기타 이슈에 더 강건하게 만드는 기능이에요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n# 쿼리 실행을 적응적으로 조정하는 AQE를 활성화합니다. 이는 구성을 간소화하고 성능을 향상시킬 수 있습니다.\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n```\n\n- AQE는 실제 데이터에 적응해 셔플 파티셔닝을 조정하고, 불균형 조인을 처리하며, 정렬을 최적화할 수 있습니다.\n\n## 12. 메모리 관리 지정\n\n- 적절한 메모리 관리는 메모리 집약적인 작업에서 특히 효과적인 성능 개선을 위해 스파이지를 방지할 수 있습니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n# RDD 저장소에 예약된 메모리 분수를 구성합니다.\n\nspark.conf.set(“spark.memory.fraction”, “0.6”)\nspark.conf.set(“spark.memory.storageFraction”, “0.5”)\n\n이러한 설정은 실행 메모리와 저장소 메모리 사이의 균형을 맞추어 셔플 및 캐싱 중 디스크 스파일을 줄이는 데 도움이 됩니다.\n\n# 읽어 주셔서 감사합니다\n\n이 글이 마음에 드시면:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 👏 여러 번 박수로 지지를 보여주세요!\n- 이 안내서를 친구들과 공유해도 좋아요.\n- 여러분의 피드백은 소중합니다. 앞으로의 글에 영감을 주고 안내해 줍니다.\n- 또는 메시지를 남겨주세요: https://www.linkedin.com/in/kevinchwong\n","ogImage":{"url":"/assets/img/2024-07-09-IntensiveSparkOptimizationCourse_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-IntensiveSparkOptimizationCourse_0.png","tag":["Tech"],"readingTime":43},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<p><img src=\"/TIL/assets/img/2024-07-09-IntensiveSparkOptimizationCourse_0.png\" alt=\"Intensive Spark Optimization Course\"></p>\n<h1>로컬에서 플레이그라운드 설정하기</h1>\n<ul>\n<li>Docker Desktop을 설치합니다.</li>\n<li><code>docker run -p 8888:8888 jupyter/pyspark-notebook</code>을 실행합니다.</li>\n<li>다음 메시지가 표시되면 브라우저에서 주피터 랩을 열기 위해 URL 중 하나를 붙여넣습니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\">서버에 액세스하려면 브라우저에서 이 파일을 엽니다:\n    <span class=\"hljs-attr\">file</span>:<span class=\"hljs-comment\">///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html</span>\n또는 다음 <span class=\"hljs-variable constant_\">URL</span> 중 하나를 복사하여 붙여넣습니다:\n    <span class=\"hljs-attr\">http</span>:<span class=\"hljs-comment\">//3c331b638888:8888/lab?token=a88888b6aa6620fc976588ba58817f3b14ea0674bdc77f72</span>\n    <span class=\"hljs-attr\">http</span>:<span class=\"hljs-comment\">//127.0.0.1:8888/lab?token=a88888b6aa6620fc976588ba58817f3b14ea0674bdc77f72</span>\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h1>SparkSession 초기화하기</h1>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">SparkSession</span>\nspark = <span class=\"hljs-title class_\">SparkSession</span>.<span class=\"hljs-property\">builder</span>.<span class=\"hljs-title function_\">appName</span>(<span class=\"hljs-string\">\"Spark Test\"</span>).<span class=\"hljs-title function_\">getOrCreate</span>()\n</code></pre>\n<h1>데이터프레임</h1>\n<h2>1. 데이터프레임 생성하기</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>컬럼 사용</li>\n</ul>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">types</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">StructType</span>, <span class=\"hljs-title class_\">IntegerType</span>, <span class=\"hljs-title class_\">StringType</span>\n\n# 데이터를 튜플의 리스트로 정의\ndata = [(<span class=\"hljs-string\">\"James\"</span>, <span class=\"hljs-number\">34</span>), (<span class=\"hljs-string\">\"Anna\"</span>, <span class=\"hljs-number\">20</span>), (<span class=\"hljs-string\">\"Lee\"</span>, <span class=\"hljs-number\">30</span>)]\n\n# 컬럼 사용\ncolumns = [<span class=\"hljs-string\">\"Name\"</span>, <span class=\"hljs-string\">\"Age\"</span>]\ndf = spark.<span class=\"hljs-title function_\">createDataFrame</span>(data, schema=columns)\n</code></pre>\n<ul>\n<li>스키마 사용</li>\n</ul>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">types</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">StructType</span>, <span class=\"hljs-title class_\">IntegerType</span>, <span class=\"hljs-title class_\">StringType</span>\n\n# 데이터를 튜플의 리스트로 정의\ndata = [(<span class=\"hljs-string\">\"James\"</span>, <span class=\"hljs-number\">34</span>), (<span class=\"hljs-string\">\"Anna\"</span>, <span class=\"hljs-number\">20</span>), (<span class=\"hljs-string\">\"Lee\"</span>, <span class=\"hljs-number\">30</span>)]\n\n# 스키마 사용\nschema = <span class=\"hljs-title class_\">StructType</span>([\n    <span class=\"hljs-title class_\">StructField</span>(<span class=\"hljs-string\">\"Name\"</span>, <span class=\"hljs-title class_\">StringType</span>(), <span class=\"hljs-title class_\">True</span>),\n    <span class=\"hljs-title class_\">StructField</span>(<span class=\"hljs-string\">\"Age\"</span>, <span class=\"hljs-title class_\">IntegerType</span>(), <span class=\"hljs-title class_\">True</span>)\n])\ndf = spark.<span class=\"hljs-title function_\">createDataFrame</span>(data, schema=schema)\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>RDD 사용</li>\n</ul>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">types</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">StructType</span>, <span class=\"hljs-title class_\">IntegerType</span>, <span class=\"hljs-title class_\">StringType</span>\n\n# 데이터를 튜플의 리스트로 준비\ndata = [(<span class=\"hljs-string\">\"James\"</span>, <span class=\"hljs-number\">34</span>), (<span class=\"hljs-string\">\"Anna\"</span>, <span class=\"hljs-number\">20</span>), (<span class=\"hljs-string\">\"Lee\"</span>, <span class=\"hljs-number\">30</span>)]\n\n# <span class=\"hljs-variable constant_\">RDD</span> 사용\nrdd = spark.<span class=\"hljs-property\">sparkContext</span>.<span class=\"hljs-title function_\">parallelize</span>(data)\nschema = <span class=\"hljs-title class_\">StructType</span>([\n    <span class=\"hljs-title class_\">StructField</span>(<span class=\"hljs-string\">\"이름\"</span>, <span class=\"hljs-title class_\">StringType</span>(), <span class=\"hljs-title class_\">True</span>),\n    <span class=\"hljs-title class_\">StructField</span>(<span class=\"hljs-string\">\"나이\"</span>, <span class=\"hljs-title class_\">IntegerType</span>(), <span class=\"hljs-title class_\">True</span>)\n])\ndf = spark.<span class=\"hljs-title function_\">createDataFrame</span>(rdd, schema=schema)\n</code></pre>\n<pre><code class=\"hljs language-js\">df.<span class=\"hljs-title function_\">show</span>()\n\n# 출력\n+-----+---+\n| 이름|나이|\n+-----+---+\n|<span class=\"hljs-title class_\">James</span>| <span class=\"hljs-number\">34</span>|\n| <span class=\"hljs-title class_\">Anna</span>| <span class=\"hljs-number\">20</span>|\n|  <span class=\"hljs-title class_\">Lee</span>| <span class=\"hljs-number\">30</span>|\n+-----+---+\n</code></pre>\n<h2>2. 데이터프레임 표시</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\">df.<span class=\"hljs-title function_\">printSchema</span>();\n<span class=\"hljs-title function_\">print</span>(df.<span class=\"hljs-property\">schema</span>);\n<span class=\"hljs-title function_\">print</span>(df.<span class=\"hljs-property\">columns</span>);\ndf.<span class=\"hljs-title function_\">describe</span>().<span class=\"hljs-title function_\">show</span>();\n</code></pre>\n<pre><code class=\"hljs language-js\">#결과\n\n## df.<span class=\"hljs-title function_\">printSchema</span>()\nroot\n |-- <span class=\"hljs-title class_\">Name</span>: string (nullable = <span class=\"hljs-literal\">true</span>)\n |-- <span class=\"hljs-title class_\">Age</span>: long (nullable = <span class=\"hljs-literal\">true</span>)\n\n## <span class=\"hljs-title function_\">print</span>(df.<span class=\"hljs-property\">schema</span>)\n<span class=\"hljs-title class_\">StructType</span>([\n  <span class=\"hljs-title class_\">StructField</span>(‘<span class=\"hljs-title class_\">Name</span>’, <span class=\"hljs-title class_\">StringType</span>(), <span class=\"hljs-title class_\">True</span>),\n  <span class=\"hljs-title class_\">StructField</span>(‘<span class=\"hljs-title class_\">Age</span>’, <span class=\"hljs-title class_\">LongType</span>(), <span class=\"hljs-title class_\">True</span>)\n])\n\n## <span class=\"hljs-title function_\">print</span>(df.<span class=\"hljs-property\">columns</span>)\n[‘<span class=\"hljs-title class_\">Name</span>’, ‘<span class=\"hljs-title class_\">Age</span>’]\n\n## df.<span class=\"hljs-title function_\">describe</span>().<span class=\"hljs-title function_\">show</span>()\n+-------+----+-----------------+\n|summary|<span class=\"hljs-title class_\">Name</span>|              <span class=\"hljs-title class_\">Age</span>|\n+-------+----+-----------------+\n|  count|   <span class=\"hljs-number\">3</span>|                <span class=\"hljs-number\">3</span>|\n|   mean|<span class=\"hljs-variable constant_\">NULL</span>|             <span class=\"hljs-number\">28.0</span>|\n| stddev|<span class=\"hljs-variable constant_\">NULL</span>|<span class=\"hljs-number\">7.211102550927978</span>|\n|    min|<span class=\"hljs-title class_\">Anna</span>|               <span class=\"hljs-number\">20</span>|\n|    max| <span class=\"hljs-title class_\">Lee</span>|               <span class=\"hljs-number\">34</span>|\n+-------+----+-----------------+\n</code></pre>\n<h2>3. 컬럼 선택</h2>\n<pre><code class=\"hljs language-js\">df.<span class=\"hljs-title function_\">select</span>(df[<span class=\"hljs-number\">0</span>]).<span class=\"hljs-title function_\">show</span>();\ndf.<span class=\"hljs-title function_\">select</span>(df.<span class=\"hljs-property\">Name</span>).<span class=\"hljs-title function_\">show</span>();\ndf.<span class=\"hljs-title function_\">select</span>(df[<span class=\"hljs-string\">\"Name\"</span>]).<span class=\"hljs-title function_\">show</span>();\ndf.<span class=\"hljs-title function_\">select</span>(<span class=\"hljs-string\">\"Name\"</span>).<span class=\"hljs-title function_\">show</span>();\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\">#<span class=\"hljs-title class_\">Output</span>\n+-----+\n| <span class=\"hljs-title class_\">Name</span>|\n+-----+\n|<span class=\"hljs-title class_\">James</span>|\n| <span class=\"hljs-title class_\">Anna</span>|\n|  <span class=\"hljs-title class_\">Lee</span>|\n+-----+\n</code></pre>\n<h2>4. 데이터 필터링</h2>\n<pre><code class=\"hljs language-js\"># 데이터 필터링\ndf.<span class=\"hljs-title function_\">filter</span>(df[<span class=\"hljs-number\">1</span>] > <span class=\"hljs-number\">25</span>).<span class=\"hljs-title function_\">show</span>()\ndf.<span class=\"hljs-title function_\">filter</span>(df.<span class=\"hljs-property\">Age</span> > <span class=\"hljs-number\">25</span>).<span class=\"hljs-title function_\">show</span>()\ndf.<span class=\"hljs-title function_\">filter</span>(df[<span class=\"hljs-string\">\"Age\"</span>] > <span class=\"hljs-number\">25</span>).<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<pre><code class=\"hljs language-js\">#<span class=\"hljs-title class_\">Output</span>\n+-----+---+\n| <span class=\"hljs-title class_\">Name</span>|<span class=\"hljs-title class_\">Age</span>|\n+-----+---+\n|<span class=\"hljs-title class_\">James</span>| <span class=\"hljs-number\">34</span>|\n|  <span class=\"hljs-title class_\">Lee</span>| <span class=\"hljs-number\">30</span>|\n+-----+---+\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>5. 파일에 DataFrame 작성하기</h2>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-title class_\">JSON</span> 파일 작성\ndf.<span class=\"hljs-property\">write</span>.<span class=\"hljs-title function_\">json</span>(<span class=\"hljs-string\">\"test123.json\"</span>)\n\n# <span class=\"hljs-title class_\">Parquet</span> 파일 작성\ndf.<span class=\"hljs-property\">write</span>.<span class=\"hljs-title function_\">parquet</span>(<span class=\"hljs-string\">\"test123.parquet\"</span>)\n</code></pre>\n<h2>6. 파일을 DataFrame으로 읽기</h2>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-title class_\">JSON</span> 파일 읽기\ndf_json = spark.<span class=\"hljs-property\">read</span>.<span class=\"hljs-title function_\">json</span>(<span class=\"hljs-string\">\"test123.json\"</span>)\n# <span class=\"hljs-title class_\">Parquet</span> 파일 읽기\ndf_parquet = spark.<span class=\"hljs-property\">read</span>.<span class=\"hljs-title function_\">parquet</span>(<span class=\"hljs-string\">\"test123.parquet\"</span>)\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>7. 새로운 복합 데이터 유형을 포함한 새로운 열 추가</h2>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> struct\ndf2 = df.<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">\"NameAndAge\"</span>, <span class=\"hljs-title function_\">struct</span>(df.<span class=\"hljs-property\">Name</span>, df.<span class=\"hljs-property\">Age</span>))\ndf2.<span class=\"hljs-title function_\">show</span>()\ndf2.<span class=\"hljs-title function_\">printSchema</span>()\n</code></pre>\n<pre><code class=\"hljs language-js\"># 출력\n+-----+---+-----------+\n| <span class=\"hljs-title class_\">Name</span>|<span class=\"hljs-title class_\">Age</span>| <span class=\"hljs-title class_\">NameAndAge</span>|\n+-----+---+-----------+\n|<span class=\"hljs-title class_\">James</span>| <span class=\"hljs-number\">34</span>|{<span class=\"hljs-title class_\">James</span>, <span class=\"hljs-number\">34</span>}|\n| <span class=\"hljs-title class_\">Anna</span>| <span class=\"hljs-number\">20</span>| {<span class=\"hljs-title class_\">Anna</span>, <span class=\"hljs-number\">20</span>}|\n|  <span class=\"hljs-title class_\">Lee</span>| <span class=\"hljs-number\">30</span>|  {<span class=\"hljs-title class_\">Lee</span>, <span class=\"hljs-number\">30</span>}|\n+-----+---+-----------+\n\n# 스키마 출력\nroot\n |-- <span class=\"hljs-title class_\">Name</span>: string (nullable = <span class=\"hljs-literal\">true</span>)\n |-- <span class=\"hljs-title class_\">Age</span>: long (nullable = <span class=\"hljs-literal\">true</span>)\n |-- <span class=\"hljs-title class_\">NameAndAge</span>: struct (nullable = <span class=\"hljs-literal\">false</span>)\n |    |-- <span class=\"hljs-title class_\">Name</span>: string (nullable = <span class=\"hljs-literal\">true</span>)\n |    |-- <span class=\"hljs-title class_\">Age</span>: long (nullable = <span class=\"hljs-literal\">true</span>)\n</code></pre>\n<h1>쿼리: 그룹화 및 집계</h1>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>1. count()</h2>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> pyspark.sql <span class=\"hljs-keyword\">import</span> SparkSession\n<span class=\"hljs-keyword\">from</span> pyspark.sql.functions <span class=\"hljs-keyword\">import</span> count\n\n<span class=\"hljs-comment\"># 스파크 세션 초기화</span>\nspark = SparkSession.builder.appName(<span class=\"hljs-string\">\"그루핑 및 집계\"</span>).getOrCreate()\n\n<span class=\"hljs-comment\"># 데이터프레임 생성</span>\ndata = [(<span class=\"hljs-string\">\"James\"</span>, <span class=\"hljs-string\">\"Sales\"</span>, <span class=\"hljs-number\">3000</span>),\n        (<span class=\"hljs-string\">\"Michael\"</span>, <span class=\"hljs-string\">\"Sales\"</span>, <span class=\"hljs-number\">4600</span>),\n        (<span class=\"hljs-string\">\"Robert\"</span>, <span class=\"hljs-string\">\"Sales\"</span>, <span class=\"hljs-number\">4100</span>),\n        (<span class=\"hljs-string\">\"Maria\"</span>, <span class=\"hljs-string\">\"Finance\"</span>, <span class=\"hljs-number\">3000</span>),\n        (<span class=\"hljs-string\">\"James\"</span>, <span class=\"hljs-string\">\"Sales\"</span>, <span class=\"hljs-number\">3000</span>),\n        (<span class=\"hljs-string\">\"Scott\"</span>, <span class=\"hljs-string\">\"Finance\"</span>, <span class=\"hljs-number\">3300</span>),\n        (<span class=\"hljs-string\">\"Jen\"</span>, <span class=\"hljs-string\">\"Finance\"</span>, <span class=\"hljs-number\">3900</span>),\n        (<span class=\"hljs-string\">\"Jeff\"</span>, <span class=\"hljs-string\">\"Marketing\"</span>, <span class=\"hljs-number\">3000</span>),\n        (<span class=\"hljs-string\">\"Kumar\"</span>, <span class=\"hljs-string\">\"Marketing\"</span>, <span class=\"hljs-number\">2000</span>),\n        (<span class=\"hljs-string\">\"Saif\"</span>, <span class=\"hljs-string\">\"Sales\"</span>, <span class=\"hljs-number\">4100</span>)]\ncolumns = [<span class=\"hljs-string\">\"employee_name\"</span>, <span class=\"hljs-string\">\"department\"</span>, <span class=\"hljs-string\">\"salary\"</span>]\ndf = spark.createDataFrame(data, schema=columns)\n\n<span class=\"hljs-comment\"># 그룹화 및 count 수행</span>\ngrouped_df = df.groupBy(<span class=\"hljs-string\">\"department\"</span>).count()\ngrouped_df.show()\n</code></pre>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># 결과</span>\n\n+----------+-----+\n|department|count|\n+----------+-----+\n|     Sales|    <span class=\"hljs-number\">5</span>|\n|   Finance|    <span class=\"hljs-number\">3</span>|\n| Marketing|    <span class=\"hljs-number\">2</span>|\n+----------+-----+\n</code></pre>\n<h2>2. max(), min(), avg(), sum()</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\"># 그룹별로 그룹화 및 최댓값 찾기\nmax_df = df.<span class=\"hljs-title function_\">groupBy</span>(<span class=\"hljs-string\">\"department\"</span>).<span class=\"hljs-title function_\">max</span>(<span class=\"hljs-string\">\"salary\"</span>).<span class=\"hljs-title function_\">alias</span>(<span class=\"hljs-string\">\"max_salary\"</span>)\nmax_df.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<pre><code class=\"hljs language-js\"># 결과\n\n+----------+-----------+\n|department|<span class=\"hljs-title function_\">max</span>(salary)|\n+----------+-----------+\n|     <span class=\"hljs-title class_\">Sales</span>|       <span class=\"hljs-number\">4600</span>|\n|   <span class=\"hljs-title class_\">Finance</span>|       <span class=\"hljs-number\">3900</span>|\n| <span class=\"hljs-title class_\">Marketing</span>|       <span class=\"hljs-number\">3000</span>|\n+----------+-----------+\n</code></pre>\n<h2>3. agg() + F.max(), F.count() 등…</h2>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span> <span class=\"hljs-keyword\">import</span> functions <span class=\"hljs-keyword\">as</span> F\n\n# 여러 가지 집계 동작 수행\nagg_df = df.<span class=\"hljs-title function_\">groupBy</span>(<span class=\"hljs-string\">\"department\"</span>).<span class=\"hljs-title function_\">agg</span>(\n    F.<span class=\"hljs-title function_\">count</span>(<span class=\"hljs-string\">\"salary\"</span>).<span class=\"hljs-title function_\">alias</span>(<span class=\"hljs-string\">\"count\"</span>),\n    F.<span class=\"hljs-title function_\">max</span>(<span class=\"hljs-string\">\"salary\"</span>).<span class=\"hljs-title function_\">alias</span>(<span class=\"hljs-string\">\"max_salary\"</span>),\n    F.<span class=\"hljs-title function_\">min</span>(<span class=\"hljs-string\">\"salary\"</span>).<span class=\"hljs-title function_\">alias</span>(<span class=\"hljs-string\">\"min_salary\"</span>),\n    F.<span class=\"hljs-title function_\">sum</span>(<span class=\"hljs-string\">\"salary\"</span>).<span class=\"hljs-title function_\">alias</span>(<span class=\"hljs-string\">\"total_salary\"</span>),\n    F.<span class=\"hljs-title function_\">avg</span>(<span class=\"hljs-string\">\"salary\"</span>).<span class=\"hljs-title function_\">alias</span>(<span class=\"hljs-string\">\"average_salary\"</span>)\n)\nagg_df.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\"># 결과\n+----------+-----+----------+----------+------------+--------------+\n|department|count|max_salary|min_salary|total_salary|average_salary|\n+----------+-----+----------+----------+------------+--------------+\n|     <span class=\"hljs-title class_\">Sales</span>|    <span class=\"hljs-number\">5</span>|      <span class=\"hljs-number\">4600</span>|      <span class=\"hljs-number\">3000</span>|       <span class=\"hljs-number\">18800</span>|        <span class=\"hljs-number\">3760.0</span>|\n|   <span class=\"hljs-title class_\">Finance</span>|    <span class=\"hljs-number\">3</span>|      <span class=\"hljs-number\">3900</span>|      <span class=\"hljs-number\">3000</span>|       <span class=\"hljs-number\">10200</span>|        <span class=\"hljs-number\">3400.0</span>|\n| <span class=\"hljs-title class_\">Marketing</span>|    <span class=\"hljs-number\">2</span>|      <span class=\"hljs-number\">3000</span>|      <span class=\"hljs-number\">2000</span>|        <span class=\"hljs-number\">5000</span>|        <span class=\"hljs-number\">2500.0</span>|\n+----------+-----+----------+----------+------------+--------------+\n</code></pre>\n<h2>4. agg() + collect_list() 및 collect_set()</h2>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> collect_list\n\n# <span class=\"hljs-title class_\">GroupBy</span> 및 리스트 수집 수행\ncollected_list_df = df.<span class=\"hljs-title function_\">groupBy</span>(<span class=\"hljs-string\">\"department\"</span>).<span class=\"hljs-title function_\">agg</span>(\n  <span class=\"hljs-title function_\">collect_list</span>(<span class=\"hljs-string\">\"salary\"</span>),\n  <span class=\"hljs-title function_\">collect_set</span>(<span class=\"hljs-string\">\"salary\"</span>)\n)\ncollected_list_df.<span class=\"hljs-title function_\">show</span>(truncate=<span class=\"hljs-title class_\">False</span>)\n</code></pre>\n<pre><code class=\"hljs language-js\"># 결과\n+----------+------------------------------+-------------------+\n|department|<span class=\"hljs-title function_\">collect_list</span>(salary)          |<span class=\"hljs-title function_\">collect_set</span>(salary)|\n+----------+------------------------------+-------------------+\n|<span class=\"hljs-title class_\">Sales</span>     |[<span class=\"hljs-number\">3000</span>, <span class=\"hljs-number\">4600</span>, <span class=\"hljs-number\">4100</span>, <span class=\"hljs-number\">3000</span>, <span class=\"hljs-number\">4100</span>]|[<span class=\"hljs-number\">4600</span>, <span class=\"hljs-number\">3000</span>, <span class=\"hljs-number\">4100</span>] |\n|<span class=\"hljs-title class_\">Finance</span>   |[<span class=\"hljs-number\">3000</span>, <span class=\"hljs-number\">3300</span>, <span class=\"hljs-number\">3900</span>]            |[<span class=\"hljs-number\">3900</span>, <span class=\"hljs-number\">3000</span>, <span class=\"hljs-number\">3300</span>] |\n|<span class=\"hljs-title class_\">Marketing</span> |[<span class=\"hljs-number\">3000</span>, <span class=\"hljs-number\">2000</span>]                  |[<span class=\"hljs-number\">3000</span>, <span class=\"hljs-number\">2000</span>]       |\n+----------+------------------------------+-------------------+\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>5. agg() + 사용자 정의 집계 함수 (UDAF)</h2>\n<ul>\n<li>때로는 내장 함수만으로 복잡한 집계를 수행하기에 충분하지 않을 수 있습니다. Spark를 사용하면 사용자 정의 집계 함수를 만들 수 있습니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\"># 초기 데이터\n|부서     |직원 이름        |급여    |\n|----------|-------------|------|\n|<span class=\"hljs-title class_\">Sales</span>    |<span class=\"hljs-title class_\">James</span>        |<span class=\"hljs-number\">3000</span>  |\n|<span class=\"hljs-title class_\">Sales</span>    |<span class=\"hljs-title class_\">Michael</span>      |<span class=\"hljs-number\">4600</span>  |\n|<span class=\"hljs-title class_\">Sales</span>    |<span class=\"hljs-title class_\">Robert</span>       |<span class=\"hljs-number\">4100</span>  |\n|<span class=\"hljs-title class_\">Finance</span>  |<span class=\"hljs-title class_\">Maria</span>        |<span class=\"hljs-number\">3000</span>  |\n|<span class=\"hljs-title class_\">Sales</span>    |<span class=\"hljs-title class_\">James</span>        |<span class=\"hljs-number\">3000</span>  |\n|<span class=\"hljs-title class_\">Finance</span>  |<span class=\"hljs-title class_\">Scott</span>        |<span class=\"hljs-number\">3300</span>  |\n|<span class=\"hljs-title class_\">Finance</span>  |<span class=\"hljs-title class_\">Jen</span>          |<span class=\"hljs-number\">3900</span>  |\n|<span class=\"hljs-title class_\">Marketing</span>|<span class=\"hljs-title class_\">Jeff</span>         |<span class=\"hljs-number\">3000</span>  |\n|<span class=\"hljs-title class_\">Marketing</span>|<span class=\"hljs-title class_\">Kumar</span>        |<span class=\"hljs-number\">2000</span>  |\n|<span class=\"hljs-title class_\">Sales</span>    |<span class=\"hljs-title class_\">Saif</span>         |<span class=\"hljs-number\">4100</span>  |\n</code></pre>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> pandas_udf, <span class=\"hljs-title class_\">PandasUDFType</span>\n<span class=\"hljs-keyword\">from</span> pandas <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">DataFrame</span>\n\n@<span class=\"hljs-title function_\">pandas_udf</span>(<span class=\"hljs-string\">\"double\"</span>)\ndef <span class=\"hljs-title function_\">mean_salary</span>(<span class=\"hljs-attr\">s</span>: pd.<span class=\"hljs-property\">Series</span>) -> <span class=\"hljs-attr\">float</span>:\n <span class=\"hljs-keyword\">return</span> s.<span class=\"hljs-title function_\">mean</span>()\nudaf_df = df.<span class=\"hljs-title function_\">groupBy</span>(<span class=\"hljs-string\">\"department\"</span>).<span class=\"hljs-title function_\">agg</span>(\n  <span class=\"hljs-title function_\">mean_salary</span>(df[<span class=\"hljs-string\">\"salary\"</span>]).<span class=\"hljs-title function_\">alias</span>(<span class=\"hljs-string\">\"average_salary\"</span>)\n)\nudaf_df.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\"># 결과\n+----------+--------------+\n|department|average_salary|\n+----------+--------------+\n|   <span class=\"hljs-title class_\">Finance</span>|        <span class=\"hljs-number\">3400.0</span>|\n| <span class=\"hljs-title class_\">Marketing</span>|        <span class=\"hljs-number\">2500.0</span>|\n|     <span class=\"hljs-title class_\">Sales</span>|        <span class=\"hljs-number\">3760.0</span>|\n+----------+--------------+\n\n## <span class=\"hljs-number\">6.</span> <span class=\"hljs-title function_\">agg</span>() + 복잡한 조건: <span class=\"hljs-title function_\">when</span>()\n\n- 때로는 조건에 따른 합계나 평균과 같은 복잡한 조건이 집계 중에 필요할 수 있습니다.\n\n# 초기 데이터\n+----------+-------------+------+\n|department|employee_name|salary|\n+----------+-------------+------+\n|     <span class=\"hljs-title class_\">Sales</span>|        <span class=\"hljs-title class_\">James</span>|  <span class=\"hljs-number\">3000</span>|\n|     <span class=\"hljs-title class_\">Sales</span>|      <span class=\"hljs-title class_\">Michael</span>|  <span class=\"hljs-number\">4600</span>|\n|     <span class=\"hljs-title class_\">Sales</span>|       <span class=\"hljs-title class_\">Robert</span>|  <span class=\"hljs-number\">4100</span>|\n|   <span class=\"hljs-title class_\">Finance</span>|        <span class=\"hljs-title class_\">Maria</span>|  <span class=\"hljs-number\">3000</span>|\n|     <span class=\"hljs-title class_\">Sales</span>|        <span class=\"hljs-title class_\">James</span>|  <span class=\"hljs-number\">3000</span>|\n|   <span class=\"hljs-title class_\">Finance</span>|        <span class=\"hljs-title class_\">Scott</span>|  <span class=\"hljs-number\">3300</span>|\n|   <span class=\"hljs-title class_\">Finance</span>|          <span class=\"hljs-title class_\">Jen</span>|  <span class=\"hljs-number\">3900</span>|\n| <span class=\"hljs-title class_\">Marketing</span>|         <span class=\"hljs-title class_\">Jeff</span>|  <span class=\"hljs-number\">3000</span>|\n| <span class=\"hljs-title class_\">Marketing</span>|        <span class=\"hljs-title class_\">Kumar</span>|  <span class=\"hljs-number\">2000</span>|\n|     <span class=\"hljs-title class_\">Sales</span>|         <span class=\"hljs-title class_\">Saif</span>|  <span class=\"hljs-number\">4100</span>|\n+----------+-------------+------+\n\n&#x3C;!-- <span class=\"hljs-variable constant_\">TIL</span> 수평 -->\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">ins</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">\"adsbygoogle\"</span>\n     <span class=\"hljs-attr\">style</span>=<span class=\"hljs-string\">\"display:block\"</span>\n     <span class=\"hljs-attr\">data-ad-client</span>=<span class=\"hljs-string\">\"ca-pub-4877378276818686\"</span>\n     <span class=\"hljs-attr\">data-ad-slot</span>=<span class=\"hljs-string\">\"1549334788\"</span>\n     <span class=\"hljs-attr\">data-ad-format</span>=<span class=\"hljs-string\">\"auto\"</span>\n     <span class=\"hljs-attr\">data-full-width-responsive</span>=<span class=\"hljs-string\">\"true\"</span>></span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">ins</span>></span></span>\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">script</span>></span><span class=\"javascript\">\n(adsbygoogle = <span class=\"hljs-variable language_\">window</span>.<span class=\"hljs-property\">adsbygoogle</span> || []).<span class=\"hljs-title function_\">push</span>({});\n</span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">script</span>></span></span>\n\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> when\n\n# 조건부 집계\nconditional_agg_df = df.<span class=\"hljs-title function_\">groupBy</span>(<span class=\"hljs-string\">\"department\"</span>).<span class=\"hljs-title function_\">agg</span>(\n    <span class=\"hljs-title function_\">sum</span>(<span class=\"hljs-title function_\">when</span>(df[<span class=\"hljs-string\">\"salary\"</span>] > <span class=\"hljs-number\">3000</span>, df[<span class=\"hljs-string\">\"salary\"</span>])).<span class=\"hljs-title function_\">alias</span>(<span class=\"hljs-string\">\"sum_high_salaries\"</span>)\n)\nconditional_agg_df.<span class=\"hljs-title function_\">show</span>()\n\n# 결과\n\n+----------+-----------------+\n|department|sum_high_salaries|\n+----------+-----------------+\n|     <span class=\"hljs-title class_\">Sales</span>|            <span class=\"hljs-number\">12800</span>|\n|   <span class=\"hljs-title class_\">Finance</span>|             <span class=\"hljs-number\">7200</span>|\n| <span class=\"hljs-title class_\">Marketing</span>|             <span class=\"hljs-variable constant_\">NULL</span>|\n+----------+-----------------+\n\n## <span class=\"hljs-number\">6.</span> <span class=\"hljs-title function_\">agg</span>() 이후 <span class=\"hljs-title class_\">GroupBy</span>에서 <span class=\"hljs-variable constant_\">RDD</span> <span class=\"hljs-title class_\">Map</span> 함수 사용하기\n\n- 경우에 따라 매핑 함수를 <span class=\"hljs-title class_\">GroupBy</span>와 결합하여 집단화된 데이터에 대한 직접적인 집계가 아닌 작업을 수행할 수 있습니다.\n\n&#x3C;!-- <span class=\"hljs-variable constant_\">TIL</span> 수평 -->\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">ins</span> <span class=\"hljs-attr\">class</span>=<span class=\"hljs-string\">\"adsbygoogle\"</span>\n     <span class=\"hljs-attr\">style</span>=<span class=\"hljs-string\">\"display:block\"</span>\n     <span class=\"hljs-attr\">data-ad-client</span>=<span class=\"hljs-string\">\"ca-pub-4877378276818686\"</span>\n     <span class=\"hljs-attr\">data-ad-slot</span>=<span class=\"hljs-string\">\"1549334788\"</span>\n     <span class=\"hljs-attr\">data-ad-format</span>=<span class=\"hljs-string\">\"auto\"</span>\n     <span class=\"hljs-attr\">data-full-width-responsive</span>=<span class=\"hljs-string\">\"true\"</span>></span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">ins</span>></span></span>\n<span class=\"xml\"><span class=\"hljs-tag\">&#x3C;<span class=\"hljs-name\">script</span>></span><span class=\"javascript\">\n(adsbygoogle = <span class=\"hljs-variable language_\">window</span>.<span class=\"hljs-property\">adsbygoogle</span> || []).<span class=\"hljs-title function_\">push</span>({});\n</span><span class=\"hljs-tag\">&#x3C;/<span class=\"hljs-name\">script</span>></span></span>\n\n</code></pre>\n<h1>데이터</h1>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>department</th><th>employee_name</th><th>salary</th></tr></thead><tbody><tr><td>Sales</td><td>James</td><td>3000</td></tr><tr><td>Sales</td><td>Michael</td><td>4600</td></tr><tr><td>Sales</td><td>Robert</td><td>4100</td></tr><tr><td>Finance</td><td>Maria</td><td>3000</td></tr><tr><td>Sales</td><td>James</td><td>3000</td></tr><tr><td>Finance</td><td>Scott</td><td>3300</td></tr><tr><td>Finance</td><td>Jen</td><td>3900</td></tr><tr><td>Marketing</td><td>Jeff</td><td>3000</td></tr><tr><td>Marketing</td><td>Kumar</td><td>2000</td></tr><tr><td>Sales</td><td>Saif</td><td>4100</td></tr></tbody></table>\n<h1>GroupBy 후 map 작업 적용</h1>\n<pre><code class=\"hljs language-python\">result_rdd = df.groupBy(<span class=\"hljs-string\">\"department\"</span>).agg(\n  collect_list(<span class=\"hljs-string\">\"salary\"</span>)\n).rdd.<span class=\"hljs-built_in\">map</span>(\n  <span class=\"hljs-keyword\">lambda</span> x: (x[<span class=\"hljs-number\">0</span>], <span class=\"hljs-built_in\">max</span>(x[<span class=\"hljs-number\">1</span>]))\n)\n\nresult_df = spark.createDataFrame(result_rdd, [<span class=\"hljs-string\">\"department\"</span>, <span class=\"hljs-string\">\"max_salary\"</span>])\nresult_df.show()\n</code></pre>\n<h1>결과</h1>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>department</th><th>max_salary</th></tr></thead><tbody><tr><td>Sales</td><td>4600</td></tr><tr><td>Finance</td><td>3900</td></tr><tr><td>Marketing</td><td>3000</td></tr></tbody></table>\n<h1>조회: 다른 것</h1>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>1. rollup()과 cube()</h2>\n<ul>\n<li>Rollup()은 다차원 집계를 생성하고 Excel의 소계와 유사한 계층적 요약을 제공합니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\"># 초기 데이터\n|department|employee_name|salary|\n|----------|-------------|------|\n| <span class=\"hljs-title class_\">Sales</span>    | <span class=\"hljs-title class_\">James</span>       | <span class=\"hljs-number\">3000</span> |\n| <span class=\"hljs-title class_\">Sales</span>    | <span class=\"hljs-title class_\">Michael</span>     | <span class=\"hljs-number\">4600</span> |\n| <span class=\"hljs-title class_\">Sales</span>    | <span class=\"hljs-title class_\">Robert</span>      | <span class=\"hljs-number\">4100</span> |\n| <span class=\"hljs-title class_\">Finance</span>  | <span class=\"hljs-title class_\">Maria</span>       | <span class=\"hljs-number\">3000</span> |\n| <span class=\"hljs-title class_\">Sales</span>    | <span class=\"hljs-title class_\">James</span>       | <span class=\"hljs-number\">3000</span> |\n| <span class=\"hljs-title class_\">Finance</span>  | <span class=\"hljs-title class_\">Scott</span>       | <span class=\"hljs-number\">3300</span> |\n| <span class=\"hljs-title class_\">Finance</span>  | <span class=\"hljs-title class_\">Jen</span>         | <span class=\"hljs-number\">3900</span> |\n| <span class=\"hljs-title class_\">Marketing</span>| <span class=\"hljs-title class_\">Jeff</span>        | <span class=\"hljs-number\">3000</span> |\n| <span class=\"hljs-title class_\">Marketing</span>| <span class=\"hljs-title class_\">Kumar</span>       | <span class=\"hljs-number\">2000</span> |\n| <span class=\"hljs-title class_\">Sales</span>    | <span class=\"hljs-title class_\">Saif</span>        | <span class=\"hljs-number\">4100</span> |\n</code></pre>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> sum\n\n# <span class=\"hljs-title class_\">Rollup</span> 예제\nrollup_df = df.<span class=\"hljs-title function_\">rollup</span>(<span class=\"hljs-string\">\"department\"</span>, <span class=\"hljs-string\">\"employee_name\"</span>).<span class=\"hljs-title function_\">sum</span>(<span class=\"hljs-string\">\"salary\"</span>)\nrollup_df.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\"># 결과\n+----------+-------------+-----------+\n|department|employee_name|<span class=\"hljs-title function_\">sum</span>(salary)|\n+----------+-------------+-----------+\n|     <span class=\"hljs-title class_\">Sales</span>|        <span class=\"hljs-title class_\">James</span>|       <span class=\"hljs-number\">6000</span>|\n|      <span class=\"hljs-variable constant_\">NULL</span>|         <span class=\"hljs-variable constant_\">NULL</span>|      <span class=\"hljs-number\">34000</span>|\n|     <span class=\"hljs-title class_\">Sales</span>|         <span class=\"hljs-variable constant_\">NULL</span>|      <span class=\"hljs-number\">18800</span>|\n|     <span class=\"hljs-title class_\">Sales</span>|      <span class=\"hljs-title class_\">Michael</span>|       <span class=\"hljs-number\">4600</span>|\n|     <span class=\"hljs-title class_\">Sales</span>|       <span class=\"hljs-title class_\">Robert</span>|       <span class=\"hljs-number\">4100</span>|\n|   <span class=\"hljs-title class_\">Finance</span>|         <span class=\"hljs-variable constant_\">NULL</span>|      <span class=\"hljs-number\">10200</span>|\n|   <span class=\"hljs-title class_\">Finance</span>|        <span class=\"hljs-title class_\">Maria</span>|       <span class=\"hljs-number\">3000</span>|\n|   <span class=\"hljs-title class_\">Finance</span>|        <span class=\"hljs-title class_\">Scott</span>|       <span class=\"hljs-number\">3300</span>|\n|   <span class=\"hljs-title class_\">Finance</span>|          <span class=\"hljs-title class_\">Jen</span>|       <span class=\"hljs-number\">3900</span>|\n| <span class=\"hljs-title class_\">Marketing</span>|         <span class=\"hljs-variable constant_\">NULL</span>|       <span class=\"hljs-number\">5000</span>|\n| <span class=\"hljs-title class_\">Marketing</span>|         <span class=\"hljs-title class_\">Jeff</span>|       <span class=\"hljs-number\">3000</span>|\n| <span class=\"hljs-title class_\">Marketing</span>|        <span class=\"hljs-title class_\">Kumar</span>|       <span class=\"hljs-number\">2000</span>|\n|     <span class=\"hljs-title class_\">Sales</span>|         <span class=\"hljs-title class_\">Saif</span>|       <span class=\"hljs-number\">4100</span>|\n+----------+-------------+-----------+\n</code></pre>\n<ul>\n<li>Cube(): Cube는 다차원 집계를 생성하고 지정된 그룹화 열의 다중 조합을 통해 통찰을 제공합니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\"># 초기 데이터\n+----------+-------------+------+\n|department|employee_name|salary|\n+----------+-------------+------+\n|     <span class=\"hljs-title class_\">Sales</span>|        <span class=\"hljs-title class_\">James</span>|  <span class=\"hljs-number\">3000</span>|\n|     <span class=\"hljs-title class_\">Sales</span>|      <span class=\"hljs-title class_\">Michael</span>|  <span class=\"hljs-number\">4600</span>|\n|     <span class=\"hljs-title class_\">Sales</span>|       <span class=\"hljs-title class_\">Robert</span>|  <span class=\"hljs-number\">4100</span>|\n|   <span class=\"hljs-title class_\">Finance</span>|        <span class=\"hljs-title class_\">Maria</span>|  <span class=\"hljs-number\">3000</span>|\n|     <span class=\"hljs-title class_\">Sales</span>|        <span class=\"hljs-title class_\">James</span>|  <span class=\"hljs-number\">3000</span>|\n|   <span class=\"hljs-title class_\">Finance</span>|        <span class=\"hljs-title class_\">Scott</span>|  <span class=\"hljs-number\">3300</span>|\n|   <span class=\"hljs-title class_\">Finance</span>|          <span class=\"hljs-title class_\">Jen</span>|  <span class=\"hljs-number\">3900</span>|\n| <span class=\"hljs-title class_\">Marketing</span>|         <span class=\"hljs-title class_\">Jeff</span>|  <span class=\"hljs-number\">3000</span>|\n| <span class=\"hljs-title class_\">Marketing</span>|        <span class=\"hljs-title class_\">Kumar</span>|  <span class=\"hljs-number\">2000</span>|\n|     <span class=\"hljs-title class_\">Sales</span>|         <span class=\"hljs-title class_\">Saif</span>|  <span class=\"hljs-number\">4100</span>|\n+----------+-------------+------+\n</code></pre>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> sum\n\n# <span class=\"hljs-title class_\">Cube</span> 예시\ncube_df = df.<span class=\"hljs-title function_\">cube</span>(<span class=\"hljs-string\">\"department\"</span>, <span class=\"hljs-string\">\"employee_name\"</span>).<span class=\"hljs-title function_\">sum</span>(<span class=\"hljs-string\">\"salary\"</span>)\ncube_df.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\"># 결과\n\n+----------+-------------+-----------+\n|부서      |직원명         |급여합계   |\n+----------+-------------+-----------+\n|      <span class=\"hljs-variable constant_\">NULL</span>|        <span class=\"hljs-title class_\">James</span>|       <span class=\"hljs-number\">6000</span>|\n|     판매 |        <span class=\"hljs-title class_\">James</span>|       <span class=\"hljs-number\">6000</span>|\n|      <span class=\"hljs-variable constant_\">NULL</span>|         <span class=\"hljs-variable constant_\">NULL</span>|      <span class=\"hljs-number\">34000</span>|\n|     판매 |         <span class=\"hljs-variable constant_\">NULL</span>|      <span class=\"hljs-number\">18800</span>|\n|      <span class=\"hljs-variable constant_\">NULL</span>|      <span class=\"hljs-title class_\">Michael</span>|       <span class=\"hljs-number\">4600</span>|\n|     판매 |      <span class=\"hljs-title class_\">Michael</span>|       <span class=\"hljs-number\">4600</span>|\n|     판매 |       <span class=\"hljs-title class_\">Robert</span>|       <span class=\"hljs-number\">4100</span>|\n|      <span class=\"hljs-variable constant_\">NULL</span>|       <span class=\"hljs-title class_\">Robert</span>|       <span class=\"hljs-number\">4100</span>|\n|   재무   |         <span class=\"hljs-variable constant_\">NULL</span>|      <span class=\"hljs-number\">10200</span>|\n|   재무   |        <span class=\"hljs-title class_\">Maria</span>|       <span class=\"hljs-number\">3000</span>|\n|      <span class=\"hljs-variable constant_\">NULL</span>|        <span class=\"hljs-title class_\">Maria</span>|       <span class=\"hljs-number\">3000</span>|\n|      <span class=\"hljs-variable constant_\">NULL</span>|        <span class=\"hljs-title class_\">Scott</span>|       <span class=\"hljs-number\">3300</span>|\n|   재무   |        <span class=\"hljs-title class_\">Scott</span>|       <span class=\"hljs-number\">3300</span>|\n|      <span class=\"hljs-variable constant_\">NULL</span>|          <span class=\"hljs-title class_\">Jen</span>|       <span class=\"hljs-number\">3900</span>|\n|   재무   |          <span class=\"hljs-title class_\">Jen</span>|       <span class=\"hljs-number\">3900</span>|\n| 마케팅  |         <span class=\"hljs-variable constant_\">NULL</span>|       <span class=\"hljs-number\">5000</span>|\n| 마케팅  |         <span class=\"hljs-title class_\">Jeff</span>|       <span class=\"hljs-number\">3000</span>|\n|      <span class=\"hljs-variable constant_\">NULL</span>|         <span class=\"hljs-title class_\">Jeff</span>|       <span class=\"hljs-number\">3000</span>|\n| 마케팅  |        <span class=\"hljs-title class_\">Kumar</span>|       <span class=\"hljs-number\">2000</span>|\n|      <span class=\"hljs-variable constant_\">NULL</span>|        <span class=\"hljs-title class_\">Kumar</span>|       <span class=\"hljs-number\">2000</span>|\n+----------+-------------+-----------+\n상위 <span class=\"hljs-number\">20</span>개 행만 표시\n</code></pre>\n<h2>2. groupBy() + pivot()</h2>\n<ul>\n<li>Pivoting을 사용하면 행을 열로 변환하여 피벗 테이블과 유사한 방식으로 데이터를 요약할 수 있습니다. 종종 두 열 간의 관계를 이해하는 데 사용됩니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\"># 초기 데이터\n+----------+-------------+------+\n|부서     |직원명        |급여   |\n+----------+-------------+------+\n|     판매 |        <span class=\"hljs-title class_\">James</span>|  <span class=\"hljs-number\">3000</span>|\n|     판매 |      <span class=\"hljs-title class_\">Michael</span>|  <span class=\"hljs-number\">4600</span>|\n|     판매 |       <span class=\"hljs-title class_\">Robert</span>|  <span class=\"hljs-number\">4100</span>|\n|   재무  |        <span class=\"hljs-title class_\">Maria</span>|  <span class=\"hljs-number\">3000</span>|\n|     판매 |        <span class=\"hljs-title class_\">James</span>|  <span class=\"hljs-number\">3000</span>|\n|   재무  |        <span class=\"hljs-title class_\">Scott</span>|  <span class=\"hljs-number\">3300</span>|\n|   재무  |          <span class=\"hljs-title class_\">Jen</span>|  <span class=\"hljs-number\">3900</span>|\n| 마케팅  |         <span class=\"hljs-title class_\">Jeff</span>|  <span class=\"hljs-number\">3000</span>|\n| 마케팅  |        <span class=\"hljs-title class_\">Kumar</span>|  <span class=\"hljs-number\">2000</span>|\n|     판매 |         <span class=\"hljs-title class_\">Saif</span>|  <span class=\"hljs-number\">4100</span>|\n+----------+-------------+------+\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\"># <span class=\"hljs-title class_\">Pivot</span> 예시\npivot_df = df.<span class=\"hljs-title function_\">groupBy</span>(<span class=\"hljs-string\">\"department\"</span>).<span class=\"hljs-title function_\">pivot</span>(<span class=\"hljs-string\">\"employee_name\"</span>).<span class=\"hljs-title function_\">sum</span>(<span class=\"hljs-string\">\"salary\"</span>)\npivot_df.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<pre><code class=\"hljs language-js\"># 결과\n+----------+-----+----+----+-----+-----+-------+------+----+-----+\n|department|<span class=\"hljs-title class_\">James</span>|<span class=\"hljs-title class_\">Jeff</span>| <span class=\"hljs-title class_\">Jen</span>|<span class=\"hljs-title class_\">Kumar</span>|<span class=\"hljs-title class_\">Maria</span>|<span class=\"hljs-title class_\">Michael</span>|<span class=\"hljs-title class_\">Robert</span>|<span class=\"hljs-title class_\">Saif</span>|<span class=\"hljs-title class_\">Scott</span>|\n+----------+-----+----+----+-----+-----+-------+------+----+-----+\n|     <span class=\"hljs-title class_\">Sales</span>| <span class=\"hljs-number\">6000</span>|<span class=\"hljs-variable constant_\">NULL</span>|<span class=\"hljs-variable constant_\">NULL</span>| <span class=\"hljs-variable constant_\">NULL</span>| <span class=\"hljs-variable constant_\">NULL</span>|   <span class=\"hljs-number\">4600</span>|  <span class=\"hljs-number\">4100</span>|<span class=\"hljs-number\">4100</span>| <span class=\"hljs-variable constant_\">NULL</span>|\n|   <span class=\"hljs-title class_\">Finance</span>| <span class=\"hljs-variable constant_\">NULL</span>|<span class=\"hljs-variable constant_\">NULL</span>|<span class=\"hljs-number\">3900</span>| <span class=\"hljs-variable constant_\">NULL</span>| <span class=\"hljs-number\">3000</span>|   <span class=\"hljs-variable constant_\">NULL</span>|  <span class=\"hljs-variable constant_\">NULL</span>|<span class=\"hljs-variable constant_\">NULL</span>| <span class=\"hljs-number\">3300</span>|\n| <span class=\"hljs-title class_\">Marketing</span>| <span class=\"hljs-variable constant_\">NULL</span>|<span class=\"hljs-number\">3000</span>|<span class=\"hljs-variable constant_\">NULL</span>| <span class=\"hljs-number\">2000</span>| <span class=\"hljs-variable constant_\">NULL</span>|   <span class=\"hljs-variable constant_\">NULL</span>|  <span class=\"hljs-variable constant_\">NULL</span>|<span class=\"hljs-variable constant_\">NULL</span>| <span class=\"hljs-variable constant_\">NULL</span>|\n+----------+-----+----+----+-----+-----+-------+------+----+-----+\n</code></pre>\n<h2>3. 윈도우 함수: partitionBy() + row_number()/rank().over(w)</h2>\n<ul>\n<li>윈도우 함수는 현재 행과 관련된 \"윈도우\"에 대해 계산을 수행할 수 있어 전통적인 group-by 작업보다 더 유연성을 제공합니다. 이는 러닝 토탈, 이동 평균 또는 이전 및 다음 행에 액세스하는 데 특히 유용합니다.</li>\n</ul>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\"># 초기 데이터\n| 부서      | 직원 이름       | 급여  |\n|----------|---------------|------|\n| <span class=\"hljs-title class_\">Sales</span>    | <span class=\"hljs-title class_\">James</span>         | <span class=\"hljs-number\">3000</span> |\n| <span class=\"hljs-title class_\">Sales</span>    | <span class=\"hljs-title class_\">Michael</span>       | <span class=\"hljs-number\">4600</span> |\n| <span class=\"hljs-title class_\">Sales</span>    | <span class=\"hljs-title class_\">Robert</span>        | <span class=\"hljs-number\">4100</span> |\n| <span class=\"hljs-title class_\">Finance</span>  | <span class=\"hljs-title class_\">Maria</span>         | <span class=\"hljs-number\">3000</span> |\n| <span class=\"hljs-title class_\">Sales</span>    | <span class=\"hljs-title class_\">James</span>         | <span class=\"hljs-number\">3000</span> |\n| <span class=\"hljs-title class_\">Finance</span>  | <span class=\"hljs-title class_\">Scott</span>         | <span class=\"hljs-number\">3300</span> |\n| <span class=\"hljs-title class_\">Finance</span>  | <span class=\"hljs-title class_\">Jen</span>           | <span class=\"hljs-number\">3900</span> |\n| <span class=\"hljs-title class_\">Marketing</span>| <span class=\"hljs-title class_\">Jeff</span>          | <span class=\"hljs-number\">3000</span> |\n| <span class=\"hljs-title class_\">Marketing</span>| <span class=\"hljs-title class_\">Kumar</span>         | <span class=\"hljs-number\">2000</span> |\n| <span class=\"hljs-title class_\">Sales</span>    | <span class=\"hljs-title class_\">Saif</span>          | <span class=\"hljs-number\">4100</span> |\n\n</code></pre>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">window</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Window</span>\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> col, row_number\n\nwindowSpec = <span class=\"hljs-title class_\">Window</span>.<span class=\"hljs-title function_\">partitionBy</span>(<span class=\"hljs-string\">\"department\"</span>).<span class=\"hljs-title function_\">orderBy</span>(<span class=\"hljs-title function_\">col</span>(<span class=\"hljs-string\">\"salary\"</span>).<span class=\"hljs-title function_\">asc</span>())\ndf_with_row_number = df.<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">\"row_number\"</span>, <span class=\"hljs-title function_\">row_number</span>().<span class=\"hljs-title function_\">over</span>(windowSpec))\ndf_with_row_number.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<pre><code class=\"hljs language-js\"># 결과\n| 직원 이름      | 부서        | 급여  | row_number |\n|---------------|------------|------|------------|\n| <span class=\"hljs-title class_\">Maria</span>         | <span class=\"hljs-title class_\">Finance</span>    | <span class=\"hljs-number\">3000</span> |      <span class=\"hljs-number\">1</span>     |\n| <span class=\"hljs-title class_\">Scott</span>         | <span class=\"hljs-title class_\">Finance</span>    | <span class=\"hljs-number\">3300</span> |      <span class=\"hljs-number\">2</span>     |\n| <span class=\"hljs-title class_\">Jen</span>           | <span class=\"hljs-title class_\">Finance</span>    | <span class=\"hljs-number\">3900</span> |      <span class=\"hljs-number\">3</span>     |\n| <span class=\"hljs-title class_\">Kumar</span>         | <span class=\"hljs-title class_\">Marketing</span>  | <span class=\"hljs-number\">2000</span> |      <span class=\"hljs-number\">1</span>     |\n| <span class=\"hljs-title class_\">Jeff</span>          | <span class=\"hljs-title class_\">Marketing</span>  | <span class=\"hljs-number\">3000</span> |      <span class=\"hljs-number\">2</span>     |\n| <span class=\"hljs-title class_\">James</span>         | <span class=\"hljs-title class_\">Sales</span>      | <span class=\"hljs-number\">3000</span> |      <span class=\"hljs-number\">1</span>     |\n| <span class=\"hljs-title class_\">James</span>         | <span class=\"hljs-title class_\">Sales</span>      | <span class=\"hljs-number\">3000</span> |      <span class=\"hljs-number\">2</span>     |\n| <span class=\"hljs-title class_\">Robert</span>        | <span class=\"hljs-title class_\">Sales</span>      | <span class=\"hljs-number\">4100</span> |      <span class=\"hljs-number\">3</span>     |\n| <span class=\"hljs-title class_\">Saif</span>          | <span class=\"hljs-title class_\">Sales</span>      | <span class=\"hljs-number\">4100</span> |      <span class=\"hljs-number\">4</span>     |\n| <span class=\"hljs-title class_\">Michael</span>       | <span class=\"hljs-title class_\">Sales</span>      | <span class=\"hljs-number\">4600</span> |      <span class=\"hljs-number\">5</span>     |\n\n</code></pre>\n<ul>\n<li>Rank() 함수를 위해</li>\n</ul>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">window</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">Window</span>\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> rank, col\n\nwindowSpec = <span class=\"hljs-title class_\">Window</span>.<span class=\"hljs-title function_\">partitionBy</span>(<span class=\"hljs-string\">\"department\"</span>).<span class=\"hljs-title function_\">orderBy</span>(<span class=\"hljs-title function_\">col</span>(<span class=\"hljs-string\">\"salary\"</span>).<span class=\"hljs-title function_\">desc</span>())\ndf_with_rank = df.<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">\"rank\"</span>, <span class=\"hljs-title function_\">rank</span>().<span class=\"hljs-title function_\">over</span>(windowSpec))\ndf_with_rank.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<pre><code class=\"hljs language-js\"># 출력\n+-------------+----------+------+----------+\n|employee_name|department|salary|      rank|\n+-------------+----------+------+----------+\n|          <span class=\"hljs-title class_\">Jen</span>|   <span class=\"hljs-title class_\">Finance</span>|  <span class=\"hljs-number\">3900</span>|         <span class=\"hljs-number\">1</span>|\n|        <span class=\"hljs-title class_\">Scott</span>|   <span class=\"hljs-title class_\">Finance</span>|  <span class=\"hljs-number\">3300</span>|         <span class=\"hljs-number\">2</span>|\n|        <span class=\"hljs-title class_\">Maria</span>|   <span class=\"hljs-title class_\">Finance</span>|  <span class=\"hljs-number\">3000</span>|         <span class=\"hljs-number\">3</span>|\n|         <span class=\"hljs-title class_\">Jeff</span>| <span class=\"hljs-title class_\">Marketing</span>|  <span class=\"hljs-number\">3000</span>|         <span class=\"hljs-number\">1</span>|\n|        <span class=\"hljs-title class_\">Kumar</span>| <span class=\"hljs-title class_\">Marketing</span>|  <span class=\"hljs-number\">2000</span>|         <span class=\"hljs-number\">2</span>|\n|      <span class=\"hljs-title class_\">Michael</span>|     <span class=\"hljs-title class_\">Sales</span>|  <span class=\"hljs-number\">4600</span>|         <span class=\"hljs-number\">1</span>|\n|       <span class=\"hljs-title class_\">Robert</span>|     <span class=\"hljs-title class_\">Sales</span>|  <span class=\"hljs-number\">4100</span>|         <span class=\"hljs-number\">2</span>|\n|         <span class=\"hljs-title class_\">Saif</span>|     <span class=\"hljs-title class_\">Sales</span>|  <span class=\"hljs-number\">4100</span>|         <span class=\"hljs-number\">2</span>|\n|        <span class=\"hljs-title class_\">James</span>|     <span class=\"hljs-title class_\">Sales</span>|  <span class=\"hljs-number\">3000</span>|         <span class=\"hljs-number\">3</span>|\n|        <span class=\"hljs-title class_\">James</span>|     <span class=\"hljs-title class_\">Sales</span>|  <span class=\"hljs-number\">3000</span>|         <span class=\"hljs-number\">3</span>|\n+-------------+----------+------+----------+\n</code></pre>\n<h1>최적화 I: 무게 감소</h1>\n<h2>0. 불필요한 원시 데이터 제거</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>1. DataFrame이 여러 번 액세스 될 때 캐시합니다.</h2>\n<pre><code class=\"hljs language-js\">df.<span class=\"hljs-title function_\">cache</span>();\ndf.<span class=\"hljs-title function_\">count</span>();\n</code></pre>\n<pre><code class=\"hljs language-js\">#출력\n<span class=\"hljs-number\">3</span>\n</code></pre>\n<h2>2. 적절한 파일 형식 사용하기</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>압축 파일은 파일의 입출력 및 메모리를 절약할 수 있어요.</li>\n<li>압축 해제된 파일은 CPU를 절약할 수 있어요.</li>\n</ul>\n<pre><code class=\"hljs language-js\">df.<span class=\"hljs-property\">write</span>.<span class=\"hljs-title function_\">parquet</span>(<span class=\"hljs-string\">\"output.parquet\"</span>);\n</code></pre>\n<h2>3. 스키마 수동 지정하기</h2>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">types</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">StructType</span>, <span class=\"hljs-title class_\">StructField</span>, <span class=\"hljs-title class_\">IntegerType</span>, <span class=\"hljs-title class_\">StringType</span>\nschema = <span class=\"hljs-title class_\">StructType</span>([\n    <span class=\"hljs-title class_\">StructField</span>(<span class=\"hljs-string\">\"id\"</span>, <span class=\"hljs-title class_\">IntegerType</span>(), <span class=\"hljs-title class_\">True</span>),\n    <span class=\"hljs-title class_\">StructField</span>(<span class=\"hljs-string\">\"name\"</span>, <span class=\"hljs-title class_\">StringType</span>(), <span class=\"hljs-title class_\">True</span>),\n    <span class=\"hljs-title class_\">StructField</span>(<span class=\"hljs-string\">\"age\"</span>, <span class=\"hljs-title class_\">IntegerType</span>(), <span class=\"hljs-title class_\">True</span>)\n])\ndf = spark.<span class=\"hljs-property\">read</span>.<span class=\"hljs-title function_\">schema</span>(schema).<span class=\"hljs-title function_\">csv</span>(<span class=\"hljs-string\">\"path/to/file.csv\"</span>)\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>4. 조기에 필요한 열 선택하기</h2>\n<p>데이터 처리 파이프라인에서 메모리 사용량을 줄이기 위해 필요한 열만 미리 선택하세요.</p>\n<pre><code class=\"hljs language-js\">df.<span class=\"hljs-title function_\">select</span>(<span class=\"hljs-string\">\"dept_name\"</span>, <span class=\"hljs-string\">\"name\"</span>).<span class=\"hljs-title function_\">filter</span>(<span class=\"hljs-string\">\"dept_id >= 102\"</span>).<span class=\"hljs-title function_\">show</span>();\ndf.<span class=\"hljs-title function_\">select</span>(<span class=\"hljs-string\">\"dept_name\"</span>, <span class=\"hljs-string\">\"name\"</span>)\n  .<span class=\"hljs-title function_\">filter</span>(df.<span class=\"hljs-property\">dept_id</span> >= <span class=\"hljs-number\">102</span>)\n  .<span class=\"hljs-title function_\">show</span>();\n</code></pre>\n<pre><code class=\"hljs language-js\">#출력\n+---------+----+\n|dept_name|name|\n+---------+----+\n|<span class=\"hljs-title class_\">Marketing</span>|<span class=\"hljs-title class_\">Jane</span>|\n|  <span class=\"hljs-title class_\">Finance</span>| <span class=\"hljs-title class_\">Joe</span>|\n+---------+----+\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>5. 필터를 조인이나 집계 전에 빠르게 적용하세요.</h2>\n<pre><code class=\"hljs language-js\">df.<span class=\"hljs-title function_\">filter</span>(<span class=\"hljs-string\">\"age > 25\"</span>).<span class=\"hljs-title function_\">join</span>(df_other, <span class=\"hljs-string\">\"id\"</span>).<span class=\"hljs-title function_\">show</span>();\n</code></pre>\n<h2>6. 큰 데이터셋 수집 방지를 위해 limit() 사용하기</h2>\n<ul>\n<li>큰 데이터셋에 collect()를 사용하지 않도록 주의하여 메모리 부족 오류를 방지하세요.</li>\n</ul>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\">df.<span class=\"hljs-title function_\">filter</span>(<span class=\"hljs-string\">\"age > 30\"</span>).<span class=\"hljs-title function_\">limit</span>(<span class=\"hljs-number\">100</span>).<span class=\"hljs-title function_\">collect</span>();\n</code></pre>\n<h2>7. Using spark.sql(): Catalyst optimizer for Complex Queries</h2>\n<ul>\n<li>Leverage Spark SQL for complex queries, which might be more readable and can benefit from the Catalyst optimizer.</li>\n</ul>\n<pre><code class=\"hljs language-js\">df.<span class=\"hljs-title function_\">createOrReplaceTempView</span>(<span class=\"hljs-string\">\"table\"</span>);\nspark.<span class=\"hljs-title function_\">sql</span>(<span class=\"hljs-string\">\"SELECT id, sum(value) FROM table GROUP BY id\"</span>).<span class=\"hljs-title function_\">show</span>();\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>8. RDD 사용: reduceByKey를 사용한 집계</h2>\n<ul>\n<li>집계 작업을 수행할 때 reduceByKey를 사용하는 것이 groupBy보다 더 효율적일 수 있습니다.</li>\n</ul>\n<pre><code class=\"hljs language-python\">rdd = df.rdd.<span class=\"hljs-built_in\">map</span>(<span class=\"hljs-keyword\">lambda</span> x: (x[<span class=\"hljs-number\">0</span>], x[<span class=\"hljs-number\">1</span>]))\nreduced = rdd.reduceByKey(<span class=\"hljs-keyword\">lambda</span> a, b: a + b)\nreduced.toDF([<span class=\"hljs-string\">\"key\"</span>, <span class=\"hljs-string\">\"value\"</span>]).show()\n</code></pre>\n<h1>최적화 II: 파티션 없이 병렬화 없음</h1>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>1. 테이블 분할: partitionBy()</h2>\n<ul>\n<li>데이터프레임을 디스크에 저장할 때 빠른 후속 읽기를 위해 분할을 사용하세요.</li>\n</ul>\n<pre><code class=\"hljs language-js\">df.<span class=\"hljs-property\">write</span>.<span class=\"hljs-title function_\">partitionBy</span>(<span class=\"hljs-string\">\"year\"</span>, <span class=\"hljs-string\">\"month\"</span>).<span class=\"hljs-title function_\">parquet</span>(<span class=\"hljs-string\">\"path/to/output\"</span>);\n</code></pre>\n<h2>2. 스튜 관리를 위한 Salting 키</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>조인 연산 중 데이터 스큐가 발생하는 경우, 하나 이상의 키 값이 다른 것들보다 훨씬 더 많은 데이터를 가지고 있을 때 발생합니다.</li>\n<li>예를 들어 \"customer_id\"를 기준으로 조인을 수행하고, 대부분의 거래가 소수의 고객에 속해 있다면 이러한 소수의 키는 다른 키들에 비해 훨씬 많은 양의 데이터를 가지고 있을 것입니다. 이로 인해 일부 작업(큰 키를 처리하는 작업)이 훨씬 더 오랜 시간이 걸리고 병목 현상이 발생할 수 있습니다.</li>\n<li>이 문제를 해결하는 방법은 skewed 데이터를 관리하기 위해 키에 임의의 접두사를 추가하는 것입니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> monotonically_increasing_id, expr\ndf.<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">\"salted_key\"</span>,\n    <span class=\"hljs-title function_\">expr</span>(<span class=\"hljs-string\">\"concat(name, '_', (monotonically_increasing_id() % 10))\"</span>)\n).<span class=\"hljs-title function_\">groupBy</span>(<span class=\"hljs-string\">\"salted_key\"</span>).<span class=\"hljs-title function_\">count</span>().<span class=\"hljs-title function_\">select</span>(<span class=\"hljs-title function_\">sum</span>(<span class=\"hljs-string\">\"count\"</span>)).<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<pre><code class=\"hljs language-js\"># 결과\n+----------+\n|<span class=\"hljs-title function_\">sum</span>(count)|\n+----------+\n|         <span class=\"hljs-number\">3</span>|\n+----------+\n</code></pre>\n<ul>\n<li>데이터 로딩의 균형을 어떻게 맞출까요?</li>\n</ul>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> monotonically_increasing_id, expr\ndf.<span class=\"hljs-title function_\">withColumn</span>(<span class=\"hljs-string\">\"salted_key\"</span>,\n    <span class=\"hljs-title function_\">expr</span>(<span class=\"hljs-string\">\"concat(name, '_', (monotonically_increasing_id() % 10))\"</span>)\n).<span class=\"hljs-title function_\">groupBy</span>(<span class=\"hljs-string\">\"salted_key\"</span>).<span class=\"hljs-title function_\">count</span>().<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<pre><code class=\"hljs language-js\"># 결과\n+----------+-----+\n|salted_key|count|\n+----------+-----+\n|   <span class=\"hljs-title class_\">James</span>_6|    <span class=\"hljs-number\">1</span>|\n|   <span class=\"hljs-title class_\">James</span>_4|    <span class=\"hljs-number\">1</span>|\n|   <span class=\"hljs-title class_\">James</span>_0|    <span class=\"hljs-number\">1</span>|\n+----------+-----+\n</code></pre>\n<h1>최적화 III: Shuffling을 최소화하는 전략</h1>\n<pre><code class=\"hljs language-js\">### <span class=\"hljs-title class_\">Shuffling</span> 최소화 전략\n- **<span class=\"hljs-title class_\">Broadcast</span> 변수 사용**\n  - 데이터셋이 작은 경우 <span class=\"hljs-title class_\">Shuffling</span>을 피하기 위해 모든 노드에 브로드캐스트합니다.\n- **파티션 튜닝**\n  - 작업 및 데이터 규모에 맞게 파티션 수를 조정합니다.\n- **변환 최적화**\n  - <span class=\"hljs-title class_\">Shuffling</span>을 필요로 하는 넓은 변환을 최소화하기 위해 작업을 계획합니다.\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>섞기</h2>\n<ul>\n<li>섞기는 데이터가 서로 다른 파티션에 재분배되는 과정입니다.</li>\n<li>이는 데이터를 실행자 간이나 심지어 기계 간에 이동하는 것을 포함합니다.</li>\n<li>네트워크 및 디스크 I/O 측면에서 가장 비용이 많이 드는 작업 중 하나입니다.</li>\n</ul>\n<h2>섞기의 목적</h2>\n<ul>\n<li>데이터 재분배: 조인, 그룹화, 집계 및 재분할과 같은 넓은 변환을 용이하게 합니다.</li>\n<li>부하 분산: 클러스터 전체에 데이터와 작업 부담을 균등하게 분배합니다.</li>\n<li>동시성: 병렬 처리를 강화하고 리소스 활용을 최적화합니다.</li>\n<li>데이터 지역성 최적화: 데이터가 처리될 위치에 가까이 이동하도록 합니다. 네트워크 트래픽을 줄입니다.</li>\n</ul>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>셔플링의 고민거리</h2>\n<ul>\n<li>자원 소모가 많음: 상당한 네트워크 대역폭과 디스크 I/O를 사용합니다.</li>\n<li>지연 시간 증가: 특히 대량의 데이터셋인 경우 처리 시간이 상당히 증가합니다.</li>\n<li>병목 현상 발생 가능성: 적절히 관리되지 않으면 전체 시스템 성능을 느리게 만들 수 있습니다.</li>\n</ul>\n<h2>1. 작은 DataFrame과 큰 DataFrame을 조인할 때 데이터 셔플링을 최소화하기 위해 브로드캐스트 조인을 사용합니다.</h2>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">SparkSession</span>\n<span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> broadcast\n\n# 스파크 세션 초기화\nspark = <span class=\"hljs-title class_\">SparkSession</span>.<span class=\"hljs-property\">builder</span>.<span class=\"hljs-title function_\">appName</span>(<span class=\"hljs-string\">\"브로드캐스트 조인 예제\"</span>).<span class=\"hljs-title function_\">getOrCreate</span>()\n# 직원용 큰 <span class=\"hljs-title class_\">DataFrame</span> 생성\ndata_employees = [(<span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">\"John\"</span>, <span class=\"hljs-number\">101</span>),\n                  (<span class=\"hljs-number\">2</span>, <span class=\"hljs-string\">\"Jane\"</span>, <span class=\"hljs-number\">102</span>),\n                  (<span class=\"hljs-number\">3</span>, <span class=\"hljs-string\">\"Joe\"</span>, <span class=\"hljs-number\">103</span>),\n                  (<span class=\"hljs-number\">4</span>, <span class=\"hljs-string\">\"Jill\"</span>, <span class=\"hljs-number\">101</span>),\n                  # 더 많은 레코드가 있다고 가정\n                  ]\ncolumns_employees = [<span class=\"hljs-string\">\"emp_id\"</span>, <span class=\"hljs-string\">\"name\"</span>, <span class=\"hljs-string\">\"dept_id\"</span>]\ndf_employees = spark.<span class=\"hljs-title function_\">createDataFrame</span>(data_employees, columns_employees)\n# 부서용 작은 <span class=\"hljs-title class_\">DataFrame</span> 생성\ndata_departments = [(<span class=\"hljs-number\">101</span>, <span class=\"hljs-string\">\"인사\"</span>),\n                    (<span class=\"hljs-number\">102</span>, <span class=\"hljs-string\">\"마케팅\"</span>),\n                    (<span class=\"hljs-number\">103</span>, <span class=\"hljs-string\">\"금융\"</span>),\n                    (<span class=\"hljs-number\">104</span>, <span class=\"hljs-string\">\"IT\"</span>),\n                    (<span class=\"hljs-number\">105</span>, <span class=\"hljs-string\">\"지원\"</span>)\n                    ]\ncolumns_departments = [<span class=\"hljs-string\">\"dept_id\"</span>, <span class=\"hljs-string\">\"dept_name\"</span>]\ndf_departments = spark.<span class=\"hljs-title function_\">createDataFrame</span>(data_departments, columns_departments)\n# 브로드캐스트 조인 수행\ndf_joined = df_employees.<span class=\"hljs-title function_\">join</span>(<span class=\"hljs-title function_\">broadcast</span>(df_departments), <span class=\"hljs-string\">\"dept_id\"</span>)\ndf_joined.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\">#출력\n+-------+------+----+---------+\n|dept_id|emp_id|  이름|dept_name|\n+-------+------+-----+--------+\n|    <span class=\"hljs-number\">101</span>|     <span class=\"hljs-number\">1</span>| 존|       인사|\n|    <span class=\"hljs-number\">102</span>|     <span class=\"hljs-number\">2</span>| 제인|     마케팅|\n|    <span class=\"hljs-number\">103</span>|     <span class=\"hljs-number\">3</span>| 조|     금융|\n|    <span class=\"hljs-number\">101</span>|     <span class=\"hljs-number\">4</span>| 질|       인사|\n+-------+------+-----+--------+\n</code></pre>\n<ul>\n<li>직원 — 직원 세부 정보를 담은 작은 데이터셋입니다.</li>\n<li>부서 — 부서 세부 정보를 담은 큰 데이터셋입니다.</li>\n</ul>\n<p>두 데이터셋을 부서 ID를 기준으로 조인하되, 부서 데이터셋을 크게 섞지 않도록 하는 것이 목표입니다.</p>\n<h2>2. 파티션 조정: 병렬성 증가를 위한 다시 분할</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>데이터 프레임의 파티션을 재분할하여 병렬성을 높이거나 셔플 비용을 줄일 수 있습니다.</li>\n<li>그러나 여전히 전체 셔플을 유발할 수 있습니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\"># 병렬성을 높이기 위한 재분할 예제\ndf = spark.<span class=\"hljs-title function_\">createDataFrame</span>([\n  (<span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'foo'</span>), (<span class=\"hljs-number\">2</span>, <span class=\"hljs-string\">'bar'</span>), (<span class=\"hljs-number\">3</span>, <span class=\"hljs-string\">'baz'</span>), (<span class=\"hljs-number\">4</span>, <span class=\"hljs-string\">'qux'</span>)\n], [<span class=\"hljs-string\">\"id\"</span>, <span class=\"hljs-string\">\"value\"</span>])\ndf_repartitioned = df.<span class=\"hljs-title function_\">repartition</span>(<span class=\"hljs-number\">10</span>)  # 파티션 수 증가\n</code></pre>\n<h2>3. 파티션 튜닝: 파티션 감소를 위한 Coalesce</h2>\n<ul>\n<li>전체 셔플 피하기: coalesce는 대규모 데이터 세트를 필터링한 후 파티션 수를 줄이고 싶을 때 셔플 비용을 피해야 할 때 최적입니다.</li>\n<li>전형적인 사용 사례: 많은 파티션이 부분적으로 채워지거나 비어있는 상태로 남을 수 있는 대규모 DataFrame을 필터링한 후 사용됩니다. coalesce는 네트워크 오버헤드를 줄이고 비용 효율적으로 리소스를 관리하는 데 도움이 됩니다.</li>\n</ul>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\"># 대규모 셔플링 없이 파티션 수를 줄이기 위한 코어스 예시\ndf_filtered = df.<span class=\"hljs-title function_\">filter</span>(<span class=\"hljs-string\">\"id > 1\"</span>)\ndf_coalesced = df_filtered.<span class=\"hljs-title function_\">coalesce</span>(<span class=\"hljs-number\">2</span>)  # 파티션 수 줄이기\n\n</code></pre>\n<h2>4. 변환 최적화를 통해 데이터 셔플링 최소화하기</h2>\n<ul>\n<li>최적화된 변환을 통해 Apache Spark에서 셔플링을 최소화하는 것은 Spark 애플리케이션의 성능을 향상시키는 중요한 측면입니다.</li>\n<li>변환 최적화는 데이터 처리 작업을 구조화하여 클러스터 전체에서 불필요한 데이터 이동을 줄이는 것을 포함하며, 이는 리소스를 많이 사용하고 실행 속도를 늦출 수 있습니다.</li>\n<li>이를 달성하는 방법을 보여주는 몇 가지 전략과 코드 예제는 다음과 같습니다:</li>\n</ul>\n<h2>4–1. 일찍 필터링하기</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>데이터 처리 파이프라인에서 조인 또는 집계와 같은 후속 작업에서 나중에 섞어야 하는 데이터 양을 줄이기 위해 필터를 가능한 한 빨리 적용하세요.</li>\n</ul>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> pyspark.sql <span class=\"hljs-keyword\">import</span> SparkSession\n\n<span class=\"hljs-comment\"># 스파크 세션 초기화</span>\nspark = SparkSession.builder.appName(<span class=\"hljs-string\">\"Shuffling 최소화\"</span>).getOrCreate()\n\n<span class=\"hljs-comment\"># DataFrame 생성</span>\ndata = [(<span class=\"hljs-string\">\"John\"</span>, <span class=\"hljs-string\">\"금융\"</span>, <span class=\"hljs-number\">3000</span>), (<span class=\"hljs-string\">\"Jane\"</span>, <span class=\"hljs-string\">\"마케팅\"</span>, <span class=\"hljs-number\">4000</span>), (<span class=\"hljs-string\">\"Joe\"</span>, <span class=\"hljs-string\">\"마케팅\"</span>, <span class=\"hljs-number\">2800</span>), (<span class=\"hljs-string\">\"Jill\"</span>, <span class=\"hljs-string\">\"금융\"</span>, <span class=\"hljs-number\">3900</span>)]\ncolumns = [<span class=\"hljs-string\">\"이름\"</span>, <span class=\"hljs-string\">\"부서\"</span>, <span class=\"hljs-string\">\"연봉\"</span>]\ndf = spark.createDataFrame(data, schema=columns)\n\n<span class=\"hljs-comment\"># 넓은 변환 전에 미리 필터링</span>\nfiltered_df = df.<span class=\"hljs-built_in\">filter</span>(df[<span class=\"hljs-string\">\"연봉\"</span>] > <span class=\"hljs-number\">3000</span>)\n\n<span class=\"hljs-comment\"># 이제 집계 수행</span>\naggregated_df = filtered_df.groupBy(<span class=\"hljs-string\">\"부서\"</span>).avg(<span class=\"hljs-string\">\"연봉\"</span>)\naggregated_df.show()\n</code></pre>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-comment\"># 출력</span>\n+----------+-----------+\n|부서      |avg(연봉)  |\n+----------+-----------+\n| 마케팅  |     <span class=\"hljs-number\">4000.0</span>|\n| 금융    |     <span class=\"hljs-number\">3900.0</span>|\n+----------+-----------+\n</code></pre>\n<h2>4-2. 가능한 경우 RDD/넓은 변환 사용하기</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>좁은 변환, 예를 들어 <code>map</code> 및 <code>filter</code>와 같은 작업은 개별 파티션에서 작동하며 데이터 셔플을 필요로하지 않습니다. 가능한 경우 이러한 작업을 넓은 변환 대신 사용하십시오.</li>\n</ul>\n<pre><code class=\"hljs language-js\"># 셔플을 발생시키지 않고 새로운 열을 만들기 위해 map을 사용합니다\nrdd = df.<span class=\"hljs-property\">rdd</span>.<span class=\"hljs-title function_\">map</span>(lambda <span class=\"hljs-attr\">x</span>: (x.<span class=\"hljs-property\">Name</span>, x.<span class=\"hljs-property\">Salary</span> * <span class=\"hljs-number\">1.1</span>))\nupdated_salaries_df = spark.<span class=\"hljs-title function_\">createDataFrame</span>(\n  rdd, schema=[<span class=\"hljs-string\">\"Name\"</span>, <span class=\"hljs-string\">\"UpdatedSalary\"</span>]\n)\nupdated_salaries_df.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<pre><code class=\"hljs language-js\"># 결과\n+----+------------------+\n|<span class=\"hljs-title class_\">Name</span>|     <span class=\"hljs-title class_\">UpdatedSalary</span>|\n+----+------------------+\n|<span class=\"hljs-title class_\">John</span>|<span class=\"hljs-number\">3300.0000000000005</span>|\n|<span class=\"hljs-title class_\">Jane</span>|            <span class=\"hljs-number\">4400.0</span>|\n| <span class=\"hljs-title class_\">Joe</span>|<span class=\"hljs-number\">3080.0000000000005</span>|\n|<span class=\"hljs-title class_\">Jill</span>|            <span class=\"hljs-number\">4290.0</span>|\n+----+------------------+\n</code></pre>\n<h2>4-3. Boardcasting join으로 불필요한 셔플을 피하세요</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>조인 작업시, 한 데이터셋이 다른 데이터셋보다 현저히 작을 때 브로드캐스트 조인을 사용하여 큰 데이터셋을 셔플링하지 않도록 합니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> pyspark.<span class=\"hljs-property\">sql</span>.<span class=\"hljs-property\">functions</span> <span class=\"hljs-keyword\">import</span> broadcast\n# df_small이 df_large보다 훨씬 작다고 가정합니다\ndf_small = spark.<span class=\"hljs-title function_\">createDataFrame</span>(\n  [(<span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">\"HR\"</span>), (<span class=\"hljs-number\">2</span>, <span class=\"hljs-string\">\"마케팅\"</span>)], [<span class=\"hljs-string\">\"id\"</span>, <span class=\"hljs-string\">\"부서\"</span>]\n)\ndf_large = spark.<span class=\"hljs-title function_\">createDataFrame</span>(\n  [(<span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">\"존\"</span>), (<span class=\"hljs-number\">2</span>, <span class=\"hljs-string\">\"제인\"</span>), (<span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">\"조\"</span>), (<span class=\"hljs-number\">2</span>, <span class=\"hljs-string\">\"질\"</span>)],\n  [<span class=\"hljs-string\">\"부서ID\"</span>, <span class=\"hljs-string\">\"이름\"</span>]\n)\n# 조인 최적화를 위해 작은 <span class=\"hljs-title class_\">DataFrame</span>을 브로드캐스트합니다\noptimized_join_df = df_large.<span class=\"hljs-title function_\">join</span>(<span class=\"hljs-title function_\">broadcast</span>(df_small), df_large.부서<span class=\"hljs-variable constant_\">ID</span> == df_small.<span class=\"hljs-property\">id</span>)\noptimized_join_df.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<pre><code class=\"hljs language-js\"># 결과\n\n+------+----+---+------+\n|부서<span class=\"hljs-variable constant_\">ID</span>|이름| id|  부서|\n+------+----+---+------+\n|     <span class=\"hljs-number\">1</span>| 존|  <span class=\"hljs-number\">1</span>|   <span class=\"hljs-variable constant_\">HR</span>|\n|     <span class=\"hljs-number\">2</span>|제인|  <span class=\"hljs-number\">2</span>|마케팅|\n|     <span class=\"hljs-number\">1</span>| 조|  <span class=\"hljs-number\">1</span>|   <span class=\"hljs-variable constant_\">HR</span>|\n|     <span class=\"hljs-number\">2</span>| 질|  <span class=\"hljs-number\">2</span>|마케팅|\n+------+----+---+------+\n</code></pre>\n<h2>4-4. 전략적으로 파티션 나누기</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>만약 넓은 변환을 사용해야 한다면, 나중에 조인 또는 집계할 키를 기반으로 데이터를 파티션으로 나누세요. 이 전략을 사용하면 동일한 키를 가진 행을 동일한 파티션에 함께 두어 셔플링을 줄일 수 있습니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\"># 셔플링 최소화를 위해 집계 전에 파티션 재분배\nrepartitioned_df = df.<span class=\"hljs-title function_\">repartition</span>(<span class=\"hljs-string\">\"Department\"</span>)\naggregated_df = repartitioned_df.<span class=\"hljs-title function_\">groupBy</span>(<span class=\"hljs-string\">\"Department\"</span>).<span class=\"hljs-title function_\">avg</span>(<span class=\"hljs-string\">\"Salary\"</span>)\naggregated_df.<span class=\"hljs-title function_\">show</span>()\n</code></pre>\n<pre><code class=\"hljs language-js\"># 결과\n+----------+-----------+\n|<span class=\"hljs-title class_\">Department</span>|<span class=\"hljs-title function_\">avg</span>(<span class=\"hljs-title class_\">Salary</span>)|\n+----------+-----------+\n|   <span class=\"hljs-title class_\">Finance</span>|     <span class=\"hljs-number\">3450.0</span>|\n| <span class=\"hljs-title class_\">Marketing</span>|     <span class=\"hljs-number\">3400.0</span>|\n+----------+-----------+\n</code></pre>\n<h1>성능 모니터링 및 세부 조정</h1>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>1. 메모리 관리</h2>\n<pre><code class=\"hljs language-js\">spark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(“spark.<span class=\"hljs-property\">executor</span>.<span class=\"hljs-property\">memory</span>”, “4g”)\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(“spark.<span class=\"hljs-property\">driver</span>.<span class=\"hljs-property\">memory</span>”, “2g”)\n</code></pre>\n<h2>2. 작업 및 스테이지 모니터링</h2>\n<ul>\n<li>Spark UI를 사용하여 응용 프로그램 내의 작업 및 스테이지의 성능을 모니터링합니다.</li>\n<li>Spark UI에 액세스하려면 다음으로 이동하십시오: http://[your-spark-driver-host]:4040</li>\n<li>Executor 메트릭 분석: 각 executor의 메트릭을 모니터링하여 메모리 사용, 디스크 스피릴 및 가비지 수집에 대한 통찰을 얻을 수 있습니다.</li>\n</ul>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\"># 자세한 실행자 지표를 수집하도록 <span class=\"hljs-title class_\">Spark</span>를 구성합니다\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.executor.metrics.pollingInterval\"</span>, <span class=\"hljs-string\">\"5000\"</span>)\n</code></pre>\n<h2>3. SQL 성능 튜닝</h2>\n<ul>\n<li>SQL 실행 계획을 이해하고 최적화하기 위해 <code>EXPLAIN</code> 계획을 활용하세요.</li>\n</ul>\n<pre><code class=\"hljs language-js\">df.<span class=\"hljs-title function_\">explain</span>(“formatted”)\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\">== 물리적인 계획 ==\n* 기존 <span class=\"hljs-variable constant_\">RDD</span> 스캔 (<span class=\"hljs-number\">1</span>)\n\n\n(<span class=\"hljs-number\">1</span>) 기존 <span class=\"hljs-variable constant_\">RDD</span> 스캔 [코드 생성 <span class=\"hljs-variable constant_\">ID</span> : <span class=\"hljs-number\">1</span>]\n출력 [<span class=\"hljs-number\">3</span>]: [이름 #<span class=\"hljs-number\">4628</span>, 부서 #<span class=\"hljs-number\">4629</span>, 급여 #4630L]\n인수: [이름 #<span class=\"hljs-number\">4628</span>, 부서 #<span class=\"hljs-number\">4629</span>, 급여 #4630L],\n applySchemaToPythonRDD에 있는 <span class=\"hljs-title class_\">MapPartitionsRDD</span>[<span class=\"hljs-number\">693</span>]에서\n                       at &#x3C;알 수 없음>:<span class=\"hljs-number\">0</span>, <span class=\"hljs-title class_\">ExistingRDD</span>, <span class=\"hljs-title class_\">UnknownPartitioning</span>(<span class=\"hljs-number\">0</span>)\n</code></pre>\n<h2>4. 동적 할당</h2>\n<ul>\n<li>워크로드에 따라 스파크가 실행자 수를 동적으로 조정할 수 있도록 동적 할당을 활성화합니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\">spark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.dynamicAllocation.enabled\"</span>, <span class=\"hljs-string\">\"true\"</span>);\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.dynamicAllocation.minExecutors\"</span>, <span class=\"hljs-string\">\"1\"</span>);\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.dynamicAllocation.maxExecutors\"</span>, <span class=\"hljs-string\">\"20\"</span>);\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.dynamicAllocation.executorIdleTimeout\"</span>, <span class=\"hljs-string\">\"60s\"</span>);\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.shuffle.service.enabled\"</span>, <span class=\"hljs-string\">\"true\"</span>);\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>5. 데이터 지역성</h2>\n<ul>\n<li>저장 및 처리 장치 간에 데이터가 이동해야 하는 거리를 최소화하여 데이터 지역성을 최적화합니다.</li>\n</ul>\n<pre><code class=\"hljs language-js\">spark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.locality.wait\"</span>, <span class=\"hljs-string\">\"300ms\"</span>);\n</code></pre>\n<h2>6. Garbage Collection Tuning</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\">- 가비지 컬렉터 설정을 조정하여 메모리 관리를 최적화하고 일시 중지 시간을 줄일 수 있습니다.\n\n# 더 나은 지연 시간을 위해 <span class=\"hljs-variable constant_\">G1GC</span> 사용\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.executor.extraJavaOptions\"</span>, <span class=\"hljs-string\">\"-XX:+UseG1GC\"</span>)\n# 짧은 일시 중지를 위해 명시적인 <span class=\"hljs-variable constant_\">GC</span> 설정 구성\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.executor.extraJavaOptions\"</span>, <span class=\"hljs-string\">\"-XX:MaxGCPauseMillis=100\"</span>)\n\n## <span class=\"hljs-number\">7.</span> 데이터 직렬화 세부 조정\n\n- 데이터 직렬화는 분산 애플리케이션의 성능에 중요한 역할을 합니다. <span class=\"hljs-title class_\">Spark</span>는 두 가지 직렬화 도구를 지원합니다:\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\"># 더 나은 성능과 효율성을 위해 <span class=\"hljs-title class_\">Kryo</span> 직렬화 프로그램 사용\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.serializer\"</span>, <span class=\"hljs-string\">\"org.apache.spark.serializer.KryoSerializer\"</span>)\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.kryo.registrationRequired\"</span>, <span class=\"hljs-string\">\"true\"</span>)\n\n# <span class=\"hljs-title class_\">Kryo</span>와 사용자 정의 클래스 등록\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MyClass</span>:\n    def <span class=\"hljs-title function_\">__init__</span>(self, name, id):\n        self.<span class=\"hljs-property\">name</span> = name\n        self.<span class=\"hljs-property\">id</span> = id\nspark.<span class=\"hljs-property\">sparkContext</span>.<span class=\"hljs-title function_\">getConf</span>().<span class=\"hljs-title function_\">registerKryoClasses</span>([<span class=\"hljs-title class_\">MyClass</span>])\n</code></pre>\n<h2>8. 네트워크 구성 최적화</h2>\n<ul>\n<li>네트워크 설정은 특히 대규모 배포에서 성능에 중대한 영향을 미칠 수 있습니다:</li>\n</ul>\n<pre><code class=\"hljs language-js\"># 네트워크 타임아웃 설정을 조정하여 대규모 클러스터에서 불필요한 작업 실패를 피하십시오\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.network.timeout\"</span>, <span class=\"hljs-string\">\"800s\"</span>)\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.core.connection.ack.wait.timeout\"</span>, <span class=\"hljs-string\">\"600s\"</span>)\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h2>9. 고급 Spark SQL 튜닝</h2>\n<ul>\n<li>Catalyst 옵티마이저 및 Tungsten 실행 엔진을 활용하면 Spark SQL의 성능을 향상시킬 수 있습니다:</li>\n</ul>\n<pre><code class=\"hljs language-js\"># 직렬 처리를 위한 전체 단계 코드 생성 활성화\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.sql.codegen.wholeStage\"</span>, <span class=\"hljs-string\">\"true\"</span>)\n\n# 조인 최적화에 유용한 테이블 브로드캐스트를 위한 최대 바이트 수 증가\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.sql.autoBroadcastJoinThreshold\"</span>, <span class=\"hljs-string\">\"10485760\"</span>)  # <span class=\"hljs-number\">10</span> <span class=\"hljs-variable constant_\">MB</span>\n</code></pre>\n<h2>10. 데이터 파티셔닝 최적화</h2>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>데이터 분배를 세밀하게 조정하여 쿼리 성능을 향상시키고 셔플 오버헤드를 줄일 수 있어요:</p>\n<pre><code class=\"hljs language-js\"># 데이터 크기 및 작업을 기준으로 수동으로 셔플 파티션 수 설정\n\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.sql.shuffle.partitions\"</span>, <span class=\"hljs-string\">\"200\"</span>)\n# 클러스터 크기 및 데이터에 맞게 조정하세요\n</code></pre>\n<h2>11. 적응형 쿼리 실행 활성화</h2>\n<ul>\n<li>적응형 쿼리 실행 (AQE)는 실행 중에 쿼리 계획을 조정함으로써 Spark SQL 쿼리를 더 빠르고 데이터 스쿠 및 기타 이슈에 더 강건하게 만드는 기능이에요.</li>\n</ul>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\"># 쿼리 실행을 적응적으로 조정하는 <span class=\"hljs-variable constant_\">AQE</span>를 활성화합니다. 이는 구성을 간소화하고 성능을 향상시킬 수 있습니다.\nspark.<span class=\"hljs-property\">conf</span>.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">\"spark.sql.adaptive.enabled\"</span>, <span class=\"hljs-string\">\"true\"</span>)\n</code></pre>\n<ul>\n<li>AQE는 실제 데이터에 적응해 셔플 파티셔닝을 조정하고, 불균형 조인을 처리하며, 정렬을 최적화할 수 있습니다.</li>\n</ul>\n<h2>12. 메모리 관리 지정</h2>\n<ul>\n<li>적절한 메모리 관리는 메모리 집약적인 작업에서 특히 효과적인 성능 개선을 위해 스파이지를 방지할 수 있습니다.</li>\n</ul>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<h1>RDD 저장소에 예약된 메모리 분수를 구성합니다.</h1>\n<p>spark.conf.set(“spark.memory.fraction”, “0.6”)\nspark.conf.set(“spark.memory.storageFraction”, “0.5”)</p>\n<p>이러한 설정은 실행 메모리와 저장소 메모리 사이의 균형을 맞추어 셔플 및 캐싱 중 디스크 스파일을 줄이는 데 도움이 됩니다.</p>\n<h1>읽어 주셔서 감사합니다</h1>\n<p>이 글이 마음에 드시면:</p>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>👏 여러 번 박수로 지지를 보여주세요!</li>\n<li>이 안내서를 친구들과 공유해도 좋아요.</li>\n<li>여러분의 피드백은 소중합니다. 앞으로의 글에 영감을 주고 안내해 줍니다.</li>\n<li>또는 메시지를 남겨주세요: <a href=\"https://www.linkedin.com/in/kevinchwong\" rel=\"nofollow\" target=\"_blank\">https://www.linkedin.com/in/kevinchwong</a></li>\n</ul>\n</body>\n</html>\n"},"__N_SSG":true}