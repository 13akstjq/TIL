{"pageProps":{"post":{"title":"Apache Airflow와 Amazon-S3를 사용한 End-to-End ETL 파이프라인 구축 하는 방법","description":"","date":"2024-07-09 09:06","slug":"2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3","content":"\n# 프로젝트 개요\n\n이 프로젝트는 Apache Airflow와 Amazon S3를 사용하여 end-to-end ETL (추출, 변환, 로드) 파이프라인을 개발하는 데 중점을 둡니다.\n\n이 파이프라인은 OpenWeather API에서 날씨 데이터를 검색하여 구조화된 형식으로 변환하고 S3 버킷에로드합니다. 이 프로젝트를 완료하면 희망하는 빈도로 예약된 파이프라인을 실행할 수 있는 완전히 기능하는 파이프라인을 보유하게 됩니다.\n\n# 프로젝트 구조\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- 날씨 데이터 가져오기: OpenWeather API에서 날씨 데이터를 가져옵니다.\n- 날씨 데이터 변환: API에서 가져온 데이터는 JSON 형식이며, 변환 작업은 JSON 개체에서 데이터프레임을 만들고 데이터프레임을 CSV 파일로 변환하는 작업을 포함합니다.\n- S3에 데이터로드: 변환된 데이터를 S3 버킷에 저장합니다.\n\n# 사전 준비 및 사용된 도구\n\n- Apache Airflow: 워크플로우를 프로그래밍 방식으로 작성, 예약, 모니터링할 수 있는 강력하고 유연한 플랫폼입니다.\n- OpenWeather API: 여러 도시의 날씨 데이터를 제공하는 서비스입니다.\n- Amazon S3: Amazon Web Services (AWS)의 확장 가능한 객체 스토리지 서비스입니다.\n- Pandas: 데이터 조작 및 분석을 위해 사용되는 Python 라이브러리입니다.\n- Boto3: Python 개발자가 S3와 같은 Amazon 서비스를 활용하는 소프트웨어를 작성할 수 있게 해주는 AWS SDK for Python입니다.\n- DAG 파일: Apache Airflow에서 작업 및 종속성을 정의하는 작업열로서 사용되는 중요한 개념인 Directed Acyclic Graph(DAG) 파일입니다.\n\n# 구현\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n- API와 연결하여 데이터를 가져오는 DAG 스크립트를 작성하세요. 데이터는 데이터프레임에 저장됩니다. DAG 코드는 이 페이지의 맨 아래에서 찾을 수 있습니다.\n- EC2 인스턴스를 생성하고 인스턴스를 시작하여 콘솔에 연결하세요. 저는 무료티어 AWS를 사용하여 이 인스턴스를 생성했습니다. 사양은 t2.micro 및 우분투 22 버전입니다.\n\n![이미지](/TIL/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_0.png)\n\n인스턴스가 실행되면 콘솔에 연결하여 다음을 설치하세요.\n\n```js\nsudo apt-get update\nsudo install python3-pip\nsudo pip install requests pandas boto3 s3fs pyarrow apache-airflow\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n3. 한 번 설치되면, Airflow가 올바르게 설치되었는지 확인하세요. airflow 명령어를 사용하여 확인하고 초기 로그인 자격 증명을 위해 스탠드얼론 명령어를 실행하십시오. 자격 증명을 복사하여 나중에 사용하세요.\n\n```bash\nairflow\nairflow standalone\n```\n\n4. 실행 중인 인스턴스에서 보안으로 이동하여 보안 그룹에 액세스하세요. 인바운드 규칙을 편집하고 새 역할을 생성하세요. \"모든 트래픽\", \"IPv4 어디서나\"로 설정하세요.\n\n5. Airflow 서버와 스케줄러를 시작하세요.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n```js\n에어플로우 스케줄러 및 에어플로우 웹서버 - 포트 8080\n\n6. 공개 IP 주소를 복사하고 포트를 추가하세요. 예: 172.31.22.254:8080. 이로써 에어플로우 어플리케이션을 열 수 있습니다. 기본 자격 증명을 사용하여 로그인하고 마음에 드는 비밀번호로 재설정하세요.\n\n7. AWS에서 데이터를 저장할 S3 버킷을 만드세요. IAM 역할을 사용하여 권한을 조정하세요. 새 IAM 역할을 만들고 S3 및 EC2에 권한을 부여하세요.\n\n8. DAG 파일에 관련 있는 S3 버킷 이름을 추가하고 저장하세요.\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n9. 인스턴스 콘솔에서 서버를 중지하고 명령을 실행하세요. 이렇게 하면 airflow의 DAGs 폴더에 액세스할 수 있어요. 원하는 경우 DAG 파일을 추가하고 필요할 때 수정할 수 있어요.\n\n```js\nairflow\ncd airflow\nls\nsudo nano airflow.cfg\n```\n\nDAGs 폴더에서 파일 이름을 조정하세요. 수정된 버퍼를 저장하고 종료하세요.\n\n10. 파일을 저장한 후 airflow 명령을 다시 실행하고 로그인하세요. 그러면 airflow 내에서 Dag 파일을 볼 수 있어요. 보이는 형태는 이렇습니다:\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n아래는 Markdown 형식으로 변경된 테이블입니다.\n\n11. 파일을 열어서 수동으로 실행할 수 있어야 합니다. Airflow의 내장 그래프 기능을 사용하여 DAG 파일의 상태를 모니터링할 수 있습니다. 초록 테두리는 성공적인 실행을 나타냅니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n12. 실행이 성공하면 데이터가 S3 버킷에 표시됩니다.\n\n![Airflow-S3](/TIL/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_4.png)\n\nDAG 파일과 설명:\n\n```js\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nimport requests\nimport pandas as pd\nimport boto3\nfrom io import StringIO\n\n# Configuration\nAPI_KEY = 'xxxxxxxxxxxxxxxxxxxxxxxxxxx'  # OpenWeather의 API 키\nCITY = 'Arizona'  # 날씨 데이터를 가져올 도시\nS3_BUCKET = 'open-weather-s3-bucket'  # 데이터가 저장될 S3 버킷 이름\nS3_KEY = 'weather_data/weather.csv'  # S3 오브젝트 키 (버킷 내 파일 경로)\n\n# DAG를 위한 기본 인수\ndefault_args = {\n    'owner': 'airflow',  # DAG 소유자\n    'depends_on_past': False,  # 작업 인스턴스는 과거 실행에 의존하지 않음\n    'start_date': datetime(2024, 7, 5),  # DAG 시작 날짜\n    'email_on_failure': False,  # 실패 시 이메일 알림 비활성화\n    'email_on_retry': False,  # 재시도 시 이메일 알림 비활성화\n    'retries': 1,  # 실패 시 재시도 횟수\n    'retry_delay': timedelta(minutes=5),  # 재시도 간의 지연\n}\n\n# 스케줄러가 없는 DAG 정의\ndag = DAG(\n    'OpenWeather_to_s3',\n    default_args=default_args,\n    description='날씨 데이터를 가져와 변환한 후 S3로 로드합니다.',\n    schedule_interval=None,  # schedule_interval을 None으로 설정하여 스케줄러 비활성화\n)\n\ndef fetch_weather_data():\n    \"\"\"OpenWeather API에서 날씨 데이터 가져오기\"\"\"\n    url = f\"http://api.openweathermap.org/data/2.5/weather?q={CITY}&appid={API_KEY}\"  # 도시와 API 키를 포함한 API 엔드포인트\n    response = requests.get(url)  # API에 GET 요청 보내기\n    data = response.json()  # 응답을 JSON으로 변환\n    return data  # 데이터 반환\n\ndef transform_weather_data(**kwargs):\n    \"\"\"가져온 날씨 데이터 변환하기\"\"\"\n    ti = kwargs['ti']  # 작업 인스턴스 가져오기\n    data = ti.xcom_pull(task_ids='fetch_weather_data')  # XCom을 사용하여 'fetch_weather_data' 작업에서 데이터 가져오기\n\n    weather = {\n        'city': data['name'],  # 도시 이름 추출\n        'temperature': data['main']['temp'],  # 온도 추출\n        'pressure': data['main']['pressure'],  # 기압 추출\n        'humidity': data['main']['humidity'],  # 습도 추출\n        'weather': data['weather'][0]['description'],  # 날씨 설명 추출\n        'wind_speed': data['wind']['speed'],  # 풍속 추출\n        'date': datetime.utcfromtimestamp(data['dt']).strftime('%Y-%m-%d %H:%M:%S')  # 타임스탬프를 읽기 가능한 날짜로 변환\n    }\n\n    df = pd.DataFrame([weather])  # 날씨 데이터를 판다스 DataFrame으로 변환\n    return df  # DataFrame 반환\n\ndef load_data_to_s3(**kwargs):\n    \"\"\"변환된 데이터를 S3 버킷에 로드하기\"\"\"\n    ti = kwargs['ti']  # 작업 인스턴스 가져오기\n    df = ti.xcom_pull(task_ids='transform_weather_data')  # XCom을 사용하여 'transform_weather_data' 작업에서 변환된 데이터 가져오기\n\n    csv_buffer = StringIO()  # 인메모리 버퍼 생성\n    df.to_csv(csv_buffer, index=False)  # DataFrame을 CSV로 버퍼에 작성\n    print(df)  # DataFrame 출력 (선택 사항)\n\n    s3_resource = boto3.resource('s3')  # boto3 S3 리소스 생성\n    s3_resource.Object(S3_BUCKET, S3_KEY).put(Body=csv_buffer.getvalue())  # CSV 데이터를 지정한 S3 버킷 및 키에 업로드\n\n# PythonOperator를 사용하여 작업 정의\nfetch_task = PythonOperator(\n    task_id='fetch_weather_data',  # 작업 ID\n    python_callable=fetch_weather_data,  # 호출 가능한 함수\n    dag=dag,  # 작업이 속한 DAG\n)\n\ntransform_task = PythonOperator(\n    task_id='transform_weather_data',  # 작업 ID\n    python_callable=transform_weather_data,  # 호출 가능한 함수\n    provide_context=True,  # 호출 가능한 함수에 컨텍스트 제공\n    dag=dag,  # 작업이 속한 DAG\n)\n\nload_task = PythonOperator(\n    task_id='load_data_to_s3',  # 작업 ID\n    python_callable=load_data_to_s3,  # 호출 가능한 함수\n    provide_context=True,  # 호출 가능한 함수에 컨텍스트 제공\n    dag=dag,  # 작업이 속한 DAG\n)\n\n# 작업 간 의존성 정의 (작업 실행 순서)\nfetch_task >> transform_task >> load_task  # 작업 실행 순서 설정: 가져오기 -> 변환 -> 로드\n```\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n설명:\n\n- Imports 및 구성: 필요한 라이브러리를 import하고 구성 변수를 설정합니다.\n- 기본 인수: DAG의 기본 인수를 정의합니다. 예를 들어, 소유자, 시작 날짜, 재시도 정책 등이 포함됩니다.\n- DAG 정의: DAG 개체를 설명과 일정 간격으로 생성합니다 (일정을 비활성화하려면 None으로 설정 가능합니다).\n- 작업 함수: 사용할 Python 함수를 작업으로 정의합니다.\n\n- fetch_weather_data: OpenWeather API에서 날씨 데이터를 가져옵니다.\n- transform_weather_data: 가져온 데이터를 Pandas DataFrame으로 변환합니다.\n- load_data_to_s3: 변환된 데이터를 S3 버킷에 로드합니다.\n\n5. 작업 생성: PythonOperator를 사용하여 정의된 함수를 호출하는 작업을 생성합니다.\n\n<!-- TIL 수평 -->\n\n<ins class=\"adsbygoogle\"\n     style=\"display:block\"\n     data-ad-client=\"ca-pub-4877378276818686\"\n     data-ad-slot=\"1549334788\"\n     data-ad-format=\"auto\"\n     data-full-width-responsive=\"true\"></ins>\n\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n\n6. 종속성 설정: 비트 시프트 연산자를 사용하여 작업을 실행할 순서를 정의하세요.\n\n# 자료들\n","ogImage":{"url":"/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_0.png"},"coverImage":"/TIL/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_0.png","tag":["Tech"],"readingTime":9},"content":"<!doctype html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n</head>\n<body>\n<h1>프로젝트 개요</h1>\n<p>이 프로젝트는 Apache Airflow와 Amazon S3를 사용하여 end-to-end ETL (추출, 변환, 로드) 파이프라인을 개발하는 데 중점을 둡니다.</p>\n<p>이 파이프라인은 OpenWeather API에서 날씨 데이터를 검색하여 구조화된 형식으로 변환하고 S3 버킷에로드합니다. 이 프로젝트를 완료하면 희망하는 빈도로 예약된 파이프라인을 실행할 수 있는 완전히 기능하는 파이프라인을 보유하게 됩니다.</p>\n<h1>프로젝트 구조</h1>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>날씨 데이터 가져오기: OpenWeather API에서 날씨 데이터를 가져옵니다.</li>\n<li>날씨 데이터 변환: API에서 가져온 데이터는 JSON 형식이며, 변환 작업은 JSON 개체에서 데이터프레임을 만들고 데이터프레임을 CSV 파일로 변환하는 작업을 포함합니다.</li>\n<li>S3에 데이터로드: 변환된 데이터를 S3 버킷에 저장합니다.</li>\n</ul>\n<h1>사전 준비 및 사용된 도구</h1>\n<ul>\n<li>Apache Airflow: 워크플로우를 프로그래밍 방식으로 작성, 예약, 모니터링할 수 있는 강력하고 유연한 플랫폼입니다.</li>\n<li>OpenWeather API: 여러 도시의 날씨 데이터를 제공하는 서비스입니다.</li>\n<li>Amazon S3: Amazon Web Services (AWS)의 확장 가능한 객체 스토리지 서비스입니다.</li>\n<li>Pandas: 데이터 조작 및 분석을 위해 사용되는 Python 라이브러리입니다.</li>\n<li>Boto3: Python 개발자가 S3와 같은 Amazon 서비스를 활용하는 소프트웨어를 작성할 수 있게 해주는 AWS SDK for Python입니다.</li>\n<li>DAG 파일: Apache Airflow에서 작업 및 종속성을 정의하는 작업열로서 사용되는 중요한 개념인 Directed Acyclic Graph(DAG) 파일입니다.</li>\n</ul>\n<h1>구현</h1>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ul>\n<li>API와 연결하여 데이터를 가져오는 DAG 스크립트를 작성하세요. 데이터는 데이터프레임에 저장됩니다. DAG 코드는 이 페이지의 맨 아래에서 찾을 수 있습니다.</li>\n<li>EC2 인스턴스를 생성하고 인스턴스를 시작하여 콘솔에 연결하세요. 저는 무료티어 AWS를 사용하여 이 인스턴스를 생성했습니다. 사양은 t2.micro 및 우분투 22 버전입니다.</li>\n</ul>\n<p><img src=\"/TIL/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_0.png\" alt=\"이미지\"></p>\n<p>인스턴스가 실행되면 콘솔에 연결하여 다음을 설치하세요.</p>\n<pre><code class=\"hljs language-js\">sudo apt-get update\nsudo install python3-pip\nsudo pip install requests pandas boto3 s3fs pyarrow apache-airflow\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ol start=\"3\">\n<li>한 번 설치되면, Airflow가 올바르게 설치되었는지 확인하세요. airflow 명령어를 사용하여 확인하고 초기 로그인 자격 증명을 위해 스탠드얼론 명령어를 실행하십시오. 자격 증명을 복사하여 나중에 사용하세요.</li>\n</ol>\n<pre><code class=\"hljs language-bash\">airflow\nairflow standalone\n</code></pre>\n<ol start=\"4\">\n<li>\n<p>실행 중인 인스턴스에서 보안으로 이동하여 보안 그룹에 액세스하세요. 인바운드 규칙을 편집하고 새 역할을 생성하세요. \"모든 트래픽\", \"IPv4 어디서나\"로 설정하세요.</p>\n</li>\n<li>\n<p>Airflow 서버와 스케줄러를 시작하세요.</p>\n</li>\n</ol>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<pre><code class=\"hljs language-js\">에어플로우 스케줄러 및 에어플로우 웹서버 - 포트 <span class=\"hljs-number\">8080</span>\n\n<span class=\"hljs-number\">6.</span> 공개 <span class=\"hljs-variable constant_\">IP</span> 주소를 복사하고 포트를 추가하세요. 예: <span class=\"hljs-number\">172.31</span><span class=\"hljs-number\">.22</span><span class=\"hljs-number\">.254</span>:<span class=\"hljs-number\">8080.</span> 이로써 에어플로우 어플리케이션을 열 수 있습니다. 기본 자격 증명을 사용하여 로그인하고 마음에 드는 비밀번호로 재설정하세요.\n\n<span class=\"hljs-number\">7.</span> <span class=\"hljs-variable constant_\">AWS</span>에서 데이터를 저장할 <span class=\"hljs-variable constant_\">S3</span> 버킷을 만드세요. <span class=\"hljs-variable constant_\">IAM</span> 역할을 사용하여 권한을 조정하세요. 새 <span class=\"hljs-variable constant_\">IAM</span> 역할을 만들고 <span class=\"hljs-variable constant_\">S3</span> 및 <span class=\"hljs-title class_\">EC2</span>에 권한을 부여하세요.\n\n<span class=\"hljs-number\">8.</span> <span class=\"hljs-variable constant_\">DAG</span> 파일에 관련 있는 <span class=\"hljs-variable constant_\">S3</span> 버킷 이름을 추가하고 저장하세요.\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ol start=\"9\">\n<li>인스턴스 콘솔에서 서버를 중지하고 명령을 실행하세요. 이렇게 하면 airflow의 DAGs 폴더에 액세스할 수 있어요. 원하는 경우 DAG 파일을 추가하고 필요할 때 수정할 수 있어요.</li>\n</ol>\n<pre><code class=\"hljs language-js\">airflow\ncd airflow\nls\nsudo nano airflow.<span class=\"hljs-property\">cfg</span>\n</code></pre>\n<p>DAGs 폴더에서 파일 이름을 조정하세요. 수정된 버퍼를 저장하고 종료하세요.</p>\n<ol start=\"10\">\n<li>파일을 저장한 후 airflow 명령을 다시 실행하고 로그인하세요. 그러면 airflow 내에서 Dag 파일을 볼 수 있어요. 보이는 형태는 이렇습니다:</li>\n</ol>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>아래는 Markdown 형식으로 변경된 테이블입니다.</p>\n<ol start=\"11\">\n<li>파일을 열어서 수동으로 실행할 수 있어야 합니다. Airflow의 내장 그래프 기능을 사용하여 DAG 파일의 상태를 모니터링할 수 있습니다. 초록 테두리는 성공적인 실행을 나타냅니다.</li>\n</ol>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ol start=\"12\">\n<li>실행이 성공하면 데이터가 S3 버킷에 표시됩니다.</li>\n</ol>\n<p><img src=\"/TIL/assets/img/2024-07-09-End-to-EndETLPipelinewithApacheAirflowandAmazon-S3_4.png\" alt=\"Airflow-S3\"></p>\n<p>DAG 파일과 설명:</p>\n<pre><code class=\"hljs language-js\"><span class=\"hljs-keyword\">from</span> datetime <span class=\"hljs-keyword\">import</span> datetime, timedelta\n<span class=\"hljs-keyword\">from</span> airflow <span class=\"hljs-keyword\">import</span> <span class=\"hljs-variable constant_\">DAG</span>\n<span class=\"hljs-keyword\">from</span> airflow.<span class=\"hljs-property\">operators</span>.<span class=\"hljs-property\">python_operator</span> <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">PythonOperator</span>\n<span class=\"hljs-keyword\">import</span> requests\n<span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\n<span class=\"hljs-keyword\">import</span> boto3\n<span class=\"hljs-keyword\">from</span> io <span class=\"hljs-keyword\">import</span> <span class=\"hljs-title class_\">StringIO</span>\n\n# <span class=\"hljs-title class_\">Configuration</span>\n<span class=\"hljs-variable constant_\">API_KEY</span> = <span class=\"hljs-string\">'xxxxxxxxxxxxxxxxxxxxxxxxxxx'</span>  # <span class=\"hljs-title class_\">OpenWeather</span>의 <span class=\"hljs-variable constant_\">API</span> 키\n<span class=\"hljs-variable constant_\">CITY</span> = <span class=\"hljs-string\">'Arizona'</span>  # 날씨 데이터를 가져올 도시\n<span class=\"hljs-variable constant_\">S3_BUCKET</span> = <span class=\"hljs-string\">'open-weather-s3-bucket'</span>  # 데이터가 저장될 <span class=\"hljs-variable constant_\">S3</span> 버킷 이름\n<span class=\"hljs-variable constant_\">S3_KEY</span> = <span class=\"hljs-string\">'weather_data/weather.csv'</span>  # <span class=\"hljs-variable constant_\">S3</span> 오브젝트 키 (버킷 내 파일 경로)\n\n# <span class=\"hljs-variable constant_\">DAG</span>를 위한 기본 인수\ndefault_args = {\n    <span class=\"hljs-string\">'owner'</span>: <span class=\"hljs-string\">'airflow'</span>,  # <span class=\"hljs-variable constant_\">DAG</span> 소유자\n    <span class=\"hljs-string\">'depends_on_past'</span>: <span class=\"hljs-title class_\">False</span>,  # 작업 인스턴스는 과거 실행에 의존하지 않음\n    <span class=\"hljs-string\">'start_date'</span>: <span class=\"hljs-title function_\">datetime</span>(<span class=\"hljs-number\">2024</span>, <span class=\"hljs-number\">7</span>, <span class=\"hljs-number\">5</span>),  # <span class=\"hljs-variable constant_\">DAG</span> 시작 날짜\n    <span class=\"hljs-string\">'email_on_failure'</span>: <span class=\"hljs-title class_\">False</span>,  # 실패 시 이메일 알림 비활성화\n    <span class=\"hljs-string\">'email_on_retry'</span>: <span class=\"hljs-title class_\">False</span>,  # 재시도 시 이메일 알림 비활성화\n    <span class=\"hljs-string\">'retries'</span>: <span class=\"hljs-number\">1</span>,  # 실패 시 재시도 횟수\n    <span class=\"hljs-string\">'retry_delay'</span>: <span class=\"hljs-title function_\">timedelta</span>(minutes=<span class=\"hljs-number\">5</span>),  # 재시도 간의 지연\n}\n\n# 스케줄러가 없는 <span class=\"hljs-variable constant_\">DAG</span> 정의\ndag = <span class=\"hljs-title function_\">DAG</span>(\n    <span class=\"hljs-string\">'OpenWeather_to_s3'</span>,\n    default_args=default_args,\n    description=<span class=\"hljs-string\">'날씨 데이터를 가져와 변환한 후 S3로 로드합니다.'</span>,\n    schedule_interval=<span class=\"hljs-title class_\">None</span>,  # schedule_interval을 <span class=\"hljs-title class_\">None</span>으로 설정하여 스케줄러 비활성화\n)\n\ndef <span class=\"hljs-title function_\">fetch_weather_data</span>():\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"OpenWeather API에서 날씨 데이터 가져오기\"</span><span class=\"hljs-string\">\"\"</span>\n    url = f<span class=\"hljs-string\">\"http://api.openweathermap.org/data/2.5/weather?q={CITY}&#x26;appid={API_KEY}\"</span>  # 도시와 <span class=\"hljs-variable constant_\">API</span> 키를 포함한 <span class=\"hljs-variable constant_\">API</span> 엔드포인트\n    response = requests.<span class=\"hljs-title function_\">get</span>(url)  # <span class=\"hljs-variable constant_\">API</span>에 <span class=\"hljs-variable constant_\">GET</span> 요청 보내기\n    data = response.<span class=\"hljs-title function_\">json</span>()  # 응답을 <span class=\"hljs-title class_\">JSON</span>으로 변환\n    <span class=\"hljs-keyword\">return</span> data  # 데이터 반환\n\ndef <span class=\"hljs-title function_\">transform_weather_data</span>(**kwargs):\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"가져온 날씨 데이터 변환하기\"</span><span class=\"hljs-string\">\"\"</span>\n    ti = kwargs[<span class=\"hljs-string\">'ti'</span>]  # 작업 인스턴스 가져오기\n    data = ti.<span class=\"hljs-title function_\">xcom_pull</span>(task_ids=<span class=\"hljs-string\">'fetch_weather_data'</span>)  # <span class=\"hljs-title class_\">XCom</span>을 사용하여 <span class=\"hljs-string\">'fetch_weather_data'</span> 작업에서 데이터 가져오기\n\n    weather = {\n        <span class=\"hljs-string\">'city'</span>: data[<span class=\"hljs-string\">'name'</span>],  # 도시 이름 추출\n        <span class=\"hljs-string\">'temperature'</span>: data[<span class=\"hljs-string\">'main'</span>][<span class=\"hljs-string\">'temp'</span>],  # 온도 추출\n        <span class=\"hljs-string\">'pressure'</span>: data[<span class=\"hljs-string\">'main'</span>][<span class=\"hljs-string\">'pressure'</span>],  # 기압 추출\n        <span class=\"hljs-string\">'humidity'</span>: data[<span class=\"hljs-string\">'main'</span>][<span class=\"hljs-string\">'humidity'</span>],  # 습도 추출\n        <span class=\"hljs-string\">'weather'</span>: data[<span class=\"hljs-string\">'weather'</span>][<span class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'description'</span>],  # 날씨 설명 추출\n        <span class=\"hljs-string\">'wind_speed'</span>: data[<span class=\"hljs-string\">'wind'</span>][<span class=\"hljs-string\">'speed'</span>],  # 풍속 추출\n        <span class=\"hljs-string\">'date'</span>: datetime.<span class=\"hljs-title function_\">utcfromtimestamp</span>(data[<span class=\"hljs-string\">'dt'</span>]).<span class=\"hljs-title function_\">strftime</span>(<span class=\"hljs-string\">'%Y-%m-%d %H:%M:%S'</span>)  # 타임스탬프를 읽기 가능한 날짜로 변환\n    }\n\n    df = pd.<span class=\"hljs-title class_\">DataFrame</span>([weather])  # 날씨 데이터를 판다스 <span class=\"hljs-title class_\">DataFrame</span>으로 변환\n    <span class=\"hljs-keyword\">return</span> df  # <span class=\"hljs-title class_\">DataFrame</span> 반환\n\ndef <span class=\"hljs-title function_\">load_data_to_s3</span>(**kwargs):\n    <span class=\"hljs-string\">\"\"</span><span class=\"hljs-string\">\"변환된 데이터를 S3 버킷에 로드하기\"</span><span class=\"hljs-string\">\"\"</span>\n    ti = kwargs[<span class=\"hljs-string\">'ti'</span>]  # 작업 인스턴스 가져오기\n    df = ti.<span class=\"hljs-title function_\">xcom_pull</span>(task_ids=<span class=\"hljs-string\">'transform_weather_data'</span>)  # <span class=\"hljs-title class_\">XCom</span>을 사용하여 <span class=\"hljs-string\">'transform_weather_data'</span> 작업에서 변환된 데이터 가져오기\n\n    csv_buffer = <span class=\"hljs-title class_\">StringIO</span>()  # 인메모리 버퍼 생성\n    df.<span class=\"hljs-title function_\">to_csv</span>(csv_buffer, index=<span class=\"hljs-title class_\">False</span>)  # <span class=\"hljs-title class_\">DataFrame</span>을 <span class=\"hljs-variable constant_\">CSV</span>로 버퍼에 작성\n    <span class=\"hljs-title function_\">print</span>(df)  # <span class=\"hljs-title class_\">DataFrame</span> 출력 (선택 사항)\n\n    s3_resource = boto3.<span class=\"hljs-title function_\">resource</span>(<span class=\"hljs-string\">'s3'</span>)  # boto3 <span class=\"hljs-variable constant_\">S3</span> 리소스 생성\n    s3_resource.<span class=\"hljs-title class_\">Object</span>(<span class=\"hljs-variable constant_\">S3_BUCKET</span>, <span class=\"hljs-variable constant_\">S3_KEY</span>).<span class=\"hljs-title function_\">put</span>(<span class=\"hljs-title class_\">Body</span>=csv_buffer.<span class=\"hljs-title function_\">getvalue</span>())  # <span class=\"hljs-variable constant_\">CSV</span> 데이터를 지정한 <span class=\"hljs-variable constant_\">S3</span> 버킷 및 키에 업로드\n\n# <span class=\"hljs-title class_\">PythonOperator</span>를 사용하여 작업 정의\nfetch_task = <span class=\"hljs-title class_\">PythonOperator</span>(\n    task_id=<span class=\"hljs-string\">'fetch_weather_data'</span>,  # 작업 <span class=\"hljs-variable constant_\">ID</span>\n    python_callable=fetch_weather_data,  # 호출 가능한 함수\n    dag=dag,  # 작업이 속한 <span class=\"hljs-variable constant_\">DAG</span>\n)\n\ntransform_task = <span class=\"hljs-title class_\">PythonOperator</span>(\n    task_id=<span class=\"hljs-string\">'transform_weather_data'</span>,  # 작업 <span class=\"hljs-variable constant_\">ID</span>\n    python_callable=transform_weather_data,  # 호출 가능한 함수\n    provide_context=<span class=\"hljs-title class_\">True</span>,  # 호출 가능한 함수에 컨텍스트 제공\n    dag=dag,  # 작업이 속한 <span class=\"hljs-variable constant_\">DAG</span>\n)\n\nload_task = <span class=\"hljs-title class_\">PythonOperator</span>(\n    task_id=<span class=\"hljs-string\">'load_data_to_s3'</span>,  # 작업 <span class=\"hljs-variable constant_\">ID</span>\n    python_callable=load_data_to_s3,  # 호출 가능한 함수\n    provide_context=<span class=\"hljs-title class_\">True</span>,  # 호출 가능한 함수에 컨텍스트 제공\n    dag=dag,  # 작업이 속한 <span class=\"hljs-variable constant_\">DAG</span>\n)\n\n# 작업 간 의존성 정의 (작업 실행 순서)\nfetch_task >> transform_task >> load_task  # 작업 실행 순서 설정: 가져오기 -> 변환 -> 로드\n</code></pre>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<p>설명:</p>\n<ul>\n<li>\n<p>Imports 및 구성: 필요한 라이브러리를 import하고 구성 변수를 설정합니다.</p>\n</li>\n<li>\n<p>기본 인수: DAG의 기본 인수를 정의합니다. 예를 들어, 소유자, 시작 날짜, 재시도 정책 등이 포함됩니다.</p>\n</li>\n<li>\n<p>DAG 정의: DAG 개체를 설명과 일정 간격으로 생성합니다 (일정을 비활성화하려면 None으로 설정 가능합니다).</p>\n</li>\n<li>\n<p>작업 함수: 사용할 Python 함수를 작업으로 정의합니다.</p>\n</li>\n<li>\n<p>fetch_weather_data: OpenWeather API에서 날씨 데이터를 가져옵니다.</p>\n</li>\n<li>\n<p>transform_weather_data: 가져온 데이터를 Pandas DataFrame으로 변환합니다.</p>\n</li>\n<li>\n<p>load_data_to_s3: 변환된 데이터를 S3 버킷에 로드합니다.</p>\n</li>\n</ul>\n<ol start=\"5\">\n<li>작업 생성: PythonOperator를 사용하여 정의된 함수를 호출하는 작업을 생성합니다.</li>\n</ol>\n<!-- TIL 수평 -->\n<p><ins class=\"adsbygoogle\" style=\"display:block\" data-ad-client=\"ca-pub-4877378276818686\" data-ad-slot=\"1549334788\" data-ad-format=\"auto\" data-full-width-responsive=\"true\"></ins></p>\n<script>\n(adsbygoogle = window.adsbygoogle || []).push({});\n</script>\n<ol start=\"6\">\n<li>종속성 설정: 비트 시프트 연산자를 사용하여 작업을 실행할 순서를 정의하세요.</li>\n</ol>\n<h1>자료들</h1>\n</body>\n</html>\n"},"__N_SSG":true}